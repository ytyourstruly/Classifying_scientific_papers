BERT (Birectional Encoder Representations from Transformers

A random sample of the tokens in the input sequence is selected and replaced with the special token
[MASK].

 The MLM objective is a cross-entropy loss on predicting the masked tokens


RoBERTa [25] significantly improves the robustness of
BERT using a set of model design choices and training strategies, such as modifying a few key hyperparameters, removing
the next-sentence pre-training objective and training with much larger mini-batches and learning rates