Categorizing Motion:
Story-Based Categorizations
Dissertation
(Draft Version: January 7, 2023)
This is a provisional draft of a doctoral dissertation to
D .
be submitted for review at the Mathematics and Nature
Sciences Faculty of the University of Tübingen for the
Degree of Doctor in Natural Sciences (Dr. rer. nat.)
Juan Purcalla Arrufi
from Barcelona, Catalonia (Spain)
Tübingen
Supervisor: Dr. Alexandra Kirsch
Legal Notices
D .
Copyright © 2022–2023 Juan Purcalla Arrufi. All rights reserved.
This doctoral thesis contains original research by Juan Purcalla Arrufi in collaboration with
his supervisor, Dr. Alexandra Kirsch, part of which has been already published in the following
papers. In brackets, we indicate in which chapters of this work such research has been primarily
included.
• Purcalla Arrufi and Kirsch (2017) (Chapters 8 and 9)
• Purcalla Arrufi and Kirsch (2018a) (Chapters 8, 9 and 12)
• Purcalla Arrufi and Kirsch (2018b) (Chapter 11)
• Papenmeier, Purcalla Arrufi, and Kirsch (2022); Collaboration with Dr. Frank Papenmeier.
(Chapter 10)
Whenever I use or refer to other scientists’ results, I indicate it with the corresponding citation
to the best of my knowledge. In case that, inadvertendly, someone might be miscited or due
citation might be omited, please contact me, ORCID Nr. 0000-0002-3307-655X, for correction.
All figures (pictures and graphics) and tables in this work are licensed by Juan Purcalla Arrufi
under CC BY 4.0, except when otherwise noted as indicated in the caption through the “(Source:
[. . .])” inscription.
Scripture quotations are from the World English Bible (WEB) under public domain.
Abstract
Our most primary goal is to provide a motion categorization for moving entities. A motion
categorization that is related to how humans categorize motion, i.e., that is cognitive plausible,
and can take advantage of the computing power; for example, that we can use in reasoning
algorithms. In short, a motion categorization that is both attractive tFo psychologists and com-
puter researchers: both applicable in human cognition and artificial intelligence. In Part III, we
fulfil this goal not only by providing six of such motion categorizations, e.g., Motion-RCC and
Motion-OPRA , but, even more, providing a method for generating eventually an endless number
1 A
of motion categorizations; we call the categorizations created by our method ‘story-based cate-
gorizations’ (Ch. 9). The key of our method is that we generate the motion categorizations from
already existing spatial categorizations—concretely, those called —and, of
spatial representations
such spatial categorizations, we find a great number in the literature.
The very first motivation for motion categorizations that are cognitively plausible is to apply
them in human-aware navigation. In that sense, the story-based categorizations bridge the gap
between the human understanding of motion and the computer requirements gfor information
processing. Beyond that, the motivation for motion categorization is, on itsoown, overwhelming;
D .
we see categorization as the first step in knowledge representation—the most basic cognitive
task. Motion categorization provides a solution for processing the flood of motion data that is
being currently generated by the myriad of navigation and motion control systems.
Another important goal, which serves the primary onoe, is to relate the understanding and ter-
minology of categorization in psychology with thosesin artificial intelligence. We devote Part II to
present the foundations of categorization according to psychology, and to explain how computer
scientists deal with spatial and motion categorization, even mentioning underlying linguistic as-
pects. In that way, later on Chapter 7, we can introduce our elementary categorization model
under consideration of the mosst basic elements in categorization. We use this model to describe
and deal with the story-based categorizations throughout this work.
Finally, in Part IV,hwe verify that the story-based categorizations are categorizations,
not just
but display very convenient properties. Most importantly, they reflect categorization aspects of
human cognition, and they are endowed with the powerful reasoning operations of the
qualita-
. We present additional properties in a broad summary in Chapter 12: story-based
tive calculi
categorizations can achieve extraordinary variety of categorization criteria, they are applicable
in high dimensional spaces, they can categorize any sort of trajectories (even when entities are
motionless), they are suitable for decision-making and control, and they can categorize motions
of multiple (more than two) entities.
Our achievements are far from obvious in the light of the challenges that motion categorization
poses (Ch. 3). Thus, in the process of achieving the main goals, we made several additional
contributions which we list in Section 1.1. Last but not least, my overarching goal is that you
enjoy your reading: Have a blessed journey on the quest for motion categorization!
D .
Acknowledgements
First and foremost, I thank Jesus Christ, my lord and saviour, for His amazing grace, which
has given me—a sinner—eternal life. I love you! His grace was also enough to complete this
dissertation; He marvellously provided everything I needed: inspiration, wisdom, endurance,
patience, finances, and the right people around me. Amongst such peoFple He provided and who
helped me achieve this goal are following:
My supervisor, Dr. Alexandra Kirsch. She has mentored my research from the very beginning.
In fact, she was the one who hired me for the two-years Ph.D. position which ended up in
this dissertation. Here, I want to explicitly acknowledge, that she gave a full-paid position
to all her Ph.D. students, which, sadly, it is not the norm in our academical landscape. I
appreciate her feedback and correction; they have been instrumental for me to clearly highlight
my research contributions and to achieve a mature argumentative balance. I extraordinarrily
appreciate her forbearance throughout these over eight years of supervision: she was always
available for questions. Thank you, Alex. Bless your heart!
My wife, Irene. She has proved to be my God-given wife throughout this last years of
doctorate. Three years ago, she married a man who was “about to finish” the doctorate, but
D .
this prospect delayed beyond any imagination. Despite this initialndisappointment, her love,
encouragement, and patience grew day by day, steadily, to the point that she sacrificed herself
for me to finish the doctorate: seeing that I was so excruciatinm gly slow writing my dissertation due
to my part-time jobs, she compelled me to devote mysoelf fully to the writing, and she provided
with her job for the household. Thus, I could fullysdevote to the writing for over one year, and
finish it. For that reason, she deserves to be acknowledged as co-author of this dissertation.
Irene, I am humbled by your love; I hope I get to show you such a great love.
My colleagues, Alisa V., Roman R., Nataliia K., and Stefan B. They made my long office
hours enjoyable, and I learned sa lot in our scientific-technical discussions. A special mention for
Alisa, who often shared with me literature, links, and any kind of information which could be a
blessing to me—Thathpushed me forward. Thanks!
My beloved and loving family: dad, mum, sister. The passion they showed to see this work
finished was one of my greatest motivations. They did not get tired to ask “When will you finish
your doctorate?”.
My parents-in-law, Peter and Olga, which I unconditionally love.
Moi svekry, Petr i Ol~ga, kotoryh (cid:31) bezuslovno l(cid:24)bl(cid:24).
D .
Contents
Legal Notices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Abstract 3
Acknowledgements 5
I Motivation, Problem Definition, and Novel Contributions 11
1 Introduction 13
1.1 Our Novel Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2 What and How We will Categorize 19
2.1 What We Categorize: . . . . . . . . . . . . . . . . . . /. . . . . 19
Motion Scenarios
2.2 How We Categorize: Assumptions . . . . . . . . . . . . . . . . . . .r. . . . . . . 21
D .
3 Challenges in Motion Categorization 23
3.1 High Dimensional States Space . . . . . . . . . . . . .m . . . . . . . . . . . . . . . 23
3.2 Variety of Categorization Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.3 Loose Cognitive Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.4 Disciplinary Fragmentation and Tension . . . . . . . . . . . . . . . . . . . . . . . 26
II Foundations of Catego/rization 29
4 Categorizations, Categories, and Concepts in Human Cognition 31
4.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.2 Classical Model and Classification . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.3 Category Typicality and Membership . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.4 [Dis-]Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.5 Boundary models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.6 Between-Category Structure: Hierarchical Taxonomy . . . . . . . . . . . . . . . . 52
4.7 Categorization in Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
5 Spatial Categorizations 57
5.1 Introduction and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.2 Spatial Representations: Notorious Examples . . . . . . . . . . . . . . . . . . . . 60
5.3 Properties of Qualitative Spatial Representations . . . . . . . . . . . . . . . . . . 63
5.4 Overview on Spatial Representations . . . . . . . . . . . . . . . . . . . . . . . . . 64
8 CONTENTS
6 Motion Categorizations 67
6.1 Survey of Qualitative Motion Representations . . . . . . . . . . . . . . . . . . . . 67
6.2 Identified Shortcomings and our Story-Based Solutions . . . . . . . . . . . . . . . 69
6.3 A Taxonomy of Qualitative Relations of Motion . . . . . . . . . . . . . . . . . . . 69
6.4 Expanding Qualitative Representations of Motion . . . . . . . . . . . . . . . . . . 70
6.5 Motion and spatial categorization: linguistic comparison . . . . . . . . . . . . . . 76
III Developing Story-Based Categorizations 81
7 Describing [Motion] Categorizations: A Framework 83
7.1 The Categorization Process: Formalization . . . . . . . . . . . . . . . . . . . . . 84
7.2 Link to Psychological and Philosophical Terms . . . . . . . . . . . . . . . . . . . 90
7.3 Example: Scenario Categorization by QTC . . . . . . . . . . . . . . . . . . . . 91
B21
7.4 Testing Dissimilarities and Traditional Models . . . . . . . . . . . . . . . . . . . . 94
8 Stories and Temporal Sequences of Relations 101
8.1 Motivation and Background for Temporal Sequences . . . . . . . . . . . . . . . . 101
8.2 Temporal Sequences: a Qualitative Trajectory Description . . . . . . . . . . . . . 103
8.3 Transitions between relations: CND . . . . . . . . . . . . . . . . . . . . . . . . . 107
8.4 Dominance Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
8.5 Stories: From Scenarios to Temporal Sequences . . . . . . . . . . . . . . . . .4. . 111
8.6 Stories become Categories: Story-based categorizations . . . . . . . . . . u. . . . . 118
9 Creating Story-Based Categorizations:
Bare and Beaded 123
D .
9.1 Finding the Stories Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Σ s
9.2 Stories- of Spatial Representations . . . . . . . . . . . . . . . . . . . . . . . . . 124
R m
9.3 Stories- of Motion Representations . . . . . . . . . . . . . . . . . . . . . . . . . 134
9.4 Motivation to Expand Bear into Beaded Categorizations . . . . . . . . . . . . . . 140
9.5 Defining beaded story-based categocrizations: ( ) . . . . . . . . . . . . . . . . 140
S R
s i j
9.6 Motion- of Spatial Representaitions . . . . . . . . . . . . . . . . . . . . . . . . . 142
9.7 Motion- of Motion Representations . . . . . . . . . . . . . . . . . . . . . . . . . 144
R t
IV Validating Story-Based Categorizations 149
10 Experimental Evidence of Stories 151
10.1 About Cognitive Plausibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
10.2 Experimental Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
10.3 Stimuli and Apparatus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
10.4 Appearance of the Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
10.5 Experimental Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
10.6 General Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
11 Story-Based Categorizations as Qualitative Calculi 171
11.1 Qualitative Calculus: Definition and Types . . . . . . . . . . . . . . . . . . . . . 171
11.2 Categorization Formalisms as Qualitative Calculi . . . . . . . . . . . . . . . . . . 175
11.3 Story-Based Categorizations are Qualitative Calculi . . . . . . . . . . . . . . . . 176
11.4 Converse Relations in Story-Based Categorizations . . . . . . . . . . . . . . . . . 178
CONTENTS 9
11.5 Composition in Story-Based Categorizations . . . . . . . . . . . . . . . . . . . . . 179
11.6 Examples of Narrative and Standard Composition . . . . . . . . . . . . . . . . . 186
12 Additional Applications and Properties 191
12.1 Creating a Variety of Categorization Criteria . . . . . . . . . . . . . . . . . . . . 191
12.2 High Dimensional Categorizations of Motion . . . . . . . . . . . . . . . . . . . . . 196
12.3 Categorization of Motionless Entities . . . . . . . . . . . . . . . . . . . . . . . . . 197
12.4 Qualitative Description of General Trajectories . . . . . . . . . . . . . . . . . . . 197
12.5 Decision-Making and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
12.6 Motion Categorization for Multiple Entities . . . . . . . . . . . . . . . . . . . . . 202
12.7 Inspirational Cognitive and Mathematical Properties . . . . . . . . . . . . . . . . 204
V Conclusion 211
13 Summary and Conclusion 213
13.1 Review of our Main Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
13.2 Tackled Challenges in Motion Categorization . . . . . . . . . . . . . . . . . . . . 214
13.3 Overcome Shortcomings in Motion RepreseAntations . . . . . . . . . . . . . . . . . 216
13.4 Epilogue: Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
VI Appendices R 219
A Mathematical Formulae and Proofs / 221
A.1 Multiplicativity measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
A.2 General Properties of Temporal Sequences of Relations . . . . . . . . . . . . . . . 222
D .
A.3 Generation of QTC Stories . . . . . . . . . . . . . . . . . .n. . . . . . . . . . . . 225
A.4 Kinematics of the 2-D Uniform Motion of two Entities . . . . . . . . . . . . . . . 225
A.5 From Categorial to Kinematic Coordinates . . . m. . . . . . . . . . . . . . . . . . 232
A.6 Similarity . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . 233
B Experiment Data 235
B.1 Stimuli Traces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
Bibliography : 261
D .
Part I
Motivation, Problem Definition, and
Novel Contributions
D .
D .
Chapter 1
Introduction
This work originated in a research team of human-computer interaction—Jun. Prof. Alexandra
Kirsch’s team at the University of Tübingen—More concretely, I began this research to meet the
needs in human-aware navigation. As Kruse et al. (Concl., 2013) remark in their well-known
survey, all of about 120 surveyed papers dealt with “individual domains and challenges, [but] a
holistic theory of human-aware navigation was not attempted yet.” As in all those papers, in the
very beginning of my research, I was also concerned with a very specific case of human-aware
navigation, namely, I was trying to implement human-aware crossing situations between humans4
and robots.
Along this endeavour, I realized that a first step in reacting to a crossing situa3tion could
be the assessment, i.e., the classification or categorization, of that crossing situation. That
lead me to a search for categorizations of two moving entities and I encountered the promising
D .
‘qualitative representations of motion’, such as QTC (Van de Weghe 2s004; e.g., Bellotto 2012;
Lichtenthäler et al. 2013), but I found them wanting: In the crossinog paradigm it became evident
to me that motion situations with the same QTC category might evolve differently; that is, QTC
did not reliably inform about the future position states of a current motion situation. Such
a categorization is cognitively disputable, because information is encoded in our mind mainly
in order to anticipate events, so that our behaviour can be decided (Butz and Kutter (2016,
Sect. 10.2.1); Sect. 12.7.4.B); if a motionhcategorization loses relevant information about the
future position states, great are the c/hances that such categorization is cognitively deficient.
Hence, I reoriented my research to develop categorizations that classify the motion situations
according to the expected future position states. In other words, I sought the answer to the
question: How can we categorize motion situations, so that each category corresponds to a spe-
cific spatial state in the future? As a base for such categorizations I took the
qualitative spatial
, because they were successfully researched and applied in Artificial Intelligence
representations
(AI) for decades (Chen et al. 2015). Therefore, the family of categorizations we obtained, the
‘ ’, are attractive both from the perspective of AI and from the per-
story-based categorizations
spective of human cognition, which make them fit for implementing human-aware navigation,
amongst others.
In this research converge two capital topics in cognitive sciences and artificial intelligence:
and . We work in a highly multidisciplinary field with methodological ten-
categorization motion
sions between disciplines, as we exemplary see in the editorial debate of the journal
Cognitive
(Ross 2019; Katsikopoulos 2019). Alike the paper that sparked the debate (Kirsch
Processing
2019), we research the fuzzy border between engineering and science. For that reason, we clarify
our contribution: this work stands at the overlap of the two major goals of cognition:
construc-
14 CHAPTER 1. INTRODUCTION
and (Gärdenfors 2004, Sect. 1.1.1). First, as a work, we offer
tive explanatory constructive
motion categorizations that can be used in artificial intelligence, and engineering, e.g., robotics
and navigation. Second, as an work, we inspect the cognitive plausibility of our
explanatory
categorizations, and test it experimentally.
The relevance of each topic on its own— and —is enormous, as we
categorization motion
justify below. Furthermore, since we bring them together, our research has a surplus value:
indeed, is an infrequent categorization theme. For instance, in psychology, Medin and
motion
Heit (1999, Sect. I.C) observe that categorization research is predominantly on
natural objects
and artificially created categories, as opposed to categorization on events or motion. Likewise, in
AI, we found an overwhelming majority of versus categorizations (See Section 6.1).
spatial motion
We help to correct this deficiency by providing the story-based categorizations, which are both
a motion categorization with cognitive plausibility in psychology (Ch. 10), and a qualitative
representation of motion with reasoning capabilities in AI (Ch. 11).
1.0.1 Relevance of Categorization
Concerning the relevance of in cognition, Cohen and Lefebvre (2005, p. 2) assert
categorization
“[Categorization] is the basis for the construction of our knowledge of the world. It is the most
basic phenomenon of cognition, and consequently the most fundamental problem of cognitive
science.”. Harnad puts it more bluntly: “cognition is categorization: to cognize is to categorize”
(Harnad 2017).
Similarly, some decades before, pioneers in categorization research, E. Rosch and Lloyd (1978,
Intr., p.1), stated “categorization is important”, and they specified “[It is] a basic task of all
organisms (indeed, one mark of living things)”. Thus, we observe an augm3enting emphasis in
the relevance of categorization in cognition throughout the years. Also,gin artificial intelligence
we find similar claims: “The organization of objects into categoriesois a vital part of knowledge
D .
representation” (Russell and Norvig 2014c, Sect. 2).
The reasons for that relevance are clear: categories allowoan agent not to perceive every object
or event as unique, but to group them according to their use or effect (e.g., Bruner et al. 1956,
p. 245; Gerrig and Zimbardo 2005, pp. 229f.). For example, when we look at a bunch of grapes,
we are not overwhelmed by fifty different objects dangling in it; instead, we essentially overlook
the differences between each danglingsobject, and see fifty objects of the same kind: ‘grape’—
they fall into the same category. Thhus, by categorizing, we save cognitive resources—‘
cognitive
’ (E. Rosch 1978, p./28)—, we can store the bunch of fifty different objects (each with
economy
its own features) in a more parsimonious representation, i.e., fifty times an object of one kind,
i.e., “fifty times grape”.
Beyond , categories are essential for and (Russell
cognitive economy reasoning decision-making
and Norvig 2014c, Sect. 2 and 5; Medin and Heit 1999, Sect. II.A.1). For instance, if an agent
intends to produce wine, it could apply the rule ‘we can produce wine, if, and only if we have
grapes’ to plan the next action: look for grapes. Remarkably, the execution of such action
depends fully on the ability to cope with the category .
grape
Summarizing, categorizations are an asset in artificial intelligence. Indeed, any sensor, either
robotic or human, is inundated by data with no direct meaning in itself, but its numerical value.
This requires both —reducing the amount and the degree of detail of the data—,
simplification
and —endowing data with a more straightforward meaning. As we have seen,
conceptualization
a meaningful categorization provides both in one stroke.
The type of categorizations we introduce in this work, the story-based, not only simplify, the
raw motion data a sensor or tracking device provides—as any motion categorization does (e.g.,
QTC, Van De Weghe et al. 2005; QRPC, Glez-Cabrera et al. 2013), but, more importantly, they
produce meaningful categories which allow for reasoning and decision-making about moving
entities (Sect. 12.5). We also show how story-based categories reflect the inner structure of
motions: categories correspond to relevant kinematic attributes, the (e.g.,
featural variables
Sects. 9.2.1.C and 9.2.2.C). Reflecting the relevant attributes is, for E. Rosch (1978, p. 28), one
of the two main principles characterizing categorizations: “categories map the perceived world
structure as closely as possible. [. . .] by the mapping of categories to given attribute structures”.
A Minimal History of Categorization
The contemporary history of categorization helps us appreciate the current relevance of the
discipline. It was as recently as the 1970’s, when the millennia-long standing view on concepts
was successfully challenged (See Medin and Heit 1999, Brief History; detailed acccount Murphy
2002, Ch. 2). Mostly the work of E. Rosch (e.g., 1973; 1975; 1978), but also the contributions
of her research partner C. B. Mervis (1976), as well as A. J. Wilkins (1971); E. E. Smith,
E. J. Shoben, and L. J. Rips (1973; 1974); and M. E. McCloskey and S. Glucksberg (1978),
amongst others, motivated the crumbling of the —the view that are
classical view definitions
the appropriate way to characterize categories and, therefore, categories have perfectly delimited
borders—which had been the implicit view in all categorization work since, at least, Aristotle
(See transl., Apostle 1980, pp. 6, 19–20).
Wrapping up, as Murphy (2002, Ch. 1) states about the study of concepts (i.e., categoriza-
tions): “a topic that seemed straightforward in 1960 has turned out to be a much deeper and
richer scientific problem than researchers expected”, so that in the current state of the art “no4
theory has explanation for all findings within a topic”, and, what is more, neither the ubasic
even
questions have been fully answered. We are confident that this work will shed more light on the
topic, as well, as rise more interesting research questions.
D .
A Note on the Classical View
We carefully avoid the classical model when we model the story-based categorizations: we make
membership gradation available (Ch. 7). It might seem then contradictory, that our simplified
version of the membership function (also called ‘categorization rule’, Eq. (7.1)) does not reflect
a graded membership. As said, it is a membership function; nevertheless, gradation is
simplified i
readily available in the ‘ ’—the metric space underlying the membership function—
featural space
in which we can define a similarity functtion based on a featural distance. Hence, when needed,
we can obtain a graded membersh/ip function for story-based categorizations by means of the
featural distance. In fact, we do apply the gradation properties of story-based categorization
when we experimentally test the cognitive plausibility of these categorizations: we generate
comparison stimuli equally similar to a reference stimulus (Sect. 10.3.3). However, we believe
that implementing a graded membership function of the story-based categorizations provides
no added value to our theoretical analysis; it makes mostly sense when modelling experimental
results on category membership. That is why we leave it as a future practical enhancement.
1.0.2 Relevance of Motion
Concerning the relevance of , we observe, in the last decades, a marked increase in motion
motion
related research in artificial intelligence and neighbouring disciplines. Arguably, the explosion of
research in autonomous vehicles has brought the major focus on motion analysis. Above all, the
integration of autonomous agents in day-to-day life, e.g., robots or vehicles, heightens the need for
a formalization of the entities motion (Kurata and Shi 2008a)—most notably in machine-human
interaction. Not least, the notorious rise in the ability to detect and record motion has ushered a
16 CHAPTER 1. INTRODUCTION
new era of motion analysis (Delafontaine et al. 2011); indeed, not only tracking devices, but the
locating possibilities of smartphones and their societal pervasiveness, furnishes a massive amount
of position data, and, hence, motion data that can be exploited (e.g., Roor 2018)
In this sense, a cognitively plausible motion categorization, such as, the story-based catego-
rizations, is an asset when processing motion data: they simplify kinematic floating point data
into a reduced finite set of concepts that largely reflect a human understanding of the moving
system, so we can more straightforwardly implement human-like navigation rules; in addition,
they lessen the overhead of floating point computations.
The understanding of motion is also a fundamental ingredient of human cognition. First of
all, motion attracts infants attention earlier (from birth) and more powerfully than most other
factors (Haith 1980; Fantz and Nevis 1967). At 5 months age, infants remember much better
the actions in events than the objects, people, or other elements involved (Bahrick et al. 2002;
Perone et al. 2008). Consequently, motion related concepts are amongst the first to be acquired
(Mandler 2012); Mandler uses the early concept ‘ ’ as a vivid example: “[Furniture] may
furniture
mean [for the child] no more than ‘things that don’t move’”. F
Motion has even a more profound role both in artificial intelligence and cognitive sciences.
On the one hand, regarding artificial intelligence, we have two different approaches: the
mental
and the one, according to Russell and Norvig’s classification (Russell and Norvig
behavioural A
2014b, Sec. 1). The behavioural approach concentrates on issues such as perception, action, and
manipulation in which motion is an essential component. Practical examples are, classification
of video images, navigation in dynamical environments or human-aware navigation. On the
other hand, regarding cognitive science, we find behavioural approaches, often termred ‘
R embed-
’. There, the system plays a basic role in cognition through its couplingg to mental and
ded motor
perceptual systems. In that way becomes integral part of cogniti/ve research. Further-
motion
more, we notice that in both artificial intelligence and cognitive science a shift from mental to
behavDioural paradigm; thus, motion seems to gain relevance.
1.1 Our Novel Contributions
In this work, we present the results of our research as a monograph, integrated with the current
state-of-the-art in related scientific disciplines, because we prioritize understandability and read-
ability. Unfortunately, this mighthrend our own contribution unclear, hard to distinguish from
the other scientists’ results. F/or this reason, we itemize here our novel contributions, which we
introduce in the following chapters.
Primary chontributions (Parts III and IV)
• We presented a method for generating the , which are motion
story-based categorizations
categorizations generated from spatial and motion representations (Ch. 9).
We create two types of story-based categorizations. First, The story-based cat-
– bare
egorizations, which are the simplest variant, and are noted as Stories- . Second, the
story-based categorizations, noted as Motion- ; they are a refinement of the
beaded
categories in which the time precedence (past and future) in a bare category is
bare
distinguished.
We obtain both bare and beaded categorizations for the spatial representations RCC
and OPRA , and the bare categorization for the motion representation QTC . That
1 B21
is, we obtain Stories-RCC (Sect. 9.2.1), Motion-RCC (Sect. 9.6.1), Stories-OPRA
(Sect. 9.2.2), Motion-OPRA (Sect. 9.6.2), Stories-QTC (Sect. 9.3.1).
1 B21
1.1. OUR NOVEL CONTRIBUTIONS 17
We showed the attractive properties and varied possibilities of story-based represen-
tations (Ch. 12)
Story-based categorizations can be tailored to many purposes, since these catego-
rizations, as a whole, possess a great variety of categorization criteria which can
be combined (Sect. 12.1)
We can create story-based categorizations that work in high dimensional spaces
and describe motions with multiple entities Sects. 12.2 and 12.6
• We proved experimentally that the story-based categorizations Stories-RCC and Stories-OPRA
are (Ch. 10).
cognitively plausible
• We proved mathematically that, in most cases (possibly all), the story-based categorizations
are (Ch. 11)
qualitative calculi
We provided a method to help compute the composition in the story-based catego-
rizations (Sect. 11.5).
Additional contributions (Part II)
• We presented comprehensive surveys in general categorization (Ch. 4), in spatial catego-
rization (Ch. 5), and in motion categorization (Ch. 6).
We related the concepts in general categorization to the particular case of spatial and
– r
motion categorization; specifically, we looked at qualitative representations from the
perspective of human cognition, illustrating basic psychological and linguistic aspects.
• We worked out an elementary categorization model that enables us too formally deal with
D .
categorizations and relate them to the psychological understanding of categorization Sects. 7.1
and 7.2. The model can reproduce all the basic psychological effects of gradation by means
of a featural distance upon which a similarity can be defined. Moreover, it is designed with
the qualitative representations in mind, so that we can identify their main constituents: cat-
egories, features, categorical regions; and we can obtain their important functions: feature
extraction, and feature based categorizatsion.
We applied our categorizati/on model to the spatial categorizations RCC (Sect. 9.2.1.A)
and OPRA (Sect. 9.2.2.A), to the motion categorization QTC (Sect. 7.3), and to the
1 s B
story-based categorizations Stories-RCC (Sect. 9.2.1.C) and Stories-OPRA (Sect. 9.2.2.C).
t 1
By doing so, twe obtained their feature extraction function, along with their features,
and their featural categorization function.
D .
Chapter 2
What and How We will Categorize
2.1 What We Categorize: Motion Scenarios
In this work, we categorize the simultaneous motion of entities that is described by
A instantaneous
of position and velocity. We call such an instantaneous motion description ‘
values motion sce-
’ (Fig. 2.1) The standard motion scenario contains two entities, and ; it is described by
nario k l
four real vector values: . The vectors describe the positions of the entities.
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)x )
k k l l k l
However, we often need extra positional information (e.g., orientations, sizes) that is relevant
for categorization; such information is provided by additional parameters. The vectorsg
((cid:126)v , (cid:126)v )
3 k l
describe two real velocities (Fig. 2.1). /
We focus on two entities because the categorization of two entities’ motionrcan be generalized
to three or mDore entities by considering the pairs of entities in a scen.ario; we illustrate this
generalization method with three entities in Section 12.6. Moreover, our categorization method
is based on qualitative representations (Ch. 5) which are mosmtly created for two entities—In
Dylla et al. (2017, Tab. II), we find only about representations for three entities. As a
15%
final reason, the properties of qualitative representations are studied only for two entities; the
formalization of three entities relations is inexhaustive (ibid., p. 6).
Although we develop our theory and methods in a general n-dimensional kinematic space,
all our examples and illustrations—inctluding our validation experiment (Ch. 10)—are in two
dimensions for the sake of simplic/ity: both computations and visualization are easier in two
dimensions. Nonetheless, in Section 12.2, we give an example of a three-dimensional motion
categorization obtained with our method.
Motion scenarios have an elementary role in motion description. The trajectories of entities
(cid:126)v
(cid:126)v
(cid:126)x
(cid:126)x
A of two entities and in two dimensions. The scenario is
characterized by their instantaneous positions, and , and velocities, and .
(cid:126)x (cid:126)x (cid:126)v (cid:126)v
k l k l
20 CHAPTER 2. WHAT AND HOW WE WILL CATEGORIZE
y y y y
l l
k l
k k
x x x x
(1) (2) (3) (4)
y y y y
l l
k l
k k k
x x x x
(5) (6) (7) (8)
A categorization problem for motion scenarios (the circles represent the entities
and the vectors represent the velocities): How should we group such motion scenarios according
to their similarity?
in time, , can be seen as a continuous sequence of motion scenarios
(cid:126)x (t), (cid:126)x (t), . . . , (cid:126)x (t)
k k k 4
1 2 n
{ }
for every instant . For that reason, we can also describre trajecto-
(cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ; . . . ; (cid:126)x R, (cid:126)v t
k k k k k k u
1 1 2 2 n n
{ }
ries qualitatively by means of a categorization of motion scenarios, if we apply thge categorization
at every instant of the trajectory (See Section 12.4).
The entities of a motion scenario need not be punctual: they can bre , but they cannot
regions
alter Dtheir shape, i.e., they can only be ‘ ’. To desc.ribe the shape and size of the
rigid regions
regions, we need additional parameters. For example, if entities are discs, we need their radii—
That is the case of a disc story-based categorization obtained in this work: Stories-RCC (See
Section 9.2.1).
In any case, entities can only translate, i.e., they cannot spin. Otherwise, we should add the
as an extra value toithe motion scenario. Summarizing, we concentrate our
spin angular velocity
research on .
rigid non-spinning enthities
We justify the separation/of translation and spinning (or rotation) as a common practice
in classical mechanics, and, more importantly, as an observed cognitive phenomenon. Indeed,
infants can independently perceive the path of an object, and the manner this object moves
along the path,tfor example, by spinning (Pulverman et al. 2008; Pruden et al. 2013). That
is, the traditional separation of translation and rotation of a movement is more than a practical
mathematical device, but a cognitive feature present from infancy on.
In Chapter 7, we flesh out a detailed categorization model, which we also apply to motion
scenarios. Here, nonetheless, we sketch motion categorization as an introduction. To categorize
a , means that we label it with a category . We choose
motion scenario K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) M
k k l l i
the category from a pool of categories, the ‘ ’ ; which is finite, and contains
categories set m
categories, .
= M , M , . . . , M
1 2 m
M { }
As an illustration, we present here a most simple motion categorization for scenarios: .
Gaping
The motion categorization considers how the distance between the entities instantly
Gaping
changes, i.e., the sign of d . It consists of three possible categories ,
(cid:126)x (cid:126)x = M , M , M
l k 1 2 3
dt
(cid:107) − (cid:107) M { }
also called . A motion scenario belongs to a certain category according to following
( ), (0), (+)
{ − }
2.2. HOW WE CATEGORIZE: ASSUMPTIONS 21
rules:
or The distance between entity and decreases (2.1a)
M ( ) k l ( (cid:126)x (cid:126)x < 0)
1 l k
− dt (cid:107) − (cid:107)
or The distance between entity and remains constant (2.1b)
M (0) k l ( (cid:126)x (cid:126)x = 0)
2 l k
dt (cid:107) − (cid:107)
or The distance between entity and increases (2.1c)
M (+) k l ( (cid:126)x (cid:126)x > 0)
3 l k
dt (cid:107) − (cid:107)
Gaping
rization (Eq. (2.1)). The scenarios (1), (3), (4), and (6), belong to category ; the scenarios
(7) and (3) belong to category ; and the scenarios (2) and (5) belong to category T.
M M
2 3
Later, as we formalize motion categorization, we thoroughly present some well-known simple
motion categorizations for motion scenarios (e.g., QTC and QTC in Sections 6.4 and 7.3).
B21 B22
Note that as we concentrate on the categorization of motion scenarios, we implicitly limit the
effects of acceleration in our system. Truly, if the entities should experience extreme accelerations,
then the changes in the scenario’s configuration would be so abrupt that the categorization of
scenarios would seem pointless.
2.2 How We Categorize: Assumptions
Any categorization model has to make assumptions on how the categorization happens—we rsee
that in detail in Chapter 4. For example, the assumes that humans extract
prototype model g
the features of entities belonging to the same category and store their central feature values as
a particular entity in our mind: the category’s ; furthermore when a new entity is to
prototype
be categorized, the prototype model assumes that humans compare in a featural space the new
D .
entity with the stored prototypes by means of a n(See details, Sect. 4.4.2).
similarity function
These are no petty cognitive assumptions!
We have also based our categorization of motion scenarmios in some assumptions. On a com-
putational level our central assumption is that we alreaody have spatial categorizations available
in the form of (Ch. 5). Wse use such spatial representations, e.g., RCC
spatial representations
and OPRA , to generate our motion representations. On a cognitive level, we firstly assume that
humans perceive motion scenarios and, consequently, can categorize them; secondly, we assume
that humans are able to and do infer the past and future trajectory of the entities in a scenario.
D .
Chapter 3
Challenges in Motion Categorization
Besides the relevance of the topic (Ch. 1), motion categorization has challenges that make it, at
the same time, more difficult and more exciting to treat than other categorization domains.
3.1 High Dimensional States Space
In the first place, the high number of dimensions that describe a motion scenario are a challenge
for researchers in motion categorization—Two entities moving in a two-dimensional space are
described as a point in R , i.e., entities position coordinates velocity coordinates ;
2 (2 + 2 / )
× g
three entities in a three dimensional space as a point in R . For that reason, both mental and
graphical representation of the space to categorize fail as a means to obtain categorizations.
D .
We overcome this complexity by using the well-founded n (also called
spatial categorizations
spatial , Hernández 1994) as the starting point of the categorization
qualitative representations m
process. Out of them we obtain motion categorizations mby applying a general method, the
, that greatly reduces the mental demoands of the researcher.
story-based method
3.2 Variety of Categorization Criteria
A further challenge of categorizsing motion is the variety of possible categorizations; we illustrate
almost identical, differing only in the speed of ; consequently, each pair may become a category.
However—assuming velocities remain constant—in both scenarios A and C the vehicle would
cross before without colliding, while in scenario B the vehicles collide, and in D the vehicle
l k
crosses behind .
than the two previous, (A, B) and (C, D)? Theoretically, we cannot find an absolute scale of
meaningfulness for categorizations of motion; each categorization might be meaningful depending
on how useful it is for a certain agent according to its task at hand, and thus we might end up with
multitude of categorizations, each useful for a different purpose (e.g., Sect. 4.4.4). It is true that,
in our categorization experiment (Ch. 10), we found that motion categorization Motion-OPRA
had a much higher saliency than Motion-RCC (Sect. 10.6), but the stimuli were not general
enough to make an overall statement about meaningfulness of categorizations.
24 CHAPTER 3. CHALLENGES IN MOTION CATEGORIZATION
y y
k k
v v
k k
x x
v v
l l
l l
A) B)
y y
k g k
v v
k k
D .
x m x
v v
l o l
l c l
C) D)
A challenge for motion categorization: 4 scenarios (A, B, C, D) with two moving
vehicles and , with velocity vectors and . Each pair (A, B) and (C, D) has identical
k l (cid:126)v (cid:126)v
k l
positions, velocity angles, and fulfils —each pair differs only in the speed of , i.e.,
(cid:126)v > (cid:126)v k
k l
(cid:107) (cid:107) (cid:107) (cid:107)
in .
(cid:126)v
(cid:107) (cid:107)
Source: Purcalla Arrufi and Kirsch (2018a)
3.3. LOOSE COGNITIVE STRUCTURE 25
In any case, a model of motion categorization must address the variety of categorizations: it
must be flexible enough to generate the most varied categorizations. We posit that the variety of
motion categorizations originates primarily in the variety of underlying spatial categorizations.
and , and, again, we can categorize B as a single category, due to the spatial
before behind
category , i.e., collision. This is a compelling reason to use spatial categorizations as the
overlap
basis of our method to generate categorizations of motion: spatial categorizations are a source
of variety for the resulting motion categorizations.
In addition to spatial categorizations, another source of variety in motion categorization are
the attributes of the moving entities. For example, if the entities have volume, there are many pos-
sible types of collisions between entities, which produce many types of motion categorizations—
Think of the many ways a foot can hit a ball. But, if the entities are seen as punctual, there
is only one possible type of collision: to be at the same point; in that case, another aspects
determine motion categorization.
Notably, spatial attributes and spatial categorizations are coupled: the considered attributes
of the moving entities determine the spatial categorizations that we use. If the entities have
volume, we will tend to use a spatial relation that considers volume, e.g., RCC (as in Wu et al.
2014); if the entities relative orientation is relevant, e.g., ships, we will use a categorization that
considers orientation, e.g., OPRA (as in Dylla et al. 2007).
Since our method can use any spatial categorization to create motion representations, our
method can create motion categorizations tailored to the attributes of the involved entities.
In Section 12.1.1, we see how two story-based categorizations, Motion-RCC and Motion-OPrRA ,
u 1
3.3 Loose Cognitive Structure
D .
Evidence has accumulated that though language influences cognitive tasks—and notably catego-
rization—language actually reflects universals in cognitive stm ructures (Kess 1992, Ch. 8). In Sec-
tion 6.5, we infer a universal property concerning motioon categorization from L. Talmy’s (2000)
work in cognitive linguistics, namely, that humanss categorize motion quite loosely: we do not
have a systematic cognitive structure for motion categorization, as we have it for spatial catego-
rization.
The attempts to formalize motion categorization are hampered by its loose cognitive struc-
ture. For example, if we understake to formalize motion categorization for motion scenarios, we
would strive to thte kinematic space of motion scenarios; in other words, we would like to
partition
assign a category—at least one—to any motion scenario. However, according to Talmy (2000a),
humans do not linguistically partition the motion space, so that for most motion categorizations
some regions in the motion space (i.e., some motions) remain linguistically uncategorized (See
Fig. 3.2).
In contrast, spatial categorization is linguistically reflected by a partition of the space medi-
ated by the prepositions, to each position can be assigned a category. For example, let us take
a spatial categorization based on of entities to an observer. Given two entities and
proximity A
, and an observer , the categorization assigns three possible categories: is than
B O A nearer B
(to the observer ), is than , and are . Whatever the positions of
O A farther B A B equidistant A
and relative to , we can always assign a category: ‘nearer’, ‘farther’, or ‘equidistant’—i.e.,
B O
the categorization the space.
partitions
This is a first hurdle in the formalization of motion categorization: to overcome the loose
structure provided for our own cognitive system in order to achieve a systematic motion catego-
26 CHAPTER 3. CHALLENGES IN MOTION CATEGORIZATION
k l k l k l
Crossing motion Parallel motion Curved motion
A. B. C.
The first two motions, A and B, are linguistically categorized, respectively, as
‘crossing’ and ‘parallel’ motion. The last motion, C, is linguistically uncategoTrized; we use the
generic term ‘curved’.
rization. F
3.4 Disciplinary Fragmentation and Tension
As we presented in Section 1.0.1, categorization is an integral part of cognition, and, as such, is
handled virtually by any discipline in cognitive science. Sadly, as each cognitive discipline usually
develops its own categorization paradigm and methodology, categorization research 4becomes
—‘ ’ occurs when disciplines develop their own research
fragmented Disciplinary fragmentation
isolated, or quite independent, from each other (See Dawson 2013, Ch. 1;3See also Schwartz
1997, p. 25).
TDhe first most dramatic effect of disciplinary fragmentation is.that, since the other disciplines
remain unaware of the results in one certain discipline, they cannot , , or such
verify refine refute
results. This is notably the case of qualitative represenm tation and reasoning amongst others in
spatial domain (QSR). Since the birth of qualitative representations—commonly accepted to
be J. F. Allen’s (1983) intervals theory—AI researchers had been arguing that such qualitative
representations and their associated reasoning were “cognitively plausible” (Freksa 1992a, p. 3)
or “cognitively adequate” (Hernándeze1994, p. 4). Yet not until the mid of the ’s, psychologists
began to proof in humans the etxperimental validity of such claims (Mark and Egenhofer 1994;
1995; Knauff et al. 1995; /1997).
A fragmentation had occurred between AI and psychology in the field of spatial categorization
concerning cognitive plausibility of QSR. The QSR scientists had been making human-cognitive
claims about their spatial representations, and had not bothered to reach out to the psychologists
to verify them. Knauff, Rauh, and Renz help us grasp such situation:
“Whether or not a formal approach to spatial relations is a cognitively adequate
[. . .] model of human spatial knowledge is more often based on the intuition of the
researchers than on empirical data.” (1997)
“Qualitative spatial reasoning is often considered to be akin to human reasoning.
This, however, is mostly based on the intuition of researchers rather than on empirical
data.” (2000)
We faced an unpleasant situation. On the one hand, researchers in QSR emphasize how
central is the link with human cognition; for example, in the words of A. G. Cohn (1995), one of
the most cited QSR researchers:
3.4. DISCIPLINARY FRAGMENTATION AND TENSION 27
“The principal goal of Qualitative Reasoning (QR) is to represent not only our
everyday commonsense knowledge about the physical world, but also the underlying
abstractions used by engineers and scientists when they create quantitative models.”
On the other hand, QSR researchers do not readily verify their claims by psychological
experimentation—or, at least, these researchers do not call loud enough for a verification—
(See notable exceptions such as Mark and Egenhofer 1994; 1995). For instance, as of 2004, one
of the most basic spatial representations, RCC, created in 1992, had not yet been checked for
cognitive plausibility with respect to reasoning (Knauff et al. 2004, p. 182). Still more recently,
researchers criticize startled that so few studies tackle the cognitive plausibility of qualitative
spatial representations, all the more because such representations are key in human-computer
interaction (Yang et al. 2015, Intr., ).
Since most people involved in studies of cognitive plausibility of qualitative representations
are not the ones developing them, we regrettably get the impression that the QSR community
has a marginal interest on studying cognitive plausibility and their main concern
experimentally F
is mathematical formalism; even though, they paradoxically continue to aim to model human
cognition, or, equivalently, common-sense (e.g., Wolter and Wallgrün 2012; Dylla et al. 2017).
Anyway, we wrap up this critical view of disciplinary fragmentation between qualitative calculi
and psychology with the hope-giving words of Klippel et al. (2013, Sect. 2.2):
“While the number of behavioral [i.e., psychological] validations of spatial calculi
is small compared to the number of proposed formalism, there is an active community
that performs research on refining and tailoring formalisms through validating their u
cognitive adequacy.”
A second dramatic effect of fragmentation is interdisciplinary tension. Wohen disciplines use a
D .
different methodology or terminology, then statements in one discipline can be misunderstood or
disputed by other disciplines. For example, if we use the label ‘cognitive’, a psychologist might
understand that we talk about “human cognition”, but an AI researcher might understand that
we talk about “knowledge representation”—The difference is substantial.
More concretely, cognitive science has two prevailing goals: and
constructive explanatory
(Gärdenfors 2004, Sect. 1.1.1). On the one hsand cognition in AI is mostly constructive, they
create models to process information—though not necessarily mimicking human cognition—while
cognition in psychology is explanator/y because they describe human cognitive processes. Thus,
the use of the term ‘cognitive’ by:an AI researcher might bewilder (or even irritate) a psychologist;
we see such a tension in the editorial debate of the journal (Ross 2019;
Cognitive Processing
Katsikopoulos 2019), which was sparkled by A. Kirsch’s (2019) paper.
Thank God, we can defuse tension: if we are transparent in our terminology (clearly defining
the terms we use), and if we found our methodology on solid principles, which need be openly
and unambiguously stated. Even so, in my opinion, the key to eliminate fragmentation and
the consequent tension is to strive to our cognitive result into the other disciplines,
integrate
i.e., to explicitly show the links, commonalities, and differences of our results with respect to
others—This is oftentimes a hard task.
In this work, we do strive to integrate our results, which originate in the field of artificial
intelligence, into other cognition fields such as psychology and linguistics: we invested effort
in linking our definitions and models to well-known results and models in categorization (e.g.,
Sects. 6.5, 7.2 and 7.4).
D .
Part II
Foundations of Categorization
D .
D .
Chapter 4
Categorizations, Categories, and
Concepts in Human Cognition
God said, “Let there be light,” and God said, “Let there be light,” and there
there was light. was light.
God saw the light, and saw that it was God saw that the light had a very dis-
good. God divided the light from the tinctive feature than darkness: ‘Goodness’. 4
darkness. Based on this feature, God established utwo
God called the light “day”, and the categories: and .
light darkness 3
darkness he called “night”. God labelled the category ‘day’, and
light
the category ‘night’.
darkness o
D .
Genesis 1:3–4 (WEB) s
Genesis 1:o3–4 (Cognitive Paraphrase)
In this chapter, we summarize previous research inocategorization. Firstly, we present the
in the literature and explain how wesapply them in this work. We aim at clarity
basic terms
and consistency; we also aim to make this work understandable and integrable in any discipline
that researches categories and concepts. Secondly, we mention major experimental results in
human categorization; which belong virtually to regardless of what
/ every human categorization
items they categorize. We mindsthese results in human categorization as we develop our motion
categorizations because one strong motivation for categorizing motion is to apply it in human
computer interaction.h
4.1 Terminology
All throughout this work, we use profusely the terms and ‘ ’. A
categorization category category
is any of the groups that result from a ; that is, any of the groups in which our mo-
categorization
tion categorizations arrange according to similarities, commonalities, or shared
motion scenarios
properties. Yet, depending on the context, we use alternative terms for /
category categorization
because they are the most common terms in another disciplines: We might use the terms ‘concep-
t/conceptualization’, common in linguistics; or ‘class/classification’, common in machine learn-
ing; and, often, we use the terms ‘ ’/‘ ’, common in
qualitative representation qualitative relation
artificial intelligence, to name spatio-temporal categorizations and categories (Sect. 5.1).
32 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
Importantly, the term ‘ ’ is a twin term of ; it has a similar meaning as ,
concept category category
but, depending on the author, it has diverse nuances. Murphy (2002, Ch. 1, p. 5) provides basic
definitions that reflect the core understanding in psychology and cognitive science (e.g., Gerrig
and Zimbardo 2005, p. 230; Medin and Coley 1998, Sect. II): are classes of things, and
categories
are the mental representations of the categories—the descriptions for the categories.
concepts
‘Concept’ is, thus, a sophisticated term. It not only captures the commonalities and features in
a category, and how they are interrelated (Sloman et al. 1998), it also works as the “categorization
rule” (Bower and Clapper 1989). For that reason, we avoid the term : we advance
concept
simplicity by using more specific terms.
In our case, we are primarily interested in deciding the of items to a certain
membership
category; we intend to assign each motion scenario to a certain motion category. Thus, for us,
the categorization rule is an essential component of a categorization. By the term ‘
categorization
’ we refer to the function that decides the category of any item (disregarding whether the
rule
decision is deterministic or probabilistic, crisp or fuzzy). In that sense, ‘ ’ is the set of
category
items grouped by the categorization rule into the same value, and a ‘ ’ consists of
categorization
a categorization rule, plus all categories that such rule creates: This is the most elementary
framework we can build to categorize items (More details in Section 7.1).
4.2 Classical Model and Classification
Before we delve into the nuances of how humans categorize and which categorization models
try to conform to it, we have to mention the oldest and simplest model: the or
classical model
. r
classical view
D .
The ‘ ’ is equivalent to say that humans categorize in the same way mathe-
classical model s
maticians assign set membership; in set theory, membership is only a Boolean value: ‘ ’, if a
o 1
member, ‘ ’, if not a member. In that sense, categories can be assimilated to crisp sets and the
most basic operations with categories are the set operations: union , intersection ,
A B A B
∪ ∩
and set difference ; we can thus applycthe powerful machinery of set theory to work with
A B
categories. Furthermore, under a classiical model, we can represent knowledge as a hierarchi-
cal tree of categories (i.e., a hierarchical taxonomy) which has many advantageous processing
properties (Sect. 4.6.1).
Therefore, the classical model is the most popular one used in artificial intelligence: it is cheap
to machinally implement and to maintain, and it has powerful reasoning capabilities relying on
first order logict(Russell and Norvig 2014c). Mind you, computer scientist use unwillingly the
term ‘categorization’ but rather ‘classification’. When an AI researcher means to perform a
“classification” of certain items, chances are that she refers to a categorization according to the
classical model.
As we said in Sect. 1.0.1, p. 15, the classical view is outdated in human cognition as a precise
account of categorization. As we will immediately see in Section 4.3, humans categorize in a
graded manner; for example, is (in the Western culture) a member of the category
apple fruit
in a higher degree than (McCloskey and Glucksberg 1978). Actually, gradation is
avocado
one of the best established facts in human categorization. Does it mean that the classification
work in AI is futile? By no means! The classical model is the most elementary approach to
categorization, and, in that respect, we can always resort to it as most economical implementation
which under certain conditions (e.g., artificial categories, or negligible boundary effects) can be
effective enough.
4.3. CATEGORY TYPICALITY AND MEMBERSHIP 33
4.3 Category Typicality and Membership
4.3.1 Typicality and Prototypes
Humans perceive some items as of a category; those items are the [highly]
the best examples
‘ ’ category members, for example, as fruit, or as furniture—They are often
typical apple chair
called also ‘ ’. They play a pivotal role in categorization; even they underlie a
prototypical items
traditional categorization model, the ‘ ’ (details in Sect. 4.4.2).
protype model
Some items are less typical members, for example, as fruit, or as furniture. Others
olive rug
are , for example, as fruit, or as furniture (E. Rosch and
extremely atypical beet telephone
Mervis 1975; McCloskey and Glucksberg 1978). This property of how items fit a category is
called ‘ ’, or ‘ ’.
typicality prototypicality
Typicality is a continuous value, i.e., it is graded (e.g., Barsalou 1985; E. H. Rosch 1973);
subjects can numerically evaluate the typicality of an item with respect to a category. For exam-
ple, subjects can assign typicality a value in the unit interval : is extremely typical,
[1.0, 0.0] 1.0
and is extremely atypical, i.e., unrelated to the category. In McCloskey and Glucksberg
0.0
(1978), subjects evaluate the typicality of certain items as fruit; for instance, obtains an
apple
average typicality of , , ; and as furniture, for instance, ,
0.991 olive 0.338 beet 0.241 chair 0.994
, (we have normalized the original values to the unit interval).
rug 0.583 telephones 0.291
Typicality is, thus, a well-established property of categorizations, and, arguably, the most
influential. E. Rosch (1978, p. 38) states, “the prototypicality of items within a category can
be shown to affect virtually all the major variables used as measures in psychological research.”
Murphy (2002, Ch. 2, p. 22), restates it, “[t]ypicality differences are probably the strongestuand
most reliable effects in the categorization literature.” We present a non-exhaustive list of how
typicality influences cognitive processes:
• TypicalDitems are those that subjects most rapidly recognize as ca
tegory members (E. H.
Rosch 1973, p. 141; see Rips et al. 1973, pp. 9, 19; c.f., Wilkins 1971).
• Typical items are more frequently produced as spontaneous examples of the category
(Mervis et al. 1976)—which is called ‘ item dominaonce ’; for example, if subjects are asked to
give examples of fruit, they will much more probably mention than . Indeed,
s apple olive
in the study of Battig and Montague (1969) with 442 Subjects, mentioned in
e 97% apple
their fruit list, appearing in almost as the first mentioned fruit; while was only
60% olive
mentioned by three subjects, i.e., , never in the first place.
0.7%
• Subjects learn sooner typical items than atypical ones as members of the category. Both
when growing up, subjects learn prototypical values at younger age than they learn the
boundaries of categories (Mervis et al. 1975), and at any age (in natural categories, Heider
1972, p. 19), even when atypical items are shown more frequently (in artificial categories,
E. Rosch, Simpson, et al. 1976, p. 498).
Even though and, thus, the existence of real in human cognition
typicality prototypical items
are far beyond any empirical question, the existence of prototypes in our mental
abstracted
representations of categories is heatedly debated; it is a decisive issue in the controversy of two
main categorization models, the and theory (Sect. 4.4.2).
prototype exemplar
A. What makes something ‘typical’ ?
We have expounded on typicality and their effects on cognitive processes, but without addressing
the question of what makes an item typical in a category.
34 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
E. Rosch, Simpson, et al. (1976, pp. 492f.) mention essentially two different sources of typi-
cality. First, when considering ‘ ’, the most typical items (i.e.,
continuous attributes prototypical
) are the ones having feature values close to the average values of the category members;
items
implying that an is some sort of average of the category members. This
abstracted prototype
occurs, for example, with sizes of animals (Rips et al. 1973, p. 19) or facial features (Reed 1972).
Moreover, the most typical item in a certain category has very different feature values than the
items in other categories.
Second, when considering ‘ ’ (‘is’ or ‘is not’), the most typical item is the
binary attributes
one having most common features with the category members (Note that the common features
can be disjoint). Moreover, the most typical item in a certain category has the least common
features with items in different categories. For example, if we consider the strings ,
T w = AABDEE
, as members of a category, then, the string would be the prototype,
w = AABKLM w = ZXYKEE w
2 3 1
because it shares with , and with ; and share only . Coincidentally, in this
AAB w EE w w w K
2 3 2 3
case, the shared features of ( and ) are disjoint, i.e., have nothing in common.
w AAB EE
Interestingly, those two sources of typicality can be summarized in one simple principle:
‘ ’ (E. Rosch 1977, p. 36), also called ‘ ’ (Posner and Keele 1968). If we
centrality central tendency
represent the items of a category with continuous attributes in their multidimensional featural
space, the prototypical element lies at the centre of the points cloud of items. Likewise, it
occurs for items of a certain category with binary attributes: when we optimally represent their
dissimilarities in a continuous metric space (using the MDS technique), prototypical items appear
also central.
We can, thus, give an equivalent definition of typicality based on : the mrost typical
R similarity
of a category is the item being in average to all items in the catgegory and
most similar most
to all items in the other categories.
dissimilar /
Even though predicts typicality in ‘ ’ categories (e.g.,
central tendency common taxonomic
and ), it fails to predict typicality in other type of categories, such as goal-derived
fruitD furniture .
categories (Barsalou 1985), in which typicality is rather prednicted by the ‘ ’. For example,
ideal
the category has as a typical member which is not the average features of the current
Christian m
Christians, but rather an member, namely, Jem sus. In other words, one considers a person
ideal
to be more a member of the category o in the measure such person is similar to Christ
Christian
(Oxford English Dictionary n.d.), instead sof comparing him to the average Church member.
Finally, and might fail to predict typicality for subjects that are un-
central tendency ideal
knowledgeable about a categoryt. In that situation, subjects resort to the most primitive source
of typicality: ‘ ’ (/e.g., Lynch et al. 2000), which amounts to ‘ ’ (Murphy 2002,
familiarity frequency
p. 31). Accordingly, the category prototype is the item that the subject has experienced (e.g.,
seen, heard, or read) more frequently. We can resort to , even if we do not personally
familiarity
know the item or its features: it suffices that, in our everyday life, we hear the item’s name more
often than the other items in order to deem it prototypical.
4.3.2 Membership
Besides typicality—but extremely related to it—a chief term in categorization is ‘ ’,
membership
i.e., whether an item belongs or not to a category. This question is answered by the
categorization
, either deterministically or probabilistically, either fuzzy or crisp. For example, is a
rule apple
? Or, is a ?
fruit olive fruit
From a practical stance, determining membership might be the most relevant question in
categorization, and it is indeed the one we answer when we create our motion categorizations
in Chapter 9. In fact, determining membership has so impacted research that, though catego-
rization influences many psychological areas, in the decade of the 80’s most research on concepts
4.3. CATEGORY TYPICALITY AND MEMBERSHIP 35
concentrated on the categorization rules (Medin 2011).
A right answer to the membership question is vital in daily circumstances. For instance,
both humans and animals search tirelessly for items belonging to the category ; also,
edible
when doing the shopping, we purchase items belonging to the categories in the grocery list
( , , . . . ); and, particularly in navigation, an autonomous vehicle might categorize a
apple olive
motion scenario as or , in order to start an avoidance manoeuvre.
collision collision-free
A. Gradation
According to the , category membership is a binary value: either
classical categorization theory
an item belongs or not to the category—there is no intermediate value. Even more, Tall items
belonging to a category are equivalent members of the category: an item cannot be a preferential
or a marginal member. This idea harmonizes with logical thinking, and, notably, with the
classical set theory, where membership of an item to a set is a logical value, either or
true
. However, category membership in real world is not a logical valuF e, as the seminal work of
false
McCloskey and Glucksberg (1978) clearly shows (See also, Lakoff 1973). They related to
typicality
and gave evidence that as much as typicality is graded, so it is membership
category membership
(Fig. 4.1; see also, Barsalou 1983). They showed that subjects agreed almost to on the
100%
category membership of both most typical items (typicality values – ), as belonging to the
1.0 0.9
category, and the most atypical items (typicality values – ), as not belonging. However, as
0.1 0.0
we approached the middle typicality ( – ), on average of the subjects disagreed about
0.5 0.3 40%
the membership of the items. For example, the majority of subjects accepted as member
olive r
of the category, but as much of of them did not; in a similar vein, the majority
fruit 40% g
rejected as , but of the subjects accepted it as member of such category. In
rug furniture 48%
contrast, all subjects agreed that belongs to the category , and almost all ( )
apple fruit 88%
agreed that does not.
beet
D .
Summarizing, the boundaries of category membership can be so that virtually half
nfuzzy
of the subjects may accept item membership, while the other half may reject it. Moreover,
the gradation on category membership occurs not only for members, but it is equally valid for
non-members: the more typical a non-member is, theohigher percent of subjects classify it as
non-member, and, conversely, the more typical a member is, the lower percent of subjects classify
it as non-member—As Barsalou (1985) exemplifies it, “ is a better non-member of
chair birds
than is ”.
butterfly
B. Membership and simpilarity: Family resemblance
Comparing membershhip and typicality is straightforward, because both are scalar values assigned
to every single item in a category. In contrast, comparing membership and similarity is complex
because similarity is a binary relation between items.
A fundamental result on similarity and membership is the so called ‘ ’: The
family resemblance
average within-category similarity should be higher than the average between-category similarity.
That is, category members are in average more similar than non category members. (ibid., p. 630;
originally, E. Rosch and Mervis 1975).
4.3.3 Historical remarks
Typicality, gradation, and fuzziness in categories are far from obvious. Though now are com-
mon knowledge, they originated in the paradigm shift of categorization, which began 1970s. A
notorious testimony of this paradigm shift are the words of E. H. Rosch (1973, Abstr.):
36 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
0.94 0.83 0.72 0.61 0.50 0.39 0.28 0.17 0.06
Normalized Item Typicality
D .
pihsrebmeM
metI
ni
tnemeergasiD
noinipo
ytirojam
morf
gnireffid
stcejbus
fo
Disagreement in Item Membership
related to its Category Typicality
It shows, for each typicality interval, the percent of subjects that disagreed with
the membership decision of the majority. For example, for the typicality interval ,
[0.44, 0.33]
which has label , the average of subjects that contradicted the membership classification of
0.39
the majority is . That is, if the majority accepted an item as category member, on average
36%
the rejected the item as member of such category, and vice versa.
36%
typicality normalized to the unit interval.
[P]sychological and linguistic research has tended to treat categories (whether
perceptual or semantic) as though they were internally unstructured—that is, as
though they were composed of undifferentiated, equivalent instances—and as though
category boundaries were always “well-defined”. [. . .] It is the contention of the
present paper that most “real” categories are highly structured internally and do not
have well-defined boundaries; thus, we may presently have a quite distorted view of
how real categories are learned and how they function in cognitive processes.
Once we have settled the experimental facts in categorization: typicality, gradation, and
fuzziness; we confront the awkward task to cognitively model it. To that end, two tools are
frequently used: and . Similarity is widely used to account for category
similarity fuzzy logic
membership and comparison effects between items—we expand on it in the next section. Fuzzy
logic is used to account for decision making, inference, and, more generally, operations with
categories (e.g., Klir and Bělohlávek 2011, esp. Ch. 4, 5; Douven 2020; Lakoff 1973; c.f., Zadeh
1972).
4.4 [Dis-]Similarity
In the previous section, we showed that a category has internal structure: the members of
a category, though having binding commonalities (i.e., ), are perfectly
family resemblance not
equivalent; they are graded according to . We also showed that both family resemblance
typicality
and typicality can be defined in terms of . It follows that, as a very first step in modelling
similarity
4.4. [DIS-]SIMILARITY 37
a category, we can define a ‘ ’, to compare pairs of items, , and
similarity function S(A, B) (A, B)
use it to establish the category membership and the internal category structure—As is the case
in many categorization models (Goldstone 1994; Kruschke 2001).
Actually, researchers often work with an alternative to similarity: the ‘ ’
dissimilarity function
(e.g., Krantz and Tversky 1975; Klippel et al. 2012, p. 244), Dissimilarity has certainly
D(A, B)
very convenient advantages over similarity. Foremost, from a practical view, dissimilarity be-
haves like a , which is a very intuitive concept and has a firm mathematical foundation
distance
(p. 42); moreover, distances enable the powerful technique of ‘ ’ (MDS)
multidimensional scaling
(Section 4.4.1.A).
At any rate, we can easily switch between dissimilarity and similarity because they are in-
versely related: the higher the dissimilarity the lower the similarity. This relation can be specified
by means of a monotone decreasing function , so that ; for example,
f (x) D(A, B) = f (S(A, B))
typically used are if , or (Mair et al.
f (x) = 1/x S(A, B) > 0 f (x) = log (x/max S(A, B))
A,B
2022). In fact, due to their direct relation, sometimes scientists use the generic term ‘ ’
proximity
to refer to any of them, similarity or dissimilarity (Borg et al. 2018,F p. 2). In this work, we
will explicitly mention either ‘similarity’ and ‘dissimilarity’, but always bear in mind that they
are directly related. Even more, we can translate any comparison statements obtained using
similarity and express them equivalently in terms of similarity, and vice versa (Tab. 4.1); such
equivalence preserves the information about [dis-]similarity rankings.
Similarity Dissimilarity
is more similar to than to is less dissimilar to than to
A B C A B C
≡ u
is less similar to than to is more dissimilar to than to
A B C A B C
is the most similar item to is the least dissimilar item to 3
A B A B
is the least similar item to is the most dissimilar item to
A B A B
D .
The statements in the same row are equivalent: on the left row are expressed with
the term ‘similarity’; on the right row are expressed with the term ‘dissimilarity’
The similarity usually outputs a real value, which beolongs to an ‘ ’ (Stevens 1946;
ordinal scale
See, Sternberg and Pickren 2019, pp. 100–104): wescan similarity values, i.e., given two
compare
pairs of items, and , we can establish which relation is true, either
(A, B) (C, D) e S(A, B) S(C, D)
or . Consequently, we can items with respect to similarity: the higher
S(C, D) S(A, B) t rank
the value the higher the similarity. For example, if we had , we
/ S(A, B) S(A, D) S(A, C)
: ≤ ≤
could say that is the most similar item to , is less similar, and is the least similar.
C A D B
Nevertheless, other operations, such as adding or multiplying similarities, are not necessary
S(orange, grapefruit) =
and in the 5-point scale unrelated, highly related.
2.69 S(orange, coconut) = 1.33 0 = 4 =
Based in these values, it makes, however, no sense to affirm that an is twice similar to a
orange
that it is to a .
grapefruit coconut
In any case, the similarity function, or simply, the ‘ ’, is arguably the most intuitive
similarity
tool to model experimental results on categorization—as Murphy and Medin (1985, p. 291) put
it: “Perhaps the most powerful explanation of conceptual coherence is that objects, events, or
entities form a concept because they are similar to one another”. Indeed, the main types of
categorization theories, i.e., classical, prototype, and exemplar model, can be modelled using
a similarity function (Medin and Heit 1999, Sect. I.B). Even, not only models, but most
different sorts of categories, including ad-hoc categories (Barsalou 1983), seem to possess an
intern similarity function. In fact, E. J. Wisniewski (2002, pp. 467f.) finds in his survey
only
theories of concept structure and models of concept acquisition and categorization that are
38 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
Item Featural item
A A
feature Features separation
extraction based on type
Item Featural item
B B
ordinal/cardinal nominal/binary
Metric distance Matching function
d(A, B) F (A ∩ B, A\B, B\A)
judgement function
−x
g(x) = e Similarity functions
−d (A,B)
S(A, B) = e S(A, B) = F (A ∩ B, A\B, B\A)
Overall similarity
S(A, B)
Schema of computing the similarity of two items and . The dashed lines
indicate controverted steps (Sect. 4.4.4).
grounded in the similarity. About 20 years before, Murphy and Medin (1985) came to the same
conclusion: all accounts on categorization “rely directly or indirectly on the notion of similarity”.
It seems, thus, that similarity as a ground of categorization is a thread throughout modern
D .
research history. n
Being similarity so prominent in categorization for the last decades, we might be astounded
at this ancient remark of F. Attneave (1950) (emphasis added):
The question ‘What makes things seem alike or seem different?’ is one so funda-
mental to psychology that psychologists have been naïve enough to ask it.
very few
We are aware of devoted to a consideration of the problem[.]
only two papers
From which we also dra:w a lesson: how decisive is, in science, to concretely define, measure, or
falsifiably model the key concepts. We can retrospectively affirm that, in psychology, the striving
to model has unlocked a wealth of insight.
simihlarity
Concerning our motion categorizations, they possess also similarity functions (Sect. 7.1.3);
such functions arise effortless from the metric structure of the categorizations, as shown in
Metric models. Because of the explanatory power of similarity, its pervasiveness, and success
in categorization modelling (See Murphy 2002, p. 481), we consider the presence of a similarity
function a strength of our model.
4.4.1 Modelling and Computing Similarity
Similarity, as an ordered relation or a graded value, has been proved to have experimental
significance. Subjects understand and perform tasks that consist on assessing the similarity of
items (See tasks, Dunn-Rankin et al. 2004, pp. 27–34). One type of assessment task is comparing
similarity between items, i.e., choosing the most similar item (e.g., Tversky and Gati 1978) or
4.4. [DIS-]SIMILARITY 39
‘ ’ items according to similarity, i.e., ordering them without giving a numerical value;
ranking
another type is numerically ‘ ’ items according to similarity (e.g., Gati and Tversky 1984).
rating
We apply such insights about similarity in this research: we experimentally check whether
our story-based motion categorizations are cognitively plausible (Ch. 10). To that end, we make
the subjects choose the more similar of two items compared to a reference item. Subsequently, in
order to evaluate the data, we transform the pairwise similarity choices into a similarity rating
(Sect. 10.2.1). Therefore, we can mathematically transform the similarity assessments between
types: pairwise comparisons, ordinal and numerical ranking.
Once we have experimental similarities, one of the greatest, and most
Featural Similarity
hotly debated challenges is to model the experimental similarity assessments by means of a
similarity function (see more in Sect. 4.4.2). The purpose is that, given two items,
S(A, B) A
and , we theoretically reproduce their experimental similarity values , i.e., their ordering or the
rating, by means of the similarity function. The first step towards the computation of similarity
is, almost inescapably, the extraction of from items. Indeed,
relevant features feature extraction
is one of the earliest processes common to all models of categorization and concept acquisition
(See, E. J. Wisniewski 2002, pp. 474f.); actually, it is a basic assumption of cognitive psychology
that items in the world can be described using features (E. Wisniewski 2001, p. 5433). Surely
because features seem a , readily accessible property of items (e.g., E. Rosch 1978,
transparent
but see Sect. 4.4.4). Based on extracted features a value for the similarity is computed (See
Fig. 4.2).
We encounter, however, two distinct types of features that we cannot process with the usame
methods: and features:
nominal ordinal 3
• ‘ ’ (e.g., eye colour) cannot be assigned a meaningful varlue. For example,
Nominal features
the eye D colour may have following values (brown, blue, green); a n.umerical assignment of
values, e.g., ( , , ) is as valid as any other permutation, e.g.,
brown = 1 blue = 2 green = 3
( , , ); the value works only as identifier, but has no meaning
brown = 3 blue = 1 green = 2
in itself. A particular case are the ‘ ’ (e.g., can fly vs. cannot fly) they have
binary features
only boolean values.
• ‘ ’ (e.g., beauty, brightness) can be ranked, i.e., ordered, (e.g., sun, head-
Ordinal features
light, candle, and glowing coal ctan be ranked according to brightness). An important
particular case are the ‘ / ’, they are expressed numerically (e.g., tempera-
cardinal features
ture, length), so they allow arithmetical operations.
Interestingly, ordinal features can often be turned into cardinal by rating them against a
maximum and minimum value. For instance, considering the maximum brightness, value ,
sun 1.0
and , the minimum brightness, value , we can cardinally rate brightness: ,
pitch black 0.0 sun = 1.0
, , ). As a consequence, we can eventually treat
headlight = 0.3 candle = 0.1 glowing coal = 0.01
ordinal features as cardinal ones (See, Bartoshuk 2019, pp. 103f.).
Summarizing, features (eventually also features) use operations incompati-
cardinal ordinal
ble with the features. This has rather unpleasant consequences for the computation of
nominal
similarity: We must use different mathematical operations to compute the similarity depending
on the features types. Cardinal features can be represented in a coordinate space, and, conse-
quently, their similarities can be based on ; we have then the so called ‘ ’
metric distances geometric
(Tversky 1977), ‘ ’ (Hahn and Heit 2001), or ‘ ’ (Kruschke 2001)
spatial multidimensional scaling
models—we call them ‘ ’ models. Nominal features allow only ; consequently,
metric set operations
their similarities are based on union , intersection , and set difference .
A B A B A B
∪ ∩ \
40 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
All considered, we might question how well-founded is comparing similarities obtained from
so different models (Fig. 4.2). In practice, we find examples where both models are efficaciously
combined (e.g., Volkert, Müller, and Kirsch 2018).
A. Metric models
‘ ’ (also called ‘ ’ (Tversky and Gati 1982) or, as a superordinate
Metric models geometric models
concept, ‘ ’ (Tversky and Krantz 1970; E. E. Smith and Medin 1981, pp.
dimensional models
102ff.)) base on two assumptions. First, an item—or rather, its relevant features—can be repre-
sented as a point in a real coordinate space, commonly called ‘ ’ (e.g., Nosofsky
psychological space
2011; Tversky and Hutchinson 1986). For example, in order to represent a beep sound, we only
need two features: and ; accordingly, we can represent it in a two-dimensional coor-
pitch volume
dinate space. Second, we assume that the between items is strictly increasing with
dissimilarity
respect to the between such featural points.
metric distance d(A, B)
Thus, in practice, we have following requirements for metric models: features must be
car-
, and we have to choose a distance between features, a ‘ ’, widely known as
dinal featural distance
‘ ’. The common distances used in categorization experiments are general-
psychological distance
ized Minkowski metrics (Kruschke 2001), not only because of their computational practicability
(e.g., Ashby 1992), but also for the convenient mathematical psychological properties (Beals,
Krantz, and Tversky 1968; see, Glazer and Nakamoto 1991).
(cid:34) (cid:35)1/p
(cid:88) r
R p (4.1)
d(A, B) = c w a b p 1 u
i i i
| − | ≥ g
i 3
The Minkowski distance—also known as or distance—measures the distance be-
L p power g
tween, , the features of item , and , the features of item . The weights , ‘
a A b Bo w attention
i i i
D .
{ } { }
’, may refer to changes in the attention to different featusres, and is a general sensitivity
weights c
parameter (Nosofsky 1992a, p. 366f.). o
We further require a function, , that relates the psychological distance to the similarity:
g(x) m
(4.2)
S(A, B) = g(d(A, B))
Such function is called ‘ ’ (e.g., Ennis 1992). Since dissimilarity is
g(x) jhudgement function
strictly increasing with the psychological distance , the judgement function must be strictly
/ d
decreasing (e.g., 1 , −x ). Remarkably, in human and even animal psychology, the featural
e:
1+x
distance relpates to the judged similarity by means of an exponential decay
d(A, B) S(A, B)
g(x) = e
−x α,
> 0
: the ‘
universal law of generalization
’ (Shepard 1987; compare with 1957;
e.g., Perrin 1992, p. 124). Accordingly, we obtain the following closed formula for similarity:
−d (A,B) (4.3)
S(A, B) = e α > 0
Metric models have successfully fitted wide range of data, moreover, they have proved to
distinguish ‘ ’ (types of features that can only exist simultaneously, such as the
integral dimensions
pitch and volume of a sound, or, the hue and brightness of a colour) from ‘ ’
separable dimensions
(those that can exist independently, e.g., colour brightness and sound volume) through the value
of , i.e., through the norm type. Separable dimensions fit better , city-block distance, while
p p = 1
integral dimensions fit better , i.e., Euclidean metric (Hahn and Chater 1997, pp. 55–57).
p = 2
Another interesting property of the metric models, is that they reproduce a ‘
multiplicative
’ rule—frequently used in modelling (Nosofsky 1992b; see also, E. E. Smith and Medin
similarity
1981, pp. 154–156)—, that is, the overall similarity, , can be computed as the product
S(A, B)
4.4. [DIS-]SIMILARITY 41
of the similarity for each feature , (e.g., Nosofsky 1986). This is the case, when the
i s(a , b )
i i
exponent in Equation (4.3) equals the power in the distance (Eq. (4.1)) then
α p d(A, B)
Eq. (4.1)
α=p
N N
(cid:0)
Eq. (4.3)
−c
α[ (cid:80)N
w |a −b |
p] (cid:0)
1 α (cid:89)
−d
(a ,b )
(cid:89)
S(A, B) = e i i i = e i i = s(a , b )
i i
i i
Nonetheless, metric models have their weaknesses (e.g., Maddox 1992, pp. 162–165; Hahn
and Chater 1997, pp. 57–60). Amongst others, the term , which is the ground
psychological space
concept of the metric models, is, at most, defined; and the mathematical properties of
vaguely
the metric distances are incompatible with numerous experimental psychological resultTs.
It is an abundantly used concept with a long tradition in psychology—at
Psychological Space
least from F. Attneave (1950)—(e.g., Shepard 1957; Nosofsky 1986; Pothos and Wills 2011b).
It is also called ‘ ’ (Rips et al. 1973), or ‘ ’ (E. J. Wisniewski 2002,
semantic space mental space
p. 478). Briefly, it is the space where the representation of stimuli can more naturally reproduce
the experimental similarity assessments using a certain dissimilarity function called
psychological
— are a kind of psychological spaces.
distance metric models
Unfortunately, this concept is rather indefinite in psychology, as U. Hahn and N. Chater
(1997, p. 91) state:
[T]he notion of psychological space is not particularly well defined: there are no
commitments as to what exactly this space is, whether it is a long-term representation
or not, nor whether it is explicitly similarity that is represented here or whethe3r the
representation of similarity it generates is merely a by-product of a general scheme
for the representation of objects.
D .
We illustrate the indefiniteness of the concept ‘psychological space’ by means of two examples.
In his pioneer work, Attneave (1950) had subjects compare pamrallelograms that differed in
tilt
(the angle between the base and the other side) and in obtaining dissimilarities
side length
between the stimuli. He noticed that representing the stimuli, i.e., the parallelograms, as points
in a two-dimensional space with coordinates and resulted in a better analogy
ariea tilt angle
between distance and dissimilarity than usinge instead of . Again, he noticed that
side length area
such analogy could be further improvedtby using the city-block metric to measure the distance
between stimuli. In sum, Attneav/e concluded that subjects compared the parallelograms in a
two-dimensional consisting of and as coordinates, in which
psychological space area tilt angle
they used the metric, as a .
city-block psychological distance
In the second example, Rips, Shoben, and Smith (1973) measured the paired dissimilarities
between certain mammals (e.g., bear, lion, horse, sheep), subsequently they fit the dissimilarities
to the Euclidean distances of points in a two-dimensional space using .
multidimensional scaling
In the best fit, the authors argue that the horizontal axis corresponds to , animals to the
size
left (e.g., bear, deer, horse) are larger than those to the right (e.g., dog, rabbit, mouse); and
the vertical axis corresponds to , animals to the bottom are wilder (e.g., lion, cat, bear),
predacy
while to the top are farm animals (e.g., goat, pig, ship) (similar results with animals, López et al.
1997; Shoben 1976). In short, Rips et al. consider that subjects mentally represent mammals
in a two-dimensional space (the psychological space) with the features and , as
size predacy
coordinates, and Euclidean metric as distance (the psychological distance).
These examples epitomize how is rather a pragmatic construct than an
psychological space
empirical reality: it is the coordinates space in which items are featurally represented, and it
is endowed with the distance that most accurately reproduces the experimental dissimilarity
42 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
judgements. The coordinates used are sometimes precise measurable quantities (such as and
tilt
in Attneave (1950)), but often they are also abstract values obtained by numerical analysis,
area
e.g., multidimensional scaling, that can only be intuitively and approximatively interpreted (such
as and in Rips et al. (1973))—sometimes even lacking intuitive interpretation.
predacy size
Accordingly, what motivates the use of psychological spaces is not a solid cognitive theory,
but rather the researchers’ necessity for a featural explanation of the similarity. In the words of
Rips, Shoben, and Smith (ibid., pp. 13, 14) (emphasis added):
While we have argued that [dissimilarity] distance can be conceptualized as [a
psychological] distance, this translation of constructs to a gain in pre-
has not led
dictability. [. . .] [W]e have retained this construct because to capture
it seemed
certain conceptual relations between the noun pairs [.]
One basic component of is the distance ,
Violation of Distance Axioms metric spaces d
which is directly related to the dissimilarity between items. Notably, has sharp
D(A, B) F d
defined mathematical properties, i.e., the distance axioms (e.g., Carothers 2000, p. 37):
i.
self-distance constancy d(A, A) = 0
ii. : ,
minimality d(A, B) d(A, A)
iii. :
symmetry d(A, B) = d(B, A)
iv. :R
triangle inequality d(A, B) + d(B, C) d(A, C) u
≥ g
These distance axioms impose constraints on the similarity function, because, in metric models,
similarity is computed from a distance using a strictly decreasing function, (e.g., Eq. (4.3)).
g(x)
Each distance axiom above yields its respective similarity constraint below marked with the same
D .
item label but capitalized.
I.
self-similarity constancy S(A, A) = S(B, B) A, B
II.
maximality S(A, A) S(A, B)
≥ s
III.
symmetry S(A, B) = S(B, A) e
IV. Using the most general judgement function , no constraint can be derived from the
/ g(x)
(item iv.). However, making assumptions on the form of , we can
triangle inequality: g(x)
obtain constraipnts on experimental similarity values.
a. If −x α with , the triangle inequality for distances becomes a
g(x) = e 0 α 1
≤ ≤
multiplicative inequality for similarities: (Prop. A.6.1)
S(A, C) S(A, B) S(B, C)
b. If the distance is a (Eq. (4.1)), then a tighter constraint for the
d Minkowski metric
distances, the ‘ ’, can be derived from the triangle inequality (Tversky
corner inequality
and Gati 1982). The has a powerful practical purpose: we can use
corner inequality
it to rebut the triangle inequality using experimental dissimilarity values without the
need to specify or know the function .
g(x)
It is experimentally verified that similarity properties I. and III. do not always hold (Tversky 1977;
Tversky and Gati 1978), the triangle inequality has also been rebutted experimentally by Tversky
and Gati (1982) under the conditions in IV.b.. Briefly, it seems that the , i.e.,
metric assumption
that dissimilarity is strictly increasing related to a distance, is only an acceptable approximation,
but not a faithful psychological model. For that reasons, the metric models have been modified
4.4. [DIS-]SIMILARITY 43
to accommodate the discrepancies, mainly extending the distance with asymmetrical terms (e.g.,
Krumhansl 1978; Appelman and Mayzner 1982), but often opening more questions than solving
them (see critique, Corter 1987; 1988; Krumhansl 1988).
Finally, Nosofsky (1991a) sets a milestone in the attempts to create valid modified dis-
tances, as he notes that various modifications to the metric models (e.g., distance-density model,
Krumhansl 1978; hybrid tree-euclidean Carroll 1976)—and even some nominal models (e.g., the
contrast model, Tversky 1977)—are particular cases of a general model due to E. W.
additive
Holman (1979), where
S(A, B) = F [s(A, B) + r(A) + c(B)]
is a strictly increasing function, is a similarity function, and
F s(A, B) symmetric r(A) c(B)
are bias of the particular items. The key point is that the additive extra terms, and , account
r c
for the and violations of .
asymmetry self-similarity
Summarizing, a distance-based similarity is not an ideal psychological model, though in most
cases a very good approximation, as Tversky and Gati (1978) explain:
Although the violations of [similarity properties that base on distance] are sta-
tistically significant and experimentally reliable [. . .], the effects are relatively small.
Consequently, [such properties] may provide good first approximations to similarity
data. [Multidimensional] [s]caling models that are based on these assumptions, there-
fore, should not be rejected off-hand. A Euclidean map may provide a very useful
and parsimonious description of complex data, even though its underlying assump- r
tions (e.g., symmetry, or the triangle inequality) may be incorrect. At the same timge,
one should not treat such a representation, useful as it might be, as an adequate
psychological theory of similarity.
D .
B. Nominal models
When features are nominal or binary values, the items are
described as a . As example, we show how
set of features i
s 1
Keren and Baggen (ibid.) described e digits
seven-segmented
(the numbers appearing in digital distplays) using their seg-
2 3
ments as features. They labelled ea/ch segment with an integer
: 4
such labels. For instance, the digit and the digit
1 = 3 , 6
h { }
. 5 6
6 = 1 , 2 , 5 , 7 , 6 , 4
{ }
Since, in nominal models, items are described as sets of fea-
tures, we can only apply for whatsoever compu-
set operations
tation between items, particularly, when computing similarity. Features ( – )
That is, we are restricted to union , intersection , of a seven-segmented nu-
X Y X Y
∪ ∩
and set difference . On this fact bases the most influen- meral according to Keren
X Y
tial model to compute similarity between items with nominal and Baggen (ibid.)
or binary features: A. Tversky’s (1977) ‘ ’. He
contrast model
postulated certain cogent assumptions for the similarity function, , between two items A
S(A, B)
and B: , , , , and . Because of relevance
matching monotonicity independence invariance solvability
and simplicity, we present only the first two ones.
The assumption holds that the similarity of nominal values is only a function of
matching
following set operations, namely , , and . That is,
A B A B B A
∩ \ \
44 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
(4.4)
S(A, B) = F (A B, A B, B A)
∩ \ \
It seems perfectly reasonable that the ‘ ’ of both items and , i.e., ,
common features A B A B
should mainly determine how similar two items are—things are similar inasmuch as they have
commonalities. It seems also reasonable that the features belonging exclusively to an item, i.e.,
the ‘ ’, and , also influence similarity—for example and
distinctive features A B B A horse zebra
\ \
have countless common features; however, the very distinctive feature of differentiate
being striped
unequivocally both concepts. Finally, Tversky excluded the whole features set, i.e., , from
A B
the computation of similarity—this decision is disputable, we cannot find convincing arguments
for this exclusion, neither Tversky does in his paper (See Sect. Contrast Model’s Criticism).
The monotonicity assumption holds that the similarity function in Equation (4.4) is mono-
tone with respect to inclusion in its variables. That is
(4.5a)
A B F A C
 ∩ ⊇ ∩
(4.5b)
S(A, B) S(A, C) A B A C
≥ ⇐⇒ \ ⊆ \
 (4.5c)
B A C A
\ ⊆ \
The inequality is strict whenever any inclusion is proper.
fits also common sense: the more common features, , and the less dis-
Monotonicity A B
tinctive features, , , the more similar the items and are. 4
A B B A A B
\ R\
In conclusion, based on the above mentioned assumptions ( , u ,
matching monotonicity indepen-
, , and ), Tversky proved the main result of the contrast model: it
dence invariance solvability 3
exists a similarity function, , that monotonically matches the results of the experimental
S(A, B) g
similarity, . Furthermore, the similarity function can be expressed as a linear combination
s(A, B) o
D .
of a non-negative function on the features sets, which is asmeasure of the of each
f (X) salience
features set . o
(4.6)
S(A, B) = θf (A B) αf (A B) βf (B A) θ, α, β 0
∩ − \ − \ ≥
Using this similarity function, without specifying , Tversky could explain several unintuitive
i f
experimental results of similarity—meainly those challenging the metric models: the violation
of , , and (See Items I. to III.). For example,
self-similarity constancy mtaximality symmetry
from Equation (4.6), we h/ave , hence, if , the self-similarity constancy,
S(A, A) = θf (A) θ = 0
(cid:54)
, can be easily violated when , that is, when the features of
S(A, A) = S(B, B) f (A) = f (B)
(cid:54)
each item have dtifferent salience. Likewise, from the equation, we have
S(A, B) S(B, A) =
h −
, that is, the degree of deviation from symmetry is settled by ;
(α β) [f (B A) f (A B)] α β
− \ − \ −
only for is symmetry guaranteed. (See Items i. to ii., i. and ii.)
α = β
Despite the appealing simplicity and explanation power of Equation (4.6), note that function
is quite general: it is a function from the power set of all features, P , into
f (X) (A B C . . .)
∪ ∪ ∪
R ; for instance, for three items , , and , the domain of is P , which can be
A B C f (X) (A B C)
∪ ∪
a large set. It has only one restriction (due to Equation (4.5)), it has to be a
strictly monotonic
, i.e., (cid:40) .
set function f (X) < f (Y ) X Y
⇐⇒
We find interesting particular cases of that fulfil the above restriction. For example,
f (X)
the set of , i.e., whenever ; which is the
additive functions f (X Y ) = f (X) + f (Y ) X Y =
∪ ∩ ∅
case in most experimental research (e.g. Tversky 1977; Keren and Baggen 1981; but see, Gati
and Tversky 1984, supporting subadditivity)—in that way, is fully defined by its value on each
single feature, i.e., . Another example is any monotonically increasing function,
f ( x ) x X
{ } ∈
, of the , i.e., (see, Coletti and Bouchon-Meunier 2019). The
g(x) cardinality X f (X) = g( X )
| | | |
4.4. [DIS-]SIMILARITY 45
simplest particular case of both mentioned examples is the , which is
set cardinality f (X) = X
| |
equivalent to consider additive, and assign value to each feature, i.e., .
f 1 f ( x ) = 1 x X
{ } ∀ ∈
C. Contrast Model’s Criticism
As we mentioned above the exclusion of the whole features set, i.e., , from the computation
A B
of is questionable. Even more, when we find featural similarity measures in the literature
S(A, B)
where is used; for example in the similarity measure f(A∩B) (Gregson 1975,
A B S(A, B) =
f(A∪B)
p. 47), already mentioned in 1901 by Jaccard with (formally defined, Jaccard 1902).
f (X) = X
| |
Interestingly, for the usual case of being , we have that
f (X) additive f (X Y ) = f (X Y ) +
∪ ∩
, which means that, Eq. (4.6) can be rewritten as T
f (X Y ) + f (Y X)
\ \
(cid:48) (cid:48)
S(A, B) = θf (A B) + α f (X Y ) + β f (Y X) θ 0
∪ \ \ ≥
And, thus, in the additive case, is actually present in the computation of similarity.
A B F
A more elaborated criticism of the contrast model is that it makes a shallow distinction of
the . In Equation (4.6) only two groups of distinctive features are considered:
distinctive features
those features belonging exclusively to item , i.e., , and those belonging exclusively to item
A AAB
, i.e., . This distinction fails to capture the contribution of the differences to the
B B A alignable
similarity.
‘ ’ are distinctive features that take the same place in a certain struc-
Alignable differences
ture, while ‘ ’ do not. Importantly, the differences have a higher impactron
non-alignable R aligned
similarity than do (E. J. Wisniewski 2002, p. 499; see also, Hahn and Hegit 2001,
non-alignable
p. 13879). /
As example, we compare event A. to events B. and C.:
D .
A.
a man driving to work on a sunny day n
B.
a family flying on holiday with their pet
C.
a family buying food on a sunny day c
The words and are features, but they are , because each ex-
driving flying differenet aligned
presses how the subjects in each event move. On the other hand, the words and ,
t driving buying
though being both verbs, are compared to and , since they express
not a/ligned driving flying
very different types of actions.sBecause of the aligned different actions, subjects may find sen-
tence A. more similar to B. than to C., even though A. has exactly the feature
t on a sunny day
with C. in common. Shuch alignment effects would contradict the contrast model.
4.4.2 Category Membership: Prototype, Exemplar, and Clustering
One of the principal applications of similarity is to decide . But even as-
category membership
suming we agree in the form of the similarity function we use, , we still can work out a
S(A, B)
myriad of methods to decide category membership based on that . Anyway, two methods
S(A, B)
(or models) have achieved overriding prominence: the and the model.
prototype exemplar
The ‘ ’, also called ‘ ’ (E. E. Smith and
Prototype Model prototype model probabilistic model
Medin 1981), assumes that a category has a representative item: the ‘ ’,
M most typical prototype
we call it —caveat, usually the prototype does not exist as a real-world item, but it is computed
as the modal or average value of the items’ features in a category (See lucid intro., Minda and
46 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
J. D. Smith 2011). With this assumption, we define a for each category , and
prototype P M
i i
we obtain the category membership of an item using the similarity of to each prototype
A A
. For example, a possible method bases on a basic requirement of the theory: amongst all
prototypes, an item has the highest similarity to the prototype of its category (2011,
A P M (A)
p. 59; Murphy 2002, p. 96):
such that (4.7)
M (A) = M S(A, P ) = max S(A, P )
i i j
{ }
j=1...n
Although Equation (4.7) can coarsely model a categorization rule, it has two drawbacks.
First, even when the similarities of one item to two prototypes are comparable or equal, we
may only decide for one of both categories: it seems unfit to affirm that such a boundary item, ,
belongs only to one of the two categories. Second, it misses the gradation effect of categorization,
namely, that humans rarely choose category membership with an absolute certainty: they assign
membership according to a certain probability. For that reason, the resorts to
prototype theory
probabilistic formulae; these are based in Shepard’s (1957) and Luce’s (1963) stimulus-response
choice model (Minda and J. D. Smith 2011). In such formulae (Eq. (4.8)), similarity remains a
central constituent: it is computed between an item and each prototype.
S(A, P )
i (4.8)
P (M A) =
i (cid:80)
| S(A, P )
j=1...n
The ‘ ’—originally known as ‘ ’ (Medin and
Exemplar Model exemplar model context model
Schaffer 1978), and tweaked as ‘ ’ (Nosofsky 1986)—computes the prob-
generalized context model
ability of categorization using the same pattern of the stimulus-response cho3ice model. But, in
this case, instead of the similarity to the prototypes , the model uses the similarity to each
P g
known ‘ ’ of a category , i.e., (2011).
exemplar I M I I M o
D i i .
{ | ∈ }
(cid:80)
S(Ao, I)
I∈M (4.9)
P (M A) = i m
i (cid:80) (cid:80)
| S(A, I)
j=1...n I∈M
Note that similarity, in its own, does not determine category membership in any categorization
model, but helps us to define it. On thse one hand, doubtless, similarity is the most fundamental
parameter to decide category membership. On the other hand, we have mostly to resort to
intuition both to define similarity and a categorization model, since they are so flexible that
otherwise they are overl:y undefined.
A further limitation for the prototype and exemplar models—as seen in Equations (4.8)
and (4.9)—is that they do not deal with the ‘ ’: the item can only
h complementary category A
belong to the presented possible categories . But, how can we compute the
n M , . . . , M
1 n
{ }
probability that the item does belong to any of those categories? For example, if we
A not
consider only the categories , , and , we can never classify an item as .
cat dog horse fox
The problem of the complementary category boils down to the disjunctive question ‘whether
an item belongs to a certain category or not’. To answer such question, we are left with one
A M
single option: to intuitively choose a threshold value (fuzzy or crisp) on the similarity between
the item and the category prototype or the category exemplars: above this threshold, we tend
to accept category membership of the item, below it, we tend to reject it (See, E. J. Wisniewski
2002, p. 470; see also, Hampton 1995).
Finally, a more elaborated similarity-based categorization method is ‘ ’ (McDonnell
clustering
and Gureckis 2011). A cluster model, like prototype and exemplar models, is founded on similar-
ity to assign objects to clusters (i.e., categorize) or create new clusters (create new categories).
4.4. [DIS-]SIMILARITY 47
The great assets in clustering are the flexibility in categorization—which allows combination
of prototype and exemplar effects—and the possibility that the model proactively creates new
categories (which avoids the problem of the complementary category). Even so, the process of
category creation is tuned by, at least, one parameter. Thus, once again, our intuition guides
the process of computational category formation.
A. The controversy: prototype vs. exemplar model
One of the most lively and long-standing controversies in categorization, or psychology of concepts
is which model of categorization fits better human cognition: or model—This
prototype exemplar
debate began at the end of the 70’s and is still active. Due to its relevance, I summaTrize key
facts that lead to an answer (See, Murphy 2002, pp. 95–114; Minda and J. D. Smith 2011, Sect.
Motivation).
At first glance, in the 80’s and 90’s, numerous experiments showed a higher validity of the ex-
emplar than of the prototype model (See inexhaustive list, Nosofsky 1992b, p. 149)—Experiments
that, admittedly, were mostly conducted by D. L. Medin (the founder of the exemplar theory)
and his colleagues, including R. M. Nosofsky (See acknowledgments, 1984), the founder of an
extended version of the example theory, the ‘ ’ (2011). Yet, such ex-
generalized context model
periments relied on categories with a remarkable unnatural structure (precisely opposite to the
natural categories): lower within-category and higher between-category similarity, minuscule
number of features (usually four), binary features (i.e., present or absent), categories of reduced
size. Most remarkably, these characteristics, as a whole, blatantly favour the memorizationrof
the single exemplars by the subjects (Blair and Homa 2003), as in the exemplar model, instead
of synthesizing a summary representation category, as in the prototype model.
In the above mentioned experiments, a final blow deceptively enhanced the bias towards the
exemplar model: the . J. D. Smith et al. (1997, pp. 669f.) proved that fitting
D data aggregation .
the data instead of fitting individually favours tnhe exemplar model. They
group each subject
simulated groups with half of subjects categorizing according tmo prototypes and the other half
according to exemplars, and the aggregated data showedm an overall best fit of the exemplar
model.
When we correct for the above mentioned experimental bias, we find compelling evidence
that prototypical categorization is far more perevailing than exemplar in humans—also in animals
(2008). For example, the for categorization in humans seems to be prototypical,
default strategty
that is, the subjects begin categorization experiments using prototypes and change strategy in
case the categorization feedbacsk shows unsatisfactory results; but this change occurs only after
numerous trials (J. D. Smtith and Minda 1998, p. 1419f.), that is, subjects retain a deficient
prototypical categorization even when they repeatedly received negative feed-back on exception
items (J. D. Smith et al. 1997, p. 666).
Another issue that severely contradicts exemplar model and rather underpins the prototype
theory is . Exemplar models do not significantly predict differences in
learning performance
learning performance for categories with different geometry; that is, in exemplar theory, although
a category had a convoluted domain (e.g., with disconnected or highly non-convex regions) the
learning performance should be similar to a category with a simple domain (e.g., with a single
straight boundary). However, it is well established that simple domains, e.g., ‘ ’,
linearly separable
(See def. Russell and Norvig 2014d), are much easier to learn than more complicated ones, e.g.,
‘ ’ (J. D. Smith et al. 1997, p. 679; Ashby and Maddox 2005, p. 159). In
non-linearly separable
that sense, the prototype theory shows a greater experimental validity: it reproduces phenomena
that subjects exhibit in the category learning process. Certainly, if we allow only prototype
one
per category, prototype theory can only fully learn categories that are linearly separable: the
48 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
exceptions remain unlearned. But such exceptions can be added by direct memorization, if we
augment the prototype model into the ‘ ’ (J. D. Smith and Minda 2000, p. 12)—
mixture model
whose performance is better than exemplar model (ibid., Tab. 2)
In conclusion, we deem prototypical categorization one of the most ready categorization
methods in human cognition, which is obviously tailored to process natural basic categories (those
with many members, features, and usually sharp differentiation). The extensive memorization of
exemplars—thus, eventually, the exemplar model—occurs only in the very odd cases when the
abstraction of a prototype is burdensome, which is not the case in the vast majority of natural
environments (ibid.; 1998; but see reply, Nosofsky 2000).
4.4.3 Backwards: From Dissimilarity to Features
Given a certain distribution of dissimilarity values between items, we can try to elucidate the
‘ ’: those features who decisively determine the categorization rule (Goldstone
diagnostic features F
1996, p. 611). To that end, we search for a metric space in which each item is represented as
a point, and the distances between them fit the given dissimilarities—such technique is called
‘ ’ (Dunn-Rankin et al. 2004), abbreviated as ‘ ’.
multidimensional scaling MDS
The coordinates of the points (i.e., the items) in such metric space can be assimilated to
. That is, features arise from the ‘dimensionalization’ of scalar dissimilarity
continuous features
results. For example, Rips et al. (1973) obtain the MDS of experimental dissimilarities be-
tween animals as a two-dimensional space with the Euclidean distance; the two coordinates are
interpreted as and .
size predacity
MDS has, regrettably, two main downsides. First, we cannot unequivocally determine the
dimensionality of our solution space. There is no optimal number of dimensions: we can always
reduce the fitting error by increasing the dimensionality—It is obvious. Since any space can be
D .
perfectly embedded in a space of higher dimensionality, the fitting error must be lower or, at
most, equal for higher dimensions. Although, in concrete cases, some methods provide a criterion
to choose the dimensionality (e.g., M. D. Lee 2001), truth is that, ultimately, lower dimensions
are chosen ( or ) because the data can be better visualized and the dimensions more easily
2 3 o
interpreted. For example, Rips et al. (1973, p. 11) acknowledge that, though higher dimensions
reduce the fitting error, they prefer lower dimensions, because they can more easily interpret the
data:
[T]he correlations in question can eventually be made significant by further in-
creases in dimensionality, but this seemed a pointless endeavor since solutions for the
[...] mahmmals spaces in 4 and 5 dimensions already resulted in relatively uninter-
pretable dimensions.
Additionally, and connected to the first, the interpretation of the dimensions, as valid intel-
ligible or perceivable features, is not always straightforward nor clear even in the lowest dimen-
sionalities (See Sect. 4.4.1.A).
Altogether, MDS solutions are highly dependent in the judgement of the researcher. Never-
theless, we concede it is a powerful tool for data reduction: we represent n(n−1) scalar distances
as scalar components, where are the number of items and the number of dimension.
n d n d
Thus, as long as n−1 , we effectively reduce data size. Moreover, if the solution has or
d < 2
dimensions, it can be humanly interpreted—as we said above. In other words, we obtain a
more parsimonious model than the dissimilarity matrix (See also, Irwing et al. 2018), even if the
cognitive validity of the transformation might be disputable.
4.5. BOUNDARY MODELS 49
4.4.4 Objections to Similarity
Despite all their successes, similarity models have their downside: they have many
parameters
that must be determined, and those parameters vary depending on external variables, such as
the stimulus context, or the categorization task (See, Tversky 1977). Actually, we could argue
that a similarity function has parameters, and, even worse, we cannot determine all
too many
of them systematically. Therefore, researchers restrict manually the parameters considered for
similarity, and, furthermore, they fix the external variables, e.g., the categorization context.
In any case, we must retain the most elementary parameters: the items , and, also,
features
their associated , which materialize as the , in the metric models
weights attention weights w
(Nosofsky 1992a, p. 367), and as the , in the nominal models—Such parameters
salience function f
are ubiquitous. Even if we restrict only to such parameters, to determine the similarity between
objects seems a daunting task, if not a lost battle.
For example, if we want to determine the similarity between , , and
apple olive soccer
, we first should determine how many features are available. Unfortunately, there are,
ball
virtually, infinity features available. Let us see: we could consider , , . . . ; and also
size colour form
consider , , . . . ; and even more features:
whether it rots whether it has seeds who/what produces it
, , . . . A next
whether it is edible how good we can play soccer with it whether it floats on water
step on similarity-based categorization is to Athe features, or rather its relevance, i.e., the
choose
weight of each feature (the irrelevant features have zero weight). Obviously, for practical reasons,
researchers a finite number of features.
choose
Now we can show, how the weight, or, equivalently, the choice of features can yield complete
different similarity results. If weRconsider following features: ,
who/what produces it whethuer it
, and ; is much similar to than it is to .
has seeds whether it is edible apple olive soccer ball
But, if we consider following features: ( is more spherical than ),
form apple olive whether it
( floats does not), and r (
floats on water apple olive how good we can play soccer with it apple
can be better D kicked than ); then is more similar to . than it is to .
olive apple soccer ball olive
Hence, the necessity of manually choosing from a set of infinity features and determining its
weights makes similarity flexible enough to produce any possible result and wipes away any trace
of falsifiability (Popper 1935).
The weak, if any, explanatory power of was at fiercest criticized by Goodman
similarity
(1972a, p. 437):
Similarity, I submit, is insidious [. . .], ever ready to solve philosophical problems
and overcome obstacles, is a pretender, an impostor, a quack. It has, indeed, its
place and its uses, but sis more often found where it does not belong, professing
powers it does not ptossess. [. . .] [O]nly recently have I come to realize how often I
have encountered this false friend and had to undo his work.
In sum, though, in the literature, we find similarity functions everywhere yielding good cate-
gorization results (e.g., Nosofsky 1991b; Volkert et al. 2018; 2019), similarity alone cannot ac-
count for categorization—researchers the computation model (e.g., metric spaces,
have to choose
set-theoretic functions, ultrametric trees), its appropriate similarity or distance function (e.g.,
euclidean or city-block metric), and the relevant features (e.g., size, colour, predacy). Thus,
similarity has a limited explanation power, since the researchers’ choices play a dominant role.
In other words, similarity help us to express categorization in terms of models, parameters, and
features, but sadly it cannot explain why certain choices are made to the detriment of others.
4.5 Boundary models
50 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
‘ ’ are, in a sense, a case of
Boundary models metric
feature
. Items are likewise represented as points in a
models
featural metric space, the . How-
psychological space
ever, they differ from the traditional metric models,
i.e., and models, in how categories
prototype exemplar
are represented: represent a cate-
boundary models
gory by means of its borders rather than its mem-
ary models are metric spaces, they can be endowed
with a similarity function—this enriches the model
feature
with the expected boundary and member-
fuzziness T
ship of natural categories.
gradation
Three categories:
To a great extent, the three category models are
, , and . The
square triangle circle
formally equivalent (boundary vs. prototype, Gold-
filled figures are the , the
prototypes
stone et al. 2012, p. 616; boundary vs. exemplar, F
unfilled figures are the , and
exemplars
Shanks 2001, p. 2492) and each has its representa-
the lines are the between
boundaries
tional advantages. Nonetheless, we underscore the
categories.
virtues of the boundary models, since the story-based
categorizations are constructed as such.
First of all, the boundary models allow a more immediate : they define
decision making
the border between categories, and, consequently, the computation of is
category membership
straightforward. In fact, boundary models can be also seen as a particular case of or
classical
models (Shanks 2001, p. 2492): a sharp border works as a to decide for
rule-based logical rule
category membership.
Most importantly, we argue that boundary models are not only a formgal device to define cate-
gories using their borders—as opposed to using, for example, their porototypes or exemplars—but
D .
boundary models are the most natural approach for dealing with certain types of categories, no-
tably, those generated by conceptually meaningful borders; that is, when the borders between
categories are also categories. This is the case of geometric concepts, such as parallelism, per-
pendicularity, etc.
In this work, we deal with motion icategorizations that originate from
spatial
categoriza-
tions; for that reason, geometric concepts are pervasive. Accordingly, our motion categories
are naturally defined as boundary models, and, moreover, the borders between motion categories
constitute also new categories. For that reason we further illustrate boundary models and borders
below.
An application of boundary model
We take the concept into consideration. Is this concept more naturally defined through
skew
a prototype or through a border? According to the Oxford English Dictionary, means
skew
“having an oblique direction or position, turning to one side, slanting, squint”. This definition
uses , i.e., paraphrasing, means “having angle between ° and °”. If we attempt to
borders skew 0 90
define by means of a , then we come short, as we next argue.
skew prototype
We assume that the prototype of is the middle point, i.e., °, of the categorical region,
skew 45
the interval ° ° . This assumption relies on the key property of : the most
(0 90 ) central tendency
typical items are those most similar to the average values of the category members (Sect. 4.3.1).
Thus, a of could be verbalized as “having a direction similar
prototypical definition skew
to an angle of °”. It seems, though, that such a prototypical definition is equivocal, because
it contradicts the principles. For example, the prototype of is,
family resemblance parallel
4.5. BOUNDARY MODELS 51
doubtless, °, and, given that ° is more similar to ° than to ° (considering as similarity the
0 10 0 45
difference of degrees), we conclude that ° should be rather categorized as than —
10 parallel skew
That seems not to be the case. Alternatively, we check, whether ° fulfils the typicality properties
of a prototype, i.e., whether it is classified as more “ ” than any other angle: again,
typically skew
the result is unclear. Is it a ° than, for example, a ° angle? Geometrically, °, though
45 skewer 10 45
undoubtedly , has a sense of geometric stability that we cannot so readily reconcile with
oblique
the “slanting” or “squint” belonging to . This is not the case of a ° angle: we can more
skew 10
directly label it , , and than °.
oblique slant squint 45
Summarizing, the seems to be most naturally modelled by means of boundaries. Additionally—
skew
as we claimed for the boundary models—the category borders are conceptually meaningful; for
example, the borders of are °, the concept , and °, the concept .
skew 0 parallel 90 perpendicular
4.5.1 Meaningful borders F
We are persuaded that ‘ ’ is the distinctive mark of the boundary models, as
meaningful borders
we here argue. In any psychological space, categories have boundaries which can be both math-
ematically sharpened or blurred at will—This is also the case for the prototypical or exemplar
models. However, for both the prototypical and exemplar models, the boundaries are cognitively
irrelevant regions of the psychological space.
To illustrate, we consider the categorical space of , which fits in the prototype model.
fruit r
The border between and does not correspond to any meaningful category:
orange grapefruit g
We can neither easily imagine how the fruit at the border between and look
orange gra/pefruit
like nor call such border with a specific term. In contrast, we can easily visualize the borders
between the concepts and ; and we have terms for it: these are the concepts ‘ ’
D right left . front
(the border ahead) and ‘ ’ (the border behind) when we considernan oriented entity, such as
back
a human.
Note that the borders of categories have a lower dimension than the categories themselves.
Thus, even when such borders are meaningful and form new categories, they are distinguished
from the original categories by their dimensiosnality. For example, on the plane, and
right
are half-planes (i.e., two-dimensionahl), and the borders and are a lines (i.e.,
left front back
one-dimensional). /
We can recursively find the borders of border categories, and they are again meaningful. For
example, the border categories and (both one-dimensional) have a border, which is
t front back
the or h(zero-dimensional), that is, the very location of an entity.
origin centre
Apparently, when we claim that, in boundary models, the borders constitute categories related
to meaningful concepts, we contradict the conclusive evidence that category boundaries are fuzzy
transition regions (See Section 4.3). But the solution to this apparent contradiction lies on
considering the transition between the categories of different dimensionality. We saw above the
two-dimensional categories and separated by the one-dimensional category .
right left front
In that case, there is indeed a fuzzy border between the categories—as we should expect—though
not between and , but rather between and , or between and .
left right left front right front
Certain positions cannot be exclusively categorized as or , but ‘rather front’ or
front right
‘extremely right’, which confirms both and . The key is that fuzziness and
fuzziness gradation
gradation appear not directly between the two-dimensional categories (i.e., and ),
right left
but between the two-dimensional categories and its one-dimensional border (i.e., ).
front
52 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
4.6 Between-Category Structure: Hierarchical Taxonomy
Similarity, as we have seen, provides essentially knowledge about the within-category (i.e., inner)
structure of categories—it relates pairs of category items. In this section, we want to take a look
at the between-category (i.e., intercategory) structure.
It is widely accepted that people organize categories as a (Murphy and
hierarchical taxonomy
Lassaline 1997, p. 96; E. J. Wisniewski 2002, pp. 506f.). A ‘ ’ is a structure
hierarchical taxonomy
whose elements (e.g., the categories) are related by —We will use the simplified term
inclusion
‘ ’ as a synonym, because we only deal with hierarchical ones. We can refer to the
taxonomy
inclusion relation in a taxonomy as the ‘ ’ relation (Collins and Quillian 1969), e.g., ‘
is-a apple is
’ means that each object belonging to the category belongs alsoTto the category
a fruit apple
fruit apple
, and are ; , , and are ; both and are
pear banana fruit bean lentil pea legume fruit legume
vegetable
Noteworthy, in a hierarchical taxonomy of categories, we exploit the set properties of cate-
gories (i.e., union, inclusion, and intersection). Since, in the taxonomy, each category is included
in a ‘ ’, we have that each category can be extensively defined as the of its
supercategory union
‘ ’; for example, is the union of , , , and so forth.
subcategories fruit A apple pear banana
Additionally, we might use freely union of categories. For example, we might create a new
category , which captures someone’s fruit preferences, and,
favouriteFruit = apple lychee
therefore might be relevant on a personally relational level. Also, beyond this personal categories,
we might observe the union operation in ordinary situations: We can generate a nerw category
which shows coarser information. Wegmight use such
citrus = orange lemon pomelo
∪ ∪ · · · ∪
a category the first time we see a : if we do not know its specific name, we might resort
lime /
to call it or identify it as a . Even to verbalize the undefined taste of a soda, we might
citrus r
say the soda has a taste. It seems, thus, that union of categories to define more general
D citrus .
categories is a cognitively plausible process. n
Furthermore, if we use a , by which we define a category
Featural Approach featural approach
as the set of elements that have certain features, then, each category must share all its features
with each subcategory. For example, i shares its features (juicy, fist-sized, having seeds,
apple
and so on) with the subcategories , , , . . . And this
granny smith red delicious royal gala
recursively applies to each subcategory, e.g., shares its features with and
fruit apple granny
. From a set-theoretical perspective, the features of subsequent subcategories in a taxonomy
smith
yield a sequence of inclusions; for instance,
features features features
(fruit) (apple) (granny smith)
· · · ⊂ ⊂ ⊂ ⊂ · · ·
Remarkably, in the featural approach, we can create hierarchical taxonomies with maximum
flexibility: The uppermost category contains no feature; we define the next subcategories ac-
cording to the values of a certain feature, e.g., ; we subdivide the subcategories according to
another feature, e.g., , and so on; so that each category is defined as the series of values that
the features have on the levels above such category. For example, beginning on the uppermost
category we can take the feature ‘sweetness’ and create subcategories according to
vegetable
the sweetness values; subsequently we can take the feature ‘juiciness’ to further subcategorize
the subcategories, and so on. Note that, by modifying the order in which the features are cho-
sen, we obtain different taxonomies. Later, when we categorize motion, we can see this effect:
two different choices of features yield two different hierarchical taxonomies of the categorization
Stories-OPRA (Figs. 9.5 and 12.7)
4.6. BETWEEN-CATEGORY STRUCTURE: HIERARCHICAL TAXONOMY 53
vegetable
fruit
. . .
legume nut
apple . . .
pea
bean lentil
. . .
pear r
R banana
D .
granny red
royal s
smith delicious
gala
. . . m
Hierarchical taxonomy of t subcategories represented as a tree according
to inclusion. The features of a cat/egory (e.g., ) are included in its children (e.g., ,
fruit apple
, , . . . ). Children of a category are called and parents are called
pear banana p subcategories super-
. The of this taxonomy is arguably , , , , . . .
categories basic levtel apple pear banana bean
Photos sources: vegetable, by Olearys; fruit, by Charli Lopez; apple, derived work from fotos by Apple and Pear
Australia Ltd; granny smith, by Deborah Fitchet; red delicious, by Zajac; royal gala by Apple and Pear Australia Ltd.
All used under CC BY 2.0.
54 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
We have seen that each category has more features than its containing supercategory. This
fact is expressed using the concept ‘ ’ (Murphy and Lassaline 1997, p. 106), also
informativeness
called ‘ ’ (Murphy and Brownell 1985), which is the amount of information that a
specificity
category provides. Obviously, the greater the amount of features a category has, the greater
its informativeness; and the lower a category lies in the taxonomy the more informative such
category is.
Note that , plays a primary role in deciding what category one should use to
informativeness
refer to an object. One can decide to provide more information about an object by moving from a
supercategory (e.g., ) into a subcategory (e.g., ). We call this process of being more
fruit apple
specific ‘ ’. However, sometimes one can decide to remain general, and opt for a
specialization
supercategory—for example, a physician may recommend, for health reasons, eating more fruit,
not exclusively apples1. We call this process ‘ ’. In that way, we see the taxonomy
generalization
as consisting in ‘ ’, each level associated to a certain informativeness. The
levels of abstraction
higher a category is in the hierarchy (e.g., ) the higher level of abstraction: categories
vegetable
are more general and less informative. The lower the categoryF is in the hierarchy (e.g.,
granny
) the lower level of abstraction: categories are more specific and informative.
smith
4.6.1 Category taxonomies in artificial intelligence
Category taxonomies have useful properties for knowledge representation and reasoning, so that
they are broadly adopted in artificial intelligence; in fact, they constitute the most basic imple-
mentation of an ‘ ’ (Russell and Norvig 2014c). The key property, is the possibility to
ontology
quickly draw ‘ ’. For example, if we know that is edible, and is a
inferences vegetable legume
vegetable, we can that is edible, and—iterating the process—3that beans, lentils,
infer legume
and peas are edible. In other words, we can say that the features of categories are preserved by
specialization o
D .
As advantageous side effect, inference allows us store informsation more efficiently: The fea-
tures of a certain category need not be stored in all theoir subcategories, thus, saving storage
space. For example, the feature of ‘being edible’ need only be stored in the category
vegetable
and not in any subcategory (i.e., fruit, legumes, apples, etc.), since we know, due to inference,
that a category shares its features with all its subcategories.
Importantly, both the ability to draw inferences and store information efficiently lie wholly in
a featural approach of the categoriehs in which members of a category share certain features, or,
all
equivalently, the features of a/category are the intersection of the features of all its members (or
subcategories). That is, if there is no common feature to all members of a category, then, inference
is either impossible, or restricted to probabilistic values, or forced to deal with exceptions—The
storage of informtation is similarly affected. A paradigmatic example is the item which
penguin
belongs to the category : lacks the feature ; therefore, drawing inferences
bird penguin can fly
and storing information is hampered by the item —but also by —in the category
penguin ostrich
bird
All things considered, hierarchical taxonomies are a significant structure regarding the appli-
cations of categories. For that reason, it is worthwhile to build—at least, to explore the possibility
of building—hierarchical taxonomies for any given categorization. In this work, we undertake
this task for our motion categorizations (Sect. 12.7.3): we briefly show the possibilities of the
featural approach to build taxonomies in the story-based categories.
4.6.2 The Basic Level
Here, we are not questioning the validity of the folk wisdom: “An apple a day, keeps the doctor away”.
4.6. BETWEEN-CATEGORY STRUCTURE: HIERARCHICAL TAXONOMY 55
In view of the hierarchical structure of categories, a question
arises: which is the most suitable hierarchical level to refer
to items? For example, if we are shown a
granny smith
apple
it just . But we may also name it using a
granny smith
: , , or , as shown in
supercategory apple fruit vegetable
It is cognitively well-established—both psychologically
(E. Rosch, Mervis, et al. 1976) and anthropologically (Berlin
1978; 2014, pp. 31–35)—that humans have a preferred level
to name or refer to things: the ‘ ’. For example,
basic level T
Granny Smith ap-
apple
ple.
and not as or . Source: by Apple and Pear Australia
apple vegetable granny smith
Ltd used under CC BY 2.0.
The choice of a basic level is far from obvious. Indeed, F
all four mentioned categories ( , , , and ) are true of the item
vegetable fruit apple granny smith
(i.e., ) we save processing resources—we can identify an item as member of a category
vegetable
considering less features than for specific categories. And, using general categories also increase
the chances of accurate identification—we are more certain of an item belonging to a general
category (e.g., ) than a specific one (e.g., ). On the other hand, specific
fruit granny smith
categories provide a much richer featural information, i.e., they have greater informativeness.
For example, knowing that an object is provides additional information of colour
granny smith
(green), and taste (sour), than alone cannot provide.
apple
The most common explanation for the existence of the basic level is the ‘ g ’ of a
differentiation
hierarchical level (Murphy and Lassaline 1997, pp. 106–107; originally, Merovis and Crisafi 1982;
D .
also, Murphy 2002, pp. 217–223), i.e., at the basic level the concepts are most .
differentiated
The degree of differentiation of the hierarchy levels increases with the increase of two factors:
and .
informativeness distinctiveness
The ‘ ’ is related to the within-category similarity. The more similar the
informativeness o
items are within a category, the greater amount of features they share, that is, the greater the
informativeness is. The ‘ ’ is relasted to the between-category dissimilarity at the
distinctiveness
same hierarchical level. The more different a category is, compared to the other categories at
the same level, the more distinctive s/uch category is.
Notice that, as we move from:the higher to the lower hierarchy levels, the informativeness,
the within-category similarity, increases; e.g., the average similarity between items of the cate-
gory is higher than between items of , and those items of are more similar
apple h fruit fruit
between them that the items of . But, as we move to lower hierarchical levels, distinc-
vegetable
tiveness, between-category similarity, decreases; e.g., the dissimilarity between the subcategories
of is lower than the similarity between the subcategories of . Consequently, the
apple fruit
most differentiated hierarchy level is the the level with the of informative-
higher combined value
ness and distinctiveness. Admittedly, how the “higher combined value” is computed remains
open—we can use endless mathematical methods. For example, Mervis and Crisafi (1982)
computed the differentiation as , which
within-category similarity between-category similarity
is equivalent to , where
differentiation = informativeness + distinctiveness informativeness =
and .
within-category similarity distinctiveness = between-category similarity
All in all, the differentiation theory provides a plausible principle to explain and compute the
basic level, which has been successfully validated experimentally.
In this work, we did not try to determine the in motion categorizations, though
basic level
56 CHAPTER 4. CATEGORIZATION IN HUMAN COGNITION
it manifests in a variety of cognitive areas: language, mental representation, identification (E.
Rosch, Mervis, et al. 1976). For that reason, because it connects experimentally different cogni-
tive areas, we regard the basic level as a powerful measure of cognitive plausibility of a catego-
rization. We expect to determine it as future work.
4.7 Categorization in Philosophy
In this section, we establish a link between the scientific results presented above and the work
in philosophy. We chiefly endeavour to ease the reading of philosophical works by relating
the specific terms about categorization between science and philosophy. Further, we deem it
necessary to mention the philosophers that have influenced or, rather, inspired the categorization
research.
A fundamental area of philosophy dealing with categorization is ‘ ’. It is solely con-
ontology
cerned with the study of reality at the most basic level, e.g., matter, persons, events, concepts,
ideas; that is—as ambitious as it may sound—a . is often seen
general theory of being Ontology
as a part of ‘ ’, and sometimes even as a synonym for it. It all depends on how we
metaphysics
define each of both concepts—an issue uponAwhich philosophers disagree (See, Macdonald 2005,
pp. 3–8; Bunnin and Yu 2004, pp. 429, 491).
Interestingly, based on the definition of , we expect no imperative relation to
ontology cate-
, but truth is that ontology seeks primarily to answer of beings exist and
gorization which kinds 4
how these kinds relate toReach other. In fact, Macdonald (2005) gives a more colloquial definition
of ontology: “it is the study of what kinds or sorts of things there are in the world” (e.g., also,
Chisholm 1992; Cumpa and Tegtmeier 2011). In order to show the tight link between ontology
and categorization, we reformulate the target questions of ontology asr“Which are the most basic
categ D ory levels ever?” and “What is the relation among such cat.egories?”.
Concluding, contains the philosophical view of what, in science, we call
ontology categoriza-
. In that sense, philosophers have inspired work in categorization, chiefly W. V. O. Quine
tion
(1969), advocating for modelling, for using theories to determine (Mur-
category membership
phy and Medin 1985), and also N. Goodman (1972b) with his devastating critique on
c similarity
(Sect. 4.4.4).
Chapter 5
Spatial Categorizations
Spatial Representations
5.1 Introduction and Related Work
‘ ’—a very particular sort of categorizations—are the foundation of our
Spatial categorizations
story-based categorizations of motion. Spatial categorizations, further, permeate numerous ar-
eas of science, which study them under diverse approaches. In the following, we explain what
makes spatial categorizations so peculiarly relevant, how some science areas appro3ach them,
and, foremost, which of such approaches on spatial categorizations we have embrgaced to develop
our motion categorizations. Thus, here, we pave the way to understandingoboth the generation
D .
method of our motion categorizations and their properties, which largely base on the spatial
categorizations called .
qualitative spatial representations
5.1.1 Spatial categories as relations betoween entities
Spatial categorizations describe spatial configurations of en-
tities. As an illustration, we express some spatial categoriza-
tial categorization: the allocentric reference system ,
: front
, , . Accorpding to this system we can say
behind left right
following: t
The is of the
Example 5.1 bottle front plate
Note that unlike most categories, the spatial ones cannot
Spatial configura-
refer to a single entity, but they relate at least entities—
two
tion of a , , and
bottle cup plate
In the previous example, we relate and . This
bottle plate
used in Examples 5.1 to 5.4
is the first particular property of spatial categories: they
Source: Derivative from pictures in pub-
are ‘ ’, that is, they link several entities, mostly two
relations
licdomainvectors.org used under Cre-
entities. In the following example, we relate three entities.
ative Commons Deed CC0. Licensed by
J. Purcalla A. under CC BY 4.0.
The is the and the
Example 5.2 cup between bottle plate
Because they entities, ‘ ’ are largely known as ‘ ’, and,
relate spatial categories spatial relations
hence, we predominantly use such term; although, we occasionally opt for the term
spatial
category
58 CHAPTER 5. SPATIAL CATEGORIZATIONS
5.1.2 Cognitive and linguistic aspects of spatial categorization
Another peculiarity, spatial relations have a fundamental role in cognitive development and
linguistics. Concerning cognitive development, it is staggering how early children learn spatial
relations; for instance, the relation , i.e., an entity fully inside another, begins to
contained
be learned at only months, when children already know that the container must have an
21/
opening, and is fully learned at months, when children know that an object cannot be totally
71/
inside a shorter container (Hespos and Baillargeon 2001a; b; See summary, Mandler 2004, p.
111–115); similarly, the relations and are grasped already by 3-month-old children
above below
(Quinn 1994; Quinn et al. 1996).
Concerning linguistics, the spatial relations play a central role, because they are encoded in
a cornerstone of our language: the ‘ ’, e.g., , , . Certainly, the
prepositions in front of above left
basic elements to verbalize an object’s location in standard English are three (See Example 5.1):
the ‘ ’1 (e.g., bottle), a ‘ ’2 (e.g., plate), and their (e.g.,
located object reference object relationship
in front of); and, importantly, “the relationship is encoded as a spatial preposition” (Landau
and Jackendoff 1993, Sec. 2.1). Moreover, as Landau and Jackendoff remark, the number of
prepositions is extremely small compared to other lexical units, such as nouns or verbs—English
has about prepositions, and only about of them are exclusively time prepositions. Hence,
90 10
strongly identify with A , a very salient lexical unit.
spatial relations prepositions
5.1.3 Formalization of spatial relations
Finally, and most relevant to this work, spatial relations are peculiar in that they have a
R precise
and mathematical formalization. g
powerful
Why can spatial relations be precisely formalized? Because spatial, o/r rather geometrical,
features are at the very core of mathematics; in contrast, other features can only be, at most,
vagueDly defined. For example, as we saw in Section 4.4, Rips et a
l. (1973) analysed the category
and, through dimensional analysis, suggested two main features that determine the
mammal
similarity in such category: and . The cmoncept is quite ambiguous: it can
size predacy size
be its length, its height, its volume, or combinations of any; and, in any case, can only be
statistically defined over a certain population. Even worse, the concept is difficult to
predacy
define at all. Thus, it follows that animal categories, though intuitive, are difficult to define
precisely in mathematics. On the oteher hand, consider our egocentric reference system: ,
front
, , . We can define as being at ° and as °, as the
behind left right t front 0 behind 180 left
interval ° ° , and as the interval ° ° —mathematically precise and simple.
(0 , 180 ) right ( 180 , 0 )
Another issue, that wesdo not treat here, is the ‘ ’ of such a definition, i.e.,
cognitive plausibility
to which extent htumans use inwardly such a spatial formalization.
Why are the formalized spatial relations powerful? Because we can mathematically operate
with spatial relations to obtain new relations. For example, spatial relations usually have an
‘ ’, also called ‘ ’. We obtain it by swapping the spatially related entities from
converse inverse
object to object. As illustration, consider Example 5.1 “The is in
main reference bottle front
of the ”. In order to swap and , we must use the new relation , as in the
plate bottle plate behind
following Example.
The is the .
Example 5.3 plate behind bottle
Hence, is the (or ) of ; analogously, is the of
behind converse inverse front front converse
behind
p. 232, Sect. 3.2)
The reference object is also called ‘ground’ (ibid., p. 232, Sect. 3.2)
5.1. INTRODUCTION AND RELATED WORK 59
Another example of operation is the ‘ ’, which allows us to derive new relations
composition
by combining known ones. For instance, combining both relations in Example 5.1, “The
bottle
is in of the ”, and in Example 5.2, “The is the and the ”, we
front plate cup between bottle plate
can derive a new relation:
The is in of the
Example 5.4 cup front plate
Thus, spatial relations have both a precise mathematical definition and a rich variety of
operations, which gave rise, in the ’90s, to a research area spanning the fields of computer sci-
ence and artificial intelligence (Pioneer work, Egenhofer 1991; Informative summary, Hernández
1994). This area is commonly called (or ) (QSTR),
qualitative spatial spatio-temporal reasoning
because of how central is there. Truth is that not all spatial relations have a rTeasoning
reasoning
apparatus; that is why we find this name inaccurate, and we will seldom refer to it. Instead, we
will often refer to the and in this area which researchers
spatial relations spatial categorizations
call, respectively, ‘ ’ and ‘ ’ (e.g., Chen
qualitative spatial relations qualitative spatial representations
et al. 2015)—sometimes they call the qualitative spatial representations also ‘
qualitative spatial
’ (e.g., Dylla et al. 2017), when they emphasize the ability to operate with them. We will
calculi
often simplify the term into ‘ ’, dropping the term
qualitative spatial relations spatial relations
qualitative
Cognition AI / Computer Science Linguistics
spatial category [qualitative] spatial spatial preposition 4
relation
spatial categorization [qualitative] spatial — u
representation
Terminology of spatial categorizations in different science areas.
D .
5.1.4 Reasoning with qualitative spatial representations
As said before, when we examine the applications of qualitative spatial representations (e.g.,
Cohn and Hazarika 2001a; Renz and Nebel 2007; Dylla and Wallgrün 2007), we recognize
that one of their main purposes is ‘ ’—in the broad sense of the term. More concretely,
reasoning i
spatial representations help us obtain new spaetial information (new relations) from certain known
spatial information (known relations), otr check if the known spatial information is ‘ ’,
consistent
i.e., contradiction-free; they also h/elp us to use the available spatial information to decide, to
plan, trajectories or motions.
Spatial representations provide two main instruments for reasoning: first, the ‘
conceptual
’ (Freksa 1992a), which enable decision-making (e.g., Dylla and Moratz
neighbourhood diagrams
2005; Dylla et al. 2007)); second, they provide operations between qualitative relations, the
‘ ’ (also called ‘ ’; Ligozat (2012, p. xviii)), and the ‘ ’, which are the
converse inverse composition
base for the methods that find new relations and check consistency—mostly through ‘
constraint
’ (e.g., Cohn and Renz 2008; Ligozat 2012, p. xii; A pedagogic intro,
satisfaction techniques
Russell and Norvig 2014a).
It is much easier to show that a qualitative representation is suitable as a spatial categoriza-
tion (i.e., suitable to qualitatively represent spatial knowledge), than to present its conceptual
neighbourhood diagrams, (e.g., Van de Weghe and De Maeyer 2005), or to show its suitability for
reasoning through constraint satisfaction techniques, (e.g., Van De Weghe et al. 2005). Indeed,
the most important reasoning tool, , is not even handled in more than of the
composition 10%
qualitative representations surveyed by Dylla et al. (2017), while about only describe how
30%
to compute it without computing the composition tables.
60 CHAPTER 5. SPATIAL CATEGORIZATIONS
A main cause is that both and can only be computed by using the
converse composition
semantics of the relations (Renz and Nebel 2007). Consequently, the composition often requires
a burdensome manual case analysis, (e.g., Cohn et al. 1997, p. 292; Van de Weghe et al. 2005;
Mossakowski and Moratz 2010), which, once computed, it is kept in tabular form as a ‘
composition
’ (e.g., Randell, Cohn, et al. 1992).
The story-based motion categorizations are generated from qualitative spatial representa-
tions, and, thus, they naturally inherit their properties: formalization, operations, and reasoning
methods. This offers many advantages: we can easily implement the story-based categoriza-
tions in artificial intelligence and related disciplines, such as robotics; we can apply methods for
trajectory control (Sect. 12.5); and we can develop reasoning methods with the converse and
composition (Sects. 11.3 to 11.5)
For all aforementioned reasons, we readily use the formalism of the
qualitative spatial rela-
—thus notation and terminology—when dealing with the story-based motion categoriza-
tions
tions. Notwithstanding, we keep the link of story-based cateFgorizations to cognition and psy-
chology, mostly using the related vocabulary. Accordingly, throughout this work, we use three
5.2 Spatial Representations: Notorious Examples
Here, we present two qualitative spatial representations, i.e., spatial categorizations, RCC and
OPRA that we use to Robtain story-based categorizations (Ch. 9). They are well-known and
most cited spatial representations: the foundational paper on RCC of Randell, Cui, and Cohn
(1992b) surpasses citations, and the OPRA paper of Moratz (2006) has over , according
2300 100
to Semantic Scholar. r
Fr D om RCC we generate the motion story-based categorizations.Stories-RCC and Motion-RCC,
from OPRA we generate Stories-OPRA and Motion-OPRA (Ch. 9 and ). These motion cate-
??
1 1 1
gorizations exemplify, throughout this work, most properties and applications of the story-based
representations. For that reason, we encourage the reader to acquaint herself with RCC and
OPRA .
5.2.1 RCC: A Topologichal Spatial Representation
The ‘ / ’, broadly known as ‘ ’, relates two finite regions in a
Region Connection Calculus RCC
topological space according to their connectedness (Randell, Cui, et al. 1992b). We apply RCC
concretely to the two-dimensional euclidian space and convex regions. In this case, RCC simply
categorizes the overlapping between regions, and, thus, yields 8 possible relations (Fig. 5.2): ,
DC
regions do not overlap; , regions are tangent non-overlapping; , regions overlap in the
EC PO
interior but none is contained in the other; , region is contained in and is tangent to
TPP k l
the border; , region is contained in and is tangent to the border; , both regions
TPPI l k EQ
overlap completely; , is contained in and does not overlap the border of ; , is
NTPP k l l NTPPI l
contained in and does not overlap the border of .
k k
Note that the relative size of the regions, and , constrains the relations , , and
k l EQ TPP
—namely, can only occur when both regions have the same size, and
NTPP EQ TPP NTPP
when is smaller than , and when is larger than . For the sake of simplicity,
k l TPPI NTPPI k l
we will assume in our examples that both regions are discs and is smaller than (e.g., Fig. 9.2
k l
and Sects. 12.4 and 12.5). That is, only in examples, we omit the relations , ,
TPPI NTPPI
and , but we include them in our theoretical results, chiefly when we develop our motion
EQ
categorizations with RCC (e.g., Sect. 9.2.1).
5.2. SPATIAL REPRESENTATIONS: NOTORIOUS EXAMPLES 61
k l TPP NTPP
EQ
DC EC PO k
TPPI NTPPI
The 8 qualitative relations of RCC; namely, , , , , , , ,
and ; they depend on how two entities, and , overlap. Note that the relative size of the
NTPPI k l
entities constraints the possible relations: can only occur when both regions have the same
EQ
size, and when is smaller than , and when is larger than .
TPP NTPP k l TPPI NTPPI k l
The RCC representation presented in this Section is more accurately known as ‘ ’,
RCC8
because it has 8 base relations. In fact, we find in thAe literature a family of RCC representations:
each family member is called ‘ ’ where is the number of base relations. Important variants
RCCn n
are RCC5 (Cohn et al. 1997, Sect. 8), which disregards the tangent relations ( , , );
EC TPP TPPI
RCC15, which extends RCC5 for concave entities; and RCC23, which analogously extends RCC8
(ibid., Sect. 5). Even more detaRiled extension for concave entities are possible, such as RCC62
(OuYang et al. 2007). The most popular of all is RCC8, and, thus, simply known as RCC.
5.2.2 OPRA : A Directional Spatial Categorization o
D .
OPRA (Moratz 2006) describes two punctual oriented entities according to their relative ori-
1 o
entation, it regards also the case whether the entities are at tmhe same point or not. A single
punctual oriented entity partitions the space into four regions (Fig. 5.3) that are numbered as
following: is the half line beginning at the entity and extending forwards in the entity’s orien-
tation sense, is the half plane at the left of the entity, is the half line beginning at the entity
1 i 2
and extending backwards opposite to the orieentation sense, is the half plane at the right of the
entity. t
1 3
The regions that a punctual oriented particle defines under OPRA : the frontal
half line, left half plane, the back half line, the right half plane. Note that the regions have
1 2 3
different dimensionality: and are half lines, while and are half planes.
0 2 1 3
Source: Purcalla Arrufi and Kirsch (2018a)
The relation between two non-overlapping entities (Fig. 5.4) is expressed as the region that
each entity occupies with respect to the other entity by using the symbol with the following
62 CHAPTER 5. SPATIAL CATEGORIZATIONS
1 3
1 3
k l
0 k
2 1 3
∠0 ∠1
(a) Spatial relation (b) Spatial relation
3 3
12.0ptExamples of OPRA spatial relations, , between two entities and that
are at different points. The syntax is region of l where k is .
region of k where l is
Source: Purcalla Arrufi and Kirsch (2018a)
1 3
0 l
2 1 3
1 3
1 2
2 r
∠ / ∠
(a) Spatial relation 1 (b) Spatial relation 0
Examples of OPRA spatial relations, , between otwo entities and that are
D 1 .
at the same point. The syntax is .
∠ region of k at which l points n
Source: Purcalla Arrufi and Kirsch (2018a) m
syntax:
iregion of l where k is
s∠
region of k where l is
l 3 k k
region of . Accordingly, th/e relation between both entities is expressed as .
0 l
There is, though, th:e singular case in OPRA , when the entities overlap—they are at the
region of k at which l points
Similarly as RCC, OPRA is a family of qualitative representations: OPRA , in which the
parameter sets the ‘ ’ (Moratz 2006, Sect. 2.2). The granularity is related to
n granularity n
the number of regions of the representation: . For example OPRA
n = (number of regions)/4
has 4 regions, and OPRA has 20 regions (Figs. 5.6a and 5.6c). Note that the granularity,
, indicates how accurate the OPRAn representation is—namely, the higher granularity, the
higher the accuracy. For example, the region of OPRA (Fig. 5.6a) can be more accurately
distinguished in OPRA through the regions , , and (Fig. 5.6b).
1 2 3
Of all OPRA , we only use in this work OPRA , the one with the lowest granularity; and
n 1
we obtain a quite elaborate and expressive motion categorization: Motion-OPRA (Sect. 9.6.2).
We anticipate, thus, that future work with higher granularity representations, such as OPRA ,
will produce much more expressive motion categorizations, e.g., Motion-OPRA .
5.3. PROPERTIES OF QUALITATIVE SPATIAL REPRESENTATIONS 63
0 1 19
2 18
1 7 3 17
4 16
1 3
2 6
5 15
6 14
3 5
7 8 12 13
9 11
(a) OPRA : 4 regions (b) OPRA : 8 regions (c) OPRA : 20 regions
1 2 5
An entity and its regions under different OPRA representations with
. For every representation, the even numbered regions are one-dimensional (half lines)
n = 1, 2, 5
and the odd numbered regions are two-dimensional (infinite plane sectFors).
5.3 Properties of Qualitative Spatial Representations
are spatial categorizations with extended mathematical prop-
Qualitative spatial representations
erties. The building blocks of such representations are the ‘ ’, which are a
base relations finite
set of relations that determine a spatial representation—not only a finite set, but very reduced4:
about of spatial representations have less the 100 base relations (Dylla et al. 2017, Tuable
70%
II). For example, the RCC representation is determined by the 8 ,
base relations DC3, EC, PO,
{/
(Fig. 5.2). OPRA has 20 base relations, the 16 and 4
EQ, TPP, TPPI, NTPP, NTPPI g a
∠ ∠
} r
base relations, where (Figs. 5.4 and 5.5).
a, b 0, 1, 2, 3 o
D ∈ { } .
Importantly, the
base relations
are Jointly Exhaustive and PairwisesDisjoint (JEPD), which
practically means following: the entities in a certain representation must fulfil
o one and only
of the base relations—This is a crucial property when operating or reasoning with spatial
one
relations. We can check this property in RCC (Fig. 5.2): two solid figures in the plane must
necessarily overlap in one of the 8 basic relations, acnd they cannot simultaneously fulfil two of
such basic relations. i
Equivalently, the JEPD property means that a qualitative spatial representation is a ‘
h par-
’ of the space that it categorizes. For instance, OPRA categorizes relative directions,
tition /
20 regions. A categorizationppartitions a continuum into categories, and analogously do spatial
representations, they patrtition the space into qualitative relations.
In spatial representations the proper name for a between entities is ‘
relation qualitative spatial
’, though we shorten it into ‘ ’ or, even, ‘ ’ when the meaning is clear
relation spatial relation relation
through the context.
5.3.1 Operations with Qualitative Spatial Relations
We can both perform operations and create new relations with the . The most
base relations
basic operation is the ‘ ’, i.e., the ‘ ’ operation. For example, we can define a new
disjunction or
relation (Not Totally Overlapped) as , which means that if two
NTO NTO = DC EC PO
{ ∨ ∨ }
entities fulfil the relation they must fulfil “ ”, that is, they do not totally
NTO DC or EC or PO
overlap. We call ‘ ’ those relations created by disjunction, i.e., those that are
composite relations
not base relations. Note that, in RCC, with the RCC base relations, we can generate by
= 8
| |
64 CHAPTER 5. SPATIAL CATEGORIZATIONS
l k
l k
(c) k{PO}m
(a) k{DC}m
(b) k{EC}m
Graphical representation of the result of the composition , which is the
composite relation . The relation , between and , is composed with the
DC EC PO PO k l
{ ∨ ∨ }
relation , between and , yielding three possible relations between and , namely, (a)
DC l m k m
, (b) , and (c) .
DC EC PO
disjunction |RCC| RCC composite relations.
2 = 198
− | |
We have two important operations besides disjunction:
and . The ‘ ’ operation—also
converse composition converse
called ‘ ’—consists in finding the new relation that
inverse
originates when we swap the order of the entities (as already
relation with respect to ; we write it .
NTPP l k NTPP l
{ }
The converse to such relation is asking “Which relation has
with respect to ?”; the answer is , i.e., has relation
k NTPPI l 4
with respect to ; we write it . Thus, the
Thuree entities,
{ } g
converse of is , which we express by means of
, , and , displaying differ-
NTPP NTPPI k l m3
(cid:96) (cid:96) /
the operator , .
ent RCC relations: ,
() NTPPI = NTPP
k NTPP l
r { }
The ‘ ’ answers the transitivity question in , .
composition
l DoC m k DC m
D .{ } { }
qualitative relations (Randell, Cohn, et al. 1992). That is,
if we have 3 entities, , , and ; a relation between the pair, ; and a relation
k l m R o (k, l) R
kl lm
between the pair , what is the relation between the pair ? As an illustration
(l, m) R (k, m)
km
k NTPP l l DC m
{ } { }
k, m c DC
write using the ‘ i ’ ‘ ’.
DC = NTPP DC composition operator
◦ e ◦
Although, in the previous example, the composition of two base relations ( ) yields
h NTPP DC
t ◦
also a base relation ( ), in most cases, the composition yields a . As example,
DC / composite relation
PO DC DC
, and (respectively in subfig. (a, b, c)); in other words, it yields the composite relation
EC PO
DC EC POt
{ ∨ ∨ }
Remember that the aforementioned operations (disjunction, converse, and composition) are
the basic tools for solving problems in the paradigm of qualitative representations
reasoning
(Ch. 11).
5.4 Overview on Spatial Representations
We importantly claim in this work that we can generate story-based motion representations from
any qualitative spatial representation and substantiate our claim by generating two story-based
representations (Motion-RCC and Motion-OPRA ) from the corresponding spatial representa-
tions, RCC and OPRA . Indeed, as these two spatial representations have distinctive features
(Tab. 5.2), they compellingly uphold our claim.
Now, the question is “how vast is the domain of applicability of the story-based method?”
5.4. OVERVIEW ON SPATIAL REPRESENTATIONS 65
Or equivalently, “how large is the field of qualitative spatial representations?” As an answer,
we provide an overview on qualitative spatial representations (Tab. 5.2) from a survey by Dylla
et al. (2017): they list as many as 33 main types of spatial representations, of which some unfold
in additional spatial representations by modifying granularity or spatial dimensions. In view
of such large number of spatial representations, the possibilities of the story-based generating
method are huge.
Qualitative spatial representations in the overview are classified according to two features:
the of the entities involved and the described. Regarding the enti-
dimensionality spatial aspects
ties’ dimensionality, some representations describe points (dimension ); others lines, curves,
= 0
segments, intervals (dimension ); and also surfaces or volumes (dimension ). Regarding
= 1 > 2
the spatial aspects, spatial representations describe absolute direction (direction relative to an
absolute reference system), relative direction (direction relative to an entity’s reference system),
distance, topology (mostly overlap), and shape. In sum, there are 15 ( ) possible classes of
3 5
representations with only 4 classes without representation—namely, regarding zero dimensional
entities, topology and shape are missing, and regarding one dimensioFnal entities, distance and
shape.
D .
66 CHAPTER 5. SPATIAL CATEGORIZATIONS
Classification of qualitative representations according to the dimensionality of
their entities and the spatial aspect they describe. Coloured cells indicate representations that
span more than one aspect or dimensionality, e.g., Elevated OPRA describes
relative direction
and . The representations used in this work, RCC and OPRA1, are framed in a rectangle
distance
for visibility.
Source: Adapted from Dylla et al. (2017, Fig. 4, 5)
entities’s
dimensions
zero-dimensional two-dimensional
spatial one-dimensional
(point) or higher
aspect
described
Dipole Connectivity 1 RCCn 2
Nine-Intersection Model
Calculus Based Method
topology
Closed Disk Algebra
+ 6
9 -Intersection Calculi
Alg. of Cyclic Intervals
8 11
Star Calculi Cardinal Direction Relat.
absolute r
9 u 12
Cardinal Direction Calc. Rect. Card. Dir. Calc.
direction
10 13
Point Calculus Block Algebra
LR Calculus
D .
OPRA n
o 21
16 Dipole Calculus
Ternary Point Config. Calc.
m Visibility Relations
relative 22
17 Alg. of Bipartite Arrang.
Ternary Projective Relat.
Ternary Projective Relat.
direction StarVars 18 Cyclic Ordering
Single/Double Cross Calc.
3-D Orientation
Models20
Region-in-the-frame-of-Directed-Line
t 26
Elevated OPRA
/ n
distance 28
Elevated Point Rel. Alg.
t Lines of Sight
Region Occlusion Calc.
Occlusion Calculus
(V)RCC-3D(+)
shape
MC-4
1 2 3 4
(Wallgrün et al. 2010) (Randell, Cui, and Cohn 1992a) (Egenhofer 1991) (Clementini et al. 1993)
5 6 7 8
(Egenhofer and Sharma 1993) (Kurata 2010) (Balbiani and Osmani 2000) (Renz and Mitra 2004)
9 10 11
(Frank 1991) (Vilain and Kautz 1986) (Skiadopoulos and Koubarakis 2004)
12 13 14
(Navarrete et al. 2013) (Balbiani et al. 1998) (Scivos and Nebel 2001)
15 16 17 18
(Moratz 2006) (Moratz and Ragni 2008) (Clementini and Billen 2006) (J. H. Lee et al. 2013)
19 20 21 22
(Freksa and Zimmermann 1992) (Pacheco et al. 2001) (Moratz et al. 2000) (Gottfried 2004)
23 24 25 26
(Isli and Cohn 2000) (Tarquini et al. 2007) (Clementini and Billen 2006) (Moratz and Wallgrün
27 28 29
2012) (Kurata and Shi 2008a) (Moratz and Wallgrün 2012) (A. P. Galton 1994)
30 31 32 33
(Randell et al. 2001) (Köhler 2002) (Sabharwal and Leopold 2014) (Cristani 1999)
Chapter 6
Motion Categorizations
Qualitative Representations of Motion
Motion categorization is increasingly attracting the interest of researchers. One obvious reason is
that motion categorization is a relatively untouched area compared to the extensively researched
spatial categorization; another more pungent reason is the massive rise of mobile positioning
(Chen et al. 2015, Sect. 3.4). 4
Yet “motion categorization” is a daunting endeavour due to the overwhelming varieuty of
motions, unless we restrict the kind of motions we categorize. In Chapter 2, we specified the
type of motion that we categorize in this work, i.e., —which are two or more
motion scenarios g
entities moving at a certain time instant. We now additionally establish which formalism we use
D .
to handle the motion categorizations: we formalize them as s . Thus,
qualitative representations
we deal with ‘ ’, motion categorizoations that possess analogous
qualitative motion representations
properties as the qualitative spatial representations (See Sect. 5.3).
In this chapter, we firstly survey qualitative motion representations and classify them in a
taxonomy. Next, we describe a simple mathematical ctechnique, concatenation of representations,
that is frequently used in the literature to create motion categorizations—We make extensive use
of this technique, notably, when building the story-based representations, and we apply it also in
. Finally, we compare motion and spatial categorization through cognitive linguistics; our
???? /
aim is to explain a striking result of the survey: in the literature, we find much fewer motion
representations than spatial prepresentations.
6.1 Survey of Qualitative Motion Representations
Dylla et al. (2017) surveyed a total of 40 qualitative representations from which they only classify
( ) as representations of motion (“relative motion”): QRPC (Glez-Cabrera et al. 2013),
three < 8%
RfDL-3-12 (Kurata and Shi 2008a), and, the most used, QTC (Van de Weghe 2004).
QTC refers to a varied family of representations characterized by suffixes (see concise and
complete summary, Delafontaine et al. 2011, Sect. 2): QTC , QTC . The subindex ‘ ’
Bxy C2y
specifies that only relative approaching and speeds difference between entities are considered,
while the subindex ‘ ’ specifies that, additionally, relative movement to the right or left is
considered; subindex ‘ ’ is the dimensionality of the space where the entities move (i.e.,
x = 1, 2
{ }
line or plane), and subindex ‘ ’ relates to the number of kinematic features considered.
y = 1, 2
{ }
For example, e.g., QTC (See full description in Sect. 7.3) describes the relative approaching
B21
68 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
of entities (subindex ‘ ’) in dimensions (subindex ‘ ’), but it disregards speeds difference
B 2 2
(subindex ‘ ’). We find also a three-dimensional of QTC, namely, QTC (Mavridis et al. 2015),
3D
however it is ill-defined for uniform motions and motionless particles.
The categories in a QTC representation are written as a , each element in the tuple
n-tuple
describes a kinematic feature always by means of three symbols . For example, consider
, 0, +
{− }
the kinematic feature ‘relative approaching of entity to entity ’. The ‘ ’ symbol means that
k l k
moves towards , the ‘ ’ symbol means that moves away from , and ‘ ’ means that remains
l + k l 0 k
stationary with respect to .
The RfDL-3-12 considers the straight path described by a point with respect to a two-dimensional
static region. Thus, we classify it as a ‘ ’—a subclass of the motion represen-
path representation
tations. A similar path representation is Double Cross (Freksa 1992b; Zimmermann and Freksa
1996), which is defined like the RfDL-3-12, but instead of using a region as reference, it uses a
static point to describe the straight path of a displaced punctual entity. In these path repre-
sentations much kinematic information is lost: only the entity’s displacement is measured—time
is irrelevant—therefore, speed and acceleration are disregarded. In fact, the path considered in
these representations need not be an entity moving, it suffices that it is the path between two
different landmarks. For that reason, we see these path representations rather marginally as
motion representations.
More elaborated path representations are those describing polygonal trajectories qualita-
tively, as the QMV (Qualitative Motion Vector) sequences (Musto et al. 1998, 1999). The QMV
sequences include time since they are obtained by scanning at a fixed rate the positions of an
entity’s trajectory. QMV sequences include, amongst others, qualitative information about the
speed of the entity ( , , , . . . ). Nonetheless, this qualitative description ad-
slow medium-vel fast /
dresses entity’s motion—we do not have a relation between two entities—thus, it falls
only one
without the scope of this work.
D .
In another survey of qualitative representations, Chen et al. (2015, Sect. 3.4) find
m three
motion representations: QTC (already mentioned)m, and two additional representations, Dipole
Calculus (Moratz et al. 2000) and DIA (Directoed Intervals Algebra) (Renz 2001).
Interestingly, Chen et al. and Dyllaiet al. only coincide with classifying one representation
as motion, namely, QTC, while they disagree in classifying DIA and Dipole Calculus as motion
representations. We explain such disagreement by remarking a fine distinction: representations
such as Dipole Calculus and DIA (also OPRA (Moratz 2006)) are primarily
relative direction
or
orientation
represpentations rather than
motion
representations—they ignore speed. However,
they can be used to represent moving entities by equating with (e.g.,
t orientation velocity direction
Dylla et al. 2007). For that reason, they are sometimes classified as motion representations.
Besides the mentioned motion representations, we also found one developed by Wu et al.
(2014). That is the only one, to our knowledge, that deals explicitly with regions. It combines
the spatial representation RCC with the distance between regions, hence, we call it RCC-d (see
details, Sect. 6.4.2.A).
Concluding, all the aforementioned representations, excepting the , can
path representations
be used to categorize —our research endevour (Ch. 2). Notwithstanding, from
motion scenarios
the representations that categorize motion scenarios, few can be unambiguously classified as
motion categorization, namely, QTC, QRPC, and RCC-d. Such are ‘ ’ qualitative motion
genuine
representations, the other ones are primarily directional representations.
6.2. IDENTIFIED SHORTCOMINGS AND OUR STORY-BASED SOLUTIONS 69
6.2 Identified Shortcomings and our Story-Based Solutions
Observing our survey (Sect. 6.1), we identify relevant shortcomings in the current landscape of
qualitative representations of motion.
As we can observe from our survey, the work in
Sparse work on motion representations
qualitative representations of is unusually sparse when compared to the vast research in
motion
representations (Cohn and Hazarika 2001a, Sect. 5.1.2, p. 16; Delafontaine et al. 2011,
spatial
p. 5187).
Our surveyed motion representations are
Mostly applicable in low dimensional domains
quite limited in their application domain: they are mainly restricted to point-like entities moving
in one or two dimensions (e.g., QTC , QTC , QRPC), while spatial representations deal also
C B
with region-like entities located in three-dimensional spaces (e.g., Egenhofer 1991; Albath et al.
2010) (See overview in Sect. 5.4).
Some motion categorizations (e.g., QTC )
Ill-defined categorization of motionless entities
3D
are ill-defined when at least one entity is motionleAss; they rely on the of the
motion direction
entities to determine the qualitative values, and a motionless entity ( (cid:126)) has an undefined
(cid:126)v = 0
motion direction.
In the directional categorizations, e.g., OPRA and QRPC, objects have an intrinsic orienta-
tion ; hence such categorizationRs are well-defined even when entities are motionless. We resort
(cid:126)o u
to such intrinsic orientation in Section 9.2.2.A, when we obtain Stories-OPRA .
Another problem in motion representations—which also occurs in
Neglected composition o
D .
spatial representations to a minor extent (See Section 5.1.4)—is that, fsor most representations,
researchers neglect the : neither compute their composition tables nor describe how
composition o
to do it. Truly, if we consider the motion representations, only QTC computes the com-
genuine
position tables; the rest, QRPC and RCC-d, remain, so far, without such a reasoning apparatus,
e.g., even when steps in this direction were taken (Gconzález-Cabrera et al. 2010).
Our story-based motion representations overcome these limitations by presenting a method
to create qualitative representations of motion that work in any number of dimensions, with any
kind of entities, i.e., point-like or regions, and, that can categorize scenarios with motionless
entities as long as the generapting representation is well-defined in ( (cid:126)). Likewise, we oppose
(cid:126)v = 0
the trend that the comtposition is neglected, we provide in Chapter 11 a method to ease the
computation of the composition for story-based motion representations.
6.3 A Taxonomy of Qualitative Relations of Motion
We present the above surveyed motion representations as a taxonomy according to two criteria.
First, on a higher level, we classify them according to the temporal duration of the categorized
motions. That is, some representations categorize motion (a punctual instant of
instantaneous
time), while others categorize motion in a time interval, i.e., along a trajectory. In between are
categorizations that, in addition to instantaneous motion, consider also the trajectory.
expected
Second, on a lower level, we classify them according to the kinematic features captured; for
example, some representations consider only the direction of motion, while others consider the
entities’ speeds and distances.
70 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
A.
Instantaneous motion
• Orientation of velocity vectors:
The family of representations ‘Oriented Point Relation Algebra’ (OPRA) (Mossakowski
and Moratz 2010)
• Orientation of velocity vectors compared speeds and angles:
The family of representations ‘Qualitative Trajectory Calculus’ (QTC) relations (Van
de Weghe 2004; Delafontaine et al. 2011)
• Overlapping (RCC representation) relative region approach:
RCC-d (Wu et al. 2014)
B.
Instantaneous motion + expected trajectory
• Configuration of the velocity vectors proximity and relative position to expected
crossing point: QRPC (Glez-Cabrera et al. 2013) F
• Kinematic aspects are inherited from the generating (either spatial or motion) repre-
sentation:
Story-based representations from spatial representations, e.g., Motion-RCC and
Motion-OPRA (Purcalla Arrufi and Kirsch 2018a)
Story-based representations from expanded qualitative representations of motion,
e.g., Story-QTC ( ).
??
R B
C.
Trajectory segments 3
• Landmark based and path-centred frames: r
D Double Cross (Zimmermann and Freksa 1996; Freksa.1992b) RfDL-3-12 (Kurata and
Shi 2008a; b)
• Oriented trajectory segments:
Dipole Calculus (Moratz et al. 2000)
D.
Arbitrary trajectories i
Described as temporal sequences of motion relations that belong to representations describ-
ing instantaneous motiont(item A.):
• Permutables4th order sequences of QTC relations (Delafontaine et al. 2011)
p C
• Sequtences of QTC relations with same start and end time (Hanheide et al. 2012)
h C
6.4 Expanding Qualitative Representations of Motion
In this section, we present a standard method for creating or expanding—making finer— repre-
sentations of motion. The method relies on Cartesian product, that is in the concatenation of
representations: we can concatenate arbitrary motion representations to form a new qualitative
representation. For example, if we have motion representations and , we can create a
A B
M M
new representation by concatenating to ; we obtain then the ‘ ’
product representation
B A
M M
, in which each motion relation is expressed as a tuple where
= R = (R , R )
C A B C A B
M M × M
and . Nonetheless, it is up to interpretation, whether we consider a
R R
A A B B C
∈ M ∈ M M
qualitative motion representation, or just an expansion of the representation —or,
brand new
alternatively, an expansion of .
6.4. EXPANDING QUALITATIVE REPRESENTATIONS OF MOTION 71
Now, we illustrate the method with a concrete example: we use N. Van de Weghe’s (2004)
QTC family of categorizations (Fig. 6.1). Initially, we take two qualitative motion representa-
tions, namely QTC (fully described in Sect. 7.3), and a very simple representation that we
B21
will call .
Speed
A motion relation in QTC describes two properties: , ‘relative approaching of entity
B21 k
with respect to entity ’, and, conversely, , ‘the relative approaching of entity to entity
k l l
’; each property, , takes three possible symbols . The symbol ‘ ’ means ‘moving
k , 0, +
P {− } −
towards’, ‘ ’ means ‘moving away’, and ‘ ’ means ‘remaining stationary’. Accordingly, a motion
+ 0
category is represented by the -tuple in which each element is a property. For instance, a motion
scenario where chases —that is, moves towards , and moves away from —is described as
k l k l l k
(See Figures 6.1a and 6.1b)
( , +)
A motion category in the representation simply describes which of the entities moves
Speed
faster. It also takes three possible symbols : the symbol ‘ ’ means ‘ moves faster than
, 0, + k
{− } −
’, ‘ ’ means ‘ moves slower than ’, and ‘ ’ means ‘ and move equally fast’ (See, respectively,
l + k l 0 k l
Figures 6.1a to 6.1c). F
Both QTC and are qualitative motion representations in their own right—they fulfil
Speed
B21
the properties of qualitative representations (Sect. 5.3). Thus, following the indications above,
we can create a new motion representation QTC . In , a motion relation
= Speed
C B21 C
M × M
is represented through the tuple ; for example, the tuple , which
(R , R ) (( , +), +)
QTC Speed
B21
describes following: moves towards , moves away from —as before—and moves slower
k l l k k
than (this is the new information) (See Fig. 6.1a). We can clearly appreciate the advantages of
concatenating the extra representation: in case is chasing , relation in representation
k l ( , +) r
QTC , the extra relation in representation tells us how successful the chase is—if the
Speed g
B21
relation is , may reach , otherwise, with relations ‘ ’ and ‘ ’, cannot reach
(( , +), ) k l Speed + 0 k
− − g
This “new” motion categorization QTC is already known as QTC (e.g., Van
Speed
D .
B21 B22
de Weghe et al. 2007, Sect. 3). For the sake of simplicity, in the QTC family, the relations are
written as a simple tuple, i.e., without nested tuples; for example is written .
m (( , +), ) ( , +, )
− − − −
y y o y
(cid:126)v e (cid:126)v
k k
h(cid:126)v
(cid:126)v (cid:126)v (cid:126)v
l l l
l l l
k s k k
t x x x
(a) (b) (c)
QTC = (−, +) QTC = (−, +) QTC = (−, +)
B21 B21 B21
Speed = (−) Speed = (+) Speed = (0)
QTC = (−, +, −) QTC = (−, +, +) QTC = (−, +, 0)
B22 B22 B22
Motion scenarios classified according to QTC , , and their
B21
QTC QTC . All these scenarios share the same QTC relation, but
sentation = Speed
B22 B21 B21
differ in ; consequently, they have different QTC relations.
Speed
B22
Theoretically, we can concatenate as many representations as we wish in order to obtain a new
product representations, i.e., . Every product representation
C A A A
1 2 i
M M ×M ×· · ·×M ×· · ·
is at least as fine as each of its components, that is, has at least the same motion relations
72 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
that each component representations has. For example, in any QTC motion relation,
A B22
we can find the corresponding QTC and relations: these are, respectively, the first two
Speed
B21
coefficients and the last coefficient of a QTC tuple (e.g., Fig. 6.1).
B22
In fact, the product representation is usually than any of the component repre-
finer
sentations. In other words, a category in any of the component representations is further
into the categories of another component representation . For example,
subcategorized
the QTC relation is subcategorized in three QTC relations, namely, ,
( , +) ( , +, )
B21 B22
− − −
, and (respectively, Figs. 6.1a to 6.1c).
( , +, +) ( , +, 0)
− −
Even though we might concatenate an number of motion representations
unlimited ,
{M
to generate the corresponding product representation, we are ultimately con-
, . . . , . . .
A A
2 i
M M }
strained: only a product representation with a few of those can be handTily implemented
and used. Hence, the question arises: which criteria could help us choose the more convenient
representations to concatenate?
Such a desired criterion is furnished in the next section, based on a key concept: the
multi-
of concatenated representations.
plicativity
6.4.1 Multiplicativity of Concatenated Representations
Ideally, we might expect that, in a product representation, , every concatenated
C A B
M M ×M
representation, , subdivides (i.e., subcategorizes) each motion relation in according to
B A
M M
its number of elements, . In short, we might expect following equality,
B A B
|M | |M × M |
, between the total number of relations in the product representation, ,
A B A B
|M | · |M | |M × M |
and the number of relations in the component representations, namely, angd .
A3 B
|M | |M |
For instance, in the product motion relation QTC QTC / (Fig. 6.1), we
= Speed
B22 B21g
should ideally expect that each of the QTC motion relations is subcategorized by the
2 r
3 3
B21 o
Speed Drelations. If so, QTC should have a total of 3 2 3 = . 27 motions relations—which is
B22
· n
precisely the case.
Unfortunately, the ideal case observed in QTC , QTC QTC , i.e.,
= Speed
B22 B22 B21 A
m | | | |·| | |M ×
, does not apply to every concatenated representation. Most generally, we
B A B
M | |M | · |M | o
can only confirm the inequality in Equation (6.1), which sets the upper bound for the number
of relations in the product representation. In the optimal case, where the equality holds, we say
that the representations and e are ‘ ’.
multiplicative
A B
M M
(6.1)
A B A B
|M × M | ≤ |M | · |M |
In fact, comptonent representations are often -multiplicative, that is, usually only the
non
strict inequality holds in Equation (6.1). Non-multiplicativity arises when some combinations of
relations are physically unrealizable—This occurs, for example, with the one-dimensional motion
representation QTC .
B12
QTC is a motion representation for entities moving in one dimension. It is constructed
B12
as the product representation of QTC (exactly defined as QTC but restricted to entities
B11 B21
moving in one dimension), and the representation (as defined above but likewise restricted
Speed
to entities in one dimension). If QTC and were multiplicative, then QTC
Speed =
B11 B12
| |
QTC . But QTC has only 17 relations, because 10 combinations
Speed = 3 2 3 = 27
B11 B12
| | · | | ·
Speed
B11
behaviour. The QTC relation combines smoothly with any of the relations
( , +) Speed
B11
yielding three different QTC relations: , , and (Figs. 6.2a to 6.2c).
( , +, ) ( , +, +) ( , +, 0)
B12
− − − −
Yet the QTC relation is only compatible with the relation ‘ ’, yielding .
( , 0) Speed ( , 0, )
B11
− − − −
Truly, the component ‘ ’ of necessarily means—in one dimension—that is motionless,
0 ( , 0) l
6.4. EXPANDING QUALITATIVE REPRESENTATIONS OF MOTION 73
which entails that, if moves, the relation may only be ‘ ’. Consequently, the relations
k Speed
and are physically unrealizable in one dimension.
( , 0, 0) ( , 0, +)
− −
(cid:126)v (cid:126)v (cid:126)v (cid:126)v (cid:126)v (cid:126)v (cid:126)v
k l k l k l k
x x x x
k l k l k l k l
(a) (b) (c) (d)
QTC = (−, +) QTC = (−, +) QTC = (−, +) QTC = (−, 0)
B11 B11 B11 B11
Speed = (−) Speed = (+) Speed = (0) Speed = ( − )
QTC = (−, +, −) QTC = (−, +, +) QTC = (−, +, 0) QTC = (−, 0, − )
B12 B12 B12 B12
Motion scenarios classified according to QTC , , and their
B11
QTC QTC . Scenarios (a), (b), and (c) display all three possible
sentation = Speed Speed
B12 B11
variants of the QTC relation . Scenario (d) displays the possible variant,
( , +) only Speed
B11
, of the QTC relation ; the relations and are physically unrealizable.
( ) ( , 0) Speed (+) (0) F
B11
− −
We resume our determination of the bounds for . The lower bound for occurs when
A B
|M ×M |
one of the representations contains the other, that iAs either or . In that
A B A B
M ⊂ M M ⊃ M
case, the corresponding subrepresentation does not provide any extra categorization information
at all, and, therefore, the product representation has the same number of relations as the finest
of both representations and . We express mathematically this fact in Equation (6.2)
A B
M M r
(6.2)
max( , )
A B A B
|M | |M | ≤ |M × M | /
Summarizing, we have the following bounds for :
A B
|M × M | o
D .
(6.3)
max( , )
A B A B A B
|M | |M | ≤ |M × M | ≤ |M | · |M |
Equation (6.3) hints about the criterion for choosing the representation that we should
more conveniently concatenate to . On the one hand, if and are the
multiplicative
A o A B
M M M
product representation, has the greatest pcossible number of categories (related to the
A B s
M × M
size of )—the product representation has thie greatest granularity. On the other hand, if we
M e
concatenate a subrepresentation, i.e., , the product representation remains the same.
B A
Mt ⊂ M
As a result, we should avoid the concatenation of subrepresentations and strive to concatenate
multiplicative representations.
However, not all pairs of pmotion representations are either multiplicative or is one represen-
tation a subrepresentations of the other—we have cases in between. For that reason, instead
of just defining as a boolean feature—whether a motion representation is mul-
multiplicativity
tiplicative or not—we define multiplicativity as a graded value. The bounds of in
A B
|M × M |
Equation (6.3) help us to define a ‘ ’, , with values in
measure of multiplicativity µ ( , ) [0, 1]
A B
M M
(Eq. (6.4); see proof, Prop. A.1.2). The maximum value, , is attained if the representations are
multiplicative; the minumum, , if one categorization is subcategory of the other—the higher the
value the more independent are the concatenated representations, that is, they generate more
effectively product relations by yielding finer subcategorizations.
|M ×M |
A B 1
max(|M
|,|M
|)
− (6.4)
µ ( , ) =
A B
M M 1
min(|M |,|M |)
A B
0 µ ( , ) 1
A B
≤ M M ≤
74 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
In conclusion, the , , provides us with a criterion to choose
multiplicativity µ ( , )
A B
M M
amongst representations that we want to concatenate. If we have some representation candidates
to concatenate with a motion representation , we can choose the one with
, . . . ,
B B A
1 n
{M M } M
the highest multiplicativity, i.e., . In that way, we ensure that
= max µ ( , )
B i=1...n A B
M { M M }
the concatenated representation optimizes the subcategorization of the motion categories of
in the new product representation .
A B
M × M
A. An application of multiplicativity
We illustrate here the application of multiplicativity. Consider the motion representation
QTC and two candidates for expanding it, namely, and . The
= Speed T = Gaping
B11 B B
1 2
M M
motion representation was wholly presented in Chapter 2. In brief, it takes, like ,
Gaping Speed
three possible values : the symbol ‘ ’ means that the distance between the entities
, 0, +
{− } −
decreases, the symbol ‘ ’ means that the distance increases, and the symbol ‘ ’ means that the
+ 0
distance between the entities remains constant. F
In the case we want to expand by concatenating one representation, we must choose
between or . The provides the criterion for the choice. In
multiplicativity measure
B B
1 2
M M
order to obtain the multiplicativity (Eq. (6.4)), we must compute following values: ,
= 9
|M |
, , , . We obtain
= 3 = 3 = 17 = 13 µ ( , ) = 0.44
B B A B A B A B
1 2 1 2 1
|M | |M | |M × M | |M × M | M M
and . This motivates the choice for , because it subcategorizes
µ ( , ) = 0.22
A B B A
2 1
M M M M
more effectively.
Admittedly, in this case, it suffices to choose the representation whose product representation
has more elements (i.e., ), because both representations and
> g
A B A B B
|M × M 1 | |M × M 2 | 3 M 1
have the same number of relations, i.e., three relations. Notwithstanding, the multiplica-
M g
tivity measure has the advantage that it compares also representations with different number of
relations (i.e, when ).
D .
B B
1 2 s
|M | (cid:54) |M |
6.4.2 Further types of concatenation: hybrids
So far, we have expanded motion representations by concatenating additional represen-
motion
tations. However, we can also expand a motion representation by concatenating a
spatial
representation , so that the produect representation, , contains both spatial
B h C A B
D M M × D
and motion relations. Such a representation—which we call ‘ ’—
t hybrid motion representation
retains all the properties of a genuine motion representation, for instance, the computation of
Most importanttly, a main result of this work—the story-based representations—are
beaded
mostly , as we comment at the end of this section. But first, we
hybrid motion representation
detail an example of in the literature: the already mentioned RCC-d.
hybrid representation
A. A hybrid motion representation: RCC-d
A B
following, we restrict to the case where region is smaller than . RCC-d is the Cartesian
A B
product of the representation RCC, and the representation “variation of distance
spatial motion
between regions”—we call it , equivalent to the representation but for regions.
Gaping2 Gaping
Consequently, a motion relation in RCC-d can be formalized as .
(R , M )
1 1
• is the RCC value of the motion scenario (explained in Sect. 5.2.1), which, for entity
R A
smaller than entity , can take following values: , , , , .
B DC EC PO TPP NTPP
6.4. EXPANDING QUALITATIVE REPRESENTATIONS OF MOTION 75
Simplified iconic representation of the RCC-d relations—These cases are restricted
to region being smaller than . 4
A B
(Source: Wu et al. (2014), composed of Figures 1, 2, and 3.)
• is the distance variation between regions. Although Wu et al. (ibid.) omit any definition
1 g
of the distance between regions, from fig. 1 and 2 (our Fig. 6.3) onercan infer that the
their
“distanc D e” they meant is the minimum distance between the . of the regions—
border points
notice, however, that this is not a ‘distance’ in the mathematical sense.
can take following values: ‘ ’, ‘ ’, ‘ ’; they respectively indicate if the points
M + 0 nearest
− m
of the borders move away, remain at a constant distance, or approach1.
Accordingly, the RCC-d relations are AP , LP , AO , TI ,
= (DiC, ) = (DC, +) = (DC, 0) = (EC, 0)
OI , CB , MI e, MB , AI .
= (PO, 0) = (TPP, 0) = (NTPP, +) = (NTPP, ) = (NTPP, 0)
As to the rest, RCC-d functions astany motion representation. For example, we can cal-
culate the multiplicativity of RCC/-d to see how effectively the spatial representation subcate-
gorizes the motion representation: , , , thus,
= 5 = 3 = 9
RCC Gaping2 RCC Gaping2
|M | |M | |M × M |
µ ( , ) = 0.4
RCC Gaping2
M M h
B. Hybrid story-based representations
In this work, we present two main types of story-based representations: the and the
bare beaded
ones ( ). Even though the beaded story-based representations are thoroughly expounded in
??
Section 9.5, we briefly mention them here because, in most cases, they are .
hybrid representations
A beaded story-based representation is a product representation , where is a
A B B
M × R R
spatial or motion representation, and is a very specific motion representation, namely,
, which is obtained straightforwardly—but not effortlessly—from the original spatial (or
motion) representation (Sect. 9.1). We call the motion relations in ‘ ’, and hence
Σ stories
B R
Wu et al. (2014) use the symbols d , d , d , d , d , d instead of {−, 0, +}. Note, however,
ext (cid:57) ext= ext+ int (cid:57) int= int+
that the additional identifiers ‘ext’ and ‘int’ are redundant when combined with the RCC relations; indeed, ‘ext’
associates to DC, and ‘int’ to NTPP.
76 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
the name ‘ ’. Note that when the original representation is a
story-based representations
representation, i.e., , then the product representation is also hybrid
spatial =
B B A B
R D M × R
representation, .
A B
M × D
Relations in a beaded story-based representation are expressed as where is a story,
S (R ) S
i j i
that is, a in ; and is a in . The notation is
motion relation Σ R spatial relation S (R )
R j B i j
equivalent to the traditional tuple notation —with our notation, we just highlight that
(S , R )
i j
R S
j i
6.5 Motion and spatial categorization: linguistic compari-
son T
As we mentioned in our survey (Sect. 6.2), it is odd to find a much greater number of spatial rep-
resentations than motion representations. All the more, because both categorizations are directly
related—motion is change in time—and have some fundF amental cognitive commonalities.
spatial
In principle, we might expect that the abundant work in representations should prompt
spatial
a similar number of results in representations; but this is not the case: scientists can-
motion
not lightly transfer the formalism used in qualitative spatial representations to motion repre-
sentations. We hypothesize that such transfer difficulties arise from the different way humans
cognitively process spatial and motion relations.
We can best appreciate through our language how we cognitively process motion and spatial
categorization. Indeed, the differences—but also the commonalities—between these two catego-
rizations show up when we verbalize them. For that reason, in this section, we resort to ‘
cognitive
’ to contrast space and motion relations: “[cognitive linguistics deals with] the linguis-
linguistics
tic representation of conceptual structure” (Talmy 2000b, p. 1, Introductgion). We are inspired in
our cognitive linguistic analysis by the work of L. Talmy, collectedoin his opus magnum
Toward
D .
a Cognitive Semantics n
The main insights we draw from cognitive linguistics are following. First, the powerful
mathematical formalism of , used in spatial categorization
qualitative spatial representations
(Sect. 5.1.3), essentially mirrors the properties of spatial relations in language. Second, mo-
tion relations are expressed through different linguistic devices than spatial relations, and, this
is a compelling reason why scientists csannot straightforwardly transfer the formalism of quali-
tative representations from spatial into motion categorization. Admittedly, we exemplify such
insights in English language, but they are applicable to other European languages, and possibly
to general human lingui:stics.
6.5.1 Commonalities
Before we tackle the differences, we remark the most fundamental similarity between spatial and
motion categorization: they relate at least two entities that have differentiated roles. The
often
first entity, a (called ‘ ’), is related to a second entity, a (called
‘ ’) (ibid., Ch. 5). In Example 6.1, we can see and for each categorization:
spatial (item a.) and motion (item b.).
and in spatial and motion relations.
a. The bottle is the plate. (spatial relation)
in front of
b. The alpinist the mountain. (motion relation)
climbed
6.5. MOTION AND SPATIAL CATEGORIZATION: LINGUISTIC COMPARISON 77
6.5.2 Differences
In this section, we present the differences between the linguistic description of and
spatial motion
relations. We use as a paradigm the differences between Example 6.1.a. and b..
A. Prepositions for spatial relations, verbs for motion relations
First of all, we argue that, in most cases, two entities are related mostly by means of
spatially
a . Indeed, in English—as in many other languages—places or position of objects
preposition
are predominantly represented by prepositions (Landau and Jackendoff 1993, p. 218). In Ex-
ample 6.1.a., the preposition relates the two entities spatially. Secondly, we argue
in front of
that, in most cases, two entities are related by means of a which is eventTually as-
moving verb
sisted by and . In Example 6.1.b. the two moving entities are related by means of a ,
adverb verb
such as .
climbed
One may object, though, that the spatial relation in Example 6.1.a. is also a verb, namely,
. However, if we turn both examples into (Ex. 6.2), then we
is in front of adjective phrases
see which the decisive elements are. Indeed, in Example 6.2.a. the verb ‘to be’ drops from the
sentence gracefully—it is superfluous—the preposition is, though, the key element.
in front of
Whereas in Example 6.2.b. the verb ‘to climb’, in the present participle form , cannot
climbing
be stripped from the setence: it is the necessary key element to refer to the motion.
alpinist’s
Adjective phrases obtained from Example 6.1.
Example 6.2
a. I see the the . (spatial relation)
bottle in front of plate
preposition
b. I see the the . (motion relation) r
alpinist climbing mountain
D verb .
One might further object that the preposition in Example 6.1.a. becomes the relevant element
in the spatial relation because the verb ‘to be’ is a dull verb. But notice that, whatever verb
of position we use (e.g., ‘to stand’, ‘to lie’, ‘to place’), the preposition remains most relevant.
Indeed, we can proceed as following: first, we exchange the verb ‘to be’ in Ex. 6.1.a. for ‘to lie’
and we get ‘the bottle lay the plate’; next, we turn the sentence into an adjective
in front of
phrase (Ex. 6.3.a.), and test which elemtent is most relevant by stripping it from the sentence
(Ex. 6.3.b. and c.). Then, we obs/erve that only when we strip the prepositional information
(Item c.), any spatial relation between and is lost; whereas when we strip the verbal
p bottle plate
information (Item b.), the spatial relation remains.
Example 6.3
a. I see the bottle the plate.
lying in front of
b. I see the bottle the plate.
lying in front of
c. I see the bottle the plate.*
lying in front of
Some prepositions pose a challenge because they are closely related to motion, amongst
others the preposition conveying direction; for example, or . However, such
towards through
prepositions still depend on the verb to express a motion or spatial relation (Ex. 6.4). At the
very most, just a couple of prepositions express only motion, i.e., they are not used to express
position; for instance, .
into
78 CHAPTER 6. QUALITATIVE REPRESENTATIONS OF MOTION
The prepositions below, and , express a spatial or motion rela-
Example 6.4 toward through
tion depending on the verb.
toward through
Spatial relation The child his The broken bone
is pointing toward sticks out
mom. the skin.
through
Motion relation The athlete the goal. The birds the trees.
runs toward fly through
In sum, it is virtually impossible to describe the between two objects without
spatial relation
a (as explained in Sect. 5.1.2). Likewise, it is very difficult to describe the
preposition motion
between two objects without a . This linguistic difference has profound cognitive implica-
verb
tions.
B. Cognitive differences between prepositions and verbs
According to L. Talmy (2000b, Intr. pp. 22f.) prepositions and verbs are cognitively very different
elements. Prepositions are ‘ ’, whereas verbs are ‘ ’. In another
grammatical terms lexical terms
words, prepositions form a ‘ ’ (like determiners or conjunctions do), which means
closed class
that the set of prepositions are relatively small compared to ‘ ’, such as verbs or
open classes
adjectives. Moreover, the closed classes have a very stable membership—a language rarely adopts
new prepositions. However, open classes, such as verbs, are in steady change growing with new
members; for instance, verbs can be readily created from nouns and adjectives, e.g., the
-ize
verbs.
More relevantly, the difference between and elements manifests in dif-
closed-class open-class u
ferent mathematical properties. Closed-class elements describe “largely a relativistic, topological,
qualitative, or approximative rather than a absolute, Euclidean, quantitative, or precisional”, as
open-class elements do (ibid., p. 28). Specifically, clossed-class elements comprise notions such
D .
as , region, locatedness.
partition s
Finally, in spatial relations, the roles of and are exchangeable in a
way than in motion relations. We illustrate this phenomenon in Example 6.5: when
metrical
we exchange the roles in the relation, we obtain a sentence grammatically equivalent to
spatial o
the first with a different preposition ( ); however, when we exchange roles in the
behisnd motion
relation, we are forced to use the s construction (i.e., was climbed by). As a consequence,
passive
one, but it is a different gram/matical structure: a passive sentence. Certainly, though
mountain
is the subject, it is not th:e one who performs the action, in fact, we have no verb which expresses
the mountain as a performer of the action on a climbing alpinist.
Example 6.5
Spatial relation The is the The is the
bottle in front of plate behind bottle
plate
Motion relation The the The by
alpinist climbed moun- mountain was climbed
. the .
tain alpinist
Admittedly, as noted by Talmy (1983, Sect. 3.1), spatial relations are not fully symmetric
Item a. sounds more familiar than Item b., and, therefore, we may argue that it has different
meaning nuances. Nevertheless, grammatically both expressions are equivalent. Consequently,
and ground.
6.5. MOTION AND SPATIAL CATEGORIZATION: LINGUISTIC COMPARISON 79
Example 6.6
a. The is the .
bicycle near house
b. The is the .
house near bicycle
By the way, motion relations are strongly asymmetrical, because usually one of the entities
performs a certain motion that the other does not. For example, an entity moves, while the
other remains motionless—as in Example 6.1.b.—or when an entity, e.g., a motorcycle, overtakes
another, e.g., a car. Spatial relations, however, remain to a great extent symmetrical because
both objects are motionless. We have, though, in spatial relations softer asymmetry sources
Tmobility
For instance, in Example 6.6, ‘bicycle’ sounds more common as (item a.) than as
(item b.), because ‘bicycle’ is more movable than ‘house’.
6.5.3 Implications for Qualitative Representations
Section 6.5.2 considered, it is noteworthy how good the properties of a linguistic , i.e.,
closed-class
a , fit the properties of qualitative relations (Tab. 6.1). One of such closed-class
grammatical term
elements, , is the standard way in language to express spatial relations. Consequently,
prepositions
we argue that when researchers mathematically formalize spatial relations, e.g., in a
qualitative
, they intuitively reflect the linguistic properties in their formalization.
spatial representation
Conversely, it is also remarkable how different are the properties of linguistic
open-classes
from the properties of qualitative representations. Thus, since motion is largely expressed
through (open-class elements), it becomes awkward to formalize motion in a 3qualitative
verbs
representation—Notwithstanding, researchers have constructed a handful of such qualitative mo-
tion representation.
D .
Comparison of properties shown by three domains: qualitative representations,
open-class elements, closed-class elements.
domain c
Qualitative representns. Closed-class elemts. Open-class elemts.
properties (e.g., RCC, OPRA ) (e.g., prepositions) (e.g., verbs)
e 1
Type of measure q/ualitative qualitative quantitative
Number of elements s fixed, finite, fixed, finite, variable, growing
t low number low number
Topological Structure disjoint partition partitions, regions
(amongst others)
Converse Yes Yes Arguably, No
Operations (different preposition (passive form used)
used)
Composition Yes Yes No
To wrap up, we conclude that using our language as cognitive interface, we seem to have a
more direct access to the conceptual structure of space than of motion. Therefore, the mathemat-
ical formalization of spatial relations as qualitative representations is more straightforward than,
regrettably, the formalization of motion relations as qualitative representations. Howbeit, in this
work, we provide a cognitively plausible method to generate qualitative motion representations.
D .
Part III
Developing Story-Based
Categorizations
D .
D .
Chapter 7
Describing [Motion]
Categorizations: A Framework
In some respects, it seems as if the study of con-
cepts is the study of theories that do not work for
one reason or another.
N. Braisby (2012)
“Concepts”, p. 162
In the literature, we find a great variety of categorization models, e.g., oprobabilistic models
D .
based on prototypes, based on exemplars, categorization through clustering, etc. Unfortunately,
each model is built on a particular formalization, i.e., each model is described using its own math-
ematical tools (See comprehensive survey, Pothos and Wills 2011a). This makes it a toilsome
task trying to compare, or computationally implement, categorization models; even more, be-
cause the formalization of many categorizations models is rather vague, notably, the formalization
of the prototype model (Murphy 2002, pp. 41–4s5). We avoid these drawbacks by mathematically
specifying our categorization model and rehlating it to the psychological terminology.
The precise definition of our story-based motion categorizations takes place in Chapter 9,
but, in this chapter, we provisde the mathematical building blocks that, then, enable a solid
definition of the story-basted motion categorizations. Here, we define a mathematical framework
to formalize categorizations, concretely, motion categorizations. In plain words, here we describe
the categorization process mathematically. Admittedly, our framework suits more naturally a
boundary categorization model, but it allows for modifications to accommodate other types, such
as prototype or exemplar model.
We proceed as follows. At first, in Sections 7.1 and 7.2, we define our categorization frame-
work: we express the categorization process mathematically, in particular, we link the mathemat-
ical expressions to fundamental psychological categorization terms, such as , ,
category attribute
and . Subsequently, in Section 7.3, we illustrate our framework by describing a simple mo-
feature
tion categorization, QTC , which is also a qualitative motion representation (Van De Weghe
B12
et al. 2006). Finally, in Section 7.4, we go beyond pure descriptive tasks: we use our framework
to test which similarity function and which categorization model (prototype or boundary) most
suitably describes a motion categorization; we take again as example the QTC categorization.
B12
84 CHAPTER 7. A CATEGORIZATION FRAMEWORK
We undertake to formalize categorization, because of its advantages (See, Murphy 2011): we
expect to be detailed enough, here, in our categorization framework, and, later, in our story-based
categorizations, that there be nothing left to intuition, but anyone may implement our methods
in the same way we do; so that we ensure that she obtains the same results we do (
reproducibil-
), and make the same predictions we would ( ) (Anderson 1976, p. 17–18). We
ity effectiveness
understand our categorization framework as an unambiguous formalization of the rather vague
concept of (Sect. 4.4.1.A, p. 41), and has, thus, many commonalities with
psychological space
it. All throughout our formalization and subsequent modelling, we keep Braisby’s admonishing
words in mind: we know that for some reasons our theory will be found wanting (See quotation
above).
That said, we acknowledge that our categorization framework is comparable to many imple-
mentations of a psychological space—for example, the ‘conceptual spaces’ of Gärdenfors (2004,
2014)—but with some customizations. The main virtues of our framework are following: the
terms are clearly, unambiguously, and mathematically defined; moreover, the terminology is
explained so that both researchers in psychology and AI shouldF sympathize with the framework.
7.1 The Categorization Process: Formalization
In the following, we describe the categorization process using terms related to motion catego-
rization; more accurately, we use the terms concerning because this
scenarios categorization
is the focus of this work. Thus, it might seem that we limit our description to the scenarios
categorization—It is not so. We could describe the categorization process in this chapter using
tion scenario’ by the term ‘ ’ which is a more general description of the motion of
motion state
two entities, not limited to the current positions and velocities. Even more, we could substitute
D .
‘motion scenario’ and ‘kinematic space’ by the most general terms ‘item’ and ‘items space’; and
our description would still be consistent.
Scenarios Mootion General
categorization categorization categorization
⊂ ⊂
Kinematic space h Kinematic space Items space
Motion scenario / Motion state Item
⊂ ⊂
Parallel pterms according to the degree of specificity. From most specific (‘scenarios
categorizatiotn’) to most general (‘general categorization’).
7.1.1 Basic Elements: Kinematic space & categorization rule
First off, to categorize a means that we label it with a
motion scenario K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
category . Now, in our formalization, this categorization process is defined by the basic
elements that we list below:
i. The ‘ ’ contains the categories in which a motion
categories set = M , M , . . . , M
1 2 m
M M { }
scenario can be categorized. We assume the categories are a finite number (e.g., the
nine QTC categories in Eq. (7.9))—Both the requirement of and the
cognitive economy
B12
finite number of human concepts seem to enforce this assumption.
7.1. THE CATEGORIZATION PROCESS: FORMALIZATION 85
ii. The ‘ ’, is the set of all possible motion scenarios ; they are repre-
kinematic space K
sented as a vector of positions, , and velocities, , for each entity. We call ‘
(cid:126)x (cid:126)v kinematic
’, , these parameters (positions and velocities) that characterize a motion
coordinates K
scenario—Depending on the context, we refer to them also as ‘ ’. In
kinematic variables
that sense, is a coordinate space in which each motion scenario is described as a
point.
iii. The ‘ ’ labels each motion scenario with a category. In other words, the
categorization rule f
categorization rule is a function that maps each , , into the corresponding
motion scenario K
, (e.g., Eq. (7.10)). Alternatively, we can say that the categorization
motion category M
rule defines the of the motion scenarios.
category membership
f :
µ (7.1)
K −→ M
K M
(cid:55)−→
Summarizing, a ‘ ’, K, is most basically defined as ( ,
motion categorization kinematic space
, ):
categorization rule f motion categories
K (7.2)
= ( , f , )
K M
Equivalently, we can define the motion categorization providing the instead
categorical regions
of providing the . A ‘ ’, , is the set of scenarios that belong
categorization rule categorical region K 4
to a certain category . In thisRcase, a motion categorization K is defined as (
M kinematic u space
, , ):
categorical regions K motion categories
K M
K (7.3)
D = ( , K , . . . , K , ) .
1 m s
K { } M
where −1
K = f (M ) M o
i µ i i
⊂ K ∈ M
In our most simple understanding of categorization, the categorical regions, , should verify
two properties: a motion scenario cannot belong to more than one category, i.e., categorical
K c
regions should be disjoint (Eq. (7.4a)); every motion scenario should be categorized, i.e., the
categorical regions cover the whole kinematic space (Eq. (7.4b)). Hence, the categorical regions,
, are a of the kinematic spatce .
K partition
t (7.4a)
i, j K K =
i j
h ∀ ∩ ∅
(cid:91)
(7.4b)
= K
i=1
Both equivalent definitions of motion categorization, (Eq. (7.2)) and
(f , ) ( K , . . . , K ,
µ 1 n
M { }
(Eq. (7.3)), correspond respectively to the ‘ ’, by rules, and ‘ ’, by
) intensional extensional
members, definitions of a category or concept (e.g., Houdé 2004, pp. 78f.). Note that the
intensional definition—in our case by means of the —is cognitively more
categorization rule f
useful than just specifying all category members, because it allows us to more easily grasp a
categorization, that is, to understand what makes a category different from other, or which are
the key features upon which a category is built. As Hampton (2011) acknowledges “Intensions are
of more interest to psychologists, since they reflect the way in which we represent the concept[,
i.e., the category,] internally”.
86 CHAPTER 7. A CATEGORIZATION FRAMEWORK
7.1.2 Additional Elements: Featural space & Feature extraction
The elements described in Section 7.1.1 (kinematic space, categorization rule, and categories set)
are indispensable to define a motion categorization. Nevertheless, there is no trace of two capital
concepts in categorization according to psychology: and (Sect. 4.4). Here,
features dissimilarity
we work out these additional elements (features and dissimilarity) from the basic ones (kinematic
space, categorization rule, and categories set).
Please note that we work inversely as in traditional categorization: Instead of defining features
and a feature-based dissimilarity in order to create a categorization rule based on them (Bottom-
up method in Fig. 7.1), we extract the features and a feature-based categorization rule from the
general , and, finally, define a dissimilarity in the obtained T (Top-
categorization rule featural space
down method in Fig. 7.1). At the end, the result is equivalent as we had first defined the features,
and, of course, we can apply our framework also in a context, when the features and
bottom-up
the dissimilarity are first given.
This method is the only effective one when we deal with the story-based categories,
top-down
because our method for generating story-based categorizations provides, in first place, their
categorization rule (which we call , Sect. 9.1)—This is also the case for qualitative
story map
representations (Sects. 5.4 and 6.1), where the categorization rule is defined without singling out
the features in advance.
Independent
Featural dissimi-
variables are
larity is chosen.
extracted. r
Dissimilarity Featural
Top-down
Categorization rule
measure variables
Featural dissim-
Dimensionality
D .
ilarity is chosen. is chosen. s
Categorisation rule
Pairwise Dissimilarity Featural
Bottom-up
(similarity-based)
dissimilarities measure variables
McultiDimensional Scaling
Two methods of describing a categorization: ‘ ’ and ‘ ’. We de-
scribe when we start with the categorization rule, and, from it, we derive the remaining
bottom-up t
categorization properties (first, features and, then, dissimilarity)—This is the case of story-based
representations. We desscribe when we begin with the pairwise dissimilarities and, from
top-down
them, we obtain the best fit for dissimilarity measure and features (e.g., using MDS techniques).
In Equation (7.5), we apply the top-down analysis to our categorization method; we express
the categorization rule in Equation (7.1) as a two-step process. First, the
f featural variables
are obtained from the through function —comparable
F = (ϕ , . . . , ϕ ) kinematic variables K Φ
1 n
to a . Second, based on the featural values , the is
feature extraction F motion category M
obtained by means of —comparable to a .
f feature based categorization
K −→ F −→ M
(7.5)
Φ f
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) F = (ϕ , . . . , ϕ ) M
k k l l 1 n i
(cid:55)−→ (cid:55)−→
(Eq. (7.1))
7.1. THE CATEGORIZATION PROCESS: FORMALIZATION 87
Summing up, the decomposition of the categorization rule, , as shown in Equation (7.5),
originates new elements: the , the , the
features extraction function Φ featural variables F featural
, and the . Below, we list these additional elements, we de-
space featural categorization rule f
fine them explaining how we obtain them from the categorization rule, and briefly illustrate them
with the motion categorizations QTC and Stories-RCC—one of the motion categorizations
B21
derived in this work (Sect. 9.2.1). Later in Section 7.3 we provide a full detailed example in which
we apply our categorization framework—and concretely this decomposition of the categorization
rule—to the QTC motion categorization.
B21
i. The ‘ ’ (or, simply, ‘ ’) are the set of independent variables,
featural variables features smallest
, that determine the category for any given scenario. The featural variables
F = (ϕ , . . . , ϕ )
1 n
must be obtained as function of the kinematic variables, i.e., ; we
i ϕ = ϕ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
i i k k l l
obtain them by inspection of .
When we describe the motion categorization QTC according to our
Example 7.1.i
B21
framework (Sect. 7.3), we obtain two featural variables,
ϕ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = cos( ((cid:126)x
k k k l l l
and , which are the cosine of the relative
(cid:126)x , (cid:126)v )) ϕ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = cos( ((cid:126)x (cid:126)x , (cid:126)v ))
k k l k k l l k l l
motion angles of the entities, and (Fig. 7.2, also see
γ = ((cid:126)x (cid:126)x , (cid:126)v ) γ = ((cid:126)x (cid:126)x , (cid:126)v )
∠ ∠
k l k k l k l l
− −
Eqs. 7.12). In short, the featural variables areA .
F = (ϕ , ϕ )
k l
The categorization Stories-RCC is determined by two featural variables,
Example 7.2.i
and , i.e., where and . The first variable,
d dif F = (ϕ , ϕ ) ϕ = d ϕ = dif d ((cid:126)x , (cid:126)v ;
min V 1 2 1 min 2 V min k k
, is the minimum distance between entities along
(cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x det((cid:126)x (cid:126)x , (cid:126)v (cid:126)v ) r
l l l k l R k l k
(cid:107) − (cid:107)| − − |
a uniform trajectory; the second one, (cid:107)(cid:126)v −(cid:126)v (cid:107) , is real value in
dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = l k g [0, 1]
V k k l l
(cid:107)(cid:126)v (cid:107)+(cid:107)(cid:126)v (cid:107) 3
k l
telling how different are the velocities (‘ ’ if they are equal, , and ‘ ’ if they are
0 (cid:126)v = (cid:126)v 1
k l
opposite ) (Eq. (9.3)).
(cid:126)v = α (cid:126)v r
k l
−| |
D .
ii. The ‘ ’ , is the real coordinate space generated bynthe .
featural space featural variables F
The featural variables of QTC (Ex. 7.1.i), generate the featural space
Example 7.1.ii
B21
(See Fig. 7.4), because they are cosine values.
= [ 1, 1] [ 1, 1] o
F − × − c
The featural variables of Stories-RCC (Ex. 7.2.i), generate following
Example 7.2.ii i
featural space , because the first variable is a distance, and the second
= [0, + ) [0, 1]
F ∞ ×
a speeds ratio. t
iii. The ‘ ’ , yields the values of the features for
features extractionpfunction Φ F = (ϕ , . . . , ϕ )
1 n
each motion scenario .
t K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
Φ :
(7.6)
K −→ F
K F = (ϕ , . . . , ϕ )
1 n
(cid:55)−→
The featural variables of QTC , and , along with their formulae
Example 7.1.iii ϕ ϕ
B21 k l
define the features extraction function of QTC as following (Eqs. 7.12):
B21
(cid:0) (cid:1)
(ϕ , ϕ ) = Φ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = cos( ((cid:126)x (cid:126)x , (cid:126)v )), cos( ((cid:126)x (cid:126)x , (cid:126)v ))
∠ ∠
k l k k l l l k k k l l
− −
The featural variables of Stories-RCC, and
Example 7.2.iii d ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) dif ((cid:126)x , (cid:126)v ;
min k k l l V k k
(Ex. 7.2.i), along with their formulae define the features extraction function of
(cid:126)x , (cid:126)v )
l l
Stories-RCC as following (See Eq. (9.4)):
(ϕ , ϕ ) = Φ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (d ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ), dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ))
1 2 k k l l min k k l l V k k l l
88 CHAPTER 7. A CATEGORIZATION FRAMEWORK
(cid:126)v
(cid:126)v
In this motion scenario, we show the angles and , which fully determine the
k l
scenario categorization in QTC through their cosine, and (Ex. 7.1.i)
ϕ = cos(γ ) ϕ = cos(γ )
B21 k k l T l
iv. The ‘ ’ maps the feature values into the
featural categorization rule f F = (ϕ , . . . , ϕ )
Φ 1 n
corresponding category .
M F
f :
Φ (7.7)
F −→ M
F = (ϕ , . . . , ϕ ) M
1 n i
(cid:55)−→
Here we show the featural categorization rule of QTC . The cate-
Example 7.1.iv
B12 4
gorization rule partitions the featural space in the categorical regions, as werwill see in
 /
if
M ϕ > 0, ϕ > 0
 1 k l
 if r
M ϕ > 0, ϕ = 0
 o
 2 k l
D   if .
M ϕ > 0s, ϕ < 0
 3 k l
 if
M ϕo= 0, ϕ > 0
 4 k l
if
f (ϕ , ϕ ) := M ϕ = 0, ϕ = 0
Φ k l 5 k l
 if
M ϕ = 0, ϕ < 0
 o 6 k l
 if
cM ϕ < 0, ϕ > 0
 7 k l
i if
M ϕ < 0, ϕ = 0
 8 k l
e 
 if
M ϕ < 0, ϕ < 0
9 k l
In/Section 9.2.1, we obtain the featural categorization rule, , of the
Example 7.2.iv f
: Φ
Stories-RCC categorization (which we later call ). Recognizably, the domain variables
p σ
of are the featural variables, . Here, we show only a part of
f F = (ϕ , ϕ ) = (d , dif )
Φ 1 2 min V
its image values: the categories , , , , , and .
S S S S S S
01 02 03 11 12 13
if
S d > d , dif = 0
01 min 2 V
if
S d = d , dif = 0
02 min 2 V
if
S d > d > d , dif = 0
03 2 min 4 V
 . .
. .
 . .
f (d , dif ) :=
Φ min V if
S d > d , dif > 0
 11 min 2 V
 if
S d = d , dif > 0
 12 min 2 V
 if
S d > d > d , dif > 0
 13 2 min 4 V
. .
 . .
 . .
where and , being and the radii of the entities.
d = r + r d = r r r r
2 k l 4 k l k l
| | | − |
7.1. THE CATEGORIZATION PROCESS: FORMALIZATION 89
7.1.3 Properties of Features and Definition of Dissimilarity
Here we define decisive properties of the features in our framework: and
lower dimensionality
. We devote a section to it because features are central to categorization. Features
self-density
allow us to comprehend, to conceptualize a categorization: Roughly, they are the concepts
on which a categorization bases (See Sect. 12.7.2). Furthermore, features are fundamental to
compute dissimilarity (Sect. 4.4.1).
Additionally, as a by-effect, the property of helps us to extract the features,
self-density
i.e., the featural variables. Certainly, the process of extracting the featural variables from the
categorization rule (item i., Sect. 7.1.2) is somewhat ambiguous: we can choose diverse sets,
or combinations, of featural variables which fulfil the requirement of ; by enforcing
minimality
, we reduce the ambiguity in the choice of featural variables.
self-density
The has, by definition, lower or equal dimensionality
Lower Dimensionality featural space
than the , i.e., . In fact, the eqFuality hardly ever holds,
kinematic space dim( ) dim( )
K F ≤ K
because the featural space is always a simplification of the kinematic space—the featural space
focuses only in certain aspects of the motion—that is, generally, .
dim( ) < dim( )
F K
As an illustration, consider the featural space Stories-RCC, which consists only of two real
parameters (Ex. 7.2.i); it has much lower dimensionality than the kinematic space, , which
consists of real parameters for the simple case of two entities moving in the plane. Therefore,
in most cases, the featural space has much lower dimensionality than the kinematic space, i.e.,
dim( ) dim( ) r
F (cid:28) K
Because of its much lower dimensionality, the allows a more direct under-
featural space g
F 3
standing of the categorization than the does; the numerous dimensions of
kinematic space
K g
(normally (Sect. 3.1)) make this space hard—de facto impossible—to visualize
dim( ) 8
K K ≥ o
and, thus, to treat intuitively. Furthermore, in most cases, the coordinates in , the ‘
featural
D .
’, correspond to , such as distances or angles.
coordinates natural kinematic concepts n
The featural space should have two additional characteristics that we summa-
Self-Density
rize in the property ‘ ’: it should be a metric space, and it should be .
self-density c dense-in-itself
The very purpose of this property is to endow the featural space with a
i dissimilarity function
, and, thus, to obtain a dissimilarity function in the kinematic space, —which
D(F , F ) D(K , K )
1 2 1 2
is directly related to obtaining a similartity function (See begin Sect. 4.4).
If the featural space has a metric , we can measure distances between featural points
d F
: F 1
and , i.e., , as we do in Section 7.3—Note that once we have a distance, we have
F d (F , F ) p
2 F 1 2
automatically a (Sect. 4.4). The featural distance, , results in a distance between
dissimiltarity d
scenarios, , by mapping the scenarios into the featural space through the features extraction
function . In other words, we extract the features of each scenario, = ( ), and, then, we
Φ F Φ K
i i
measure the featural distance between the extracted features.
(7.8)
d (K , K ) = d (Φ(K ), Φ(K )) = d (F , F )
K 1 2 F 1 2 F 1 2
A featural space with a metric is, additionally, when the featural variables are
dense-in-itself
values—that is, intervals of R or Q values—instead of, for instance, boolean or
continuum-like
discrete values. As an illustration, consider a featural space with two featural variables: , which
takes values in and , which takes values in ; both take values in an interval—such
[ 3, 3] ϕ [0, 2]
featural space is dense. In contrast, a featural space with the variable which takes discrete
values and , which takes discrete values is non-dense because we have discrete
3, 3 ϕ 0, 2
{− } { }
featural values.
90 CHAPTER 7. A CATEGORIZATION FRAMEWORK
A crucial consequence is following: a non-dense featural space is less appropriate to describe
human categorization. The distances and the corresponding dissimilarities in a non-dense space
lack a gradation effect, as expected in human categorization (See Sect. 4.3.2). By contrast, in a
dense space, we have a sort of continuum of distances, that is, a gradation.
In conclusion, since we have a certain flexibility in choosing the featural variables, we should
strive to define the featural variables as continuum values. This is the striving in our story-based
categorizations (e.g., Sect. 9.2). For example, for Stories-RCC we have chosen the featural
variable (cid:107)(cid:126)v −(cid:126)v (cid:107) with values in the interval , although we could have chosen the
dif = l k [0, 1]
(cid:107)(cid:126)v (cid:107)+(cid:107)(cid:126)v (cid:107)
k l
discrete variable if if . In the latter case, the featural variable would
dif = 0 (cid:126)v = (cid:126)v ; 1 (cid:126)v = (cid:126)v
V k l k l
{ (cid:54) }
not be , because it takes few discrete values, and it would separate scenarios into just two
dense
groups according to the or value: within-category gradation would be lost for such a variable.
0 1
7.2 Link to Psychological and Philosophical Terms
Our framework reproduces basic categorization principles in human cognition. Indeed, as men-
tioned in Section 4.4.1, p. 39, all models of categorization and concept acquisition lie on the
; our framework contains also a feature extraction by means of the function
feature extraction
(item iii., Sect. 7.1.2). In addition, as mentioned in Section 4.4, p. 37, many models base on
similarity; in our framework, we can create a similarity measure based on the featural distance
(Eq. (7.8)).
Consequently, we can link the elements in our framework to terms and properties in human
categorization, as we do in the following.
We identify the in with the ‘ ’, as defined in the
Features featural variables features APA
F o
, p. 370 (2007) “[A]ttribute of an object or event that plays an important
Dictionary of Psychology
D .
role in distinguishing it from other objects or events and in thenformation of category judgements.”
Whereas the and all other non-featural variables are simply ‘ ’,
kinematic variables m attributes
the are a very particular type of : they univocally determine the
featural variables m attributes
category. Still, alternative terminology is usedoin the literature, which we mention for the sake
of completeness: often researchers use thse term ‘feature’ to generally refer to ‘attribute’ (e.g.,
Colman 2015, pp. 812f.); because of this, they distinguish the relevant features by adding an
adjective such as ‘ ’, ‘ ’ (Hampton 1979), or ‘ ’ (Goldstone 1996),
characteristic defining diagnostic
e.g., ‘diagnostic features’.
Our framework stresses the feature extraction, , as an intermediate step in categorization.
Even though we do not lie on the features from start—They are subsequently extracted—we can
rightly call ouhr framework a ‘ ’. This might be seen as a limitation, or even
featural framework
as opposed to ‘ ’ methods, but truth is that modern neural networks, e.g., deep
neural networks
learning, strive to automate the finding of (Skansi 2018, p. 55), and, thus, the quest
features
after features seems a constant in categorization research (e.g., Sect. 4.4.1 and Fig. 4.2).
In our framework, we can implement a simi-
Similarity function and Psychological Space
larity function between motion scenarios, , because the featural space is a metric space:
S(K , K )
1 2
we can directly transform the distance function in the featural space into a dissimilarity, or into
a similarity. In addition, such similarity is plausible in human cognition because it is graded,
due to the self-density of the featural space.
Furthermore, we can readily identify the with the because it
featural space psychological space
is in the featural space where the dissimilarities are computed (Sect. 4.4.1.A, p. 41). We see this
clearly in Equation (7.8): the distance between scenarios, , is effectively computed in
d (K , K )
K 1 2
7.3. EXAMPLE: SCENARIO CATEGORIZATION BY QTC 91
B21
the featural space, , after the features are extracted and . In
d (ϕ , ϕ ) ϕ = Φ(K ) ϕ = Φ(K )
F 1 2 1 1 2 2
other words, scenarios with exactly the same featural values are categorized as “identical”.
The categorization rule —or rather its featural version
Standard Categorization Models f
—reveals to us which standard categorization model is at work. By inspecting the categoriza-
tion rule, we can see whether the categorization implements a rule-based model (i.e., a boundary
model, Sect. 4.5) or a similarity-based model (i.e., a metric model, Sect. 4.4.2); moreover, we can
see whether the similarity model requires exemplars or category prototypes, or maybe clustering
is performed.
Regarding philosophical terms, we already mentioned above how the and
intensional exten-
types of concept definitions are expressed respectively by each definition of categorization:
sional
given the categorization rule (Eq. (7.2)) and given the categorical regions
(f , ) ( K , . . . ,
µ 1
M {
(Eq. (7.3)).
K , )
} M
7.3 Example: Scenario Categorization by QTC
B21
We illustrate the definitions of the previous section through the well-known and simple motion
categorization QTC (Van De Weghe et al. 2006). Although it is already briefly defined in
B21
Sect. 6.4, we offer below a more extensive definition.
In QTC , each motion category is a 2-tuple , where each coefficient, , can take
( , )
B21 k l ∗
P P P
one of three symbols, . Accordingly, we have a total of 9 categories that we can alter-
+, 0, r
{ −}
natively name using the coefficients or the standard terms for motion categories (item i.,
M g
∗ i
P 3
Sect. 7.1.2), e.g., or .
( , ) M
− − g
= ( , ) o
QTC k l
M D B21 { P P } .
(7.9)
M M M M M M M n M M
1 2 3 4 5 6 7 8 9
= ( , ), ( , 0), ( , +), (0, ), (0, 0), (0, +), (+o, ), (+, 0), (+, +)
{ − − − − − m − }
The coefficients and are determined according to following rules:
k l
P P
• is the relative motion of the entity regardcing still.
k l
k s
if moves towards s
k l
if keeps distance to
0 k l
if moves away from
+ k l /
• is the relative motion of the entity regarding still:
l k
P t
if moves thowards
l k
if keeps distance to
0 l k
if moves away from
+ l k
Categorization rule f
Based on the above definitions, we can explicitly define the categorization rule .
f :
µ QTC
B21
K −→ M
(cid:126)v ((cid:126)x (cid:126)x ) (cid:126)v ((cid:126)x (cid:126)x )
k l k l k l (7.10)
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) M = (Sign( · − ), Sign( · − ))
k k l l i
(cid:55)−→ − (cid:126)v (cid:126)x (cid:126)x − (cid:126)v (cid:126)x (cid:126)x
k l k l k l
(cid:107) (cid:107)(cid:107) − (cid:107) (cid:107) (cid:107)(cid:107) − (cid:107)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
P P
k l
92 CHAPTER 7. A CATEGORIZATION FRAMEWORK
(cid:126)v
(cid:126)v
k l
In this motion scenario, we show the angles and , which fully determine
k l
the scenario’s categorization in QTC through their cosine, and (Eqs. 7.12
cos(γ ) cos(γ )
B21 k l
and Eq. (7.10))
where is the ‘ ’; it outputs a character.
Sign(x) sign symbol function
 F
if
‘ ’ x < 0
if (7.11)
Sign(x) = ‘0’ x = 0
 if
‘+’ x > 0
Feature extraction Φ and feature categorization f
The categorization rule in Equation (7.10) becomes more graspable, if we decompose it into
the feature extraction and the feature categorization , i.e., , according 4to Equa-
Φ f f = f Φ
Φ µ Φ r
tion (7.5).
By inspecting we discern the possible featural variables and 3(Eqs. 7.12), which
f ϕ ϕ
µ k l
correspond to cosine values of the relative motion angles ( and ) of the entities and .
γ γ g k l
k l
D .
(cid:126)v ((cid:126)x (cid:126)x )
k l k o (7.12a)
ϕ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) := · − = cos( ((cid:126)x (cid:126)x , (cid:126)v )) = cos(γ )
k k k l l m l k k k
(cid:126)v (cid:126)x (cid:126)x −
k l k
(cid:107) (cid:107)(cid:107) − (cid:107)
(cid:126)v ((cid:126)x (cid:126)x )
l k l (7.12b)
ϕ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) := · − = cos( ((cid:126)x (cid:126)x , (cid:126)v )) = cos(γ )
c ∠
l k k l l k l l l
(cid:126)v (cid:126)x s (cid:126)x −
l k l
(cid:107) (cid:107)(cid:107) − (cid:107)
These features, and , fulfil the aforementioned three main conditions for featural values
ϕ ϕ
k l
(item i., Sect. 7.1.2 and Sect. t7.1.3): they are the smallest set of independent variables that
determine the categorizati/on; the dimensionality of the features space, , is much
dim( ) = 2
s F
lower than the dimensionality of the kinematic space, ; they correspond to natural
p dim( ) = 8
t K
kinematic concepts, and they are self-dense (they are real values).
Now, we define the , , which maps every motion scenario into the
feature extraction function Φ
QTC featural space, , as follows.
B21 QTC
B21
Φ = [ 1, 1] [ 1, 1]
QTC (7.13)
B21
K −→ F − × −
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) (ϕ , ϕ )
k k l l k l
(cid:55)−→
with and defined, respectively, in Equations (7.12a) and (7.12b)
ϕ ϕ
k l
As last step, we obtain the , , which yields the categories
feature categorization function f
based on the features and . We deduce the following function by filling the gap in
ϕ ϕ f
k l Φ
Equation (7.13) that allow us to obtain Equation (7.10). We can visualize this function, , in
7.3. EXAMPLE: SCENARIO CATEGORIZATION BY QTC 93
B21
1.0
0.5
0.0
0.5
− o
D .
1.0
1.0 0.5 0.0 0.5 1.0
− −
Categorical Space of QTC
B21
For two entities k and l
QTC Categories
B21
M : ( , )
− −
M : ( , 0)
(+, ) ( , ) 2
− − −
M : ( , +)
M : (0, )
M : (0, 0)
M : (0, +)
M : (+, )
(+, +) ( , +)
M : (+, 0)
− 8
M : (+, +)
Featural space of QTC , , which is constituted by two variables
B21 QTC k
B21
and (Eq. (7.14)). The different coloured regions correspond to the QTC
cos(γ ) ϕ = cos(γ )
k l l B21
categories; i.e., they are . They have different dimensionality; for example,
categorical regions M
is 0-dimensional, is 1-dimensional, and is 2-dimensional.
M M
6 7
94 CHAPTER 7. A CATEGORIZATION FRAMEWORK
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
f =f ◦ Φ
µ Φ
(ϕ , ϕ ) M = ( , )
k l i k l
P P
2 2
= [ 1, 1] = , 0, +
QTC QTC
B21 B21
F − M {− }
This diagram is a particular case of Equation (7.5) applied to QTC . The cat-
B12
egorization rule categorizes a motion scenario into a QTC category
f ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ( , )
µ k k l l B12 k l
P P
(Eq. (7.10)). The categorization process is decomposed in, first, the extraction of features
, by (Eq. (7.13)), and the subsequent feature-based classification, by (Eq. (7.14)).
(ϕ , ϕ ) Φ f
k l F Φ
f = , 0, +
Φ QTC QTC (7.14)
B21 B21
F −→ M {− }
(ϕ , ϕ ) M = ( , ) = (Sign( ϕ ), Sign( ϕ ))
k l i k l k l
(cid:55)−→ P P − −
with defined in Equation (7.11) r
R Sign
At last, we have the main ingredients of a categorization: the , , and its
categorization rule f
resolution into , , and , . Thgese three functions are
feature extraction Φ feature categorization f
Φ r
D . B21
representation.
Notice how in describing QTC according to oum r framework, we have proceeded in a
B21
way (Fig. 7.1). As in any other qualitative representation of motion, QTC was
top-down
B21
defined without explicit mention of features or featural space; nevertheless, these were implicit
in the definition, and our framework brings them to light.
7.4 Testing Dissimilarities and Traditional Models
In this section, we show further capabilities of our categorization framework. We apply it to test
three dissimilarity measures and two traditional categorization models for motion scenarios—We
use as example the motion categorization QTC , which we described in Section 7.3. Concerning
B12
the dissimilarity measures, we test in which representation space the dissimilarity best captures
the properties of a categorization, either in the ( ), or in the ( ),
kinematic space featural space
K F
or in the ( ). Further, using the fittest dissimilarity, we test which traditional
categorical space
model, either or (Sects. 4.4.2 and 4.5), is more suitable to describe such
prototype boundary
motion categorization. Keep in mind that the results, in terms of , can be directly
dissimilarities
translated in terms of (as shown in Tab. 4.1).
similarities
Our prime aim is not to perform an in-depth analysis of all possible dissimilarity measures,
but to outline how dissimilarity measures behave. For that reason, we drastically restrict the
number of analyzed scenarios and dissimilarity measures: dissimilarity measures and motion
3 9
scenarios. The chosen dissimilarities and scenarios are representative enough to show common
issues, which can be generalized to categorizations other than categorization of motion scenarios.
7.4. TESTING DISSIMILARITIES AND TRADITIONAL MODELS 95
y y y
K K K
a1 a2 a3
x x x
( , )
k l k l k l
− −
y y y
K K K
b1 b2 b3
x x x
(0, 0)
k l k l k l
y y y
K K K
c1 c2 c3
9 4
x x x
(+, +) R
k l k l k l u
Motion scenarios that we use to analyze the three different dissimilarity measures
D . K
B12 s a∗
category , i.e., , to category , i.e., , and nto , i.e., . The
M ( , ) K M (0, 0) K M (+, +)
1 b∗ 3 oc∗ 9
− −
kinematic space s featural space
i QTC QTC
(cid:0) K (cid:1) s F B21 M B21
, ; , =
(cid:126)x (cid:126)v (cid:126)x (cid:126)v e (ϕ , ϕ ) (cos(γ ), cos(γ )) ( , )
k k l l k l k l k l
P P
(cid:0) t(cid:1)
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 70 ) (1.0, 125 ) (0.34, 0.57) (cos(70 ), cos( 55 )) M = ( , )
a1 1
(cid:0) / (cid:1) − − −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 0 ) (1.0, 180 ) (1.00, 1.00) (cos(0 ), cos(0 )) M = ( , )
a2 1
(cid:0) (cid:1) − −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 45 ) (1.0, 225 ) (0.71, 0.71) (cos( 45 ), cos(45 )) M = ( , )
a3 1
(cid:0) − t (cid:1) − − −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 90 ) t (1.0, 90 ) (0.00, 0.00) (cos(90 ), cos( 90 )) M = (0, 0)
b1 h 5
(cid:0) (cid:1) −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 90 ) (1.0, 90 ) (0.00, 0.00) (cos(90 ), cos(90 )) M = (0, 0)
b2 5
(cid:0) − (cid:1)
, ° ; (2,0), ° = ° °
K (0, 0) (0.6, 90 ) (1.0, 90 ) (0.00, 0.00) (cos( 90 ), cos(90 )) M = (0, 0)
b3 5
(cid:0) − − (cid:1) −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 110 ) (1.2, 70 ) ( 0.34, 0.34) (cos(110 ), cos( 110 )) M = (+, +)
c1 9
(cid:0) (cid:1) − − −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 180 ) (1.0, 0 ) ( 1.0, 1.0) (cos(180 ), cos(180 )) M = (+, +)
c2 9
(cid:0) (cid:1) − −
, ° ; (2,0), ° = ° °
K (0, 0) (1.0, 225 ) (0.6, 45 ) ( 0.71, 0.71) (cos( 135 ), cos(135 )) M = (+, +)
c3 9
− − − −
according to the QTC categorization (Eq. (7.9)). The first column, ‘ kinematic space’,
B21
displays the kinematic data (positions and velocities ); the length units are not specified,
(cid:126)x (cid:126)v
∗ ∗
one unit corresponds to the separation between grid lines at Fig. 7.6; the velocities are expressed
in polar coordinates . The featural coordinates are those obtained through
(speed, angle) ϕ
Eqs. 7.12. The categories are determined through the categorization rule in Equation (7.10)
96 CHAPTER 7. A CATEGORIZATION FRAMEWORK
We have chosen scenarios, , arrayed in three groups according to their QTC categories:
9 K
∗ B21
the group belongs to category , belongs to , and to (Fig. 7.6
K ( , ) K (0, 0) K (+, +)
a b c
− −
and Tab. 7.2). We compare all scenarios pairwise using three different dissimilarity measures: ,
, and (Eqs. 7.15). For each measure, we use the Euclidean norm, i.e.,
(cid:0)(cid:80)
(cid:1)1/2
d d (cid:126)x = x
F M 2 i
i=1
(cid:107) (cid:107)
to compute the dissimilarities. Another possibility would be to use only for , because
2 K
(cid:107) · (cid:107)
many features are (they cannot exist independently of one another); while we could use
integral
for and , because the features are (See Sect. 4.4.1.A). However, in our
d d separable
1 F M
(cid:107) · (cid:107)
example, the results are qualitatively the same independently of the norm used. We choose the
same norm for all measures, Euclidean, so that the difference lies only in the chosen coordinates.
In that way, we can better appreciate how important is the choice of the coordinate space (i.e.,
kinematic, featural, or categorical) to appropriately model the categorization. T
The dissimilarity measures are defined as follows:
(cid:2) (cid:3)1/2
2 2 2 2 (7.15a)
d (K , K ) = ((cid:126)x (cid:126)x ) + ((cid:126)v (cid:126)v ) + ((cid:126)x (cid:126)x ) + ((cid:126)v (cid:126)v )
K A B kB kA kB kA lB lA lB lA
− − − F −
(cid:2) (cid:3)1/2
2 2 (7.15b)
d (K , K ) = (ϕ ϕ ) + (ϕ ϕ )
F A B kB kA lB lA
− −
(cid:2) (cid:3)1/2
2 2 (7.15c)
d (K , K ) = (p p ) + (p p )
M A B kB kA lB lA
− −
The parameters used in Equations 7.15 mean following:
(a) and are, respectively, the of
((cid:126)x , (cid:126)v , (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v , (cid:126)x , (cid:126)v ) kinematic coordinates
kA kA lA lA kB kB lB lB
scenario and , that is, their coordinates in the kinematic space . 4
K K
A B
R K
(b) and are, respectively, the of scenagrio and ,
(ϕ , ϕ ) (ϕ , ϕ ) featural coordinates K K
kA lA kB lB 3 A B
that is, their coordinates in the featural space . They are computed from the kinematic
coordinates via Equations 7.12.
D .
(c) and are the of the scenarios and , that
(p , p ) (p , p ) categorical coordinates n K K
kA lA kB lB A B
is, the values that univocally determine the QTC category of the scenarios,
1, 0, +1
m B21
{− }
(Eq. (7.9)). For example, if scenario belongs to the category , then
( , ) K ( , +)
k l A
P P −
; and if scenario belongs to the category , then
(p , p ) = ( 1, 1) K o (+, 0) (p , p ) =
kA lA B kB lB
− c
(+1, 0)
7.4.1 Dissimilarity results: analysis
In order to analyze the dissimilarity values (Tab. 7.3) of the test scenarios (Tab. 7.2 and Fig. 7.6),
we regard as benchmpark some desirable and plausible properties for dissimilarities, which we
derive from ourtexposition in Section 4.3—More accurately, there, we mentioned the properties
of similarity in human categorization, which, here, we translate into properties for dissimilarities
(items I. to III.). A dissimilarity having such properties should suitably reflect both the inner
and the outer category structure in human categorization.
I. ( ) The lowest dissimilarity of an item should be the self-dissimilarity (Compare
minimality
with item ii. in distance axioms).
II. (‘ ’) The average within-category dissimilarity should be lower than the
family resemblance
average between-category dissimilarity (Sect. 4.3.2.B).
III. ( ) If we assume that the categories are represented by prototypes fol-
prototypes adequacy
lowing properties should apply:
A. The protypes are the central (sort of average) items of a category (Sect. 4.3.1).
7.4. TESTING DISSIMILARITIES AND TRADITIONAL MODELS 97
d (K , K ) K K K K K K K K K
K A B a1 a2 a3 b1 b2 b3 c1 c2 c3
0.00 1.75 2.81 0.70 1.94 2.91 1.24 2.59 3.01
a1
0.00 1.29 2.24 2.24 1.93 2.61 2.83 2.47
a2
0.00 3.12 2.63 1.05 3.34 2.71 1.83
a3
0.00 2.00 3.02 0.56 2.24 2.92
b1
0.00 2.26 2.20 2.24 2.62
b2
0.00 3.09 1.93 1.02
b3
0.00 1.95 2.82
c1
0.00 1.26
c2
0.00
c3
(a) Dissimilarity matrix between scenarios obtained from d (K , K ), which is the Euclidean norm applied to the
K A B
kinematic coordinates of the scenarios K and K (Eq. (7.15a)).
A B
d (K , K ) K K K K K K K K K
F A B a1 a2 a3 b1 b2 b3 c1 c2 c3
0.00 0.78 0.39 0.67 0.67 0.67 1.14 2.07 1.66
a1
0.00 0.41 1.41 1.41 1.41 1.90 2.83 2.41
a2
0.00 1.00 1.00 1.00 1.48 2.41 2.00
a3
0.00 0.00 0.00 0.48 1.41 1.00
b1
0.00 0.00 0.48 1.41 1.00
b2
0.00 0.48 1.41 1.00 r
K R
b3
0.00 0.93 0.52 g
c1 3
0.00 0.41/
c2 g
0.00
K r
c3 o
D .
(b) Dissimilarity matrix between scenarios obtained from d (K , K ), which is the Euclidean norm applied to the
F A B n
featural coordinates of the scenarios K and K (Eq. (7.15b)).
A B o
d (K , K ) K K K K Ko K K K K
M A B a1 a2 a3 b1 b2 b3 c1 c2 c3
0.00 0.00 0.00 1.41 1.41 1.41 2.83 2.83 2.83
a1
0.00 0.00e 1.41 1.41 1.41 2.83 2.83 2.83
a2 h
0.00 1.41 1.41 1.41 2.83 2.83 2.83
K t
a3 /
0.00 0.00 0.00 1.41 1.41 1.41
b1
s 0.00 0.00 1.41 1.41 1.41
b2
t 0.00 1.41 1.41 1.41
b3
0.00 0.00 0.00
c1
0.00 0.00
c2
0.00
c3
(c) Dissimilarity matrix between scenarios obtained from d (K , K ), the Euclidean norm applied to the values
M A B
(p , p ), of the scenarios K and K (Eq. (7.15c)).
k l A B
Dissimilarity matrices that pairwise relate the 9 scenarios in Items I. to III.. In
each matrix a different dissimilarity measure is used: all dissimilarities are Euclidean, but each
is computed in a different space. The level of dissimilarity is proportional to the blue saturation
of each cell: lowest dissimilarity (0.00) corresponds to white cells and highest (depending on the
measure) corresponds to blue saturated cells. The brown coloured rectangles are
within-category
dissimilarities; the rest are dissimilarities.
between-category
98 CHAPTER 7. A CATEGORIZATION FRAMEWORK
B. A prototype is, in average, least dissimilar to the items within the category, and overall
most dissimilar to the items in other categories—This is a kind of
family resemblance
property for prototypes.
C. An item’s category is determined by the least dissimilar prototype to such item
(Sect. 4.4.2). Equivalently, the category members should be less dissimilar similar
to the prototypes in their category than to those in other categories.
In the Tables 7.3a to 7.3c, we observe how all dissimilarities fulfil the first property,
Minimality
. By definition all self-dissimilarities are zero, which is the minimum possible value.
minimality
We test the second property— —aided by the brown
Family resemblance family resemblance
rectangles at the tables. The rectangles contain the within-category dissimilarities, i.e., dissim-
ilarities between scenarios belonging to the same category. For that reason, the dissimilarities
inside the brown rectangles should be lower than those outside them.
In the dissimilarity (Tab. 7.3a), the values inside the rectangles are not particularly low
in whatever measure we take (mean or median). Actually, by visual inspection, the kinematic
dissimilarities do not seem to follow any particular pattern related to category membership,
which makes an entirely useless dissimilarity with regard to the QTC categorization. This
K B12
becomes more evident, if we notice that in our examples (Fig. 7.6), the only contributions to
arise from differences in the velocity vectors—since the positions of and are the same in each
k l
scenario—and, hence, by modifying the entities’ positions alone, we could obtain anyrarbitrarily
high dissimilarity values between scenarios.
On the other hand, Tables 7.3b and 7.3c show family resemblance: we observe a clearly low
within-category and high between-category dissimilarity, though with some objections.
First, we realize that the categorical dissimilarity, , discerns no structure;
D d . intra-categorical
M s
the differences of are only due to the different categoricalnmembership, while all items of the
same category have the same dissimilarity. For example, all scenarios within the same category
have , and, between all scenarios of categorym and category there is the same
d = 0.0 ( , ) (+, +)
− −
dissimilarity, . Thus, in practice, ois a dissimilarity between categories not between
d = 2.83 d
M cM
items; it is only a little finer than the ‘ s ’ between categories which assigns
discrete dissimilarity 1.0
to items of different categories and to items of the same category. The dissimilarity takes
0.e0 d
advantage of the fact that QTC is a concatenated representation, i.e., QTC ,
t =
B12 B12 k l
P × P
so that reflects the between-category structure provided by the Cartesian product, but it
d /
obliterates all trace of swithin-category structure.
Second, the dissimilarity presents a marked effect of family resemblance. In fact, if we
t F
take has threshold, the rule correctly classifies most pair of scenarios,
τ = 1.00 d (K , K ) < τ
F A B
F F
, as belonging to the same category. Even more, any scenario but scenario belongs
(K , K ) K
A B c1
to the same category of the scenario with which it has lowest ; in other words,
If and belong to the same category
d (K , K ) < d (K , K ) K = K K K
F A B F A C C B A B
∀ (cid:54) ⇒
In this sense, would be cognitively the most adequate of all similarities but with a caveat: its
family resemblance is imperfect. For example, family resemblance is flagrantly violated by the
scenario . This scenario—which according to the categorization rule (Eq. (7.10)) belongs to
c1
the category , i.e., —has lower dissimilarity, , to all scenarios in category ,
M (+, +) d = 0.48 M
9 F 5
i.e., , than to those in its own category, and .
(0, 0) d (K , K ) = 0.52 d (K , K ) = 0.93
F c1 c3 F c1 c2
Family resemblance is deficient in QTC , because this categorization is defined as a bound-
B12
ary model in which the borders are categories: category is the border between categories
(0, 0)
7.4. TESTING DISSIMILARITIES AND TRADITIONAL MODELS 99
and . For that reason, the scenarios of category that are closer to the
( , ) (+, +) (+, +)
− −
—that is, those scenarios of with , i.e.,
boundary category (0, 0) (+, +) (ϕ > 0.5, ϕ > 0.5)
k l
− −
° ° ° ° —are in average more similar to the scenarios in
( 120 < γ < 120 , 120 < γ < 120 ) (0, 0)
k l
− −
than to those in its own category, . An analogous effect occurs with category .
(+, +) ( , )
− −
If QTC motion categorization were suitably modelled by proto-
Prototypes adequacy
B12
types, the properties recounted in Item III. should apply. However, a prototype model cannot
fully account for its category structure: The deficiencies that arose when we tested family re-
semblance reverberate throughout the testing of the .
prototypes adequacy
For a start, the categorical distance, (Tab. 7.3c), cannot define prototypes because it
displays no inner structure in the categories; only the featural distance remains as possible
distance for defining the prototypes. Accordingly, the prototypes in the featural space corre-
spond to the scenarios (i.e., , , and ); certainly, we observe that those scenarios
K K K K
∗3 a3 b3 c3
have the lowest dissimilarity to the other scenarios in their own category (Tab. 7.3b)—As
side note, the scenarios of the category are indistinguishable in the featural space: any of
(0, 0)
them have the same featural distance to any otAher scenario; we have chosen for ease of
d K
F b3
exposition.
Therefore, by definition, the prototypical scenarios, , fulfil the centrality property (item III.A.),
∗3
and, thus, have the lowest within-category dissimilarity; this is the first condition of Item III.B.4.
However, they fail to fulfil the second condition of Item III.B., namely, that the prototypesuhave
the highest between-category dissimilarity. Indeed, the highest between-category dissimilarity is
given by the scenarios: in category , and by in category —As said
K K ( , ) K (g+, +)
∗2 a2 c2
− − r
before, in the border category all scenarios have the same between-category dissimilarity,
(0, 0) o
D .
so we can choose . Sadly, we find no scenarios that completely fulfil sItem III.B.; we stick in a
b2 n
trade-off: if we increase the between-category disimilarity, the withoin-category dissimilarity will
also increase, so that we cannot increase one, while we decrease the other to find an optimum.
On top of that, the prototypes fail to fully account for category membership (item III.C.),
which is a primary goal of the prototype theory. For example, the item has a lower dis-
s K
c1
similarity to the prototype in the categorsy —what is more, to any member of such
d (0, 0)
category—than to any possible prototype in its own category , i.e., to or . Ac-
(+, +) K K
t c2 c3
tually, all members of category with featural coordinates , i.e.,
(+, +) (ϕ > 0.34, ϕ > 0.34)
/ k l
− −
° ° ° °:, have a higher dissimilarity to any of their possible prototypes
( 70 > γ > 70 , 70 > γ > 70 )
k l
− − p
than to member of the category .
any (0, 0)
Much as we explained above for the , the prototype model fails to pre-
family resemblance
cisely describe QTC because this motion categorization is defined as a boundary model.
B12
Even though we can define prototypes in QTC that retain some of their basic properties
B12
(for instance, lowest within-category or lowest between-category dissimilarity), we cannot find a
prototype that fulfils all properties. Moreover, prototypes can only approximatively account for
category membership; they fail when category members are near to category borders (as it is the
case of ).
c1
Although we did not prove it, it is evident that neither the exemplar model can provide a
detailed categorization account for similar reasons as the prototype model: we cannot reproduce
the effect of a border category by means of a number of exemplars. Technically, it is not
finite
possible because, in a real continuum, a border has always at least one topologically open side. In
sum, only the boundary model can precisely describe the categorization originated by QTC .
B12
100 CHAPTER 7. A CATEGORIZATION FRAMEWORK
7.4.2 Conclusion
By means of the simple example, QTC , we have seen how the motion categorizations behave
B12
that are created as qualitative representations. The most meaningful dissimilarity function is
based on the featural distance , and it yields acceptable results regarding the expected prop-
erties of a dissimilarity (items I. to III.): In psychological jargon, we would say that the featural
space ( , ) is the most faithful representation of the for the exemplified
d psychological space
categorization.
Prototypes can be used as a rough description of the categorization; they fulfil to a cer-
tain degree their expected properties (item III.A. to item III.C.): they have the highest family
resemblance in the category, and they can decide membership of all their category members.
Nevertheless, prototypes fail to provide a detailed categorization account because a motion
categorization—at least in the form of a qualitative representation—has ,
meaningful borders
i.e., borders that are also categories; this is the case of the category , the border between
(0, 0)
and . Moreover, as we mentioned, the border categories make also the exemplar
( , ) (+, +)
− −
model inadequate.
All counted, we have established that only the captures the categorical
boundary model
essence of a qualitative representation of motion, and, furthermore, we have also validated our
framework as a tool capable of describing aAnd analyzing motion categorizations.
D .
Chapter 8
Stories and
Temporal Sequences of Relations
The qualitative is supervenient on the quantitative. There can be
no qualitative distinctions without underlying quantitative ones.
This does not in itself necessarily imply that qualitative distinc-
tions arise from quantitative ones, [however,] that conclusion is4
all but irresistible in very many cases. u
A. Galton (2000, p. 341)
D .
So far, we have presented a broad and multidisciplinary understanding of categorization
(Ch. 4), and have explained how qualitative representations, specifically those concerning motion,
are—or, rather, can be—integrated in such categorization domain (Chs. 5 to 7). In this chapter,
we begin the genesis of the story-based representations: motion categorizations obtained from
spatial representations. We will see, as well, that we can obtain story-based representations from
certain motion representations.
8.1 Motivation ansd Background for Temporal Sequences
Remember that our two fundamental onset conditions for motion categorization are following
(Ch. 2):
. We have ‘ ’ (Sect. 2.1), which are described by instantaneous position-velocity
i motion scenarios
pairs of vectors in a certain instant (a vector pair for each entity). In the most com-
t = t
mon case of two entities, and , a motion scenario is .
k l ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
. We have a ‘ ’ ( , ) (Ch. 5). It categorizes ‘
ii qualitative spatial representation δ position
’ of two entities, into possible categories, the spatial qualitative relations
scenarios ((cid:126)x , (cid:126)x ) n
k l
. The set of all possible position scenarios is the ‘ ’ . Note
R , R , . . . , R position space
1 2 n
{ } X
that a ‘ ’ describes not only the “positions” of two entities, but
position scenario ((cid:126)x , (cid:126)x ) any
k l
(e.g., positions, orientations, sizes) that is relevant for the spatial
positional information
categorization .
102 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
(8.1)
X × X −→ D
((cid:126)x , (cid:126)x ) R
k l i
(cid:55)−→
In the literature, the spatial categorization is usually defined . For example, a
extensively
‘ ’ is a region of the ‘ ’ , i.e.,
qualitative spatial relation X position space R = ((cid:126)x , (cid:126)x ) X
i i k l i
X { ∈ ⊂ X }
(e.g., Ligozat and Renz 2004; Dylla et al. 2017). In our work, it is more convenient to define
(Eq. (8.1)); we call such categorization rule that assigns the corresponding
intensionally δ
spatial relation to each , i.e., .
R position scenario R = δ ((cid:126)x , (cid:126)x )
i i X k l
As we want to obtain the spatial relations for motion scenarios, we extend the spatial
categorization rule to motion scenarios and call it ‘ ’, . To obtain is a trivial
δ spatial map δ δ
X T
process (Eq. (8.2): first, we extract the positional information, , from the motion sce-
((cid:126)x , (cid:126)x )
k l
nario —we use a projection, —then, we apply the categorization rule to the
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) π δ
k k l l X X
extracted position scenario .
((cid:126)x , (cid:126)x )
k l
(8.2)
K −→ X × X −→ D
X X
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)x ) R
k k l l k l i
(cid:55)−→ (cid:55)−→
In sum, categorizes motion scenarios by means of a spatial representation, ;
δ spatially δ δ
4 X
assigns a spatial relation to each motion scenario. r
RR
We endeavour to categorize motion by taking advantage of the spatial r3epresentations; for
that reason, the next step is how to use spatial relations to describe motion. A most straightfor-
ward method is to describe the entities’ trajectories as a .
temporal osequence of spatial relations
D .
Based on such sequences, researchers implement successful mesthods for motion analysis: Dela-
fontaine et al. (2011) analyze complex trajectories in a squoash game by looking at its 4-relations
subsequences, Hanheide et al. (2012) compare navigation trajectories in human-robot interaction
by means of the Levenshtein distance between sequences, and Chavoshi et al. (2015) compare
dance motions through sequence alignment methods (SAM).
Therefore, we embrace temporal sequences of spatial relations as a decisive step towards
motion categorization. But, is thihs a sensible decision? How can then a motion scenario, which
occurs in a punctual time inst/ant, be expressed as a sequence of relations, which extends through-
out a time interval? The solution is to embed each scenario in a certain motion trajectory, and,
then obtain the temporal sequence of such trajectory.
Scenario Trajectory Story
scenario’s trajectory trajectory’s temporal
is generated sequence is extracted
The summarized final outcome is that we map each scenario onto a particular temporal
sequence which we call ‘story’, and this ‘story’ serves as a category for the scenario. Indeed,
most spatial representations yield a finite number of stories, and, therefore, mapping a scenario
onto a story is equivalent to label a scenario with the category .
K S K S
Interestingly, we obtain stories not only from relations, but we also obtain stories from
spatial
certain representations of , concretely, from those motion representations that categorize
motion
instantaneous motion (those motion representations listed as Items A. and B. in Section 6.3). For
that reason, we generalize Equation (8.2), in which scenarios are categorized, into Equa-
spatially
tion (8.3), in which scenarios are categorized: ‘ ’ represents the ‘ ’ of any
generally ρ qualitative map
kind of qualitative representation that categorizes scenarios, notably, ( , ) or
spatial δ motion
R D
8.2. TEMPORAL SEQUENCES: A QUALITATIVE TRAJECTORY DESCRIPTION 103
( , ) representations. Concluding, we have broadened the condition in Item . to include also
µ ii
qualitative motion representations with which create story-based motion representations.
(8.3)
K −→ R
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) R
k k l l i
(cid:55)−→
In the following, we reveal how to map scenarios onto stories. Before defining the ‘stories’, we
define temporal sequences of relations , i.e., for kind of representation (Sects. 8.2
generally any
to 8.4). Moreover, we delve into the properties of general temporal sequences: we relate them
to fundamental concepts in the field of qualitative representations, and touch on properties that
broaden our understanding of such sequences. Subsequently, we define as a particular
stories T
case of the temporal sequences of relations (Sects. 8.5 and 8.6). All throughout this chapter, we
exemplify the concepts with the spatial relation RCC (Sect. 5.2.1). Later, in Chapter 9, we build
the full-fledged stories for RCC and OPRA spatial representations, as well as stories of
motion
representations, such as QTC , which are fully developed in ?? . F
B21
8.2 Temporal Sequences: a Qualitative Trajectory Descrip-
tion
In this section, we deal with and how to describe them
trajectories of entities ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l
using sequences of qualitative representations. As seen in the previous section, qualitative rep-
resentations categorize , that is, they categorize only each instantaneous point
motion scenarios
of a trajectory, e.g., . As a result, we might describe a trajectory by assigning
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t )
k k l l 0
to every time instant in the trajectory the corresponding qualitative relation g: This lead us
t R
0 r i
directly to the concept of temporal sequence (Def. 8.2.1).
D .
We remark that a trajectory is a particular case of a more general concept, namely,
n continuous
. Thus, for the sake of generality, we often refer in the following sections to
transformations
‘continuous transformations’; but, keep in mind that our focus are trajectories.
c We borrow this term from
Definition 8.2.1 Temporal sequence of qualitative relations
Hanheide et al. (2012), and we define it as followis: A ‘ ’
s temporal sequence of qualitative relations
is a chronologically ordered sequence of qualitative relations of any kind, e.g., space or motion,
generated by a continuous transformation of two entities in a time interval . In each
/ [t , t ]
a b
temporal sequence, , we avoid relations—in case they
(R , R , . . . ,:, R , . . .) consecutive repeated
1 2 s i
appear we merge them—so tphat .
i R = R
i i+1
∀ (cid:54)
We remark following important details in the definition:
• A temporal sequence of relations is as generally defined as possible. Firstly, the sequence
describes ‘ ’, which includes (as a continuous change
continuous transformations trajectories
of positions in time), but also (in the case that entities are regions), and
deformations
even changes on non-spatial domains. Admittedly, later, we will centre on trajectories and
disregard any other kind of continuous transformations, specially deformations, because we
restrict to rigid entities (Ch. 2).
Secondly, we do not restrict to sequences of relations, temporal sequences can con-
spatial
tain any kind of qualitative relations—as long, as we do not mix kinds in the same sequence.
Although we predominantly use sequences of spatial relations, we also use sequences of
relations in certain parts of this work; for example, when we extend motion repre-
motion
sentations ( ).
??
104 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
y y y
l l l
x x x
, , ,
t = 1.5 DC t = 0.0 EC t = 2.0 DC
1 2 3
A trajectory, , of two entities, and . We display three time instants ( , ,
A 1 2
and ) of the trajectory along with their corresponding RCC relations. The entities are circular
3 F
(cid:0) (cid:1)
with radii and ; 4 2 8 .
r = 1 r = 2 K (t) = (2t + 2, t 0.5), ( t + 2, t); (2 t, 2.5), ( 1, 0)
k l A
9 9
− − − − − −
• The time interval can be freely chosen. For example, we may consider a
[t , t ] A punctual
a b
interval by setting , we may consider the limits or , and, thus,
t = t lim lim
a b t →−∞ t →+∞
a b
have a half-bounded, e.g., , or totally unbounded interval . In fact,
[t , ) ( , ) stories
∞ −∞ ∞
are defined over a totally unbounded interval .
( , )
−∞ ∞
• We avoid repetitions of consecutive relations by assigning only one symbol to each
g R
relation that occurs in a subinterval of . That means that the sequence captures
[t , t ]
a b
every transition of relations (from one relation to the next different relation )
R R
i i+1
within the continuous transformation (e.g., within the trajectories).
D .
• The definition of assumes implicitly the existence of a map
temporal sequence of relations
called that, given a continuous transformation in a certain
f K(t) = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
ρ k k l l
[ta,t
interval “generates” the corresponding temporal sequence of relations. Such map
[t , t ]
a b
exists and we describe it as follows.
h ∗
(t) (t) Σ
t K −→ R −→ (8.4)
/ TS
ρ [ta,t
K(t) = ((cid:126)x , (cid:126)v ;/(cid:126)x , (cid:126)v )(t) R(t) s = (R , R , . . . , , R , . . .)
k k l l 1 2 i
: (cid:55)−→ (cid:55)−→
t [ta,t
(cid:0) (cid:1)
In Equation (8.4), is the function that provides the qualitative
R(t) = ρ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l
relation at any time instant of the continuous transformation . We can
t ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l
call it ‘ ’ or ‘ ’. Subsequently, extracts
qualitative transformation qualitative trajectory TS
[t ,t ]
a b
from the temporal sequence of relations , i.e., the relations in the temporal order as
R(t) s
they occur in the interval .
[t , t ]
a b
Example 8.1 K (t)
(cid:0)
K (t) = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) = (2t+2, t 0.5), ( t+2, t);
A k k l l
9 9
− − − −
(cid:1)
(2 t, 2.5), ( 1, 0)
− −
2. If we apply the spatial representation RCC, i.e., , to the trajectory
ρ = δ ((cid:126)x , (cid:126)v ;
k k
RCC
8.2. TEMPORAL SEQUENCES: A QUALITATIVE TRAJECTORY DESCRIPTION 105
, we obtain the :
(cid:126)x , (cid:126)v )(t) qualitative trajectory R(t)
l l
DC t < 0
R(t) = EC t = 0
DC t > 0
R(t)
3. Finally, we apply the function to and obtain a qualitative temporal sequence
TS R(t)
[t ,t ]
a b
[t , t ] [ 1.5, 2.0]
a b
we have which applied to produces the three-elements sequence T
TS R(t) (DC, EC,
[-1.5,2.0]
DC)
TS
[t ,t ]
a b
intervals to clearly illustrate its behaviour.
(a) If we consider the interval , we have which applied to
[ 10.0, 1.0] TS R(t)
[-10.0,-1.0]
− −
produces the one-element sequence . This is the same sequence produced by any
(DC)
interval in which . A
[t , t ] t < 0
a b b
(b) If we consider the interval , we have which applied to pro-
[ 5.0, 0.0] TS R(t)
[-5.0,0.0]
duces the two-elements sequence . This is the same sequence produced by
(DC, EC)
any interval in which , e.g., the interval .
[t , t ] t = 0.0 [ 0.4, 0.0]
a b b r
− u
(c) If we consider the interval , we have which applied to produces
[0.0, 3.0] TS R(t)
[0.0,3.0]
the two-elements sequence . This is the same sequence produced by any
(EC, DC) /
interval in which and , e.g., the interval .
[t , t ] t = 0 t = 0 [0.0, 0.01]
a b a b
(cid:54) o
D .
(d) If we consider the interval , we have which applied to
[ 1.7, 20.4] TS s R(t)
[-1.7,20.4]
produces the three-elements sequence . In fact, any interval con-
(DC, EC, DC) o [t , t ]
a b
taining the instant not in the boundaries produces the sequence .
t = 0 (DC, EC, DC)
The whole process described in Items 1 to 3 is condensed in the mapping . For
RCC
s [ta,t
[ 1.5, 2.0]
e−
t (cid:0) (cid:1)
(DC, EC/, DC) = f K (t)
RCC
/ [-1.5,2.0]
The mapping for sItems 3a to 3d can be represented as following:
RCC
[ta,t
bt
h (cid:0) (cid:1)
(a)
(DC) = f K (t)
RCC
[-10.0,-1.0]
(cid:0) (cid:1)
(b)
(DC, EC) = f K (t)
RCC
[5.0,0.0]
(cid:0) (cid:1)
(c)
(EC, DC) = f K (t)
RCC
[0.0,3.0]
(cid:0) (cid:1)
(d)
(DC, EC, DC) = f K (t)
RCC
[-1.7,20.4]
A ‘temporal sequence of relations’ is a rather general term. In the following, we specify some
types of temporal sequences of relations which are specially interesting.
‘ ’ are those containing a
Definition 8.2.2 Finite and infinite sequences finite sequences
finite number of relations, and ‘ ’ ones are those containing an infinite number
infinite sequences
of relations.
106 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
For instance, all sequences obtained in Example 8.1, , , ,
(DC) (DC, EC) (EC, DC) (DC, EC, DC)
are obviously finite.
Note that these two concepts, and , express the
finite infinite sequences number of relations
that a sequence contains; what is usually called ‘ ’ of a sequence. The length of the temporal
length
sequences influences heavily their practical application. We can deal much easier with
finite
than with ones. For that reason, the most important sequences defined in this
sequences infinite
work, the ‘stories’, are finite (item i., Sect. 8.6).
A complete temporal sequence
Definition 8.2.3 Complete temporal sequence of relations
of relations is a temporal sequence in the whole unbounded time interval . All possible
( , )
−∞ ∞
complete temporal sequences define the set ∗.
We can define the complete temporal sequence of a certain trajectory by
s ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l
means of function in Equation (8.4) and seeking its limits and :
f lim lim
ρ t →−∞ t →+∞
[ta,t
] a b
(cid:0) (cid:1)
. A compelling property of complete temporal sequence is that
s = f ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) F
ρ k k l l
(-∞,∞)
they are invariant under monotone non-decreasing unbounded time transformations :
g(t)
(cid:0) (cid:1) (cid:0) (cid:1)
(8.5)
s = f ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) = f ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(g(t))
ρ k k l l A ρ k k l l
(-∞,∞) (-∞,∞)
For example, a complete story is invariant under time translations and dilations, namely,
under where .
g(t) = at + b a > 0
As example of compRlete sequence, let us consider the trajectory
K (t) = ((cid:126)xu, (cid:126)v ; (cid:126)x , (cid:126)v )(t)
A k k l l
jectory, any interval containing the instant not in the boundaries, i.e.,
[t , t ] t = 0 t = 0
a b g
and , produces the temporal sequence of relations r . That means
[t , t ] t , t = 0 (DC, EC, DC)
a b a b
that D the limit (cid:54) (cid:0) (cid:1) is the sequence . too. Hence, we can
lim
t → −∞
f K (t) (DsC, EC, DC)
a A
→ +∞ RCC [ta,t
] n
compute the temporal sequence of the trajectory :
complete o K (t)
(cid:0) (cid:1)
(DC, EC, DC) = f ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
ρ o k k l l
(-∞,∞)
Certainly, we obtain the same sequence as complete sequence, i.e., in the
s (DC, EC, DC)
interval , and as non-complete sequence in the interval . What makes
( , ) h I = [ 1.7, 20.4]
−∞ ∞ t −
then a complete sequence particular compared to a non-complete sequence?
In first place, the invariance to certain time transformations. For instance, the sequence
: (DC,
obtained frpom the interval changes into the sequence if we shift the time
EC, DC) I (EC, DC)
t 1
(cid:48) , sintce the interval becomes . In second place, if we restrict our analysis
t = t + 1.7 I [0, 22.1]
to the certain particular trajectories some non-complete sequences can never be obtained as
complete sequences. For instance, restricted to uniform linear motions the sequence
(DC, EC)
can never be a complete sequence because it ends at but a uniform linear motion in the
EC
interval which begins in the disconnected relation cannot get stuck at the tangent
( , ) DC
−∞ ∞
relation for the rest of the time.
EC
We remark that Definitions 8.2.2 and 8.2.3 are independent: we can have both finite and
infinite temporal sequences of relations that are complete sequences or not. In fact, the temporal
sequences defined in this work, the , are sequences of relations, i.e., defined in
stories complete
the time interval , and happen to be sequences (Sect. 8.5). The most rare case
( , ) finite
−∞ ∞
of temporal sequences, however, are the infinite temporal sequences that are not complete, i.e.,
which occur in a finite time interval; for that to happen, we need either entities or motions that
are extremely convoluted, actually, those only found in theoretical settings.
8.3. TRANSITIONS BETWEEN RELATIONS: CND 107
To conclude, a is a parsimonious description of the entities’
temporal sequence of relations
continuous transformations (e.g., motion trajectories) in the space of qualitative relations ,
as compared to the most detailed description, , the , which provides
R(t) qualitative trajectory
the qualitative relation at any time instant . In a temporal sequence of relations the precise
instant at which every qualitative representation occurs is lost, but a coarse temporal information
remains: the temporal order in which the qualitative relations occur (analogously to the events
models of Warglien et al. (2012)). The temporal sequence of relations is equivalent to an ordered
listing of the between relations.
transitions
8.3 Transitions between relations: Conceptual Neighbour-
hood Diagram
As seen above, temporal sequences of relations are mainly about transitions between relations.
For example, in the temporal sequence of spatial relations F (See Fig. 8.1), we
(DC, EC, DC)
recognize two transitions: and . Actually, because temporal sequences are
DC EC EC DC
(cid:32) (cid:32)
defined for continuous transformations, temporal sequences show more than just transitions:
they show (as defined below).
direct transitions
A ‘ ’, is a transition between two
Direct and indirect transitions direct transition R R
(cid:32)
a b
relations, and , that occurs when two entities go through a transformation,
R R continuous
a b 4
and there is involved. For example, the transition irs a
no intermediate relaRtion R DC EC
(cid:32)
c u
direct transition: two entities that are disconected, , move towards eacgh other
DC continuously
until they become connected through their border, —there is no other intermediate relation
EC /
involved. The transition , however, cannot be direct—it is ‘ ’—because there
DC NTPP indire r ct
(cid:32)
is no continuous trajectory that brings two disconnected entities, , into being one contained
D DC .
in the other, , without going through an intermediate relation, snuch as partial overlap .
NTPP PO
Transitions are not ‘direct’ in absolute terms. A transition might be in
m R R direct
(cid:32)
a b
a certain type of continuous transformation and min another type. For example, the
indirect
transition is direct both for translations or doilations; however, the transition
DC EC TPP
(cid:32) (cid:32)
is direct for dilations but not for translations s(Fig. 8.2).
EQ
Note, however, that if we had allowed transformations as direct transitions,
deiscontinuous
transition would be a direct transition, and, thereby, the concept ‘direct transition’ would
any t
be useless: we could instantly leap from one motion state to any other motion state in order to
generate whatsoever transitionsof relations. Therefore, is a fundamental assumption
continuity
when considering transitions of qualitative relations.
By definition, a temporal sequence of relations shows of two entities oc-
direct transitions
curring in a certain time interval (the continuous transformation is the continuous trajectory).
Therefore, if a temporal sequence happens to include an —e.g., the transition
indirect transition
, as in the sequence —such a temporal sequence cannot be
DC NTPP (PO, EC, DC, NTPP)
(cid:32)
obtained from continuous trajectories of entities. In other words, such sequence is .
unrealizable
A temporal sequence of relations is ‘
Realizable and unrealizable temporal sequences un-
’, when it does not exist any continuous transformation (e.g., trajectories) of entities
realizable
that produces such sequence. Conversely, any temporal sequence of relations that can be pro-
duced from continuous transformations is ‘ ’. Equivalently, in terms of transitions, a
realizable
temporal sequence of relations is realizable if and only if each transition is a transition.
direct
In conclusion, it is essential to know which transitions in a qualitative representation are
direct, when one uses temporal sequences of relations to describe trajectories.
108 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
TPP
x y
NTPP
x y
x y
EQ
y x
DC EC PO
NTPPI
y x
TPPI
The RCC qualitative rela-
DC EC
, , , , , and as a . The arrows
PO TPP NTPP EQ TPPI NTPPI conceptual neighbourhood diagram
correspond to , i.e., relations that are directly connected by a continuous trans-
direct transitions
formation: dashed arrows represent only deformations, solid arrows represent translations and
deformations.
The ‘ ’ (CND) is
Conceptual Neighbourhood Diagram conceptual neighborurhood diagram
simply a tool to display the of a qualitative representation. It is a graph
D direct transitions .
linking the pairs of qualitative relations that have a direct transition, i.e., pairs of relations
directly connected by . Againm , we remark a CND is not restricted to
continuous transformation
spatial representations, but it can apply to any kmind of representations; for example, motion
representations, and, more concretely, story-based representations.
Note that, as the CND displays the direct transitions, if we lift the continuity requirement
from the definition of direct transitioen, any possible transition becomes realizable, and, thus, the
CND becomes a complete graph. In that case, the CND would be pointless: it provides no extra
information about transitions.
We can apply thistCND to analyze temporal sequences. For example, the temporal sequence
(PO,
is realizable as all transitions are represented as edges in the diagram. The
TPP, NTPP, TPP)
temporal sequence is only realizable if the entities are deformed, because
(PO, TPP, EQ, PO)
the transition is represented with a dashed arrow in the diagram, and, thus, is
TPP EQ
(cid:32)
only possible by deformation. In any case, sequences such as or
(EC, PO, NTPP) (DC, PO,
are not realizable because, in the CND, it exists no direct transition (i.e., no edge)
TPP) PO
(cid:32)
, neither . Notwithstanding, such sequences can become realizable by adding
NTPP DC PO
(cid:32)
intermediate relations (bold faced) and .
(EC, PO, TPP, NTPP) (DC, EC, PO, TPP)
We can formulate the connection between and the CND in an
realizable temporal sequences
elegant way: Any temporal sequence of relations in a certain qualitative representation
realizable
is a in the CND of such qualitative representation—The CND let us visualize realizable
path
sequences as paths. Moreover, in the CND, we can visualize at once different types of direct
transitions, if we differently format the arrows connecting the relations. For example, in Fig-
ure 8.2, the direct transitions obtained only through deformation are denoted by dashed arrows,
8.4. DOMINANCE THEORY 109
while those possible by dilation or translation are denoted by solid arrows.
In this work, we certainly benefit from the CND when we generate the story-based categoriza-
tions (i.e., story-based qualitative representations of motion), because a necessary step in such
generation is finding temporal sequences of spatial relations, the stories—These are particular
paths in the CND of the spatial representation. Interestingly, the story-based categorizations are
also qualitative representations of motion, and, thus, we could also obtain their CND in order to
analyse direct transitions between their relations: we refer to such analysis in the control of tra-
jectories (Sect. 12.5). Anyway, we leave for future work building whole CNDs of the story-based
categorizations.
CND in the literature
The conceptual neighbourhood diagram (CND) is widely acknowledged as one of the most ele-
mentary concepts concerning qualitative representations. It was introduced by C. Freksa (1992a),
though another names used are ‘ ’ (Cohn et al. 1994) or ‘ ’ (1997,
transition graph continuity network
p. 295)—compare also with the similar concept ‘ ’ (Egenhofer
closest-topological-relationship-graph
and Al-Taha 1992).
When dealing with motion, the conceptual neiAghbourhood diagram becomes a ubiquitous
concept, because motion is a of the position in time (See thorough
continuous transformation
analysis, Hazarika and Cohn 2001; Cohn and Hazarika 2001b). Therefore, when we represent
motion qualitatively, we are using (at least implicitly) the CND.
Furthermore, the conceptual neighbourhood diagram is a basic tool for implementation of
decision-making, planing, and control algorithms (e.g., Dylla and Wallgrün 2007; Dylla 2008;
Dylla et al. 2012)—Qualitatively steering a moving entity consists on transitioning between
relations in the conceptual neighbourhood diagram (Sect. 12.5). As mentionerd above, CND are
D .
not only created for spatial representations; we can also display motion relations (e.g., Van de
Weghe and De Maeyer 2005).
8.4 Dominance Theory
The ‘ ’—introduced by A. Gealton (1995)—seeks a deeper mathematical insight
dominance theory
in the continuous transformations between qualitative relations than the conceptual neighbour-
hood diagrams (CND) (See didact/ical introduction, 2001). One result of the dominance theory
are the ‘ ’, which are conceptual neighbourhood diagrams with additional
dominance diagrams
information about continutity at the transition of qualitative relations. In fact, the dominance
diagrams define a topology in the qualitative representations, the , which
dominance topology
characterizes the continuous transformations of entities. On this account, the dominance theory
underlie any analysis of continuous transformations of entities, particularly, continuous trajecto-
ries.
This theory has very practical applications: it provides tools to build conceptual neighbour-
hood diagrams (CND), and determines properties of the time intervals in which the relations
occur. For example, the dominance theory provides a method for obtaining the CND of a quali-
tative representation formed by Cartesian product of properties or representations whose CNDs
are known. Such method is used by Van de Weghe 2004 to obtain the CNDs for variants of the
QTC motion representation, since such variants are all constructed through Cartesian product of
simpler properties or representations (Sect. 6.4). Many story-based categorizations are also built
as product representations (as explained in Sect. 6.4.2.B), and, consequently, we could apply the
dominance theory to construct their CNDs.
110 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
R R
1 2
(a)
R R
R R R R
1 2
1 2 1 2
t t
t t
T T
(b) (c)
A visual representation of the . When entities transition from a
relation, , into another relation, , in a certain instant (Fig. (a)), the dominance theory
R R t
1 2 T
determines which relation occurs always at the transition instant (the boundary instant): In
F t
R dominates R R t
1 2 1 T
R dominates R R
2 1 2
instant .
In this work we resort twice to the dominance theory. In Section 8.6.2 we use the
dominance
to differentiate two types of relations, the and stories. Later, in Sec-
relation rigid singleton 4
tion 11.5.2, the definition of will simplify the computation of the coumposition of
position state
stories.
We stress that the dominance theory characterizes not only continuous map onto qualitative
representations, but more generally, continuous maps into . Examples of discrete
discrete spaces
D .
spaces are the in Section 6.4, which are used to build qualitative relations.
discrete properties s
∗ n
For that reason, and in view of its limited success, we believe that the dominance theory is, at
present, undervalued, and it has still a lot to say in qualitative description.
A. Galton (Def. 2.1, 2001) defines the dominance relation as follows: a
Dominance relation i
state another state e, when can occur at the boundary of an open interval
R dominates R R
1 2 1
in which occurs, i.e.,t can occur at or .
(t , t ) R R t t
a b 2 / 1 a b
dominance relation
over the other is the one that must occur at the transition instant between both relations. For
p t
instance, if dominates , then whenever we transition from to —or vice versa—
R R R R R
1 2 1 2 1
will be the relation occurring at the boundary, i.e., at the transition point (Fig. 8.3b). If on
the contrary, dominates , then it is that will be occurring at the transition point
R R R t
2 1 2 T
(Fig. 8.3c).
In RCC, if we allow deformations, the following temporal sequence of relations
Example 8.2
is possible, . If we inspect the transition , we realize that it is the
(EQ, TPP, PO) EQ TPP
(cid:32)
state that must occur at the boundary instant, ; for instance, occurs in and
EQ t EQ (t , t ]
T a T
in . However, a transition of the form for and for would
TPP (t , t ) (t , t ) EQ [t , t ) TPP
T b a T T b
be impossible. Therefore, dominates . Analogously, dominates , because
EQ TPP TPP PO TPP
must occur at the transition boundary, , of the interval in which occurs.
t (t , . . .) PO
T T
Note that, in RCC, the states and always dominate over
Position and motion states EQ EC
their neighbour states (no matter whether deformations or translations are allowed). Accordingly,
8.5. STORIES: FROM SCENARIOS TO TEMPORAL SEQUENCES 111
the dominance relation defines two new types of states: the and the states.
position motion
A ‘ ’ is such that always dominates over its neighbour states. Thus, it can only
position state
occur in a interval . Particularly, a position state can occur in a single time instant
closed [t , t ]
a b
. For example, the relation is a position state.
t = [t , t ] EC
0 0 0
A ‘ ’ is such that always is dominated by its neighbour states. Thus, it can
motion state
only occur in an interval . Particularly, a motion state can never occur in a single
open (t , t )
a b
time instant . For example, the relation is a motion state—Do not confuse the ‘motion
t DC
states’ according to the dominance relation with the ‘motion states’ which
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
are kinematic descriptions of the entities, as in Tab. 8.1.
Not all relations in a qualitative representation need to be either position or motion states:
they can be neither. In that case, we have a ‘ ’, which can occur in
non-motion non-position state T
a interval, i.e., or . If in a qualitative representation, each relation is either
half-open (t , t ] [t , t )
a b a b
a position or a motion states then such representation constitutes a ‘ ’ according to
regular space
the dominance relation. Remarkably, both RCC (without deformations) and OPRA are .
regular
If we allow only translations and the entities have different size (Fig. 8.2), the
Example 8.3
RCC relations , , and are , while , , , and are
EC TPP TPPI position states DC PO NTPP NTPPI
. It is evident, if we think of continuous translations, that can occur in a single
motion states EC
time instant (as in Fig. 8.1), and that cannot; the latter can only occur in open intervals
PO
(See realization in Fig. 8.8).
Concluding, the dominance theory classifies the qualitative relations between entities accord-
ing to how they can occur at the boundaries of open time intervals. This is essential to analyze
or build temporal sequences of relations, because the occurrence of relations at the boundaries
of time intervals determines how relations can be strung to form temporal sequences.
Yet, behind this simple “property of time intervals”, lies a bigger picture: the ‘
dominance
’ (ibid., p. 60), which is the finest topology that makes a qualitative representation
topology
D .
continuous—specifically, it makes continuous its categorization rule. It is beyond the scope of
this work to delve into the topological properties of the story-based representations, though it is
a relevant topic for a deeper mathematical understanding of such representations.
8.5 Stories: From Scenarios tio Temporal Sequences
Here, we aim to map each motion scenario into a particular temporal sequence of qualitative
relations: the ‘story’. To that end, we use the concepts in Section 8.2 but specialized to motion
trajectories instead of general continuous transformations.
Which type of qualitative representations can we use to create a story? Whatever. We can
choose the type of qualitative representation that is appropriate for our current purpose. For this
reason, we strive to keep as as possible the qualitative representations in this section.
general
Although the examples and illustrations here have only spatial relations—mostly RCC—for the
sake of simplicity, in Section 9.3, we specifically deal with stories of qualitative relations of motion.
Throughout the section, we will use the scenario to illustrate the new defined concepts.
8.5.1 Embedding a scenario in a uniform motion
In our aim to map scenarios into temporal sequences, the critical step is to embed a motion
scenario (e.g., Fig. 8.4) into a trajectory ; because, once we map
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l k k l l
the scenario into a trajectory, we can obtain a temporal sequence rather straightforwardly. We
decide to embed a motion scenario into a trajectory, i.e., a trajectory without
uniform motion
accelerations, because of the many persuading arguments:
112 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
(cid:126)v
l l
(cid:126)v
A motion scenario of two entities and with kinematic coordinates
B B
(cid:0) (cid:1)
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = ( 2, 0), (2, 0); (4, 2), ( 1, 0)
k k l l
− −
• First of all, a motion scenario contains no information about the acceleration, only veloc-
ities. Therefore, as a best guess we can choose a central value, i.e., zero acceleration, for
the trajectory containing the scenario.
• At the very moment, when we consider it worth to categorize motion scenarios, we are
assuming a limited effect of acceleration on the entities—The extreme case is constant zero
acceleration. Conversely, if scenarios should belong to trajectories with high and greatly
variable acceleration, then the changes in the scenario’s configuration would be so abrupt
that the categorization of scenarios would be meaningless.
• Uniform motion is the state of steady motion of any entity (a person, an animal, a car); it
is a typical state, and, thus, has highly ecological validity.
D .
Finally, we have relevant computational and mathematical reasons:
• In uniform motion, we can most easily compmute the temporal sequence of relations for a
trajectory.
• And, perhaps, most decisive, we shave only cardinalities (App. A.2.1): any temporal
finite
sequence in uniform motion is finite, also the total number of sequences is finite. The latter
is a necessary condition for stories to be a (Sect. 11.1.1).
qualitative calculus
After all, by embedding scenarios into uniform motion trajectories, we are limiting our
not
categorization of scenarios to uniform motion in any case: the categorized scenario can belong
to any kind of trajectories (See examples in Sect. 12.4). The use of uniform motion embedding
is only a device that helps us categorize scenarios, and, as an advantageous effect, it enables us
to qualitatively detect accelerations when we categorize trajectories (See Sect. 12.5).
Thus, to resume, we embed the motion scenario into its uniform motion
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
trajectory , as given by the equation that
K(t) = ((cid:126)x + (cid:126)v t, (cid:126)v ; (cid:126)x + (cid:126)v t, (cid:126)v ) (cid:126)x(t) = (cid:126)x + (cid:126)v(t t )
k k k l l l 0 0
describes the position of an entity in uniform motion. This embedding is univocal, and, hence,
equivalent to the following map.
U : (t)
t (8.6)
K −→ K
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) = ((cid:126)x + (cid:126)v t, (cid:126)v ; (cid:126)x + (cid:126)v t, (cid:126)v )
k k l l k k l l k k k l l l
(cid:55)−→
Example 8.4 K
trajectory .
K (t)
8.5. STORIES: FROM SCENARIOS TO TEMPORAL SEQUENCES 113
y y y y y y
l l l l l l
x x x x x x
k k k k k k
t = 1.8, PO t = 2.0, PO t = 2.5, PO t = 2.75, EC t = 3.0, DC t = 3.2, DC
Each picture is a snapshot of the uniform motion trajectory obtained by applying
(cid:0) (cid:1)
U K ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = ( 2, 0), (2, 0); (4, 2), ( 1, 0)
t B k k l l
− −
the time interval . Below each picture, we display the time of the trajectory and its
[1.8, 3.2] t
corresponding RCC qualitative relation—The scenario occurs at (not visible in the
K t = 0
U : (t)
(cid:0) K(cid:1) −→ K
K = ( 2, 0), (2, 0); (4, 2), ( 1, 0) K (t) = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
B B k k l l
− − (cid:55)−→
= (( 2, 0) + (2, 0)t, (2, 0); (4, 2) + ( 1, 0)t, ( 1, 0))
A− − −
K (t) [1.8, 3.2]
interval .
[ 1.0, 4.0]
Note that the mapping creates a uniform trajectory , that contains rthe
U R K(t) = U (K)
t t
embedded scenario at , i.e., . Alternatively, we could have defined the mgapping
t = 0 K = K(0)
, which is the same as but with time shift . In that case the unifor/m trajectory
U U t t t
t-t t 0
→ −
reproduces the embedded scenario at , i.e., . Taking or , i.e.,
K(t) t = t K = K(t ) rU U
0 0 o t t-t
centering theDembedded scenario at
t = 0
or
t = t
, will make no differe
nce for our purpose at
0 s
hand: the generation of stories.
8.5.2 Mapping uniform motion into sequences
We obtain now a qualitative temporal sequence from the uniform trajectory
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
k k l l
defined by the embedding in Equation (8e.6); to that end, we apply onto the trajectory the
t h
map that we defined in Equation (8.4). Remember that, for the map to work, we must
f t
ρ /
[ta,t
specify the qualitative representati/on (e.g., ) and the time interval in which the
ρ ρ [t , t ]
: RCC a b
sequence is computed. In summary, starting with a given scenario , we apply upon
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
it the composition of both the map , which embeds the scenario in a uniform trajectory, and,
subsequently, we apply the map , which generates the temporal sequence in the interval
[ta,t
(Eq. (8.7)). The end result is the mapping , which assigns to a scenario
[t , t ] σ ((cid:126)x , (cid:126)v ;
a b [t ,t ] k k
a b
a temporal sequence in the interval .
(cid:126)x , (cid:126)v ) s = (R , R , . . . , R ) [t , t ]
l l i i i a b
1 2 m
(t) Σ Σ
K −→ K −→ ⊂ (8.7)
[ta,t
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) s = (R , R , . . . , R )
k k l l k k l l i i i
1 2 m
(cid:55)−→ (cid:55)−→
[t ,t ]
a b
Example 8.5 σ
[t ,t ]
a b
(cid:0)
U K ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = ( 2, 0),
t B k k l l
114 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
y y y y y y y y
l l l l l l l l
x x x x x x x x
k k k k k k k k
t = −1.0, DC t = 0.0, DC t = 1.25, EC t = 1.8, PO t = 2.4, PO t = 2.75, EC t = 3.0, DC t = 4.0, DC
Each picture is a snapshot of the uniform motion trajectory obtained by applying
(cid:0) (cid:1)
U K ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = ( 2, 0), (2, 0); (4, 2), ( 1, 0)
t B k k l l
− −
the time interval . Below each picture, we display the time of the trajectory and its
[ 1.0, 4.0] t
corresponding RCC qualitative relation—The scenario occurs at .
K t = 0
(cid:1)
(2, 0); (4, 2), ( 1, 0) [1.8, 3.2]
sequence of RCC relations is . We can express these operations by means of
(PO, EC, DC) σ
[t ,t ]
F a b
as follows:
(cid:0) (cid:1)
(PO, EC, DC) = σ ( 2, 0), (2, 0); (4, 2), ( 1, 0) = σ (K )
[1.8,3.2] [1.8,3.2] B
− −
It is a non-trivial issue to decide the specific
Meaningful intervals [t , t ] for categorization
a b
values of the time interval that map the scenario into a temporal sequence of relations.
[t , t ]
a b
As an illustration, consider scenario two scenarios at the instant : r and
R t K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
a 1 k k l l
scenario ; has the same positions as and proportiognal velocities by
K = ((cid:126)x , α(cid:126)v ; (cid:126)x , α(cid:126)v ) K K
2 k k l l 2 1 3
a factor . If we take the time interval in which yield/s a certain temporal
α [t , t ] σ (K )
a b [t ,t ] 1 g
a b
sequence, e.g., , then, by slowing enough, i.e., making arbitrarily near to
(R , R , R ) K r α
1 2 3 2 o
zero,D
σ (K )
would yield the one-element sequence
(R )
. Th
e consequence would be that if
[t ,t ] 2 1 s
a b
we categorize scenarios according to temporal sequences in intervals , scenarios with
finite [t , t ]
o a b
proportional velocities might belong to different categorimes. We deem such behaviour undesirable,
as we see both scenarios ( and ) being fully manalogous, since they differ only in how fast
K K
1 2
they temporally evolve. Now, if we take as time interval the interval , the
unbounded ( , )
s −∞ ∞
scenarios with proportional velocities, such as and , yield the same temporal sequences;
K K
1 2
formally expressed, e
s (8.8)
σ (K ) = σ (K ) K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ), K = ((cid:126)x , α(cid:126)v ; (cid:126)x , α(cid:126)v )
(-∞,∞) p1 (-∞,∞) 2 1 k k l l 2 k k l l
where (8.9)
t σ = lim t → −∞ σ
h (-∞,∞) a [t ,t ]
t → +∞ a b
Therefore, we choose the unbounded interval to generate temporal sequences of
( , )
−∞ ∞
trajectories in uniform motion by applying the mapping . In other words, we choose to
(-∞,∞)
work with because of the time invariance properties that they
complete sequences of relations
possess (See Definition 8.2.3).
Example 8.6 σ
(-∞,∞)
(cid:0) (cid:1)
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = ( 2, 0), (2, 0); (4, 2), ( 1, 0)
k k l l
− −
. Therefore, its temporal sequence of relations is obtained as
[ 1.0, 4.0] (DC, EC, PO, EC, DC) =
− (cid:0) (cid:1)
σ ( 2, 0), (2, 0); (4, 2), ( 1, 0) = σ (K )
[-1.0,4.0] [-1.0,4.0] B
− −
t 1.0 t 4.0
≤ − ≥
relation RCC remains , so we conclude that
DC
8.5. STORIES: FROM SCENARIOS TO TEMPORAL SEQUENCES 115
(DC, EC, PO, EC, DC) = σ (K ) =
(-∞,∞) B
(cid:0) (cid:1)
(8.10)
lim t → −∞ σ ( 2, 0), (2, 0); (4, 2), ( 1, 0) = σ (K )
a [t ,t ] [-1.0,4.0] B
t → +∞ a b − −
8.5.3 The story map σ, stories S , and stories set Σ
Recapitulating, in last section, we obtained the mapping which maps a scenario into a
[t ,t ]
a b
temporal sequence by evolving the scenario into a uniform motion trajectory in the interval
. Next, we saw that the unbounded interval was the most meaningful interval to
[t , t ] ( , ) T
a b
−∞ ∞
categorize scenarios using the mapping. Accordingly, the application is the most
σ σ
[t ,t ] (-∞,∞)
a b
appropriate as categorization rule for scenarios. Now, we want to use these results to establish
the in this work.
fundamental definitions
The mapping possesses appropriate categorizations
Story map σ and Stories S σ
i (-∞,∞)
properties. We display in Equation (8.11); it is obtained by mapping composition in two
(-∞,∞)
steps:
1. embeds the scenario into a uniform motion trajectory .
U ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t)
t k k l l k k l l
2. yields the complete sequence for the trajectory
f S = (R , R , . . . , R ) ((cid:126)x ,r(cid:126)v ;
ρ R i i i i k k
(-∞,∞) 1 2 m u
(cid:126)x , (cid:126)v )(t) g
l l
∗ o
(t) Σ Σ Σ
D K −→ K −→ ⊂ U t ⊂ . (8.11)
U ρ (-∞,∞) n
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(t) S = (R , R , . . . , R )
k k l l k k l l i io i i
1 2 m
(cid:55)−→ (cid:55)−→
σ := σ
(-∞,∞)
For the sake of simplicity, we denote as the mapping . We call the mapping ‘
σ σ σ story
(-∞,∞)
’ because we call ‘story’ each sequenceeof relations that the mapping generates. A
map S σ
h i
‘ ’ is, thus, the complete temporal sequence of qualitative relations generated by a uniform
story t
motion of entities.
In Equation (8.12), we repressent the compactly by hiding the intermediate steps
story map σ
of Equation (8.11): t
σ : Σ
(8.12)
K −→
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) S = (R , R , . . . , R )
k k l l i i i i
1 2 m
(cid:55)−→
Equation (8.12) is a fundamental formula in this work: it shows how a motion scenario
is mapped into a story, . This is the cornerstone of the categorization of motion
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) S
k k l l i
scenarios, as we see below. Remember that we set modest prerequisites for obtaining the story
map (See items . and ., p. 101); we only need the description of entities as motion scenarios,
i ii
and a , which provides the the function .
qualitative representation ρ
A story originates from a motion scenario by observing the time evolution of
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
the qualitative relations of motion, considering that the velocities remain unchanged. That is,
we list which past relations of motion could have occurred ( ), the current relation,
past inference
and which future relations of motion could follow ( ).
future inference
116 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
We already saw in the last section (Sect. 8.5.2) a single example of , an
Example 8.7 story
RCC story: The story obtained from the motion scenario , i.e.,
K (DC, EC, PO, EC, DC) =
σ (K )
(-∞,∞) B
stories: the stories which we obtain from the scenarios ; these are namely, ,
S K (DC) (DC,
C C
i i
, , ,
EC, DC) (DC, EC, PO, EC, DC) (DC, EC, PO, TPP, PO, EC, DC) (DC, EC, PO, TPP, NTPP,
TPP, PO, EC, DC)
Example 8.8 K (t)
the motion scenario (sketched at ). This trajectory yields the RCC story
K t = 2 S = (DC,
D K
. This is the same as the story generated by the
EC, PO, TPP, NTPP, TPP, PO, EC, DC) S
K (t)
We explicitly show here a trajectory in which the velocities are not antiparallel, as in the
other examples in this section, so that one can better grasp how the story map works in general.
Additionally, the uniform trajectory contains the embedded scenario at , ,
K (t) t = 2 K = K (2)
D D D
which means that it was embedded by . It is trivial to see that whether we embed the scenario
t-2
at , , or , , the generated story remains the same. In that way, we exemplify our
t = 0 U t = 2 U
t t-2
previous statements: first, that we can choose any to embed the scenario which generates
t-t
the story, and, second, equivalent to the first, that the stories are invariant to time translations.
The set that contains all possible stories is called ‘ ’, and we notate it
Stories set stories set
with ; it is a subset of the set of complete temporal sequences ∗. The stories rset, as any
Σ R Σ
set of categories, can be endowed with a hierarchical structure, forming a tree of subsets (i.e.,
subcategories). We call such subsets , where ‘ ’ is the name of the subset. For instance,
Σ /
∗ g
in the stories set of the spatial representation OPRA , we have, amongst others, the subsets
(crossing trajectories, both entities moving) and (parallel non-superposed trajectories)
Σ D Σ .
C P s
(Fig. 12.7). n
In Example 8.7, we have seen sixm RCC stories. Since the stories set of RCC,
Example 8.9
, contains all RCC stories, these six stories offer us a first glance into .
Σ Σ
RCC RCC
(cid:8)
(DC), (DC, EC, DC), (DC, EC, PO, EC, DC),
(cid:9)
(DC, EC, PO, TPP, PO/, EC, DC), (DC, EC, PO, TPP, NTPP, TPP, PO, EC, DC) Σ
RCC
: ⊂
We emphasize that we can use relations to create a story. Once we choose a
of any kind
specific kind of relations, e.g., spatial relations, we obtain the corresponding kind of sequence. In
this work, we study the stories of relations (Section 9.2) and stories of relations
spatial motion
(Section 9.3). There might be more types of relations that depend on the positions and velocities,
if so, the definition of stories extends to that types of relations as well.
We might add a subindex to the story map, i.e., , that indicates the qualitative represen-
tation participating in the story generation. As an illustration, the story map application
RCC
uses the qualitative spatial representation RCC to generate stories; uses the qualitative
QTC
motion representation QTC . But, often, the drops, and we have , because the qualitative
representation is clear from the context, or because we mean a general qualitative represen-
S = σ(K ) S
C C C
i i i
was obtained from scenario . We could have written , but it was clear by
K S = σ (K )
C C C
i i RCC i
the context and the name of the qualitative relations that we were dealing with RCC.
8.5. STORIES: FROM SCENARIOS TO TEMPORAL SEQUENCES 117
(cid:0) (cid:1)
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(0) = (−2, −1), (2, 0); (4, 3), (−1, 0)
C k k l l
y y y
l l l
x x x
k k k
t = 0, DC t = 2, DC t = 4, DC
(a) Story S = (DC)
(cid:0) (cid:1) T
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(0) = (−2, −0.5), (2, 0); (4, 2.5), (−1, 0)
C k k l l
y y y
l l l
x x x
k k k
t = 0, DC t = 2, EC t = 4, DC
(b) Story S = (DC, EC, DC)
(cid:0) (cid:1)
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(0) = (−2, 0), (2, 0); (4, 2), (−1, 0)
C k k l l
3 4
y y y y y
l l l l l
x x x x x
k k k k k
D .
t = 0, DC t = 1.25, EC t = 2, PO t = 2.75, EC t = 4, DC
(c) Story S = (DC, EC, PO, EC, DC)
3 m
(cid:0) (cid:1)
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(0) = (s−2, 0.5), (2, 0); (4, 1.5), (−1, 0)
C k k l l
4 i
y y y e y y y y
l l l l l l l
x xs x x x x x
k k p k k k k k
t = 0.0, DC t = 1.06, EC t = 1.3, PO t = 2.0, TPP t = 2.7, PO t = 2.94, EC t = 4.0, DC
(d) Story S = (DC, EC, PO, TPP, PO, EC, DC)
(cid:0) (cid:1)
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(0) = (−2, 0.75), (2, 0); (4, 1.25), (−1, 0)
C k k l l
y y y y y y y y y
l l l l l l l l l
x x x x x x x x x
k k k k k k k k k
t = 0.0, DC t = 1.01, EC t = 1.5, PO t = 1.71, TPP t = 2.0, NTPP t = 2.29, TPP t = 2.5, PO t = 2.99, EC t = 4.0, DC
(e) Story S = (DC, EC, PO, TPP, NTPP, TPP, PO, EC, DC)
A sample of uniform trajectories, , of two circular entities and with
C k k l l
radii and . Each trajectory displays a story. Although we show only the time interval in , the
r = 1 r = 2 [0, 4]
trajectory can be effortlessly extrapolated to the interval ; and, since each trajectory is the
( , ) K (t)
−∞ ∞
uniform embedding, , of the motion scenario (sketched at ), each obtained qualitative temporal
U K t = 0
t C
sequence is the for each scenario . In other words, these qualitative temporal sequences are
S story K
C C
i i
obtained by story map as .
σ S = σ(K )
C C
i i
118 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
Motion state Category
Categorization
term Kinematic space Categorization rule Category set
(extensional def.)
General
symbol
K f M (f , )
µ i µ
∈ K ∈ M M
Motion scenario Story
Story categorization
term Kinematic space Story map Stories set
(extensional def.)
Story-based
symbol
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) σ S Σ (σ, Σ)
k k l l i
∈ K ∈
Terms and their symbols for the ‘general’ motion categorization model (Ch. 7)
compared with analogous terms and symbols of motion categorization through stories, i.e.,
‘story-based’ categorization.
8.6 Stories become Categories: Story-based categorizations
Stories have two crucial properties that made them apt to form both a
motion categorization
and, specifically, a :
qualitative representation of motion
i. Stories are sequence of relations, i.e., stories have a finite number of relations (Prop. A.2.2).
finite
In that way, computations are facilitated or, rather, made possible: we can obtain the full
temporal sequence of each story, and we can operate with stories as with qualitative rep-
resentation, for instance, to compute inverse and composition (Ch. 11). r
ii. The set of all possible stories in uniform motion is (Prop. A.2.3). Only then makes it
finite
sense to use stories as categories or qualitative relations—It is nogt cognitively economical
to categorize with a categorization that has an infinite numbeor of categories.
D .
Consequently, we have a as we formalized it in Chapter 7, namely
motion categorization o
. The story map is the , because it maps the categorized objects (i.e.,
(σ, Σ) σ categorization rule
motion scenarios) into the categories (i.e., stories ), which form the set of stories . In brief,
S Σ
o i
the motion categorization induced by the stocries is fully defined as the categorization rule and
the set of categories (Tab. 8.1). We caill ‘ ’ any categorization that uses stories as a
Σ s story-based
categorization cue.
Moreover, stories are not only categories, but they are also qualitative relations of motion—
we show it rigorously in Chapter 11. Thus, the stories set is not only a categorization, but
: Σ
also a qualitative reppresentation of motion.
8.6.1 Types of Story-Based Categorization: Bare and Beaded
In this work, we treat two types of story-based categorizations: ‘bare’ and ‘beaded’. We ex-
pound them at length in Chapter 9. Here, we mention them in an example (Ex. 8.10) without
delving into details; we want to illustrate that not only the simple stories, , are story-based
categorizations, but, using the stories, we can create further types of story-based categorizations.
Example 8.10
scenarios, the five snapshots of the trajectory , which can be categorized according to a
K (t)
RCC.
The categorization uses only the story, , to which the scenario belongs, as category.
bare S
8.6. STORIES BECOME CATEGORIES: STORY-BASED CATEGORIZATIONS 119
(cid:0) (cid:1)
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )(2) = (2, 0.7), (1, 1); (2, 0), ( 1, 0)
D k k l l
− − −
y y y
k (cid:126)v (cid:126)v (cid:126)v
l l l
x k x x
(cid:126)v k T
l (cid:126)v l (cid:126)v l
k k
t = 2.0 t = 1.46 t = 1.2
− − −
DC EC PO
y y y
(cid:126)v (cid:126)v (cid:126)v
l l l 4
x x x
l l l
(cid:126)v
k k (cid:126)v 3
k k (cid:126)v
k /
Dt = 0.48 t = 0.00 t =
0.20
TPP NTPP TPP
y y y
(cid:126)v (cid:126)v (cid:126)v
l l h l
x x x
l l l
k (cid:126)v p
t k (cid:126)v
k (cid:126)v
t = 0.60 t = 1.18 t = 2.00
PO EC DC
The snapshots (each in a specific time ) depict the temporal
sequence of RCC relations of the uniform
S = (DC, EC, PO, TPP, NTPP, TPP, PO, EC, DC)
trajectory created by the scenario ( ). The entities and are discs with and
K t = 2 k l r = 1
D k
. We display only the sequence for the interval , but, obviously, it is the same
r = 2 [ 2.0, 2.0]
sequence as for . Therefore, this sequence is a story, .
( , ) S = σ(K )
K D
−∞ ∞
Source: Purcalla Arrufi and Kirsch (2017)
120 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
Scenario Scenario as Bare Beaded
Name Trajectory instant Category Category
K (t)
K K (0.00) S S (DC )
C C C C −
31 3 3 3
K K (1.25) S S (EC )
C C C C −
32 3 3 3
K K (2.00) S S (PO)
C C C C
33 3 3 3
K K (2.75) S S (EC )
C C C C +
34 3 3 3
K K (4.00) S S (DC )
C C C C +
35 3 3 3
, ( ), and according to its story along with the corresponding spatial
PO, EC, DC) bare category
representation ( ), e.g., . T
S (R ) beaded category S (PO)
i j C
v F
l v
l k
Two motion scenarios that seemingly have the same story , but are differently
classified according to a variety of criteria, such as Gestalt psychology, dynamic operations, or
dominance theory. The scenario left, in which both velocity vectors are equal, i.e., , can
(cid:126)v = (cid:126)v
k l
occur only in closed time intervals; such scenario is a rigid story . The scena4rio right
S = (DC)
01 r
can only occur in opened time intervals, velocity vectors are different, i.e., u ; and it is,
(cid:126)v = (cid:126)v
k l
(cid:54)
thus, a singleton story .
S = (DC) 3
Source: Purcalla Arrufi and Kirsch (2018a)
D .
The categorization uses both the story, , and tohe relation, , to which the scenario
beaded S R
i m j
t = 2 S
m C
relation , thus, its beaded category is . To some RCC relations, we have to add the
PO S (PO)
signs ‘ ’ and ‘ ’ to discern which of both reclations in the sequence is meant, the one occurring
− i
first or second. For example, the scenario at belongs to the beaded category ,
s t = 4.0 S (DC )
e C +
because this relation is the second one in the story sequence; as though the story sequence
DC
of would be . The scenario at , however, belongs to the
S (DC , EC , PO, EC , DC ) t = 0.0
C − − + +
3 /
beaded category , because it is the first relation in the story sequence.
S (DC:) DC
C s−
8.6.2 Rigid and Singleton Stories
The stories that have only relation are subdivided in either ‘rigid’ or ‘singleton’ stories. An
one
example of rigid story is , and of singleton story is (Fig. 8.9). It might
S = (DC) S = (DC)
01 11
seem bewildering, that we classify and as different stories, while they have the same one
S S
01 11
element ‘ ’. Admittedly, we are slightly refining our categorization rule, . We differentiate
DC σ
stories that according to the categorization rule should be the same story, when certain properties
hold. In the following, we explain the properties that make stories, such as ,
rigid S = (DC)
different from stories, such as .
singleton S = (DC)
A ‘ ’ is the story of two entities that move with the same velocity, i.e.,
Rigid stories rigid story
, including the case where both objects are motionless (cid:126) (Thereby, we state
(cid:126)v = (cid:126)v (cid:126)v = (cid:126)v = 0
k l k l
that spatial categorization can be seen as a particular case of motion categorization).
8.6. STORIES BECOME CATEGORIES: STORY-BASED CATEGORIZATIONS 121
A ‘ ’ is a story consisting of a single relation, where the
Singleton Stories singleton story
velocities of the entities are different, i.e., .
(cid:126)v = (cid:126)v
k l
(cid:54)
We can immediately see that the story map, , yields rigid stories for every spatial repre-
sentation. That is, every story-based motion representation contains rigid stories. However, not
every representation yields stories; for instance, OPRA yields no singleton stories,
singleton
because any trajectory in uniform motion goes at least through two different relations. In any
case, the two spatial representations used in this work, RCC and OPRA , yield singleton stories,
and, therefore, we devote next section to show the differences between singleton and rigid stories.
Differences between rigid and singleton stories
We have several ways to differentiate between rigid and singleton stories. The most direct way is
possibly the which belongs to the —It has a long
principle of the common fate Gestalt psychology
tradition and profound influence in psychology. We can also distinguish rigid and singleton stories
through dynamic operations (rigid stories react differently to accelerations than singleton stories)
or through continuous transformations (in that case we resort to the commutative diagrams and
dominance theory).
stories are integral part of ‘ ’. Indeed, they are
Gestalt psychology Rigid Gestalt psychology
central in the ‘ ’ (in German, ). This law is a particular
law of common fate gemeinsames Schicksal
case of the ‘ ’ grouping law, which is one of the four original ‘ ’
good continuation grouping laws
or ‘ ’ defined by M. Wertheimer (1923). The law of common
gestalt principles of organization
fate states that entities “moving in time in the same direction and at the same speed” appear to
belong together, to form a single unit (Colman 2015, p. 148; compare, VandenBogs 2007, p. 198).
Actually, as we have seen, Gestalt psychology does not expressly distinoguish between both
D .
types of stories: it singles out the as being extremely salient, and leaves
rigid stories n singleton
unmentioned, most assuredly because they lack such saliency.
stories
Murphy and Medin (1985, po. 295f.) explain that further differences
Dynamic operations
between categories arise “from our knowledge abosut transformations and operations associated
with [them]”; they compare it with finding “higher order features” in the categories. Concerning
the motion scenarios, which are such “operations” associated with them? We argue that they are
the operations, i.e., : stories (i.e., the motion categories) react differently
dynamic accele/rations
depending on the acceleration applied on the entities.
Applying a tangential acceleration, i.e., changing speed, reveals a difference between singleton
and rigid stories. A h remains unchanged for low enough speed changes, while
singleton story
most switch into another story when either entity changes speed, no matter how
rigid stories
low the change might be—plainly expressed, rigid stories are extremely sensitive to tangential
accelerations. This is related to the fact that the velocities in a rigid story have a tighter, a lower
dimension constraint, i.e., , than the velocities in a singleton story, e.g., . This is a
(cid:126)v = (cid:126)v (cid:126)v (cid:126)v
k l k l
(cid:107)
practical distinction, since it repercusses on navigation control (Sect. 12.5).
Rigid and singleton stories behave differently when we apply con-
Continuous operations
tinuous operations on the entities. We note that are a case of continuous
dynamic operations
operations limited to exerting forces on the particles. When we deal with continuous operations,
we can apply the insights of Sections 8.3 and 8.4: direct transitions and dominance theory. We
consider two continuous operations: translation, and speed variation (which is also a dynamic
operation, the tangential acceleration).
122 CHAPTER 8. STORIES AND TEMPORAL SEQUENCES OF RELATIONS
Most remarkably, the rigid stories are closed under translation; the non-rigid stories (e.g., the
singleton stories) are also closed under translation. Thus, there is no direct transition between a
rigid and a non-rigid story, if we allow only translations. However, the rigid non-rigid transitions
are possible, if we allow speed change.
The also differentiates between rigid and singleton stories. The rigid stories
dominance theory
dominate over the non-rigid ones, and, particularly, over the singleton stories, because the rigid
stories correspond to the single value of the parameter . Simply put, if
(cid:126)v (cid:126)v = 0 (cid:126)v (cid:126)v
k l k l
(cid:107) − (cid:107) (cid:107) − (cid:107)
we change only speed, the rigid stories can occur in closed intervals while the non-rigid stories
cannot.
All the above arguments distinguishing rigid and singleton stories are presumably related.
Through dynamic and continuous operations, we show a particular behaviour of the rigid sto-
ries that might explain the saliency captured in the law of common fate. Remember that the
are not explained from higher principles, but postulated from
Gestalt principles of organization
experimental results on human perception (see, Ullman 1979, FSect. 4.3, p. 144).
D .
Chapter 9
Creating Story-Based Categorizations:
Bare and Beaded
In Chapter 8, we have shown that are fit to categorize motion scenarios, and we have
stories A
defined stories for any type of qualitative representations. Particularly, in Section 8.6, we estab-
lished that any categorization using stories as a categorization cue is a categorization.
story-based
Further, in Section 8.6.1, we mentioned two main types of story-based categorizations: ‘bare’
(using stories, , as categories) and ‘beaded’ (using pairs stories-relations, , as categories).
S S (R )
i i j
• The story-based representations are motion categorizations in which t/he categories
bare
are simply the stories, . We call any of such representations ‘ ’, where is the
Stories-r
i o
R R
qualitatDive representation generates it. .
For example, the representations Stories-RCC or Stories-QTC . Some categories of
o B21
Stories-RCC are or , some categories of Stories-QTm C are or .
S S S S
(cid:57)
11 13 B21 00 1 1
• The story-based representation are motion categorizations in which the categories
beaded
are pairs of stories, , and generating relastions, , i.e., . We call any of such
S R S (R )
i j i j
representations ‘ ’, where is the qualitative representation generates it.
Motion- e
R R
For example, the representations Motion-RCC, or Motion-OPRA . Some categories of
Motion-RCC are or , some categories of Stories-OPRA are ( )
S (DC ) / S (PO) S 3
11 − 13 1 C21 1
or . s
S ( 2 )
T (cid:57) 1 ∠ p
In this chapter, we precisely define both types of categorizations—bare and beaded—and
create full-fledged instances of them. Concretely, we create the story-based categorizations
bare
Stories-RCC, Stories-OPRA (Sect. 9.2), and Stories-QTC (Sect. 9.3). We also create the
1 B21
story-based categorizations Motion-RCC, Motion-OPRA (Sect. 9.6), and Motion-QTC
beaded
1 B21
(Sect. 9.7).
Whatever the type of categorization (bare or beaded), the very first step in a story-based cat-
egorization is to find the —We provide an algorithm to find it in Section 9.1. When
stories set Σ
finding the stories set, we obtain, as a by-product, the story map compactly defined. Once we
have the stories set and the story map, we already have the bare story-based categorization.
A beaded story-based categorization is obtained by adding the corresponding relation to
the story given by the bare story-based categorization (Sect. 9.5). Equivalently, in the beaded
categorization, the categorization rule is the Cartesian product of the story map, , and the
qualitative representation used to create the story map, , i.e., (Sect. 9.5.1) .
ρ f = σ ρ
µ R
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
124 BARE AND BEADED
9.1 Finding the Stories Set Σ
To obtain the story of a single scenario might be laborious but straightforward: we manually ap-
ply the story map to the scenario. However, to obtain the stories set seems an overwhelming
σ Σ
process. Theoretically, we should apply the storymap to every scenario in the
σ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
kinematic space to obtain the stories set . Obviously, we cannot literally apply to “every”
Σ σ
scenario in the kinematic space, this is why we present a practical method to obtain .
Conveniently, as we find the stories set, we find also a compact form of the story map. Indeed,
when we find the stories, we find simultaneously their , i.e., the
categorical regions K , . . . , K
1 n
{ }
region for each story (Sect. 7.1, p. 85). Since the categorical regions are finite, the story
K S
i i
map is defined as a piecewise constant function in the kinematic space. TherefoTre, to obtain the
categorical regions is equivalent to obtain the story map.
Summing up, the method for generating the stories set below, determines at once the stories
set with their categorical regions { ,. . . , }, and the story map based on such categor-
Σ K K σ
1 n
ical regions. The method fully determines the —actually,
bare story-based categorization (σ, Σ)
, if we make explicit the representation —and we call such categorization ‘ ’.
(σ , Σ ) Stories-
R R
R R
9.1.1 Algorithm for stories set generation
At the beginning we have the empty stories set, , the empty regions set, ,
Σ = K =
{} 4 {}
and the story map defined by a qualitative representation (Eqs. (8.11) and (8.12))
R u
1. We pick a random motion scenario that does not belong to any
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) /
a k k l l
∈ K g (cid:83)
categorical region in the regions set . That is, we pick .
K K K K
i a i
K ∈K
o ∈ {K \ i }
D .
2. We obtain the scenario’s story with the story map, n .
S = σ (K )
oa R a
3. Add the story to the stories set,
Σ = Σ S
R R a
o∪ { }
4. We find the categorical region of the story , i.e., all scenarios in the kinematic space that
belong to such story, i.e., −1 .
K = σ (S )
a R a
5. We add the categorical region to the regions set
K = K K
s a
∪ { }
6. Repeat steps 1 to 5 until the whole kinematic space is partitioned in categorical regions,
each with its corresponding story.
At the end we have the stories set , the categorical regions
Σ = S , . . . , S K =
R 1 n
{ }
, and the story map defined according to the categorical regions.
K , . . . , K σ
1 n R
{ }
9.2 Stories- of Spatial Representations
We show now concrete examples of the stories set for spatial representations. We choose
two spatial representations which are simple but non-trivial: RCC and OPRA . Since we use
the method in Section 9.1 to generate , we will obtain for each spatial representation a
Σ bare
, namely, ‘ ’ and ‘ ’.
story-based categorization Stories-RCC Stories-OPRA
9.2. STORIES- OF SPATIAL REPRESENTATIONS 125
l l l
k k k
(a) A story S = (DC)
k k k k k
l l l l l
(b) A story S = (DC, EC, PO, EC, DC)
Examples of stories in RCC.
(Source: Purcalla Arrufi and Kirsch (2018a))
9.2.1 Stories-RCC
We generate the categorization Stories-RCC by applying the method for stories set generation
(Sect. 9.1) with the spatial representation RCC. Our start point is the spatial map
= δ
provided by RCC, i.e., ; this map relates each motion scenario with a spatial
δ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
RCC
relation RCC.
The spatial map of RCC, , and, consequently, the ensuing story map depend
δ σ
RCC RCC
heavily on the geometry of the entities. In this work, we assume that the entities ( and ) are
k l
discs with radii and , if we have to compute RCC relations and stories; because, then, we
r r g
k l
have the simplest formulation of and .
δ σ /
RCC RCC
Regarding notation, we tend to drop the suffix RCC and write simply and , whenever it
δ σ
is clear that we deal with the representation RCC.
D .
A. Spatial Map δ
RCC
For disc entities, and , the spatial relation of a motion scenario depends exclusively on their
k l
radii and on the distance between the entities’ centres: , , .
r r d((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x
k l k k l l l k
i (cid:107) − (cid:107)
t if
DC d > d
/  2
/  if
EC d = d
:  2
s 
if
PO d > d > d
p 
 2 4
t 
 if (cid:27)
t  TPP d = d
h 4 if (9.1)
δ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) := r < r
if
k k l l NTPP d > d k l
RCC
 if (cid:27)
 TPPI d = d
 4
if
r > r
 if
 NTPPI d > d k l
 4
 (cid:9)
 if if
EQ d = d (= 0) r = r
4 k l
distance at spatial relation
d = r + r , EC
2 k l
| |
distance at spatial relation
d = r r , TPP
4 k l
| − |
B. Stories Set Σ
RCC
With the spatial map , we need only apply the definition of story map (Eq. (8.11)) to
RCC
generate the stories—we show some examples of stories in Fig. 9.1. By applying our method,
Σ = S ,
0 01
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
126 BARE AND BEADED
Rigid stories (cid:126)v = (cid:126)v
k l
 ( ) ( )
DC S PO S
01 03
 ( )
 EC S
r < r
 k l
 F
( ) ( )
TPP S NTPP S
04 05
 r > r
 k l
 ( ) ( )
 TPPI S NTPPI S
 06 07
 A
 r = r
k l
 ( )
EQ S
Non-rigid stories (cid:126)v = (cid:126)v
k l g
(cid:54)
( )
 DC / S
 ( , , )
 DC EC DC S
 r
 ( , , , , )
D  DC EC PO EC DC . S
 s
 o
r < r
 k l m
( , , , , , , )
 DC EC PO TPP PO EC DC S
 m 14
( , , , , , , , , )
DC EC PO TPP NTPP TPP PO EC DC S
Σ o 15
 i r > r
 k l
 ( , e, , , , , )
 DC EC PO TPPI PO EC DC S
 h 16
 ( , , , , , , , , )
 DC EC tPO TPPI NTPPI TPPI PO EC DC S
 / 17
 : r = r
 s
k l
( , , , , , , )
DC EC PO EQ PO EC DC S
t 18
Stories set, , of RCC consists of stories. It is subdivided in rigid stories,
( stories), and non-rigid stories, ( stories). Some stories are dependent on which entity is
8 Σ 8
larger, as indicated through the radii relations ( and ). The non-rigid stories are symmetrical,
r r
k l
so we have marked bold-faced the middle relation for clarity.
9.2. STORIES- OF SPATIAL REPRESENTATIONS 127
k l
v v
k l
− o
D .
o S
11 s
Depiction of , the non-rigid stories of for two entities
11 12 13 14 15
RCC
{ }
and with and (Tab. 9.1). Two stories correspond to one-dimensional regions:
k l r < r (cid:126)v = (cid:126)v
k l k l
(cid:54)
, and . The remaining three stories
S = (DC, EC, DC) S = (DC, EC, PO, TPP, PO, EC, DC)
12 14
correspond to two-dimensional regions: , ,
S = (DC) S = (DC, EC, PO, EC, DC) S = (DC,
11 13 15
EC, PO, TPP, NTPP, TPP, PO, EC, DC)
We depict as being motionless and moving with the difference of velocities ,
l k (cid:126)v = (cid:126)v (cid:126)v
kl k l
which is an equivalent depiction as moving with and moving with . The stories depend
l (cid:126)v k (cid:126)v
l k
on the direction of .
(cid:126)v
kl
Source: Purcalla Arrufi and Kirsch (2017)
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
128 BARE AND BEADED
, the rigid stories, and , the non-rigid stories. Some stories
S , . . . , S Σ = S , S , . . . , S
02 08 1 11 12 18
} { }
are restricted to certain ratios of the entities’ radii: , , or . The ratios
r < r r > r r = r
k l k l k l
depict how the non-rigid stories originate from the motion of the two entities with different size,
r < r
k l
C. Story Map σ and Featural Variables
RCC
As we generate the RCC stories, we observe that the categorical regions are fully determined by
two real variables: the minimum distance between entities in the story, (Eq. (9.3a)), and
min
the dissimilarity between vectors, (Eq. (9.3b))—additional parameters are the radii, and
dif r
V k
, but they are constant for all stories of the same entities; for this reason, we do not consider
them featural variables. It is, thus, more natural to define the story map as the composition
of two functions: and (Eq. (9.2)). We call the first function,
σ (d , dif ) Φ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) Φ
Φ min V k k l l
(Eq. (9.4)), , because it maps the kinematic variables
feature extraction function A ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
into , which act as , or simply, as . We call the second
(d , dif ) featural variables features
min V
function, (Eq. (9.5)), ‘ ’, because it maps the featural variables into stories.
σ featural story map
Story map r
D .
(9.2)
σ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = σ (d , dif ) Φ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l Φ min V k k l l
RCC m ◦
Featural variables and feature extraction function
(9.3a)
d ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x det((cid:126)x (cid:126)x , (cid:126)v (cid:126)v )
min k k l l l k l k l k
(cid:107) − (cid:107)| − − |
(cid:126)v (cid:126)v
l k
 (cid:107) − (cid:107) (cid:126)v = 0, (cid:126)v = 0
k l (9.3b)
dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)v + (cid:126)v (cid:107) (cid:107) (cid:54) (cid:107) (cid:107) (cid:54)
V k k l l k l
(cid:107) (cid:107) (cid:107) (cid:107)
0 (cid:126)v = (cid:126)v = 0
k l
(cid:107) (cid:107) (cid:107) (cid:107)
(9.4)
Φ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (d ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ); dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ))
k k l l min k k l l V k k l l
9.2. STORIES- OF SPATIAL REPRESENTATIONS 129
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
k k l l
σ =σ ◦ Φ
RCC Φ
(d , dif ) S
min V i
RCC RCC
Categorization Stories-RCC reflecting the model in Section 7.1 ( ). The story
map of RCC, , assigns a story —a motion category—to every scenario in
σ S ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
i k k l l
RCC
the kinematic space . The story map can be expressed as a two-step process: first,
RCC
the featural variables, and , are extracted from the kinematic variables
d dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
min V k k l l
through function ; second, based on the features, the featural story map assigns a story to
Φ σ
the scenario features, i.e., it performs a feature based categorization.
Featural story map
 r
Rif 
S d > d u
 01 min 2
  g
 if 
 S d = d  3
 02 min 2 
  /
 if 
 S d > d > d 
 03 2 min 4 
 
  r
 if (cid:27)  o
 S d = d 
D   04 min 4 if r < r   .
 s
if k l if
 S d > d n dif = 0
 05 4 min V
 o
 (cid:27) 
 if m 
 S d = d 
 06 min 4 if 
 r > r 
 if m k l 
 S d > d 
 07 4 min 
 o 
 
 (cid:9) 
 if c if 
S d = d r = r
 
 08 min 4 s k l 
(9.5)
σ (d , dif ) := s
Φ min V e
 if h
 S d > d
 11 min 2
t 
 if /
 S d = d 
 12 / min 2 
 if
 S : d > d > d 
 s13 2 min 4 
p 
 (cid:27) 
t if 
S d = d 
t  14 min 4 if
r < r if
h   if k l dif > 0
S d > d V
 15 4 min
(cid:27) 
 if
 S d = d
 16 min 4 if 
 r > r
 if k l 
 S d > d
 17 4 min 
 (cid:9)
 if if 
S d = d r = r
18 min 4 k l
distance at spatial relation
d = r + r , EC
2 k l
| |
distance at spatial relation
d = r r , TPP
4 k l
| − |
The story map of Stories-RCC (Eq. (9.2)) is a clear example of the formalization of catego-
story map can be seen as a two-step process: initially, the
σ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) feature extraction
k k l l
RCC
, (Eq. (9.4)), extracts the features, and , from the motion scenario
function Φ d dif ((cid:126)x , (cid:126)v ;
min V k k
; afterwards, the featural story map (Eq. (9.5)) assigns the story based on the features
(cid:126)x , (cid:126)v ) σ
l l Φ
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
130 BARE AND BEADED
of the scenario.
9.2.2 Stories-OPRA
We generate the categorization Stories-OPRA by applying the method for stories set generation
(Sect. 9.1) to the spatial representation OPRA (Sect. 5.2.2). Our start point is the spatial
map provided by OPRA ; the map relates each motion scenario to a spatial
δ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
1 k k l l
relation.
A. Spatial Map δ
OPRA
The spatial representation OPRA describes relative orientation of entities; for that reason
OPRA requires the orientation vectors of the entities, and ; vectors that are intrinsics
(cid:126)o (cid:126)o
1 k l
to the entities. Nevertheless, we can use OPRA in motion scenarios even when entities do not
have an intrinsic orientation, e.g., simple points and circles, as we explain in the following.
In the case of moving entities, we identify the orientation vectors with the velocity vectors,
. This is the most common way humans, animals, and vehicles move: they move
(cid:126)o := (cid:126)v
∗ ∗ g
“forwards”—We leave the treatment of motions with divergent, i.e., norn-aligned, orientation and
veloc D ity for future work. For that reason, the definition of the .spatial map in Equations (9.6)
and (9.7) uses the velocity vectors, , instead of the orientation vectors, .
(cid:126)v (cid:126)o
∗ ∗
In case that at least one entity is motionless , the orientation is given by the
(cid:126)v = 0 (cid:126)o
∗ ∗
direction of its most recent past or future non-zero velocity vector . This solves the problem of
(cid:126)v
undetermined orientation for motionless entities in the course of a trajectory. Note that when a
motionless entity accelerates, it must bsegin the motion in the direction of its current orientation,
i.e., its last velocity, so that the cohndition always holds. Such requirement is consistent
(cid:126)o = (cid:126)v
∗ ∗
with our ban on entities spin/ning (Ch. 2): an entity cannot stop, turn, and start again in a
different direction, but it must start in the direction in which it most recently stopped.
As last case, itf the entity is motionless for the whole trajectory, or we want to categorize
a motion scenario alone, i.e., without trajectory, then we might question whether categorizing
according to a directional representation (i.e., OPRA ) is reasonable for an entity that neither
has intrinsic orientation vector nor moves at any time. Of course, we can always define an
arbitrary intrinsic orientation for the entity, as a workaround, but the meaningfulness of such
decision is debatable.
(cid:40)
if
dif > 0
a X
(9.6)
δ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) =
k k l l
OPRA if
1 c dif = 0
(cid:126)x (cid:126)x
l k
 (cid:107) − (cid:107) (cid:126)x = 0, (cid:126)x = 0
where k l
dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x + (cid:126)x (cid:107) (cid:107) (cid:54) (cid:107) (cid:107) (cid:54)
X k k l l k l
(cid:107) (cid:107) (cid:107) (cid:107)
0 (cid:126)x = (cid:126)x = 0
k l
(cid:107) (cid:107) (cid:107) (cid:107)
9.2. STORIES- OF SPATIAL REPRESENTATIONS 131
, , and are defined as follows:
a b c
(cid:27)
 if
0 cos(α ) > 0
∆xv if
 k sin(α ) = 0
 if ∆xv
 2 cos(α ) < 0 k
∆xv
k (9.7a)
a =
if
1 sin(α ) > 0
 ∆xv
if
3 sin(α ) < 0
∆xv
(cid:27)
 if
0 cos(α ) < 0
v ∆x if
 l sin(α ) = 0
 if v ∆x
 2 cos(α ) > 0 l
v ∆x
l T (9.7b)
b =
if
1 sin(α ) > 0
 v ∆x
if
3 sin(α ) < 0
v ∆x
where
∆(cid:126)x = (cid:126)x (cid:126)x
l k
(cid:27)
 if
0 cos(α ) > 0
vv if
 sin(α ) = 0
 if vv
 2 cos(α ) < 0
vv
(9.7c)
c =
if
1 sin(α ) > 0
 vv
if
3 sin(α ) < 0
vv
When (cid:126) or (cid:126), then, instead of the zero vector, we use its
(cid:126)v = 0 (cid:126)v = 0
k l
most recent (past or future) non-zero velocity vector or , as-
(cid:126)v (cid:126)v
k l g
suming that the motion scenario belongs to a trajectory. Otherwise, 3
we resort to defining an orientation vector or .
(cid:126)o (cid:126)o
k l
If we examine the spatial relation (Eqs. (9.6) and (9.7)), it apparently depends on
D δ .
OPRA
1 n
four real values: , and the angles , , .
dif α = (∆(cid:126)x, (cid:126)v ) α = ((cid:126)v , ∆(cid:126)x) α = ((cid:126)v , (cid:126)v )
X ∆xv ∠ k v ∆x o ∠ l vv ∠ k l
k l
However, the variable is not independent: we see that m because
α α = (α + α )
vv vv v ∆x ∆xv
l k
. Thus, the total number of is three:
((cid:126)v , (cid:126)v ) = ( ((cid:126)v , ∆(cid:126)x) + (∆(cid:126)x, (cid:126)v )) featural variables
∠ ∠ ∠
k l l k
− o
, , and .
dif α α
X ∆xv v ∆x
k l
By the way, to compute , we just need the of the sine and cosine of the vector
δ i sign
OPRA
angles, which can be efficiently calculated by means of dot product, (cid:126) (cid:126), and the
h cos( ((cid:126)a, b)) (cid:126)a b
t ∝ ·
determinant of the vectors, (cid:126) (cid:126) .
sin( ((cid:126)a, b)) det((cid:126)a, b)
B. Stories Set Σ
OPRtA
With the spatial map , we need only apply the definition of story map (Eq. (8.11)) to
OPRA
generate the stories—we show some examples of stories in Fig. 9.4. By applying our method, we
OPRA
subsets ; the subsets grouped by pairs in three superordinate groups.
• Entities cross:
both entities are moving
one entity is motionless
• Entities move parallel:
the entities’ trajectories are superposed
the entities’ trajectories are not superposed
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
132 BARE AND BEADED
Non-rigid stories (cid:126)v = (cid:126)v
k l
(cid:54)
Non-Parallel velocities (cid:126)v (cid:126)v
k l
(cid:126)v = 0, (cid:126)v = 0
k l
 (cid:54) (cid:54)
 3 0 1 1 1 1 0 3 3 3
S S
∠ ∠ ∠ ∠ ∠ (cid:57) ∠ ∠ ∠ ∠ ∠ (cid:57)
1 1 1 2 3 C1 1 3 3 3 2 1 C2 1
3 1 1 3
3 S 1 S
∠ ∠ ∠ ∠ ∠ ∠
 1 3 C10 3 1 C20
3 3 3 2 1 1 1 1 2 3
S S
∠ ∠ ∠ ∠ ∠ ∠ ∠ ∠ ∠ ∠
1 0 3 3 3 C11 3 0 1 1 1 C21
(cid:126)v = 0, (cid:126)v = 0 T
 k l
 (cid:54)
3 0 1 1 0 3
 S S
 ∠ ∠ ∠ (cid:57) ∠ ∠ ∠ (cid:57)
 1 1 1 B1 1 3 3 3 B2 1
 3 3 1 S 1 1 3 S
 ∠ ∠ ∠ ∠ ∠ ∠
 0 2 B10 0 2 B20
 3 2 1 1 2 3
S S
∠ ∠ ∠ ∠ ∠ ∠
3 3 3 B11 1 1 1 B21
(cid:126)v = 0, (cid:126)v = 0
 k l
 (cid:54)
 1 1 1 3 3 3
 S S
∠ ∠ ∠ (cid:57) ∠ ∠ ∠ (cid:57)
 1 2 3 B3 1 3 2 1 B4 1
0 2 0 2
 3 S 1 S
∠ ∠ ∠ ∠ ∠ ∠
 1 3 B30 3 1 B40
 A
3 3 3 1 1 1
S S
∠ ∠ ∠ ∠ ∠ ∠
1 0 3 B31 3 0 1 B41
Parallel velocities (cid:126)v (cid:126)v
k l
(cid:107)
 The entities’ trajectories are superposed u
 g
 2 0 3
0 S
∠ ∠ ∠ (cid:57)
0 2 T 1 /
T 0 2 g
2 S
∠ ∠ ∠
 0 2 T0 r
 o
0 2
0 S
D ∠ 2 ∠ ∠ 0 T1 .
 n
The entities’ trajectories are not superposed
 m
singleton stories
Σ m
3 3
S S
∠ ∠
 1 P2 o 3 P3
1 c 1
S S
∠ (cid:57) ∠
3 s P 2 1 P1
Rigid stories (cid:126)v = (cid:126)v
k l
(cid:126)v = 0
(cid:107) (cid:107) (cid:54)
t 2 0
Σ S 0 S S
∠ (cid:57) ∠ ∠
h E 0 E 2 E0 2 E2
1 3
S S
∠ (cid:57) ∠
3 E 1 1 E1
(cid:126)v = 0
 (cid:107) (cid:107)
 0 S 0 S 0 S 0 S
 ∠ ∠ ∠ ∠
 0 R00 1 R10 2 R20 3 R30
1 1 1 1
S S S S
∠ ∠ ∠ ∠
Σ 0 R01 1 R11 2 R21 3 R31
2 2 2 2
 S S S S
∠ ∠ ∠ ∠
 0 R02 1 R12 2 R22 3 R32
3 3 3 3
 S S S S
∠ ∠ ∠ ∠
 0 R03 1 R13 2 R23 3 R33
0 S 1 S 2 S 3 S
∠ ∠ ∠ ∠
R0 R1 R2 R3
Stories set of OPRA , divided into meaningful subsets of stories: , , ,
1 C B T
, , . A total of 50 stories, but 20 belong to , i.e., fully motionless entities.
Σ Σ Σ Σ
P E R R
9.2. STORIES- OF SPATIAL REPRESENTATIONS 133
Story
P3
D .
3 3 3
∠ ∠ ∠
3 3 3
Story
C11
k 2 3
2 3
k 0
2 k
3 3 3 2 1
∠ ∠ ∠ ∠ ∠
1 0 3 3 3
Representation of two stories of OPRA , and . We display for each story
1 P3 C11
Source: Purcalla Arrufi and Kirsch (2018a)
• Rigid stories:
both entities are moving
both entities are motionless
C. Story Map σ and Featural Variables
OPRA
In OPRA , we separate the description of the story map, , in two cases that we treat
OPRA
differently: first, the non-rigid stories ( ); second, the rigid stories ( ) (See Tab. 9.2).
(cid:126)v = (cid:126)v (cid:126)v = (cid:126)v
k l k l
(cid:54)
The rigid stories behave mostly like the spatial relation OPRA , as described in Equations (9.6)
and (9.7). For that reason, in this section, we only describe the story map for the first case, the
non-rigid stories.
The OPRA non-rigid stories can be fully determined using four real features:
• , the angle between orientations, i.e., .
α ((cid:126)v , (cid:126)v )
vv k l
To determine the category, we compute its sign, which is equivalent to the sign of .
det((cid:126)v , (cid:126)v )
k l
• , the angle between the positions’ difference and the velocities’ difference, i.e.,
∆x∆v
(∆(cid:126)x, ∆(cid:126)v)
To determine the category, we compute its sign, which is equivalent to the sign of .
det(∆(cid:126)x, ∆(cid:126)v)
• and , which are the velocity ratio for each entity.
u u
k l
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
134 BARE AND BEADED
(cid:40) (cid:40)
(cid:107)(cid:126)v (cid:107) (cid:107)(cid:126)v (cid:107)
k (cid:126)v = 0 l (cid:126)v = 0
max{(cid:107)(cid:126)v (cid:107),(cid:107)(cid:126)v (cid:107)} k (9.8a) max{(cid:107)(cid:126)v (cid:107),(cid:107)(cid:126)v (cid:107)} l (9.8b)
u = k l (cid:107) (cid:107) (cid:54) u = k l (cid:107) (cid:107) (cid:54)
k l
0 (cid:126)v = 0 0 (cid:126)v = 0
k l
(cid:107) (cid:107) (cid:107) (cid:107)
We use these ratios to know, whether and which entity is motionless, i.e., whether ;
u = 0
and, also, to know whether one entity is faster than the other: for non-rigid scenarios
, or, equivalently, .
u < 1 (cid:126)v < (cid:126)v u < 1 (cid:126)v < (cid:126)v
k k l l l k
⇔ (cid:107) (cid:107) (cid:107) (cid:107) ⇔ (cid:107) (cid:107) (cid:107) (cid:107)
Since we fully determine the OPRA story of each scenario using the fea-
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
1 k k l Tl
tures above, we have again the categorization model displayed in : first, we extract the
??
Stories-OPRA features from the scenario, which we can express as the feature extraction func-
tion ; and, second, we use the features to map the scenario into its corresponding story, which
is the featural story map of OPRA , —Since the featural variables are independent and each
1 Φ
forms at most four featural regions, we can represent as a tree (Fig. 9.5).
9.3 Stories- of Motion Representations
The method in Section 9.1 for obtaining Stories- categorizations is a general method that uses
any kind of qualitative representations. Remember that, as we mentioned in Section 8.1, the
only condition for using a certain qualitative representation, is that it categorizes scenarios—
This condition was expressed in Equation (8.3). In the specific case of a qualitative representation
that categorizes motion scenarios, Equation (8.3) can be rewritten as follows:
of motion
µ :
D . (9.9)
K −→ M s
K = ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) Mn
k k l l i
(cid:55)−→
We have just made following substitutions: the qualitative representation is expressed as
a motion representation ; the qualitative relatiom n is expressed as a motion relation ;
R M
i i
the relational map of a qualitative represenotation is expressed as the relational map of a
ρ µ
qualitative motion representation. s
The same substitutions take place in Equation (8.12) (the definition of story map), so that
we obtain following:
s σ : Σ
(9.10)
K −→
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) S = (M , M , . . . , M )
k k l l i i i i
t (cid:55)−→ 1 2 m
We can see, in Eq. (9.10), that the stories of motion representations, , are a sequence of
relations, , and, specifically, a sequence of motion relations, .
R M
i i
Now, the construct “story of motion representations” might be confusing at first glance. If a
motion representation already categorizes motion, what does a story of a motion representation
categorize? The categorize motion scenarios too. In short, we
stories of motion representations
can create a new motion categorization, e.g., Stories-QTC (Sect. 9.3.1), using the stories of
B21
another motion categorization, e.g., QTC .
B21
Notwithstanding, the process for obtaining new motion
Idempotence of Stories- Operation
categorizations by obtaining their stories cannot be applied recursively, indeed, the story of a
certain story is by definition this same story, i.e., Stories-{Stories- } Stories- . That is
R R
the Stories- operation that maps a qualitative representation into a motion representation is
idempotent
9.3. STORIES- OF MOTION REPRESENTATIONS 135
C11
u = 0, u = 0
k l
(cid:54) (cid:54)
u = 0
l S
B11
u = 0
B31
α < 0
∆x∆v
C10
u = 0, u = 0
k l
(cid:54) (cid:54)
α = 0
∆x∆v u = 0
α l S
∆x∆v B10
u = 0
B30
α > 0
∆x∆v
(cid:57)
C1 1
u = 0, u = 0
k l F
(cid:54) (cid:54)
u = 0
l S
(cid:57)
B1 1
u = 0
(cid:57)
B3 1
P2
u < 1
° k
α < 0
vv
° ° u < 1
α ( 180 , 0 ) l
vv 4
∈ − S
(cid:57)
P 2
α < 0 g
∆x∆v
T1
u < 1 g
α = 0 rigid stories
∆x∆v u = u = 1 o
α k l
∆x∆v
D Σ .
E s
u < 1 n
m S
(cid:57)
T 1
α > 0
∆x∆v m
° o
α = 0 S
vv P (cid:57) 2
u < 1
s k
s u < 1
P2
σ = α / S C2 (cid:57) 1
Φ vv
OPRA 1 :
s u = 0, u = 0
( ) k l
(cid:126)v = (cid:126)v p (cid:54) (cid:54)
k l
(cid:54) u = 0
t l S
(cid:57)
t B2 1
α > 0
vv
° ° u = 0
α (0 , 180 ) k
vv
(cid:57)
B4 1
α < 0
∆x∆v
C20
u = 0, u = 0
k l
(cid:54) (cid:54)
α = 0
∆x∆v u = 0
α l S
∆x∆v B20
u = 0
° B40
α = 180
vv
α > 0
∆x∆v
C21
u = 0, u = 0
k l
(cid:54) (cid:54)
u = 0
l S
B21
u = 0
B41
P3
α < 0
∆x∆v
α = 0
α ∆x∆v S
∆x∆v T0
α > 0
∆x∆v
P1
Featural story map for Stories-OPRA , . It maps scenarios ( )
1 Φ k l
(cid:54)
into the corresponding stories by means of following four featural variables: ;
α = ((cid:126)v , (cid:126)v )
vv k l
, where , ; and (cid:107)(cid:126)v (cid:107) .
α = (∆(cid:126)x, ∆(cid:126)v) ∆(cid:126)x = (cid:126)x (cid:126)x ∆(cid:126)v = (cid:126)v (cid:126)v u = ∗
∆x∆v l k l k ∗
max{(cid:126)v ,(cid:126)v }
− − k l
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
136 BARE AND BEADED
Motion
Story
Scenario
l l k
k l
(+,–)
(–,–) (0,–) (+,–) (+,0) (+,+)
K S
(cid:57)
1 3 1
l F
k k k k k
(–,+) (–,0) (–,–) (0,–) (+,–)
(–,0)
K S
(cid:57)
2 2 3
Two scenarios, and , with their corresponding stories based in the motion
R 1 2
representation QTC : , and . The position of each
S = σ (K ) S = σ (K )
(cid:57) (cid:57) g
B21 3 1 1 2 3 2
QTC QTC
B21 B21 3
scenario in its own story is marked with a red dashed rectangle.
D .
9.3.1 Stories-QTC
B21
As an example of stories of motion categorization, we create Stories-QTC . We apply the
B21
algorithm given in Section 9.1.1 which uses the story map. The story map is created by
o σ
c QTC
B21
means of the categorization map of QTC , which is presented as in Section 7.3, and here
iB21 µ
we will rename according to the notation in Equation (9.9).
µ e
A. Story Map σ / and Featural Variables
QTC:
B21
The QTC scenarios are mapped into different stories by means of (Tab. 9.3).
18 σ
B12
QTC
B21
QTC
B21
stories.
Similarly to the previous story maps, and , the story map is determined
σ σ σ
RCC OPRA QTC
1 B21
by a limited number of featural variables, namely, , , , (Tab. 9.4). In other words, each
α τ γ q
story is determined by 4-tuples of features. Most stories are determined by a unique tuple of
features, while some, such as , consist of several tuples. An interesting cognitive question is
(cid:57)
1 2
whether stories defined with several tuples are recognized as a simple category or subjects tend
to associate each single tuple to a unique category—We leave this question for future work.
We explain here, in detail, the featural variables of QTC :
B21
i) , ‘ ’: It is the angle of the crossing trajectories, i.e., the minimum angle
α crossing angle
between the entities’ velocities:
(cid:126)v (cid:126)v
k l ° ° (9.11)
α((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = arccos( · ) [0 , 180 ]
k k l l
(cid:126)v (cid:126)v ∈
k l
(cid:107) (cid:107)(cid:107) (cid:107)
9.3. STORIES- OF MOTION REPRESENTATIONS 137
Story description Featural variables
Story name
(as a sequence of QTC relations)
(α, τ, γ, q)
B21
Rigid stories (cid:126)v = (cid:126)v
k l
P − =
S ( , +) ( , k , v , )
(cid:57)
0 1
− ∗
= =
S (0, 0) ( , k , v , )
00 T
+ =
S (+, ) ( , k , v , )
− ∗
Non-rigid stories (cid:126)v = (cid:126)v
k l
(cid:54)
( , , , )
∗ ∗ ∗
FC
S ( , ), (0, 0), (+, +) ( , k , , )
(cid:57)
1 2
− − ∗ ∗
BC = −
( , k , , q )
A BC = + =
S ( , 0), (0, 0), (+, 0) ( , k , v , q )
(cid:57)
1 1
P +
( , , v , )
S ( , +), (0, 0), (+, ) k
∗ ∗
10 BC = + +
− − ( , k , v , q )
BC −
R = =
S (0, ), (0, 0), (0, +) ( , k , v , q )
P −3
( , , v , )
S (+, ), (0, 0), ( , +) /k
∗ ∗
12 BC −
− − ( ,gk = , v , q + )
r k
D . FC −
s ( , k , , )
S ( , ), ( , 0), ( , +), (0, +), (+, +) n
(cid:57) ∗ ∗
3 1 BC − −
− − − − o ( , k , , q )
m ∗
m FC +
( , k , , )
S ( , ), (0, ), (+, ), (+, 0), (+, +)
∗ ∗
31 o BC + −
− − − − ( , k , , q )
BC + + =
S [( , 0)], ( , ), (0, ),e(+, ), [(+, 0)] ( , k , v , q )
(cid:57)
2 1
− − − −h −
BC − + =
S [( , 0)], ( , +), (0, +), (+, +), [(+, 0)] ( , k , v , q )
(cid:57) t
2 2
− − /
BC + + +
S ( , +), ( , 0), ( , ), (0, ), (+, ) ( , k , v , q )
(cid:57) /
2 3
− :− − − − −
S ( , +),s(0, +), (+, +), (+, 0), (+, ) ( BC , k − , v + , q + )
(cid:57)
2 4 p k
− −
t BC − − =
S [(0, )], ( , ), ( , 0), ( , +), [(0, +)] ( , k , v , q )
h − − − − −
BC −
+ =
S [(0, )], (+, ), (+, 0), (+, +), [(0, +)] ( , k , v , q )
− −
BC − − +
S (+, ), (0, ), ( , ), ( , 0), ( , +) ( , k , v , q )
− − − − − −
BC −
+ +
S (+, ), (+, 0), (+, +), (0, +), ( , +) ( , k , v , q )
− −
‘∗’ in the featural variables means that any value in the corresponding feature is allowed.
Square brackets around a relation, such as [(−, 0)], means that this is a limit relation either for t → −∞ or
t → +∞.
B12
QTC
B21
. It contains stories, and the corresponding tuple of
σ 18 featural variables F = (α, τ, γ, q)
QTC
B21
that allows mapping each motion scenario into the corresponding story (See feat. var. Tab. 9.4).
We note that some stories, such as , are characterized by several featural tuples.
(cid:57)
1 2
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
138 BARE AND BEADED
According to the values that takes in the different stories, we identify four different
featural regions that categorize stories: A, P, FC, and BC. The regions are linked to the
values of in the following equation:
A ‘ ’ if ° , i.e.,
antiparallel α = 180 vˆ vˆ = 1
k l
· −
P ‘ ’ if ° , i.e.,
parallel α = 0 vˆ vˆ = +1
k l (9.12)
FC ‘ ’ if ° ° , i.e.,
front crossing 90 α < 180 1 < vˆ vˆ 0
k l
≤ − · ≤
BC ‘ ’ if ° , i.e.,
back crossing α < 90 0 < vˆ vˆ < 1
k l
ii) , ‘ ’: It indicates which of the entities goes first through the crossing point.
τ crossing delay
yields the time difference, , between the arrival of and at the crossing point
τ τ = t t k l
k l
(See full deduction of the formulae in Propositions A.4.6 and A.4.8). Note that can only
be computed when ∦ or . For the rest of stories is undetermined, but it does
(cid:126)v (cid:126)v (cid:126)v = (cid:126)v τ
k l k l
not influences categorization.
(cid:126)v (cid:126)v
k l for ∦ (9.13a)
τ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) =((cid:126)x (cid:126)x ) ((cid:126)v A (cid:126)v ) × (cid:126)v (cid:126)v
k k l l k l k l k l
− × − · (cid:126)v (cid:126)v 2
k l
(cid:107) × (cid:107)
((cid:126)x (cid:126)x ) vˆ
k l k limit for (9.13b)
τ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = − · (cid:126)v = (cid:126)v
k k l l k l
− (cid:126)v
(cid:107) (cid:107)
According to the values that takes in the different stories, we identify three different
featural regions that categorize stories:
, and −. The regions are linked to the
k k k
values of in the following equation: g
D .
+ if crosses first o
k τ < 0 k
if and cross simultaneously, i.e., they collide (9.14)
k τ = 0 k l
− if crosses first
k τ > 0 l
iii) , ‘ ’: It indicates ewhich entity moves faster.
γ relative speed
/ (9.15)
γ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v )
: k k l l V k k l l
According to the values that takes in the different stories, we identify three different
t γ
featural regions that categorize stories: +,
, and −. The regions are linked to the
v v v
k k k
values of in the following equation:
+ if , i.e., is faster
v γ > 0 (cid:126)v > (cid:126)v k
k l
(cid:107) (cid:107) (cid:107) (cid:107)
if , i.e., and move equally fast (9.16)
v γ = 0 (cid:126)v = (cid:126)v k l
k l
(cid:107) (cid:107) (cid:107) (cid:107)
− if , i.e., is faster
v γ < 0 (cid:126)v < (cid:126)v l
k l
(cid:107) (cid:107) (cid:107) (cid:107)
iv) , ‘ ’. This feature measure, whether the faster entity fully overtakes the
q normal overtaking
slower one or not, as seen, from the perpendicular perspective of the entity. Practically,
it determines the asymptotic relation of the entities, i.e., the relations for . Note
→ ±∞
that, because of motion symmetry, the relation at is always the sign-inverted
t +
→ ∞
relation at . For example, the story has at and at
t S ( , 0) t (+, 0)
(cid:57)
1 1
→ −∞ − → −∞
t +
→ ∞
9.3. STORIES- OF MOTION REPRESENTATIONS 139
Trajectory Crossing Relative Normal
Feature
Angle Precedence Speed Overtaking
Name
(ϕ ) α τ γ q
A (antiparallel)
α = 180
FC
(front crossing) if crosses
k k
° ° first
90 α < 180
Feature
≤ T
Values P (parallel) − if crosses
k l
+ if is faster
v k
° first
α = 0 k
− if is faster
v l
BC k = if k and l k q + if asympt. (+, −) or (−, +)
(back crossing) collide v = if k and l q − if asympt. (+, 0) or (0, +)
° ° equally fast = if asympt.
0 < α < 90 q
(+, +)
The features that allow a full categorization of the QTC stories. In other
i B21
words, the features that define the story map . The greyed cells illustrate that for
QTC
B21
certain trajectory angles additional features do not refine the categorization. For example, the
stories with FC trajectory angle are only subcategorized by the feature , i.e.,
crossing precedence
, while the features and havRe no further subcategorization effect.
τ γ q
D .
(cid:126)v
k n if
cos(α) (cid:107) (cid:107) (cid:126)v > (cid:126)v
 o k l
 (cid:126)v (cid:107) (cid:107) (cid:107) (cid:107)
 m l
max( (cid:126)v , (cid:126)v )  (cid:107) (cid:107)
if
k l cos(α) (cid:126)v = (cid:126)v
q((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = cos(α) (cid:107) (cid:107) (cid:107) (cid:107) = m
k l
k k l l
(cid:107) (cid:107) (cid:107) (cid:107)
min( (cid:126)v , (cid:126)v ) (9.17)
k l o
(cid:107) (cid:107) (cid:107) (cid:107)  (cid:126)v
 l if
cos(α) (cid:107) (cid:107) (cid:126)v > (cid:126)v
 l k
i (cid:126)v (cid:107) (cid:107) (cid:107) (cid:107)
s k
(cid:107) (cid:107)
From Equation (9.11), we note that
cos(α) = vˆ vˆ
t k l
/ ·
According to the values that takes in the different stories, we identify three different
s q
featural regions that categorize stories:
, and −. The regions are linked to the
t q q q
values of in thhe following equation:
if , i.e., the stories are or
q q > 0 t + (+, ) ( , +)
→ ∞ − −
if , i.e., the stories are or (9.18)
q q = 0 t + (+, 0) (0, +)
→ ∞
− if , i.e., the story is
q q < 0 t + (+, +)
→ ∞
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
140 BARE AND BEADED
1 3
3 2
1 3
1 l
3k
0 1
0 2
1 3 1
∠3 ∠ ∠1
(a) OPRA : (b) OPRA : 3 (c) OPRA :
1 1 1 1 3
Stories-OPRA : S Stories-OPRA : S Stories-OPRA : S
1 C10 1 C10 1 C10
OPRA story ; the story’s sequence is illustrated with the
1 C10 ∠ 1 ∠ ∠ 3 F
corresponding motion scenarios. In each scenario, we write both the OPRA spatial relation and
the Stories-OPRA motion relation.
9.4 Motivation to Expand Bear into Beaded Categoriza-
tions
The bare story-based categorizations fulfil the initial goals and assumptions in this uwork (Ch. 2).
Such categorizations accomplish also some basic purposes of categorization; for example,
3 cognitive
: they simplify the R kinematic space of the motion scenarios of two entities into a
economy 8
reduced number of categories (e.g., 16 categories in Stories-RCC, or 18 in Stories-QTC ); and
B21
D .
the categories, i.e., the stories, are cognitively meaningful—Thesy correspond to certain types of
motion with salient navigation features (parallelism, collision, . . . ).
Nevertheless, stories alone can very limitedly be used for decision-making. In truth, a story
corresponds to a whole trajectory, but, in order to take a control decision in a trajectory, we
should know in which stage of the trajectorycthe scenario is.
S s
C10
relations , and it depicts a collision. Each scenario in the story (Figs. 9.7a to 9.7c)
( 3 , 3 , 1 ) h
∠ ∠ ∠
1 3
is categorized with the same category according to the categorization Stories-OPRA , al-
/ S
C10 1
though the scenarios differ from each other regarding decision-making: In scenario (a), we must
control the entities tpo avoid collision, but in scenario (c), the collision threat is over—no action
is needed. t
Thus, we need some extra information besides the story in case we want to effectively use a
story-based categorization for decision-making in trajectory control. The simplest solution is to
append the qualitative relation of the scenario, , to the story, . That is, the scenario (c)
R S
i j
would be categorized as —instead of simply . Similarly, the scenario (a) would be
S ( ) S
C10 1 C10
categorized as . Now the decision rule emerges lightly: collision danger exists, if the
S ( )
C10 3
scenario belongs to category ; no collision danger if the category is .
3 1
S ( ) S ( )
∠ ∠
C10 1 C10 3
9.5 Defining beaded story-based categorizations: S (R )
i j
As suggested above, we can create a new type of story-based categorizations by appending, i.e.,
concatenating, the qualitative relation of a certain scenario to its story ; thus, the categories
R S
j i
in these new categorizations are represented as , and we call them ‘ ’. Cor-
S (R ) beaded categories
i j
respondingly, we call the new categorizations ‘ ’. Each beaded
beaded story-based categorizations
9.5. DEFINING BEADED STORY-BASED CATEGORIZATIONS: ( ) 141
S R
I J
story-based categorizations is named ‘ ’, according to the qualitative relation used to
Motion-
R R
generate it. For instance, Motion-RCC and Motion-OPRA (Sects. 9.6.1 and 9.6.2).
A story-based categorization, or simply, a is a subset of the
beaded beaded categorization
Cartesian product of the stories and their qualitative relations, . Caveat, not every re-
× R
lation is combined with any story, but only those relations belonging to the story. Due to this
asymmetry, we do not represent the pair story-relation as a tuple, , but rather in a
(S , R )
i j
functional way, .
S (R )
i j
Since the beaded categories, , are compound, we call the ‘ ’—
S (R ) R position component
i j j
because it indicates the “position” of the categorized scenario in the story’s sequence
(R , R , . . . ,
1 2
—and we call the ‘ ’ or just ‘ ’.
R ) S story component story T
n i
9.5.1 Story map, stories set, and categorization rule
Now, let us formalize the elements of a categorization. Most of its elements are the same
beaded
as those of a categorization. First of all, a beaded categorization categorizes the same objects
bare
a bare categorization does: . Second, once we have chosen a certain qualitative
motion scenarios
categorization , both types of categorizations, ‘bare’ (Stories- ) and ‘beaded’ (Motion- ),
R A R R
have the same , , and, accordingly, the same , .
story map σ stories set Σ
R R
On the other hand, the differs between both categorization types. In the
categorization rule
categorizations, the story map, , is itself the categorization rule because the categories
bare σ
are the stories. However, in the categorizations, the categorization rule is a Cartesian4
beaded
product of the qualitative map of (Eq. (8.3)) and its story map, , i.e., u
ρ σ f := σ ρ
R µ R
R ×
(Eq. (9.19))—Remember that not all possible Cartesian combinations are possible, but a story
can only be concatenated to relations that are contained in the story.
D .
f := σ ρ : Σ
µ R (9.19)
× K −→ M ⊂ × R n
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) S (R )
k k l l i j
(cid:55)−→
Since the categorization rule, , differs between bare and beaded categorizations, also the
µ m
differs. In the bare categorizations the categories set equals the stories set,
categories set
i.e., , but in the beaded categorizations theccategories set is a subset of the Cartesian
= Σ
M i
product, i.e., .
Σ s
M ⊂ × R
A method to create a beaded story-based categorization
We outline the steps that lead to a beaded story-based categorization, Motion- . In the fol-
t R
lowing sections, we use the here defined method to generate the novel motion categorizations
‘Motion-RCC’ and ‘Motion-OPRA ’.
1. We have a qualitative representation (either spatial, , or motion, ) with a catego-
R D M
rization rule (Eq. (8.2))
2. We determine the stories set associated with the representation, i.e., , and the story
map, , as we showed in Section 9.1.1—This step is equivalent to obtain the Stories-
categorization.
3. We directly create the , , if we combine both aforementioned maps,
categorization rule f σ
µ R
and , through the Cartesian product, i.e., (Eq. (9.19)).
ρ f := σ ρ motion categorization
µ R
based on the , as presented in Equation (7.1), Accordingly, a motion
categorization rule f
scenario is mapped both into a story and into a qualitative spatial relation
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) S
k k l l i
. Thus, the of the scenario is defined as , i.e., the story , at
R motion category S (R ) S
j i j i
the spatial relation .
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
142 BARE AND BEADED
4. The , , is effortless obtained from the stories set, . We obtain each
categories set Σ beaded
by appending to every story each of the story relations; so
category S = (R , R , . . . , R )
i 1 2 n
we have . The categories set is, hence, the collection of all such
S (R ), S (R ), . . . , S (R )
i 1 i 2 i n
beaded categories = S (R )
i=1...n j=1...n i j
M ∪ ∪
9.6 Motion- of Spatial Representations
We show now concrete examples of beaded story-based representations (Motion- ) obtained
from the qualitative spatial representations RCC and OPRA . In other words, we expand
the Stories-RCC and Stories-OPRA representations of Section 9.2 into the representations
Motion-RCC and Stories-OPRA .
9.6.1 Motion-RCC
Here, we apply the method for generating beaded motion categorizations to the spatial catego-
rization RCC; thus, we obtain Motion-RCC.
1. We have a spatial representation RCC, which provides a map that relates each
= δ
motion scenario with a spatial relation .
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) R
k k l l j
2. We already obtained the RCC stories and the story map in Section 9.2.1. .
Σ =4Σ Σ
0 1
RCCr
are the R and are the stories.
Σ rigid stories Σ non-rigid u
0 1
Σ = (DC), (EC), (PO), (TPP), (NTPP), (TPPI), (NTPP/I), (EQ)
{ }
S S S S S S S S
01 02 03 04 05 06 07 08
D .
Σ = (DC), (DC, EC, DC), (DC, EC, PO, EC, DC)o, (DC, EC, PO, TPP, PO, EC, DC),
{ m
S S S S
11 12 13 14
(DC, EC, PO, TPP, NTPP, TPP, PO, EC, DC), (DC, EC, PO, TPPI, PO, EC, DC),
S c S
15 16
(9.20)
(DC, EC, PO, TPPI, NTPPI, TPPIi, PO, EC, DC), (DC, EC, PO, EQ, PO, EC, DC)
S S
17 h 18
3. The categorization function, , is the Cartesian product of the RCC story map,
f σ
RCC
(Eqs. (9.2) to (9.5):), and the RCC spatial map, (Eq. (9.1)):
s δ f := σ δ
p RCC RCC RCC
4. The categtories set is obtained by expanding the stories according to the elements of their
sequences as follows:
Motion-RCC =
S (DC); S (EC); S (PO); S (TPP); S (NTPP); S (TPPI); S (NTPPI); S (EQ);
01 02 03 04 05 06 07 08
S (DC); S (DC ), S (EC), S (DC ); S (DC ), S (EC ), S (PO), S (EC ), S (DC );
11 12 − 12 12 + 13 − 13 − 13 13 + 13 +
S (DC ), S (EC ), S (PO ), S (TPP), S (PO ), S (EC ), S (DC );
14 − 14 − 14 − 14 14 + 14 + 14 +
S (DC ), S (EC ), S (PO ), S (TPP ), S (NTPP), S (TPP ),
15 − 15 − 15 − 15 − 15 15 +
S (PO ), S (EC ), S (DC ); S (DC ), S (EC ), S (PO ), S (TPPI),
15 + 15 + 15 + 16 − 16 − 16 − 16
S (PO ), S (EC ), S (DC ); S (DC ), S (EC ), S (PO ), S (TPPI ), S (NTPPI),
16 + 16 + 16 + 17 − 17 − 17 − 17 − 17
S (TPPI ), S (PO ), S (EC ), S (DC ); S (DC ), S (EC ), S (PO ), S (EQ),
17 + 17 + 17 + 17 + 18 − 18 − 18 − 18
S (PO ), S (EC ), S (DC )
18 + 18 + 18 +
(9.21)
9.6. MOTION- OF SPATIAL REPRESENTATIONS 143
y y y
x x x
k T
(a) RCC: DC (b) RCC: EC (c) RCC: DC
Stories-RCC: S Stories-RCC: S Stories-RCC: S
12 12 12
Motion-RCC: S (DC ) Motion-RCC: S (EC) Motion-RCC: S (DC )
12 − 12 12 +
RCC story . Each relation in the story’s sequence is illustrated
with a motion scenario. In each motion scenario, the corresponding relations, i.e., categories,
of three qualitative representations, i.e., categorizations, (RCC, Stories-RCC, Motion-RCC) is
shown.
For example, the relation indicates that the entities are moving in the story
S (EC) Sr
R12 12
at the moment of tangency, i.e., (Fig. 9.8b). If the spatial relation appears multiple
EC
times in the story, such as in (Fig. 9.8), we distinguish between each appearance;
DC S
for example, chronologically, is the first (Fig. 9.8a), and g , the last
S (DC ) DC S (DC )
12 − r13 +
(Fig. 9.8c). In this work, we have only observed this effect, i.e., tohat relations repeat
DC
D .
in the same story, for the representation RCC.
The total number of Motion-RCC stories is 16—as in Stories-RCC (Tab. 9.1). The total
number of Motion-RCC categories is 56, from which 8 are rigid categories. Therefore, we have
48 non-rigid categories: 9 are independent of the entities’ relative size; in 16 relations the first
entity is smaller, ; in 16 relations the fisrst entity is larger, ; in 7 relations both
r < r r > r
k l e k l
entities are equally large, . h
r = r
k l t
9.6.2 Motion-OPRA
t 1
Here, we apply the method for generating beaded motion categorizations to the spatial catego-
rization OPRA ; thus, we obtain Motion-OPRA .
1 1
1. We have a spatial representation OPRA , which provides a map that relates each
= δ
motion scenario with a spatial relation .
((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) R
k k l l j
2. We already obtained the OPRA stories and the story map in Section 9.2.2.
Σ =
OPRA
. The are ; the rest are .
Σ Σ Σ Σ Σ Σ rigid stories Σ Σ non-rigid
C B T P E R E R
∪ ∪ ∪ ∪ ∪ ∪
3. The categorization function, , is the Cartesian product of the OPRA story map,
f σ
µ 1
OPRA
(Fig. 9.5), and the OPRA spatial map, (9.6– ):
δ ?? f := σ δ
1 µ
OPRA OPRA OPRA
1 1 1
4. The categories set is obtained by expanding the stories according to the elements of their
sequences as follows:
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
144 BARE AND BEADED
3 0 1 1 1
Motion-OPRA = S ( ), S ( ), S ( ), S ( ), S ( );
(cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠
1 C1 1 1 C1 1 1 C1 1 1 C1 1 2 C1 1 3
1 0 3 3 3
S ( ), S ( ), S ( ), S ( ), S ( );
(cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠
C2 1 3 C2 1 3 C2 1 3 C2 1 2 C2 1 1
3 1 1 3
S ( ), S ( 3 ), S ( ); S ( ), S ( 1 ), S ( );
∠ ∠ ∠ ∠ ∠ ∠
C10 1 C10 C10 3 C20 3 C20 C20 1
3 3 3 2 1
S ( ), S ( ), S ( ), S ( ), S ( );
∠ ∠ ∠ ∠ ∠
C11 1 C11 0 C11 3 C11 3 C11 3
1 1 1 2 3
S ( ), S ( ), S ( ), S ( ), S ( );
∠ ∠ ∠ ∠ ∠
C21 3 C21 0 C21 1 C21 1 C21 1
3 0 1 1 0 3
S ( ), S ( ), S ( ); S ( ), S ( ), S ( );
(cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠
B1 1 1 B1 1 1 B1 1 1 B2 1 3 B2 1 3 B2 1 3
3 1 1 3
S ( ), S ( 3 ), S ( ); S ( ), S ( 1 ), S ( );
∠ ∠ ∠ ∠ ∠ ∠
B10 0 B10 B10 2 B20 0 B20 B20 2
3 2 1 1 2 3
S ( ), S ( ), S ( ); S ( ), S ( ), S ( );
∠ ∠ ∠ ∠ ∠ ∠
B11 3 B11 3 B11 3 B21 1 B21 1 B21 1
1 1 1 3 3 3 (9.22)
S ( ), S ( ), S ( ); S ( ), S ( ), S ( );
(cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠ (cid:57) ∠
B3 1 1 B3 1 2 B3 1 3 B4 1 3 B4 1 2 B4 1 1
0 2 0 2
S ( ), S ( 3 ), S ( ); S ( ), S ( 1 ), S ( );
∠ ∠ ∠ ∠ ∠ ∠
B30 1 B30 B30 3 B40 3 B40 B40 1
3 3 3 1 1 1
S ( ), S ( ), S ( ); S ( ), S ( ), S ( );
∠ ∠ ∠ ∠ ∠ ∠
B31 1 B31 0 B31 3 B41 3 B41 0 B41 1
S ( 2 ), S ( 0 ), S ( 0 ); SA ( 0 ), S ( 2 ), S ( 2 );
(cid:57) ∠ (cid:57) ∠ (cid:57) ∠ ∠ ∠ ∠
T 1 0 T 1 T 1 2 T0 0 T0 T0 2
0 2 3 1 3 1
S ( ), S ( 0 ), S ( ); S ( ); S ( ); S ( ); S ( );
∠ ∠ ∠ ∠ (cid:57) ∠ ∠ ∠
T1 2 T1 T1 0 P2 1 P 2 3 P3 3 P1 1
2 1 3 0
S ( ); S ( ); S ( 0 ); S ( ); S ( );
(cid:57) ∠ (cid:57) ∠ ∠ ∠ ∠
E 2 0 E 1 3 E0 E1 1 E2 2 4
0 R 0 0 0 1 1 1
S ( ); S ( ); S ( ); S ( ); S ( ); S ( ); S ( );
∠ ∠ ∠ ∠ ∠ ∠ u∠
R00 0 R10 1 R20 2 R30 3 R01 0 R11 1 R21 2
1 2 2 2 2 3 3 3
S ( ); S ( ); S ( ); S ( ); S ( ); S ( ); S ( );
∠ ∠ ∠ ∠ ∠ ∠ ∠
R31 3 R12 0 R12 1 R22 2 R32 3 R03 0/ R13 1
3 3
S ( ); S ( ); S ( 0 ); S ( 1 ); S ( 2 ); S ( 3 )
∠ ∠ ∠ ∠ ∠ ∠r
R23 2 R33 3 R0 R1 R2 R3
o }
D .
The total number of Motion-OPRA stories 50. The total number of Motion-OPRA
1 1
relations is 100.
S S
P3 C11
gories of Motion-OPRA ; for example, ( ) and ( ).
3 3
S S
1 c P3 ∠ 3 C11 ∠ 3
9.7 Motion- of Mtotion Representations
R /
We show now concrete examples of beaded story-based representations (Motion-) obtained from
the qualitative representation of motion QTC . In other words, we expand the bare story-based
B21
representation Stories-QTC in Section 9.3 into the beaded story-based representation Motion-QTC .
B21 B21
9.7.1 Motion-QTC
B21
The very same method we applied for Motion-RCC and Motion-OPRA in Sections 9.6.1 and 9.6.2
can be applied to Stories-QTC , and, thus, we obtain the beaded motion categorization
B21
Motion-QTC . The categorization function of Motion-QTC , , is the Cartesian prod-
B21 B21 µ
uct of the QTC story map, , and the categorization rule of QTC , which we will
B21 QTC B21
B21
call (Eq. (7.10)). Accordingly, we obtain the Motion-QTC relations. For example,
QTC B21
B21
, , , , , , . . .
S ( , 0) S (0, 0) S (+, 0) S ( , ) S ( , 0) S (+, )
(cid:57) (cid:57) (cid:57) (cid:57) (cid:57) (cid:57)
1 1 1 1 1 1 3 1 3 1 3 1
− − − − −
Although we will not examine Motion-QTC in detail, it is important for us to mention this
B21
beaded motion categorization because Motion-QTC is generated from a motion categorization,
B21
namely, QTC . In that way, we can substantiate our statement: we can both use spatial and
B21
motion categorizations to create story-based categorizations with the same methods.
9.7. MOTION- OF MOTION REPRESENTATIONS 145
Story
P3
D .
( ) ( ) ( )
3 3 3
S S S
∠ ∠ ∠
P3 3 P3 3 P3 3
Story
C11
k 2 3
2 3
k 0
2 k
( ) ( ) ( ) ( ) ( ) 3 3 3 2 1
S S S S S
∠ ∠ ∠ ∠ ∠ C11 1 C11 0 C11 3 C11 3 C11 3
OPRA stories and . For each scenario,
∠ ∠ ∠ ∠ ∠ ∠
1 P3 3 C11 1 0 3 3 3
we show the corresponding Motion-OPRA category, i.e., relation, . Such category consists
S (R )
1 i j
of the OPRA story of the scenario (e.g., ) and its OPRA relation (e.g., ).
S S R
1 i C11 1 j 3
Source: Purcalla Arrufi and Kirsch (2018a)
CHAPTER 9. CREATING STORY-BASED CATEGORIZATIONS:
146 BARE AND BEADED
9.7.2 Formalization and Featural Variables
Here, we describe a Motion- categorization using the formalism we defined in Chapter 7.
Namely, we break down its , , into the , , and
categorization rule f feature extraction function Φ
the , i.e., . We relate and to the featural
featural categorization function f f = f Φ f Φ
Φ µ Φ Φ
functions in the component categorizations, Stories- and , by exploiting that Motion- is
R R R
the Cartesian product of such two categorizations.
The categorization function of Stories- is the story map ; the categorization
σ = σ Φ
R Φ σ
R R
R ◦
function of is the qualitative map —In both cases we expressed the categorization
ρ = f Φ
Φ ρ
R ◦
rule by means of the feature extraction and the featural categorization rule. Since Motion-
Stories- , we have that the categorization rules are also concatenated . In
f = σ ρ
µ R
R × R ×
Equation (9.23), we combine the previous formulae, and use a basic property of composing
Cartesian product of functions.
(9.23)
f = σ ρ = (σ Φ ) (f Φ ) = (σ f ) (Φ Φ )
µ R Φ σ Φ ρ Φ Φ σ ρ
R R ρ R ρ R
× ◦ × ◦ × ◦ ×
Now comparing the result of Equation (9.23) and our formalization, , we can isolate
f = f Φ
µ Φ
A ◦
the formulae for the (Eq. (9.24a)), and the
feature extraction function Φ featural categorization
(Eq. (9.24b)). We have thus obtained that each function, and , is the Cartesian
function f Φ f
Φ Φ
product of the functions of the components, i.e., Stories- and .
R R
R (9.24a)
Φ = Φ Φ u
σ ρ
R × g
3 (9.24b)
f = σ f
Φ Φ Φ /
R ρ
The Cartesian product in the feature extraction function (Eq. (9.24a)) tells that the featural
Φ o
D .
variables of a Motion- categorization are the union of the feastural variables of Stories- and
R R
, i.e., . o
Motion-R Stories-R R m
R F F ∪ F
A. Motion-RCC Formalization
Motion-RCC is the concatenation of Stories-RCC and RCC. Concerning Stories-RCC, we thor-
oughly presented and in Section 9.2.1 (Eqs. (9.3a) to (9.5))—The featural variables
Φ σ
σ Φ
RCC RtCC
are and .
d dif /
min V
s (9.3a)
d ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x det((cid:126)x (cid:126)x , (cid:126)v (cid:126)v )
pmin k k l l l k l k l k
(cid:107) − (cid:107)| − − |
t (cid:126)v (cid:126)v
h l k (9.3b)
dif ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:107) − (cid:107)
V k k l l
(cid:126)v + (cid:126)v
k l
(cid:107) (cid:107) (cid:107) (cid:107)
Concerning RCC, we presented —the spatial map of RCC—in Section 9.2.1. There, we
RCC
noted that its only featural variable was the distance between entities .
d((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x
k k l l k l
(cid:107) − (cid:107)
Even so, the original featural categorization function of RCC (Eq. (9.1)) differs slightly from
the one used here (Eq. (9.26)), because in Stories-RCC each spatial categorization has a sign
indicating its position in the story (Eq. (9.26b)). For instance, appears twice in the story ,
DC S
and, hence, we add signs, or , to distinguish both appearances,
DC DC S = DC , EC, DC
− + 12 − +
{ }
(see item 4, Sect. 9.2.1)
Concluding, for the spatial representation RCC, we have following feature extraction function,
, and featural categorization function .
Φ f
RCC
RCC
(9.25)
Φ ((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = d((cid:126)x , (cid:126)v ; (cid:126)x , (cid:126)v ) = (cid:126)x (cid:126)x
k k l l k k l l l k
RCC
(cid:107) − (cid:107)
9.7. MOTION- OF MOTION REPRESENTATIONS 147
 if
DC d > d
∗ 2
 if
 EC d = d
 ∗ 2
 if
 PO d > d > d
 ∗ 2 4
if (cid:27) (9.26a)
f (d) := TPP d = d
Φ ∗ 4 if
RCC r r
 NTPP if d > d k l
 ∗ 4 ≤
 (cid:27)
if
TPPI d = d
 ∗ 4 if
r > r
if k l
NTPPI d > d
∗ 4
if for the whole spatial relation
+ d > d
 min
if for the whole spatial relation (9.26b)
:= d < d
min
∗ −
 ∅ otherwise
distance at spatial relation
d = r + r , EC
2 k l
| |
distance at spatial relation
d = r r , TPP
4 k l
| − |
From the analysis above, we see that Motion-RCC has three featural
Featural variables
variables: , , and —The additional parameters, such as , , , and , are all
d dif d d d r r
min V 2 4 k l
time-independent. Since the featural space is three-dimensional, we amply verify our claim that
(Sect. 7.1.2); in fact, the featural space is dimensionally low enough that it
dim( ) < dim( )
F K u
can be graphically visualized.
B. Motion-OPRA Formalization
D .
Motion-OPRA is the concatenation of Stories-OPRA and OPRA . Concerning Stories-OPRA ,
1 1 1 n 1
we presented non-rigid and in Section 9.2.2 (Fig. 9.5); the corresponding featural vari-
Φ σ o
σ Φ
ables are , , , and . Concerning OPRA , we presented —the spatial map
α α u u δ
vv ∆x∆v k l 1
m OPRA
of OPRA —in Section 9.2.2 (9.6– ); the corresponding categorical variables are , ,
?? α α
1 o ∆xv v ∆x
k l
and . c
dif
X s
Apparently, the tothal number of featural variables in Motion-OPRA is
Featural variables
t 1
seven (four in Stories-OPRA plus th/ree in OPRA ), but we observe that the variable is
1 / 1 vv
combination of and :(as we proved in Section 9.2.2). Therefore, since the variable
α α α
v ∆x ∆xv vv
l k
is not independent, we have a total number of six featural variables, i.e., .
dim( ) = 6
Stories-OPRA
t F 1
D .
Part IV
Validating Story-Based
Categorizations
D .
D .
Chapter 10
Experimental Evidence of
Story-Based Categorization
The fact that it is not possible to uniquely determine cognitive
structures and processes poses a clear limitation on our ability
to understand the nature of human intelligence. The realization
Rof this fact has also led to a shift in my personal goals. I am less
interested in defending the exact assumptions of the theory and
am more interested in evolving some theory that ca/n account for
important empirical phenomena. r
D .
sJ. R. Anderson (1976)
After we criticized in Section 3.4 how scanty is the experimental verification of cognitive plau-
sibility in spatial representations, one may expect thcat we experimentally verify the story-based
categorizations for cognitive plausibility—That iis the purpose of this chapter.
This chapter bases on research we did in collaboration with Frank Papenmeier (Original
paper, Papenmeier, Purcalla Arrufi, and Kirsch 2022).
10.1 About Cognitive Plausibility
We say that a certain cognitive model is ‘ ’ when it appropriately describes
cognitively plausible
human cognition. This is a key concept in science: For some researchers, the very goal of
Artificial Intelligence (AI) is to create machines that work in a manner, i.e.,
cognitively plausible
to create machines that think like humans (See, Sweeney 2003). However, as Russell and Norvig
(2014b, Sect. 1.2) advise, it is much more effective that AI concentrates on creating machines
that think—and act— . Creating machines should be the endeavour
rationally cognitive plausible
of ‘ ’. Hence, as we verify experimentally the story-based categorizations, we
cognitive modelling
look at them from a cognitive modelling perspective.
The concept ‘cognitive plausibility’ has some equivalent terms: ‘ ’ (Knauff
cognitive adequacy
et al. 1995; Renz et al. 2000; Klippel et al. 2008), ‘ ’ (Cohn and Renz 2008,
cognitive validity
Sect. 13.5), ‘ ’ (Knauff et al. 2004). From now on, we use the term ‘cognitive
psychological validity
plausibility’ even when we refer to authors that use an equivalent term.
152 CHAPTER 10. EXPERIMENTAL EVIDENCE OF STORIES
According to Knauff, Rauh, and Schlieder (1995), cognitive plausibility comprises two dif-
ferent but complementary aspects: and cognitive plausibility. ‘
conceptual inferential Concep-
’ (shortened, ‘ ’) enquires whether the categories
tual cognitive plausibility conceptual plausibility
(or classes) of a certain knowledge representation correspond to categories in human concep-
tual knowledge. For example, whether humans naturally categorize motion by means of the
Stories-OPRA categories (i.e., , , , . . . ). ‘ ’ (short-
S S S Inferential cognitive plausibility
(cid:57)
1 C11 C1 1 C21
ened, ‘ ’) enquires whether humans reason about qualitative relations sim-
inferential plausibility
ilarly as qualitative reasoning does (Sects. 5.1.4 and 5.3.1). For example, whether humans also
use the operation of qualitative representations when reasoning.
composition
In order to ascertain whether a qualitative representation is cognitively plausible we must re-
sort to experimentation with humans—As M. Knauff (1999, p. 263) simply states: “The question
whether an approach to [qualitative representations] can be claimed as cognitively [plausible] can
be answered only on psychological experiments”. In the following, we verify whether Stories-RCC
and Stories-OPRA are plausible by means of several experiments.
conceptually
10.2 Experimental Principles
10.2.1 Pairwise Comparison
Our experimental method based on comparisons of stimuli. In each trial, we presented
pairwise
the subject with three stimuli: one was the ‘ ’, and the other two were the
reference stimulus
‘ ’; then, we asked the subject to choose the comparison3stimulus that was
comparison stimuli
most similar to the reference stimulus (see trial set-ups in Fig. 10.1). g
We chose a pairwise comparison task because of the nature of oour stimuli: they are
motion
D .
scenes—And this is a key difference with most categorization experiments, which use motionless
stimuli. Indeed, motion limits the experimental methods we can use. For example, we find
inappropriate to use a typical task, in which all the stimuli are presented at once
free grouping
and each subject groups them at will (e.g., Mast et al. 2014; Renz et al. 2000). If we present
to the subjects many motion scenes at once, we might overstrain their cognitive and perceptive
capacities—Not to mention that the sismultaneous view of many motion scenes is dizzying (but
see, Yang et al. 2015). h
Another reason for pairw/ise comparison is that our stimuli are simultaneously categorized
according to two differen:t categorizations (Stories-RCC and Stories-OPRA ). However, in a free
grouping task, subjects tend to group according to just one categorization, namely, the most
salient—Human laziness restrains subcategorization. Although in a
h hierarchical free grouping
task (e.g., Burnett et al. 2005), it is possible to force the subjects to subcategorize, such grouping
task is more demanding.
At a theoretical level, the pairwise comparison is underpinned by the work of Luce (1959)
and Bradley and Terry (1952), the BTL model. By fitting the experimental data to this model,
we can relate the frequency of choice of each stimuli pair to a single scale measure called ‘
utility
’. This is useful because we can assimilate such utility ratio to the experimental similarity
ratio
between each comparison stimulus and the reference stimulus.
In some trials, we also
Comparison Stimuli, Modified Stimuli, and Reference Stimulus
included the reference stimulus as a comparison stimulus; that is, sometimes, one comparison
stimulus was identical to the reference stimulus. More concretely, we created two variants of
experimental set-ups: in one variant (named ‘a’), the comparison stimuli could only be ‘
modified
’ (see later, Sect. 10.5), that is, the stimuli obtained by modifying the reference stimulus;
stimuli
10.2. EXPERIMENTAL PRINCIPLES 153
in the other variant (named ‘b’), one of the comparison stimuli could be the identical to the
reference stimulus.
By including the reference stimuli in the comparison stimuli, we can verify whether the
categorical effects are altered when the proportion of similar stimuli to the reference stimulus
increases. Additionally, we can control more closely the experiment results, since the maximality
axiom (item II., Sect. 4.4.1.A) holds for similarity: the similarity of a stimulus to itself is higher
than to any other stimulus. For example, we can sort subjects out that do not predominantly
choose the reference object as most similar to itself; we might conclude such subjects are not
attentive to the stimuli or are performing the task carelessly.
10.2.2 Perception and Memory Tasks
The most straightforward way to present the one reference stimulus and the two comparison
stimuli is to display all three stimuli , as we do in our ‘ F ’ set-up (Fig. 10.1a). In
at once perception
such set-up, only perception is involved–there is no need to use memory. However, categorization
is mostly advantageous when we store things in memory because, then, we resort to the powerful
(Goldstone et al. 2012, p. 611; originally, E. Rosch 1978, p. 28). On that
cognitive economy
account, we created an additional experimental set-up, the ‘ ’ set-up in which the subjects
memory
were forced to store the reference stimulus in short-term memory: we introduced a s pause
between the display of the reference stimulus and the comparison stimuli (Fig. 10.1b).
We expect a different behaviour of the subjects between the perception and memory ex-
periment. We know that the short-time memory has very limited storage resources for static
situations (e.g., Cowan 2001; Luck and Vogel 1997); how much more limited, then, for dynamic
situations (e.g., Papenmeier and Huff 2014). Therefore, we guess that, due to the cognitive load,
the memory experiment might show a reduced differentiation of the categories; it might happen
D .
that only the most salient category be stored in memory, if any.
10.2.3 Experimental Hypothesis
We generated the stimuli, used as comsparison stimuli, so that each had the same
modified
similarity to the stimulus. We achieved that by applying the same amount
feature-based reference e
of to the modified stimuli with respect to the reference stimulus. But we also gen-
metric change
erated them so that they had different Stories-RCC and Stories-OPRA categories with respect to
the stimulus (See latesr Sect. 10.3.3 for more detail). In that way, if the similarity bases