The Struggle for Life among Words:
How Cognitive Selection Shape Language Evolution
Ying Li1,2*; Breithaupt Fritz3*, Cynthia Siew4, Thomas Hills5, Yanyan Chen1 & Ralph Hertwig2
1. CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy
of Sciences, Beijing, China
2. Center for Adaptive Rationality, Max Planck Institute for Human Development, Berlin,
Germany
3. Department of German Studies, Indiana University, Bloomington, USA
4. National University of Singapore, Singapore.
5. Department of Psychology, University of Warwick, Coventry, UK
Author Note
We thank Ziyong Lin and Bodo Winter for their constructive feedback.
Abstract
Like biological species, competition for survival is a constant among words in language.
Previously, it has been shown that language evolves in response to cognitive constraints and over
time becomes more learnable. Here, we use two complementary research paradigms to
demonstrate how survival of words can be predicted by psycholinguistic properties that impact
language production. In the first study, we analyzed survival of words in the context of inter-
personal communication. We used a large-scale serial reproduction experiment in which stories
were passed down along a transmission chain over multiple participants. The result shows that
words that are acquired earlier in life, more concrete, more arousing, and more emotional have
better chance to survive retellings. We reason that the same trend might scale up to language
evolution over multiple generations of natural language users. If that is correct, the same set of
psycholinguistics properties should also account for the change of word frequency in natural
language corpora over evolutionary time. That is what we found in two large historical language
corpora (study 2): early acquisition, concreteness and high arousal all predicts increasing word
frequency over the past 200 years. By bridging micro-level behavioral preferences and macro-
level language patterns, our study sheds light on the evolutionary mechanism of word competition.
Keywords: language evolution, serial reproduction, psycholinguistics, corpus analysis, linguistic
norms.
Significant Statement
Words compete for survival in each language. This article studied the cognitive mechanisms
underlying the rise and fall of English words using two complementary research paradigms: serial
reproduction experiment where a story is passed on along a diffusion chain and a quantitative
linguistic analysis of historical corpora that spans the past two centuries. We found that the
competition among word forms is closely associated with how human use language: words that
are acquired earlier in life, more concrete and higher arousing are more likely to survive and
proliferate. Our results suggest that the micro-level language production behaviors may have
scaled up to macro-level patterns in language evolution.
1. Introduction
‚ÄúA struggle for life is constantly going on amongst the words and grammatical forms in each
language. The better, the shorter, the easier forms are constantly gaining the upper hand, and they
owe their success to their own inherent virtue.‚Äù
Max M√ºller, 1870
Language change due to social, cultural and cognitive influences. Social and cultural
environment can often evolve rapidly due to the impact of, for example, war, pandemic disease,
technological innovation, or changes in social norms and values. By contrast, the mind‚Äôs cognitive
structures and constraints, due to its deep roots in our biological endowment, are much more stable
over time, and therefore capable of imposing lasting and coherent impacts on language evolution.
Cognitive selection during inter-personal communication, even very small, may be magnified into
macro-level patterns of linguistic phenomena through generations of language speakers (for a
review of evidence from experimental studies, see Scott-Phillips & Kirby, 2010). In this research,
we explored how a set of psycholinguistic factors impact survival of words in two studies: first in
the context of inter-personal communication using an serial reproduction experiment, and second
in the context of diachronic language change using three large historical corpora. Before we turn
to the analyses, we first introduce the theories that motivate it.
1.1 Cognitive Account of Language Evolution
Language evolution has long been a topic of academic debate, yet little consensus has been
reached. Chomsky (1966, 1988) proposed that human language is a product of adaptive biological
evolution. He argued for an innate endowment of language-specific knowledge which he referred
to as universal grammar. On his view, language develops in the mind of children in the same
biologically determined fashion that a chicken grows its wings. Chomsky‚Äôs theory accounts for
how human language emerged from non-language, and concerns a timescale of human species,
dating back more than 100,000 years. However, this ‚Äòall-or-nothing‚Äô theory of language evolution
seems to be less relevant when explaining how human language changes after its emergence. As
noted by Christiansen and Chater (2008), language changes much faster than genes. Hence,
language cannot provide a stable environment in which genes could have been adapted to encode
patterns for linguistic conventions (i.e., the ‚Äúmoving target argument‚Äù; Christiansen & Chater,
2008; Deacon 1997: 329).
In recent decades, instead of asking how the mind is shaped to learn language, the claim
was reverted to ask how language evolved to be learnable by humans. It has been proposed that
language is a complex adaptive system, constantly evolving under the selection pressure of human
cognitive constraints over generations of language speakers (Beckner et al, 2009). In order to
evolve, language must be used by humans. Hence, one straightforward expectation is that word
forms that are easier to master by language users are more likely to gain the upper hand. One of
the advantages of this view of language evolution is that, other than explaining how human
language emerges, it also provides a theoretical foundation to explore hypotheses on how
languages change. Evidence supporting the view that language is a living ‚Äúorganism‚Äù evolving
under cognitive constraint has been accumulating from various research paradigms, including
computational stimulations (Kirby, 2001; Monaghan, Christiansen, & Fitneva, 2011), laboratory
experiments of artificial languages (Kirby, Cornish, & Smith, 2008), and analysis of natural
language corpora (Monaghan, 2014; Hills & Adelman, 2015; Hills, Adelman, and Noguchi, 2016).
While there is likely to be a selective advantage for easier word forms, there may also be forces to
introduce and favor more complex forms. With ever new environmental challenges (e.g., war,
disease, technology, hazards), there are likely pressures to constantly adapt language to such
macro-shocks, thus yielding a more complex language.
1.2 From individual cognitive preconceptions to diachronic language pattern
One commonly used experimental paradigm aimed to explore language evolution is serial
reproduction ‚Äì a process whereby information produced by one individual is observed by the
another, who then communicates the information to a third individual, and so on along a
transmission chain. This method of serial reproduction harks back to the works by early British
anthropologists (as explained in Haddon, 1895) and became popular in psychology through the
work of Frederic Bartlett (1932). Bartlett studied the ‚Äúfate‚Äù of a story (e.g., a folk tale) that was
passed down a chain of participants. By measuring change over successive reproductions, one can
study what components of the story are either forgotten or retained. Those that survived the test of
time should, by definition, be more memorable. Both the original Bartlett study and its recent
follow-ups found that people selectively retain information that is coherent with the narrator and
re-narrator‚Äôs pre-existing preconceptions (in Bartlett‚Äôs terminology ‚Äúschemas‚Äù) and therefore
reproduce and even distort information in ways that assimilate the stories to the schemata available
to them (e.g., Barrett & Nyhof, 2001; Bangerter, 2000; Mesoudi & Whiten, 2004; Kashima, 2000).
For example, when folk tales from another culture, ‚Äòweird stories‚Äô for Bartlett‚Äôs British audience,
were passed down a transmission chain, many of the folk tales‚Äô supernatural elements were
removed and replaced (Bartlett, 1932). Similarly, Bangerter (2000) found that abstract scientific
concepts unfamiliar to participants gradually become anthropomorphic after repeated iterations.
Moussaid et al. (2015) found that messages detailing risks (e.g., the benefits and harms of a
controversial antibacterial agent) undergo change when passed from one person to the next in a
10-person transmission chain. Specifically, they tend to become shorter, gradually inaccurate, and
increasingly dissimilar between chains (Moussaid et al., 2015), as well as resistant to corrective
information (Jagiello & Hills, 2018). Furthermore, subjective perceptions of risk amplify, perhaps
due to heightened attention to negative information (Rozin & Royzman, 2001; see their negativity
bias hypothesis). Breithaupt, Li, and Kruschke (2022) found that emotional appraisals of several
selected emotions survived transmission remarkably well whereas other story features decayed.
Overall, the above studies suggest that there are a set of underlying cognitive factors that impact
which information is kept, removed, or distorted when it is transmitted from one person to the next.
Similarly, cognitive selection ‚Äìthe mind‚Äôs tendency to preferentially attend to certain kinds of
information (Hills, 2019)--may systematically favour words with certain properties in daily
communication, and over time, impact the survival of words on a time scale of language evolution.
Recent decades have seen a surging interest in explaining how individual-level behaviors
result in population-level linguistic phenomena. This interest is deeply inspired by work and
theorizing in evolutionary biology that investigates links between small-scale micro-evolutionary
processes (e.g., natural selection and mutation) and population-level macro-evolutionary patterns
in time and space, with the latter patterns generated in part by the former. The same rationale can
be applied to language evolution. The advantage of adapting an iterated learning approach to
language evolution is that it bridges the gap between behavioral patterns in small-scale information
transmission that can be observed in laboratory experiments and population-level patterns arising
from generations of language users in real life (Mesoudi & Whiten, 2008). Previous laboratory
studies using the method of serial reproduction to explore questions of language evolution typically
focus on whether inter-personal communication is sufficient to give rise to the basic design
features of human language, such as the emergence of abstract symbols (Garrod, Fay, Lee,
Oberlander & MacLeod, 2007; Healey, SwoBoda, Umata & King, 2007), compositionality1 (Kirby,
Cornish & Smith, 2008), arbitrariness2 and systematicity3 (Theisen, Oberlander & Kirby, 2010).
Few studies, however, have investigated the survival of word forms of real language in serial
reproduction experiment.
1.3 Psycholinguistic Properties that Predict Language Evolution
The present study focuses on the evolutionary success of word forms. Words compete with
each other for the limited cognitive capacities of language users. Successful words are those used
in high frequency, whereas words that are no longer in use go extinct. What makes some words
more likely to survive than others? To answer this question, consider the cognitive life cycle of
information: to make it from one individual to another, information must be attended to,
comprehended, encoded in memory, and later reproduced (Hills, 2019). Correspondingly for the
life cycle of language communication, words that are favored at each of these stages should, ceteris
paribus, have higher and increasing word-usage frequency and should be less likely to become
extinct.
1.3.1 Age of Acquisition
A set of psycholinguistic properties has been found to impact each of the aforementioned
stages of language communication. For example, words that are acquired early in childhood have
more stable representations, making early-acquired words easier and more accurately retrievable
(Juhasz, 2005), and more resistant to aging-related language depletion (Hodgson & Ellis, 1998)
and cognitive impairment (Bradley, Davies, Parris, Su, & Weekes, 2006; Holmes, Fitch, & Ellis,
2006). Consequently, early-acquired words are more resistant to evolutionary change in both form
(Monaghan, 2014) and meaning (Li & Siew, 2022).
1 Key design feature of language whereby the meaning of an expression is a function of its constituent parts and the
way the constituents are combined.
2 A sign is arbitrary when where is no inherent relationship between its form and its meaning.
3 Key design feature of language whereby a feature (e.g. house) that is common to multiple items (red house, blue
house) is represented by the same sign (house).
1.3.2 Concreteness
Concrete words are those that refer to specific objects or entities that can be perceived
through our senses. Words like dog are more vividly imagined than words like truth, and people
can easily report the difference. Compared to abstract words, concrete words are more rapidly
recognized as valid words (James, 1975), more easily recalled in memory tasks (Miller &
Roodenrys, 2009; Romani, McAlpine, & Martin, 2008) and easier to learn (De Groot & Keijzer,
2000). Language composed of more concrete words is both more interesting and easier to
understand (Sadoski, 2001). These results together provide a cognitive basis for predicting a
selective advantage for more concrete language, relative to more abstract language, during the life
cycle of language communication. Consequently, the advantage for concrete words in language
communication predicts an increased frequency of concrete words in the long run. Hills and
Adelman (2015) found such an increase in concrete words in American English over the past two
centuries. Snefjella, G√©n√©reux and Kuperman (2018) further showed that this change was not due
to a historical change in word concreteness.
1.3.3 Emotion
Emotional connotation of words has also been found to impact attention, processing and
memory. Emotions are characterized by two primary and orthogonal dimensions, namely, valence
and arousal (Russel & Barrett, 1999; Kensinger & Corkin, 2004). Negative words can be either
calming (e.g., dirt) or arousing (e.g., snake), and positive words can also be calming (e.g., sleep)
or arousing (e.g., sex). Both negative and arousing stimuli engage attention longer than other
stimuli (automatic vigilance; Erdekyi, 1974; Fox, Russo, Bowles, & Dutton, 2001). Consequently,
negative words evoke slower lexical decisions4 (Kuperman et al, 2014; Estes & Adelman, 2008)
and slower word naming5 (Algom, Chajut & Lev, 2004), and arousing words predict slower lexical
decisions, even though this effect is smaller, relative to valence (Kuperman et al, 2014).
When processing meaning of individual words, emotionality (either emotionally positive
or negative), but not elicited arousal, makes a word easier to process (Pexman & Yap, 2018; Moffat
et al, 2015; note this effect is only present for abstract words). Similarly, in memory tasks,
emotional words, but not arousing words, are better remembered (Adelman & Estes, 2013;
i.e., decide whether a string of letters is a word or nonword. It requires correct recognition of word
forms. It does not necessarily involve semantic processing.
5 i.e., reading aloud a written word
Kensinger & Corkin, 2003). However, it is worth pointing out that the above studies that found
null effect of arousal converge in the same methodological approach: they studied individual words
out of context. When examining pictures, videos, or events, arousal does facilitate memory. In fact,
current theories often attribute emotional memory enhancement mostly to arousal (Hamann, 2001;
Mather, 2007; Phelps, 2006), with valence having little or no influence independent of arousal
(Mather & Sutherland, 2009).
The precise role of emotion on language evolution is still unclear. The challenge to explore
this question is that the language changes respond not only to human cognitive patterns, but also
to shifting external socio-cultural environments. It seems possible that this issue is more salient
for valence than other linguistic properties. Consider, for instance, the evidence that life
satisfaction has increased while the rate of violence has gone down, especially since the end of the
World War II (Pinker, 2011). Hence, one may speculate that these trends imply less demand for
negative words. Indeed, Hills et al. (2019) found that the average valence of historical language
predicts change of national wellbeing in four countries (UK, US, Germany and Italy). Therefore,
the cumulative advantage of both positive and negative emotional words in language production
may be offset by a rising demand for positive words brought about by increasing life satisfaction.
1.4 Overview of Current Study
We used two studies to explore the role of psycholinguistic properties on language
evolution. In Study 1, we used a serial production experiment in which stories were passed along
a transmission chain of three generations of narrators. Based on the reviewed psycholinguistic
studies on language communication, we hypothesize that words that are more concrete and
acquired earlier in life have greater chance of survival in the transmission chain, after controlling
for frequency, grammatical category, and length of word. Additionally, we explore the role of
emotional connotations (i.e. valence and arousal) without a clear hypothesis due to mixed prior
findings on how emotion impacts various stages of language communication. In Study 2, we test
whether findings from the Study 1 can be extrapolated to the longer time scale of language
evolution. We hypothesize that the same set of psycholinguistic properties that predict word
survival in the serial production experiment also predict the increase of word frequency in natural
language corpora over the past 200 years.
2. Study 1
2.1 Materials and Methods
2.1.1 Story retelling data.
The story retelling data were collected by Breithaupt, Li, & Kruschke (2022). In one of their studies,
participants were asked to read stories created by researchers (seed stories) and retell the stories in
their ‚Äúown words‚Äù. These retold stories were then presented to the next generation of narrators
who performed the same task. In total, each seed story was passed down through 3 generations of
3 seed stories
5 affects
Stage 2 Stage 3
Stage 1
916 participants
participants participants
2695 stories
7428 stories
6474 stories
For each story,
create 5-8
variations:
97 seed stories
in total
'Arnold and his girlfriend, Elizabeth, are traveling through a country in southeastern Asia. They are both photography aficionados and
Elizabeth perfectly composes a photo of their flea-market finds for social media. Afterward, they decide to take a walk through a
neighboring street, but quickly return to the hotel after smelling something rotting. That afternoon, they travel on their way with a
Different participants were recruited for each stage of the experiment. Narrators were asked to
rented car through the countryside. Midway through their journey, the car breaks down on the road. Though Arnold tries, he can‚Äôt
the sky begins to darken. The family welcomes them in hospitably, and even prepares a meal for them. After seating them, their host
retell the story given to them in their own words.
brings out what appears to be a bunch of leafy greens with fruits. After a bit of hesitation, the hungry couple digs in.‚Äô
To prepare the original stories, Breithaupt et al. created three basic stories for one of five
emotion categories (joy, sadness, disgust, embarrassment, risk). Next, for each of the above 15
basic stories, 5-8 variations were created by changing the content of the last sentence so that these
variations differ in the intensity of its corresponding emotion6. In total there are 97 initial stories.
Each participant read and retold three story variations after being instructed to tell the story to
another person in your own words. Participants were not instructed to pay attention to any specific
6 For example, in a story created under the emotion category ‚Äúdisgust‚Äù, the authors manipulated the last sentence
so that the story were perceived with slight disgust (the protagonist eat leafy greens with fruit) to strong disgust
(the protagonist eat soup that contains the whole fruit bat). In a pretest, the researchers validated the effectiveness
of the manipulation of emotion intensity by asking other participants to read the stories and evaluate the degree
of intensity of its corresponding emotion and other measures.
aspects of the stories. The three stories that each participant received are from different emotion
categories and from within the same iteration (e.g. only second generations of retellings). In
making use of the existing data, we removed data from participants who indicated that their first
language is not English. Following these procedures, the 97 original stories were retold for a total
of 2,695 times in the first iteration, 6,474 times in the second iteration and 7,428 in the third
iteration. For more details on the corpus see Breithaupt, Li, and Kruschke (2022).
2.1.2 Psycholinguistic properties included in the current analysis.
To evaluate the age at which a word is learned we used the Age of Acquisition ratings (AoA)
collected by Kuperman et al. (2012). In their study, participants were asked to report the age (in
years) at which they thought they had learned a word7. This self-report measure of AoA has been
validated on a more objective measure based on vocabulary assessment among pupils (Pearson r
= 0.76; Brysbaert & Biemiller, 2017).
Warriner, Kuperman, and Brysbaert (2013) provided valence and arousal 8 ratings for
around 14,000 English words; and Brysbaert, Warriner and Kuperman (2014) provided
concreteness ratings for 40,000 English words. Using a neural network model to capture word
meanings in vectors, Hollis, Westbury, and Lefsrud (2016) extrapolated the above human
judgments of words to over 70,000 English words. In the present study, we take valence, arousal
and concreteness norms from Hollis et al. (2016). In addition, we compute emotionality by taking
the absolute value of the difference between the word‚Äôs valence and average valence in the dataset
so that the most negative and the most positive words have the largest scores on emotionality.
Frequency was retrieved from the Google Ngram Books Corpus (Michel et al., 2011).
Length of word was computed by counting the number of letters in a word. We also extracted
grammatical category for each word using the NLTK python package (Bird, Klein & Loper, 2009).
7 In Kuperman et al. (2012), participants were told to use the following definition when providing AoA: ‚ÄúWe mean
the age at which you would have understood that word if somebody had used it in front of you, EVEN IF YOU DID
NOT use, read, or write it at the time.‚Äù
8 Valence and arousal are the two primary and independent dimensions of emotion that have been found universal
across cultures (Russell, 1980). Valence taps into pleasantness that a word evokes, with higher-valenced words evoke
pleasant emotions and lower-valenced words evoke unpleasant emotions; while arousal taps into feeling of
excited/calm that a word evokes, with highly arousing words evoke excited, stimulated, frenzied, jittery, wide-awake
and low arousing words evoke calm, sluggish, dull and sleepy.
2.2 Analysis
For each story retelling, we first extract word types (the distinct word forms in a text).
Whether a word type in the input story is preserved in the output story was taken as the dependent
variable, with preservation coded as 1 and drop-out coded as 0. The set of psycholinguistic
properties mentioned above were included into a regression model as predictors. In addition, we
also included two variables: the number of times a word appeared in the input story and the position
1 ‚Äì 3). We included participants ID, story ID, emotion category and tagged part of speech as
random effects. Participants ID was included as a random effect to model individual differences
among participants whereby some participants use more words from the input story than others
(due to better memory or devoting more attention to the task). Story ID as a random effect models
the idiosyncratic features of a story that makes its words more likely to be reproduced than other
stories. Moreover, since the original stories were created by researchers to contain one of the five
emotion appraisals, we entered emotion category as a random effect to model potential differences
in the role of different emotions on the probability of a word‚Äôs preservation. Lastly, we include
grammatical category as a random effects since it has been found to predict language evolution,
such as borrowing of loan words between languages (Monaghan & Roberts, 2019) and change of
lexical forms (Monaghan, 2014).
2.3 Result
We first examined multicollinearity for all models. Multicollinearity can be assessed by
the variance inflation factor (VIF), which measures how much the variance of a regression
coefficient is inflated due to multicollinearity in the model. We computed VIF for each
independent variable. The smallest possible VIF value is 1, suggesting complete absence of
multicollinearity. As a rule of thumb, a VIF value that exceeds 5 indicates a problematic amount
of collinearity (James et al., 2014). We found that all independent variables had a VIF value
The mixed effect logistic regression shows that all independent variables (i.e. AoA,
concreteness, arousal, emotionality, length, frequency) except valence are significant predictors of
Among the four random effects, the emotion category of the initial story has a much smaller
standard deviation than the other three, suggesting that there is little variability in word
preservation across different emotion categories. In contrast, there is much larger variability in
word preservation across different stories, different individuals, and words of different
grammatical categories. Further analysis shows that nouns and verbs are more likely to be
Fixed Effects Odds Ratios 95% CI P
(Intercept) 0.39 0.30-0.50 <0.001
Age of Acquisition 0.98 0.97-0.98 <0.001
Valence 1.00 0.99-1.00 0.568
Arousal 1.17 1.16-1.18 <0.001
Concreteness 1.45 1.44-1.73 <0.001
Emotionality 1.55 1.39-1.73 <0.001
Length 1.05 1.04-1.06 <0.001
Word count 1.71 1.70-1.73 <0.001
Log Frequency 1.05 1.04-1.06 <0.001
Session 1.34 1.29-1.40 <0.001
Random Effects Variance Std.Dev Number
Story ID 0.23 0.48 9265
Individual ID 0.49 0.7 6126
Grammatical Category 0.10 0.31 9
Emotion Category 0.01 0.09 5
Observations
537,055
Marginal R square 11.7%
Conditional R square 29.6%
Note: all variables (except session) are scaled and centered. Word count refers to number of
times a word appears in the input story.
Looking at fixed effects, the odds ratio shows the relative change in likelihood of word
preservation for a one standard deviation increase in the predictor. The closer the odds ratio is to
1, the smaller the effect of the predictor. Age of acquisition shows a small but significant effect on
word preservation, with one standard deviation increase in age corresponding to 2% decrease in
likelihood of word preservation. This suggest that earlier acquired words (smaller AoA) have a
larger chance of being preserved in the context of a story retelling task. Furthermore, and consistent
with our prediction, concrete words are also more likely to be preserved, with one standard
deviation increase corresponding to 45% increase in the likelihood of word preservation. In
addition, words of greater emotionality (as opposed to emotionally neutral words) and words of
higher arousal are found to have a better chance of being preserved. Valence, however, is not a
significant predictor, suggesting that negative words do not have a selective advantage over
positive words.
We also entered word length, count (number of times a word appears in the input story),
and word frequency as control factors. The results show that longer words, words that appeared
multiple times in a story, and words of higher frequency in Google Ngram Corpus are more likely
to be preserved. Lastly, we include session number as the only non-linguistic factor. It shows that
people in later sessions of the transmission chain tend to keep more words: word preservation
likelihood increase by 34% from session 1 to session 2, and from session 2 to session 3. This
suggests stories become increasingly stable after each additional retelling. Also, stories became
shorter in the process of retelling, a process referred to as ‚Äúleveling‚Äù (Allport & Postman, 1947).
text label from top to bottom respectively represents the percentages of word preservation, number
of word types and number of word tokens in the corresponding grammatical categories.
3. Study 2
In Study 1, we found that when recounting stories words that are early-acquired and that
are more concrete, emotional and arousing tend to better preserve across retellings. This cognitive
selection in language production, when aggregated across generations of language speakers, may
give rise to a macro-level pattern that influences language evolution, leading to the selective
preservation of words that are early-acquired, concrete, emotional and arousing. In Study 2, we
examine this possibility using historical language corpora.
3.1 Materials and Methods
To evaluate the life and death of words over historical time, we extracted word frequency
from historical language corpora. Three corpora were used: the Corpus of Historical American
English (COHA; Davies, 2010), and two corpora from Google Books Ngram (Michel et al., 2011):
the all English corpus and the English fiction corpus.
The major difference among these three corpora is their genre compositions. COHA was
carefully curated to be genre-balanced decade by decade, with major genres including TV/movies
(subtitles), fictions, magazine, newspaper, and non-fiction. The corpus is also balanced across
decades for sub-genres and domains (e.g., sub-genres for fiction are prose, poetry, drama, etc.).
Fiction and TV/movies are the biggest genres in COHA, together accounting for 54-57% of the
total in each decade. Google Ngram English Fiction include only fiction books. The advantage of
COHA and Google Ngram English Fiction is that they have fixed genre compositions across time.
Hence, it alleviates the concerns that the historical patterns we observe (e.g. change of word
frequency) are merely an artifact of changing genre compositions. Moreover, both corpora are
large, with COHA containing around 475 million words from 1820 to 2000 and Google Ngram
English Fiction containing around 40 billion words.
The third corpus, the Google Ngram All English corpus, is made up of around 155 billion
words. It represents around 6% of all English books published over the last several hundred years.
On the one hand, this corpus has proved fruitful in capturing cultural shift such as evolution of
grammar, adoption of technology (Michel et al., 2011), and national wellbeing (Hills, Proto, Sgroi,
& Seresinhe, 2019); on the other hand, it has been criticized for containing corpus artifacts due to
its shifting sampling paradigm. For example, Pechenick, Danforth, and Dodds (2015) found clear
evidence for an explosion of scientific writings since around 1930s in Google Ngram All English,
but not in Google Ngram English Fiction. This surge of academic writings offers a natural control
for other corpora as we note below.
suffix to the end of a word is a common strategy for word nominalization. It makes a word longer
frequency for words that end with each of three common suffixes (-ism, -tion, -ity) and words
longer than 15 letters. Unsurprisingly, Google Ngram All English contains much more
nominalized words and longer words than COHA and Google Ngram English Fiction corpus, but
the difference only becomes larger in recent decades. This further suggests that the Google Ngram
All English corpus reflects an increase in scientific writing and should therefore show patterns of
age of acquisition, concreteness, and emotional valence that are likely to be different from those
found in our other two corpora. Thus, we provide the Google Ngram All English corpus for
Google Fiction corpus, which are not subject to this known sampling bias and therefore may better
reflect the language use of its time period.
suffix for nominalization (-ism, -tion, -ity) and words longer than 15 letters.
3.2 Analysis
3.2.1 Quantifying frequency change
The increase of logged word frequency from year 18009 to year 2000 were taken as the
dependent variable:
9 For COHA, we take year 1820 because COHA starts at year 1810 and the 1810 data was smaller in size than the
1820 data (words in 1820 are approximately 6 times the number of words in 1810).
ùêπùëüùëíùëû
Œîùêπùëüùëíùëûùë¢ùëíùëõùëêùë¶ = ùêøùëúùëî(ùêπùëüùëíùëû ) ‚àí ùêøùëúùëî(ùêπùëüùëíùëû ) = ùêøùëúùëî ( )
2000 1800
ùêπùëüùëíùëû
We are aware that historical word frequency may not always follow a monotonic trend. If there is
a large proportion of words with non-monotonic frequency change, representing change of word
frequency as the difference between two endpoints could be problematic. To find out the
proportion of words with stable historical frequency, monotonic frequency change and non-
monotonic frequency change, we used principal component analysis (PCA) on relative
frequencies10 between the years 1800 to 2000. PCA captures patterns of change without relying
on prior assumptions. We grouped words that score at different ranges of each principal component
The first principal component (PC1) corresponds to the absolute frequency. Words that score
highly on the opposite ends of PC1 are high frequency words like all, one, on, and low frequency
words like reformat, roadbed, histamine. The second principal component (PC2) captures
monotonic change: words with monotonically increasing frequency on one end of the PC2 and
words with monotonically decreasing frequency on the other. The third principal component (PC3)
represents an asymmetric curve, with U-curve on one end and inverted U-curve on the opposite
end. PC1 explains the most variance (48%), suggesting that rank order of word frequency is largely
preserved for most words. PC2 explains around 19% of the variance. In contrast, PC3 only explains
around 5% of the variance. Given the large variance explained by PC1 and PC2, our analysis
focusing on word frequency difference between the years 1800 and 2000 captures the dominant
pattern of word frequency change.
PC 1 PC 2 PC 3
(explains 48% of the variance) (explains 19% of the variance) (explains 5% of the variance)
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
1800 1850 1900 1950 2000 1800 1850 1900 1950 2000 1800 1850 1900 1950 2000
Top 10% 10%-20% 20%-50% 50%-80% 80%-90% Bottom 10%
10 For each word, frequencies were divided by the historical maximum so that frequencies range from 0 to 1.
11 We only show results from the Google English Fiction corpus in the main text for brevity. Results from COHA
shows a similar pattern (see details in the Supplementary Materials Section 1)
principal component analysis was conducted on relative word frequency between 1800 and 2000
from the Google English Fiction corpus. Respectively, PC1, PC2, and PC3 corresponds to word
frequency, monotonic frequency change and non-monotonic frequency change.
3.2.2 General additive model
Age of acquisition, concreteness, arousal, and valence were taken as independent variables.
We controlled for word length and log frequency at year 1800. All variables are scaled and
centered. We are aware that word meanings are constantly in flux. Therefore, for those words that
substantially changed their meaning, psycholinguistic norms such as valence, arousal and
concreteness may not apply because these norms were rated by people living today and therefore
do not necessarily reflect word meaning 200 years ago. For this reason, we quantified semantic
shift following procedures described in Li and Siew (2022) and only keep words with stable
semantic meaning (see technical details in Supplementary Materials section 5). We also performed
S3)
Preliminary analyses revealed non-linear relationships in the data. General additive models
can accommodate nonlinear relationship by adding extra fixed effects to represent quadratic, cubic,
or higher order terms of a variable to represent complex relationships. To avoid overfitting
problems, general additive models introduce a penalty on more complex curves. For each
independent term in the model, two basic statistics are provided: how well the term predicts the
dependent variable, represented by an F statistic and a p-value; and how non-linear the relationship
is, represented by the estimated degree of freedom (edf). Edf of 1 indicates a linear relationship,
with higher edf for more complex curves.
Given that results using Google Ngram English Fiction corpus and COHA are similar to
each other, we combined these two corpora in one general additive model, including corpus as a
random effect. See Supplementary Materials section 2 for model fittings to each of the three
corpora independently.
3.3 Results
The combined model with both COHA and Google Fiction accounts for 41.8% of the
the observed relations between psycholinguistic properties and frequency change are not distinct
across the two corpora. We did not include grammatical category as a random effect because it
complicates the model while only explaining 0.6% additional variance.
Combined
(Fiction & COHA)
Smooth terms Estimated F value p-value
DF
Age of Acquisition 4.46 1467.7 <0.001
Length 4.21 61.2 <0.001
Valence 4.95 81.6 <0.001
Arousal 4.95 43.3 <0.001
Concreteness 5.09 46.1 <0.001
Log Frequency (year 1800) 6.97 1436.5 <0.001
Number of Observation 23,306
Marginal R2 0.367
Conditional R2 0.436
Note: p-values for all variables are below 0.0001.
frequency, with shading indicating 95% confidence intervals. We also examined the one-to-one
relationship between change of word frequency and each independent variable separately and
present the results in the embedded miniature plots.
For each variable, in order to determine which region of the slope was significantly
increasing or decreasing, we determined the derivatives and standard errors for the derivatives
along the gradient of the slope. The derivative at one point of a curve means the slope of the tangent
line at that point. Where the 95% confidence interval for the derivatives do not overlap zero, there
indicate significant change with a solid line and regions of slope that show no significant change
with a dashed line. Therefore, the advantage of general additive model is that it shows in which
specific range an independent variable has significant impact on the dependent variable.
The same scale of y-axis is used for all subplots for easier comparison of the impact on
frequency change: the steeper the slope the larger the impact. Consistent with Study 1, we found
that earlier acquired (smaller AoA value), more concrete, and more arousing words were more
likely to increase in frequency. Unlike in Study 1 where AoA has a relatively small effect on word
survival, when looking at language evolution across two centuries, early acquisition has a large
effect on change of word frequency. This effect of AoA is monotonic. In contrast, the effect of
concreteness and arousal on change of word frequency is less straightforward. For low- to middle-
concrete words, the relationship is positive and monotonic, with increasing concreteness relating
to greater increase in word frequency. However, for middle- to high-concrete words, there is no
significance effect of increased word concreteness on the change of word frequency. Similarly, the
main effect of word arousal on increase of word frequency is mainly driven by words at the two
ends of the arousal scale.
has a monotonic, negative effect on increase of word frequency, with longer words seeing greater
reductions in word frequency. More positively-valence words have better chance to increase in
their frequency, but this effect is only significant for negative to neutral words.
We also found that words of lower frequency at year 1800 are more likely to increase in
frequency over the past two centuries. This pattern is indicative of regression to the mean. This is
counter-intuitive at first glance because one may predict the opposite pattern given the negative
correlation between log frequency in 1800 and age of acquisition. Caution must apply when
interpreting this result because our data only contains words that are still in use in the recent
decades and, therefore, systematically exclude words that are no longer in use (simply because
linguistic features such as AoA and valence norms are only available for words in use today).
Therefore, we include frequency in the model simply as a control factor in this analysis.
and six psycholinguistic properties (x-axis). All independent variables are scaled and centred
around 0. Lines are model estimates (dotted lines are estimates and solid lines indicate where the
estimates are significantly increasing or decreasing). Shading indicates 95% confidence intervals
around the estimates.
4. Discussion
What makes a word more likely to survive than others? We argue that the answer to this
question lies in both micro-level, individual‚Äôs language production and macro-level patterns of
language evolution, with the effect of the former magnified into the latter through the behavior of
generations of language users. We took a methodologically pluralistic approach to examine this
question. In Study 1, we examined word survival rate in an serial reproduction experiment where
people read stories and retold them to others. In Study 2, we analyzed the rise and fall of word
frequency across 200 years, using English corpora. Both studies converged on the same conclusion:
early acquisition, concreteness, and high arousal are the linguistic properties that give a word a
selective advantage. Our results provide empirical evidence supporting the emerging language
theory that suggests language should evolve to be easier for humans to learn, process and produce
(Beckner et al, 2009; Christiansen & Chater, 2008). Moreover, our result is strengthened by its
strong external validity, achieved by using large-scale data, examining survival of words in context
(instead of learning and producing individual words independent of context), and validation of
results across different methodologies and time frames.
The current results are consistent with previous findings on the related effects of
concreteness and early acquisition on language evolution. For example, concrete words were found
to have more stable meaning (Li & Siew, 2022), and rising in frequency over the past two centuries
(Hills & Adelman, 2015); earlier-acquired words are more resistant to both lexical (Monaghan,
2014) and semantic change (Li & Siew, 2022).
Why have the patterns of language change observed here not stabilized? That is, why has
language evolution not already arrived at a stable equilibrium among the various cognitive
selective forces for word features observed in Study 1? The answer, suggested by Hills and
Adelman (2015), is that English has seen a period of marked competition over the last 200 years
caused by an increasing amount of language production facilitated by more numerous and faster
means of language communication as well as a growing population of English language speakers.
If language production exceeds the attention capacity of humans to process it, this increases
competition and what Darwin (1859), speaking of biological species, called the ‚Äústruggle for
existence‚Äù: when competition increases, selective forces become stronger. For our purposes, more
competition for language means that more words must compete to pass through the life cycle of
language communication, successfully acquiring listeners who later become producers, much like
a parasite seeking a host. The consistency we observe between individual story retellings and the
pattern of language change observed over the last two centuries is coherent with this language
competition hypothesis. That is, the pattern of language evolution we observe over historical time
finds its root in individual cognitive selection for words that are easier to process, recall and
reproduce: words that more successfully achieve that are, by definition, better adapted for the life
cycle of language production.
Why does the effect of emotional valence on language production differ across the two
studies? The divergent result is a reminder that language evolution does not only follow cognitive
mechanisms in language production; it also responds to the shifting sociocultural environment. A
world that is unprecedentedly peaceful and safe (Pinker, 2011) is likely to reflect itself in language,
with greater demands for positive words and less for negative words (Hills et al., 2019). Perhaps
that is why we observed that positive words have larger increase of word frequency over the past
two centuries. In contrast, in the iterated learning experiment where the influence of historical
improvement of life satisfaction is precluded, we found emotionality, instead of valence, predicts
survival of words. This result is consistent with previous research showing that emotional words
are better remembered (Adelman & Estes, 2013) and easier to process (Pexman & Yap, 2018) than
neutral words.
We also have a serendipitous finding on arousal: both studies converge on the selective
advantage of arousing words. This result is in line with current theories that attribute emotional
memory enhancement to arousal (Hamann, 2001; Mather, 2007; Phelps, 2006). The underlying
assumption is that our limited memory resources are preferentially allocated to significant stimuli
(Nairne, 2010), with arousal acting as a primary index of behavioural significance (McGaugh,
2000). However, it is worth pointing out that most studies that explored the role of arousal on
memory used non-verbal input (e.g., images, videos). Studies using words as stimuli are scarce,
and often find null effect of arousal on memory (Adelman & Estes, 2013) and semantic processing
(Pexman & Yap, 2018; Moffar et al, 2015). It seems that the effect of arousal is harder to detect
when words are used as stimuli. This is probably because individual words out of context are
substantially less arousing than images and videos, so that arousal levels were generally too low
to exert effects. In contrast, both of our studies examined survival of words within the context of
meaningful communication. With an unprecedented large amount of data, we are able to detect
effects of arousal on word survival, and this effect is larger than emotionality.
Lastly, it is worth pointing out that language is evolving to become more efficient. Pilgrim
et al (2021) found a steady increase in the information density of English since 1900. Therefore,
higher survival rate in earlier-acquired, concrete and arousing words does not necessarily suggest
that English language is going down the path of simplification or degeneration. In contrast, it may
point to a more positive prospect: an enhanced communication capacity that allows humans using
simpler words to express clear but increasingly complicated ideas.