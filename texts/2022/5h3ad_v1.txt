In-Depth Analysis of a Fraction Comparison Task
In-Depth Analysis of a Fraction Comparison Task
1 *23
Nicolás Morales & David M. Gómez
(1) Department of Psychology, Universidad de Chile, Santiago, Chile.
(2) Institute of Education Sciences (ICEd), Universidad de O’Higgins, Rancagua, Chile.
(3) Millenium Nucleus for the Study of the Development of Early Math Skills (MEMAT),
Santiago, Chile.
* Instituto de Ciencias de la Educación UOH, Avda. Libertador Bernardo O’Higgins 611
oficina O743, 2841959 Rancagua, Chile, email: david.gomez@uoh.cl, tel: +56 2 3328 6065.
In-Depth Analysis of a Fraction Comparison Task
Abstract
Fraction comparison tasks are commonly used in research to assess students’ knowledge of
fractions. However, as fractions are complex multi-faceted constructs that are understood in
distinct manners by different groups of children, it is relevant to investigate the properties of
fraction comparison tasks to understand what conclusions can be validly drawn from their
outcomes. In this paper, we perform a psychometric analysis of secondary data from a
fraction comparison task performed by 403 school children from 10 to 12 years of age. The
task consisted of 24 fraction pairs to be compared. These pairs could share or not a common
component (numerator or denominator) and could be either congruent or incongruent (i.e.
items for which naive comparison of the natural numbers composing the fractions leads to the
same or different answer than comparing the fractions’ magnitudes). Results showed that
congruent items had substantially higher scores and lower discrimination indices than
incongruent items. Strikingly, congruent items without common components showed null or
negative discrimination indices, driven both by unusually low scores obtained by top-
performing children and unusually high scores obtained by low-performing children. Finally,
a confirmatory factor analysis suggested a 3-factor structure, with all incongruent items
pooled into a single factor, as the best for describing the responses of students who were not
extremely guided by item congruency. Our results confirm that fraction comparison items
elicit complex patterns of responses across different groups of students, questioning simple
interpretations of item characteristics such as congruency.
Keywords
fractions, fraction comparison, congruency, psychometric properties, item analysis, numerical
cognition
In-Depth Analysis of a Fraction Comparison Task
Fractions—and rational numbers, more generally—are a core element in school mathematics
curricula worldwide (Litwiller & Bright, 2002) and are considered crucial for achievement in
advanced mathematical topics such as algebra (Siegler et al., 2012). Fraction understanding is
necessary to pursue higher education studies in fields related to science, technology,
engineering, and nursery, among others (Davidson, 2012; Sformo, 2008). Yet, this topic has
traditionally proved to be a difficult one in school mathematics for many students.
Researchers from diverse disciplines have studied students’ errors in working with
fractions (Baroody & Hume, 1991) as well as the cognitive foundations of fraction
understanding (e.g. Bonato et al., 2007; Ischebeck et al., 2009; Meert et al., 2010; Schneider
& Siegler, 2010; Sprute & Temple, 2012; Stafylidou & Vosniadou, 2004; Vamvakoussi et al.,
2012). In this research, many studies have used fraction comparison tasks to explore
children’s understanding and mental processing of fractions (Di Lonardo Burr et al., 2022;
Gómez & Dartnell, 2019; González-Forte, Fernández, & Van Dooren, 2020; González-Forte
et al., 2020; Meert et al., 2010; Reinhold et al., 2020; Van Hoof et al., 2013). But not many
studies have studied the psychometric properties of these tasks. To our knowledge, the only
study doing this is the one by Van Hoof et al. (2015), who applied a Rasch model to a
rational number knowledge test. However, this test spanned a diversity of dimensions of this
knowledge such as size and operations and used fractions and decimals. Therefore, their
results do not speak specifically about fraction comparison section.
An adequate understanding of fractions—and, more generally, of rational numbers—
requires fluency with multiple interpretations. While the interpretation of fractions as parts of
a whole is widely known, there are several other interpretations like fractions as operators, as
measures, or as points on the number line (Kieren, 1976). Moreover, understanding fractions
as numbers requires students to redefine their concept of what a number is. This process of
conceptual change (Vosniadou, 2013) takes students from an initial concept of number,
linked to natural numbers, to a more advanced concept that includes rational numbers
(Vamvakoussi & Vosniadou, 2004; Vamvakoussi et al., 2013; Van Dooren, Lehtinen, &
Verschaffel, 2015). Conceptual change is needed to accommodate several aspects of rational
number knowledge, such as rational number size/magnitude, operations, and density (Van
Hoof et al., 2015). On top of this, as students familiarize themselves with fractions, they
learn—or develop on their own—intuitions and strategies for solving specific types of
problems (Clarke & Roche, 2009; González-Forte, Fernández, & Van Dooren, 2020;
Obersteiner et al., 2020; Rinne et al., 2017). Strategies for comparing two fractions include
the use of benchmarks (“1/3 is smaller than 1/2 and 4/7 is larger than 1/2, therefore 4/7 is
larger than 1/3”) and gap thinking (“1/3 is missing two pieces to complete the whole and 4/7
is missing three pieces, therefore 1/3 is larger than 4/7”). These strategies are not always
mathematically correct or not necessarily applicable to every fraction comparison problem.
Altogether, we conclude that learning fractions is a complex endeavor affected by diverse
aspects: students’ conceptual understanding of fractions, fractions’ multiple interpretations,
and strategies to work with them. Fraction comparison tasks, although simple on the surface,
fully embody this complexity of concepts (Stafylidou & Vosniadou, 2004), interpretations
(Fuchs et al., 2013), and strategies (Gómez & Dartnell, 2019), meaning that item-level and
participant-level variability may be substantial and interact with one another. A deeper study
of fraction comparison tasks is thus necessary to improve the validity of conclusions obtained
from their results.
In this paper, we conducted a psychometric analysis of a 24-item computerized
fraction comparison task used in previous studies and investigate its item structure. In the
following sections, we describe the two main dimensions considered in designing this task:
the presence or absence of a common component (numerator or denominator) between the
fractions to be compared and the congruency relation between the magnitudes of the fractions
and those of their natural number components.
Two Dimensions Affecting Fraction Comparison Performance
In this section, we review research related to two aspects known to affect children’s and
adults’ performance in comparing fractions. The first one is whether the fractions share or not
a common component (numerator or denominator). The second one is whether the order
relation between fractions is congruent or not with the order relation between the fractions’
education and cognitive/experimental psychology research, that have considered these
aspects in their task design.
they consider items with/without common components and congruency.
Study Components Congruency
cc ncc co in
Clarke & Roche (2009) X X X X
Meert et al. (2010) X X X
Van Hoof et al. (2013) X X X
Van Hoof, Janssen, Verschaffel, & Van Dooren X X X
(2015)
Rinne et al. (2017) X X X X
Gómez & Dartnell (2019) X X X X
González-Forte, Fernández & Van Dooren (2019) X X X X
Reinhold et al. (2020) X X X
Di Lonardo Burr et al. (2022) X X X X
Note: For each study, we indicate whether its set of items included fraction pairs with a
common component (cc), pairs without common components (ncc), congruent pairs (co), and
incongruent pairs (in).
Presence of a Common Component
One possible aspect in which fraction comparison items can vary is whether the two fractions
share or not a common component (e.g. Clarke & Roche, 2009; Di Lonardo Burr et al., 2022;
Meert et al., 2010). In the case fractions share a common component, this component can be
either their numerator (e.g., 5/7 and 5/9) or denominator (e.g., 3/8 and 6/8). Fractions may
also share no common component (e.g., 2/3 and 4/7).
Early cognitive studies about fractions focused on how the human mind represents
these numbers, and specifically if such representation works in a holistic or a componential
manner (e.g., Bonato et al., 2007). A “componential” representation referred to the case
where the natural numbers composing the fraction are mentally represented as separate
entities (e.g., 1/2 as two independent numbers 1 and 2), whereas a “holistic” representation
referred to the case where the magnitude of the fraction is represented as a single entity (e.g.,
1/2 as half of a given object or distance; Gallistel & Gelman, 2000).
To probe whether adults use a componential or a holistic representation of fractions,
Bonato et al. (2007) conducted four experiments in which participants had to compare unit
fractions (1/n) against the benchmark 1/5. They examined the presence of two experimental
effects commonly found in numerical cognition research with natural numbers: distance
effects (increased response times to comparison items that have associated smaller distances;
see Moyer & Landauer, 1967) and SNARC effects (an association effect between small
magnitudes and responses exerted by the left hand; and between large magnitudes and
responses exerted by the right hand; see Fias & Fischer, 2005). Their results pointed to the
presence of a regular distance effect, but a reversed SNARC effect. They interpreted this as
an indication that participants processed the unit fractions to be compared (e.g., 1/6 vs. 1/5) in
a componential manner (i.e., by comparing the non-common component, 6 vs. 5), and
therefore both effects were determined by the fractions’ denominators rather than their
magnitudes. Together with evidence from other tasks, this led them to conclude that
fractions’ magnitudes are not mentally represented.
Kallai and Tzelgov (2009) argued that the study by Bonato et al. (2007) had important
limitations. In particular, these authors claimed that Bonato et al.’s (2007) set of items biased
participants towards using a componential representation, and cannot be taken as proof of the
inexistence of a holistic representation. Later, Toomarian and Hubbard (2018) directly tested
this hypothesis, replicating Bonato et al.’s (2007) task both with the same and with more
complex fractions. The results confirmed Bonato et al.’s (2007) conclusions when the
original set of fractions was used but differed with the more complex fractions, supporting
the idea that Bonato et al.’s (2007) results were strongly driven by their particular item set.
Another study with unit fractions (Kallai & Tzelgov, 2012) related the likelihood of using
componential/holistic processing of fractions with their mathematical notation. They trained
adult participants to associate unit fractions with arbitrary, non-numerical symbols. Results
showed that participants’ performance with these novel symbols exhibited holistic
processing, suggesting that the visual presence of fractions’ components has a role in
affecting participants’ processing of them.
Further research has suggested that the presence of a common component in a fraction
comparison item makes participants more likely to respond with a componential approach,
whereas the opposite (i.e., a holistic approach) occurs when the fractions share no common
component (Meert et al., 2009, 2010; Sprute & Temple, 2011). This has led, in recent years,
to the consideration of learners’ strategies for fraction comparison as a crucial element. In
this light, holistic and componential processing are two possibilities available to compare
fractions, with the likelihood of their usage being affected by specific item characteristics.
It is relevant to point out that, in the cognitively-oriented literature, componential
processing typically refers to a particular fraction comparison strategy, where one fraction’s
numerator and denominator are directly compared to their counterparts of the other fraction
(e.g., answering 2/3 vs. 4/9 by comparing 2 vs. 4 and 3 vs. 9). In what follows, we call this
strategy naïve component comparison, and note it as distinct from other strategies for fraction
comparison that rely only on their components, such as cross-multiplication (e.g., answering
2/3 vs. 4/9 by comparing 2×9 vs. 3×4). Naive component comparison makes sense when
fractions share a common component, but may also be applied with fraction pairs that lack
common components.
Congruency With Natural Number Comparison
When students start learning fractions and rational numbers, they bring previous knowledge
about mathematics and numbers. This previous knowledge has been identified as a potential
barrier to learning (Hartnett & Gelman, 1998). Stafylidou and Vosniadou (2004) argued that
learning fractions requires a radical cognitive reorganization of the concept of what numbers
are. Such reorganization might lead students to generate ad hoc concepts or strategies that are
not mathematically correct (Stafylidou & Vosniadou, 2004). Naive component comparison,
introduced in the previous section, is one such ad hoc strategy that students may devise. After
observing many examples of fraction comparison where fractions share a common
denominator, learners may believe that fraction comparison simply works by comparing
fractions’ components. Choosing as larger the fraction with larger components will lead to
correct answers in some cases (e.g., 2/3 vs. 1/2), but to incorrect answers in others (e.g., 2/3
vs. 4/9). In the first case, we have that naïve component comparison leads to the same answer
that actual fraction comparison, and therefore we call this fraction pair “congruent”. In the
latter case, instead, naive component comparison leads to the opposite answer that actual
fraction comparison, and we call this fraction pair “incongruent”. Hence, naive component
comparison leads systematically to correct answers in the case of congruent pairs and to
incorrect answers for incongruent pairs.
More generally, researchers have called a rational number test item congruent if the
naive application or generalization of natural number properties leads to the mathematically
correct answer, and incongruent otherwise (Van Dooren, Lehtinen, & Verschaffel, 2015). To
assess the role of congruency in educated adults’ performance in fraction comparison,
Vamvakoussi et al., 2012) presented them with tasks about different rational number
properties including congruent and incongruent items, while measuring their accuracy as well
as their response times. Their results showed that responses to congruent items were more
accurate and quicker than responses to incongruent items, supporting the hypothesis that
previous natural number knowledge interferes with rational number processing. Similar
results emerge with school students. Van Hoof et al. (2013), using a similar paradigm to the
previous study, tested secondary school first- and fifth-year students with a fraction
comparison task. Results showed that, while accuracy in incongruent items was high for both
age groups, response times were higher for incongruent items than for congruent items,
hinting that students’ answers, albeit correct, were still disturbed by prior knowledge. Gómez
and Dartnell (2019) observed that a sizable number of fifth- to seventh-grade students
answered a fraction comparison task in a pattern consistent with naive componential
comparison.
Congruency can be identified as a specific pattern of responses within a broader
phenomenon known as Whole Number Bias or Natural Number Bias (e.g. Vamvakoussi et
al., 2012; Van Hoof et al, 2013). Research about this bias in children, adults, and expert
mathematicians, as well as theoretical discussions about its nature and origins, has been an
active area of work (Hoch et al., 2018; Morales et al., 2020; Obersteiner et al., 2013; Siegler
et al., 2012; Vamvakoussi et al., 2012; Van Hoof et al., 2013; Van Hoof et al., 2015).
Common Components and Congruency Together
Although previous studies have overall provided support for the role of congruency in
fraction comparison, a finer analysis reveals a more complex picture. This becomes evident
when considering the specific case of items without common components. Gómez and
Dartnell (2015) reanalyzed data from several fraction comparison studies, showing that
congruent items tended to be responded to more correctly and/or quickly than incongruent
items only when fractions shared a common component. When there was no common
component, this difference disappeared or was reversed, with congruent items being
responded to more incorrectly and/or slowly than incongruent ones. Later studies have agreed
with this finding: Morales, Dartnell, and Gómez (2020) showed that the response times of
adults who are proficient in mathematics showed a significant congruency effect for items
with a common component and a significant reversed congruency effect for items without.
Moreover, these effects followed different time courses along the session: the congruency
effect was significant at the beginning of the session but faded by the end of it, while the
reversed congruency effect remained present throughout the session. Therefore, while item
congruency is overall a significant predictor of performance, it cannot explain these nuanced
patterns of results.
A possible alternative explanation for the reversed congruency effect is the use of
strategies, as some of them may naturally lead to a reversed congruency effect for items
without common components. One such strategy is selecting as the larger fraction the one
with a smaller denominator, which would lead to low scores in congruent items and high
scores in incongruent ones. Another such strategy is known as gap thinking, where learners
compare fractions by focusing on “how many pieces each fraction is missing to complete the
whole” (e.g., Clarke & Roche, 2009; Morales et al., 2020; Pearn & Stephens, 2004). This
gap-based strategy has been addressed both in research with school children (González-Forte
et al., 2020) and adults (Morales et al., 2020). González-Forte et al. (2020) showed the
presence of a group of school children that seems to systematically use the gap strategy and,
therefore, exhibits better results for incongruent than for congruent items. However, this
reversed congruency effect was present not only in that group but also in another one, labeled
“reversed bias”. Morales et al. (2020) showed that Engineering undergraduates presented a
reversed congruency effect for items without common components, but scores were not
significantly modulated by whether gap thinking led to the incorrect or correct answer.
Altogether, this shows that gap thinking alone does not fully explain the reversed congruency
effect, but leaves open the possibility that a combination of several strategies might provide
such an explanation.
In summary, the presence of common components and congruency have shown to be
relevant item characteristics that affect learners’ performance in comparing pairs of fractions.
They have been revised in a number of studies with children and adults, yet it is still unclear
how these dimensions interact in affecting fraction comparison performance.
The Present Study
In the present study, we reanalyzed data from a fraction comparison task used before by
Gómez et al. (2015) and Gómez and Dartnell (2019). We take as a departure point the
dimensions of presence of a common component and congruency, and evaluate the
psychometric properties of the task. We focused on the following research questions: (1)
How do the presence of common components and congruency affect fraction comparison
items’ psychometric properties? (2) To what extent do these two dimensions explain the item
structure of the task?
Materials and Methods
Dataset
We analyzed data from a fraction comparison task performed by 502 fifth-, sixth-, and
seventh-grade Chilean students (these grades approximately correspond to the ages of 10 to
13). Students were recruited from five schools in different municipalities of Santiago, Chile.
For this study we considered only the data from students with no missing answers, reducing
the sample size to 403.
These data have been previously studied in the context of their relation to students’
outcomes in an inhibition task (Gómez et al., 2015) and the exploration of individual
differences in fraction comparison by means of clustering analysis (Gómez & Dartnell,
2019). However, the present study extends the basic psychometric analysis performed before,
by describing the discrimination indices of the test items and, more importantly, investigating
the item structure of the test.
Task and Procedure
The fraction comparison instrument included 24 pairs of fractions, listed in Appendix 1 of
Gómez and Dartnell (2019). The test included six items of each of four item types: congruent
with common components (e.g., 4/9 vs. 8/9), incongruent with common components (e.g.,
4/15 vs. 4/6), congruent without common components (e.g., 2/5 vs. 11/18), and incongruent
without common components (e.g., 6/18 vs. 5/6). The procedure was described by Gómez
and Dartnell (2019).
Data Analysis
We investigated the following properties of the fraction comparison test: item scores, item
discrimination indices, and test reliability. Item scores were calculated as the proportion of
students who answered each item correctly. To calculate item discrimination indices, we first
calculated the first and fourth quintiles of the distribution of total test mean scores and then
classified participants into one of three groups: (a) those whose total test mean score was
equal or higher to the fourth quintile, (b) those whose total test mean score was equal or
lower to the first quintile, and (c) all the rest. Item discrimination indices were obtained by
subtracting, separately for each item, the proportions of correct answers to that item between
groups (a) and (b). Test reliability was assessed using Cronbach’s alpha.
Finally, we investigated the fraction comparison test validity by performing a
confirmatory factor analysis (CFA) for the 4-factor structure obtained by crossing the
presence of common components and congruency. Additional CFAs were computed to
further explore the test factorial structure, as explained in the Results section. Following Sun
(2005), for each CFA we reported SRMR, RMSEA, CFI, TLI, Mc, the Χ² test, and the Χ²/df
ratio.
All analyses were performed using the software R version 4.1.1, and the packages
dplyr version 1.0.7 (Wickham et al., 2022), tidyr version 1.1.4 (Wickham & Girlich, 2022),
lavaan version 0.6-9 (Rosseel, 2012), lme4 version 1.1-27.1 (Bates et al., 2015), and car
version 3.0-12 (Fox & Weisberg, 2019). The analysis scripts, as well as the full dataset, can
Results
Scores for congruent items ranged from .69 to .85, whereas for incongruent items they ranged
from .33 to .40. Within each of the four item types, item scores are homogeneous. Item
discrimination indices show a different picture. Incongruent items exhibit very high indices,
ranging from .77 to .95. In contrast, congruent items show a different pattern depending on
the presence of a common component. Congruent items with a common component have
somewhat similar discrimination indices, ranging from .19 to .38, but those of congruent
items without common components show a majority of negative values and greater
variability, ranging from -.53 to .09.
Dimension Item id Fraction pair Item score Item discrimination
i6 15/17 vs. 6/17 .81 .23
i7 15/19 vs. 7/19 .83 .31
Congruent with a i9 2/11 vs. 3/11 .84 .23
common component i15 4/11 vs. 9/11 .84 .19
i18 4/8 vs. 7/8 .82 .38
i19 4/9 vs. 8/9 .85 .23
i2 1/4 vs. 1/9 .38 .84
i13 3/14 vs. 3/7 .36 .92
Incongruent with a i16 4/15 vs. 4/6 .35 .95
common component i20 5/17 vs. 5/8 .38 .83
i23 6/14 vs. 6/8 .38 .92
i24 7/10 vs. 7/15 .37 .80
Congruent without i1 1/3 vs. 5/7 .76 -.20
common components i3 10/17 vs. 3/9 .69 -.41
i4 11/18 vs. 2/5 .70 -.53
i5 12/17 vs. 5/16 .79 .02
i8 17/19 vs. 4/9 .75 -.27
i14 3/14 vs. 9/17 .78 .09
i10 2/3 vs. 5/17 .39 .88
i11 2/4 vs. 3/13 .36 .87
Incongruent without i12 2/5 vs. 4/15 .40 .84
common components i17 4/5 vs. 6/13 .33 .80
i21 5/6 vs. 6/18 .38 .86
i22 5/6 vs. 8/19 .33 .77
Alpha values for each item type. The average score and Cronbach’s Alpha of the total test
were calculated as well. A mixed logistic regression with components and congruency as
fixed factors and participants as a random factor revealed a significant interaction (Wald’s
χ²(1, N = 403) = 30.8, p < .0001). Separate regressions for items with and without common
components showed significantly higher scores for congruent items in both cases, with a
larger difference for items with a common component (with a common component: b = -2.7,
SE = 0.09, z = -31.5, p < .0001; without common components: b = -1.6, SE = 0.06, z = -25.4,
p < .0001). All four item types show adequate to excellent levels of internal consistency,
although the type of congruent items without common components shows negative
discrimination. This is reflected in the total test’s Alpha value, which turns lower than all four
item types’ Alpha values.
Average Average Cronbach’s
Item type score discrimination index Alpha
Congruent with a common component .83 .26 .77
Incongruent with a common component .37 .88 .94
Congruent without common components .74 -.22 .84
Incongruent without common components .37 .84 .95
Full test .58 .44 .73
To study the test’s item structure, we first carried out a CFA considering each item
type as a different factor. Model fit statistics were SRMR = .070, RMSEA = .029, CFI =
.999, TLI = .999, Mc = .901, χ²(246, N = 403) = 329.9, p = .0003, χ²/df = 1.34. The solution,
however, exhibited problems in the covariance matrix of the latent variables, reflected in an
extremely high correlation of .999 between the factors corresponding to the two incongruent
item types. We, therefore, performed a second CFA considering three factors by combining
together all incongruent items. Model fit statistics were SRMR = .071, RMSEA = .030, CFI =
.999, TLI = .999, Mc = .896, χ²(249, N = 403) = 337.2, p = .0002, χ²/df = 1.35. Still, the
second model also presented problems in the covariance matrix, although this time it was not
due to extreme correlation values between latent variables.
A cue to solve this new problem was given by the clustering analysis reported by
Gómez and Dartnell (2019), which showed that a cluster comprising about half of the sample
exhibited a response pattern extremely influenced by congruency, with scores of almost
100% in congruent items and almost 0% in incongruent items. To check whether this subset
of participants was driving the model fit problems, we first looked for a manner to identify
which participants to remove. We did so by computing for each participant a congruency
index, by subtracting their number of correctly answered incongruent items from their
number of correctly answered congruent items. This index ranged from -12 to +12, where
values close to +12 signal responses highly determined by congruency. In line with the
presence of the cluster detected by Gómez and Dartnell (2019), the congruency index had a
median of +9. We therefore repeated the CFAs with 4 and 3 factors, considering only
participants with a congruency index of +8 or less (n = 191). The model with 4 factors again
exhibited problems in the covariance matrix. This time, however, the model with 3 factors
converged without problems. The fit measures of the successful 3-factor model were SRMR
= .133, RMSEA = .061, CFI = .970, TLI = .967, Mc = .633, χ²(249, N = 191) = 422.6, p <
.0001, χ²/df = 1.70, suggesting an adequate—though not excellent—fit. All models using
intermediate thresholds for the congruency index led to fit problems (see Appendix A for the
model. All loadings and correlations were statistically significant (all ps < .0001). Within
each factor, loadings are relatively homogeneous as shown by their coefficients of variations
in the range 9%-15%.
Since this subsample excluded participants who were mostly guided by naive
component comparison, we repeated the mixed logistic regression analysis as well. This
analysis revealed, again, a significant interaction between components and congruency
(Wald’s χ² (1, N = 191) = 68.0, p < .0001). Separate analyses for items with and without
common components showed, this time, significantly higher scores for incongruent items in
both cases with a larger difference for items without common components (with a common
component: b = 0.29, SE = 0.11, z = 2.7, p = .008; without common components: b = 1.28,
congruent items without common components show important variation in their
discrimination indices with two items displaying negative and two items showing close-to-
zero discrimination.
with congruency indices of +8 or less (n=191).
Item
Dimension Item id Fraction pair Item score
discrimination
i6 15/17 vs. 6/17 .66 .55
Congruent with a i7 15/19 vs. 7/19 .70 .63
common component i9 2/11 vs. 3/11 .75 .52
i15 4/11 vs. 9/11 .72 .48
i18 4/8 vs. 7/8 .71 .72
i19 4/9 vs. 8/9 .72 .54
i2 1/4 vs. 1/9 .78 .66
i13 3/14 vs. 3/7 .74 .88
Incongruent with a i16 4/15 vs. 4/6 .71 .93
common component i20 5/17 vs. 5/8 .77 .61
i23 6/14 vs. 6/8 .79 .81
i24 7/10 vs. 7/15 .73 .66
i1 1/3 vs. 5/7 .52 .10
i3 10/17 vs. 3/9 .36 -.12
Congruent without i4 11/18 vs. 2/5 .38 -.27
common components i5 12/17 vs. 5/16 .59 .31
i8 17/19 vs. 4/9 .48 .00
i14 3/14 vs. 9/17 .57 .40
i10 2/3 vs. 5/17 .81 .70
i11 2/4 vs. 3/13 .76 .71
Incongruent without i12 2/5 vs. 4/15 .81 .58
common components i17 4/5 vs. 6/13 .68 .69
i21 5/6 vs. 6/18 .77 .71
i22 5/6 vs. 8/19 .69 .61
The previous analysis was conducted assuming that the item types induced by
congruency and common components explain in a meaningful manner children’s
performance in this fraction comparison task. While the literature provides plenty of evidence
that this is the case, we also performed a clustering analysis between items to explore how
items group together in a data-driven manner. For this final, exploratory analysis, and based
on our findings presented so far, we hypothesized that incongruent items would cluster more
First, all items got clustered according to congruency: there is a cluster depicted on the left
side that groups all incongruent items, and a cluster on the right that groups all congruent
items. Second, that after the initial congruent/incongruent split, the next split occurs within
the cluster of congruent items and it separates items according to whether fractions share or
not a common component. Additionally, if one cuts the dendrogram at a height that divides
incongruent items, these items do not split according to the presence of a common
component. This supports the hypothesis that incongruent items tend to behave in a similar
manner, whereas congruent items show different patterns of responses depending on the
presence of common components.
according to its type: with or without common components (cc/ncc), congruent or
incongruent (co/in).
Discussion
In the present study, we reanalyzed a dataset of a fraction comparison task performed by 403
school children. We did so by focusing on the psychometric properties of the test items and
the factorial structure of the task.
Our first research question was how the presence of common components and
congruency affected items’ scores and discrimination indices. When considering all
participants together, item scores revealed large differences between congruent and
incongruent items, with a larger difference for items with a common component. This pattern
suggests that a large number of participants answered the task by using naive component
comparison, implying that congruency plays a key role in their responses. Nonetheless, item
discrimination indices showed a more nuanced picture. While incongruent items displayed
high and similar indices (range .77–.95), congruent items exhibited different patterns
depending on the presence of a common component. Congruent items with a common
component showed positive, moderate discrimination indices (range .19–.38), whereas
congruent items without common components showed null or negative indices (range -.53–
.09). Considering a subsample of students who did not only rely on naive component
comparison for the task, item discrimination indices varied but the distinction between
congruent items without common components and the other item types remained: items of
this specific type showed substantially lower discrimination than the others, and three items
still displayed null-or-negative indices.
On face value, any pair of fractions may be considered a valid representative of the
category “fraction comparison items” and therefore it would be inadequate to discard items
with negative discrimination from such a test. Moreover, all items had exactly the same
structure, presentation, and response options, so it is unlikely that only these specific items
were misunderstood. However, the interpretation of items with negative discrimination
requires a deeper analysis. The Standards for Educational and Psychological Testing (AERA,
APA, & NCME, 2014) presents the study of response processes as a source of validity
evidence. In this case, for all pairs of fractions to be considered as instances of “fraction
comparison items”, it is important to assume that participants’ cognitive processes involved
in answering these items are similar. However, numerous investigations have shown that this
is not the case: students recur to a diversity of strategies to compare fractions (Clarke &
Roche, 2009; Pearn & Stephens, 2004; Stafylidou & Vosniadou, 2004), making “fraction
comparison” an umbrella term with little meaning from a cognitive perspective. Some
researchers have hypothesized that the unusual pattern of responses obtained in fraction
comparison items without common components, where congruent items tend to be answered
less correctly than incongruent items, stems from the use of strategies such as gap thinking
(Clark & Roche, 2009; Pearn & Stephens, 2004). Morales et al. (2020) showed that gap
thinking can lead to mathematically incorrect responses only for congruent items without
common components. In the test analyzed in this paper, we note that the two items displaying
the most negative discrimination indices (i3 and i4) are exactly the items where gap thinking
leads to the mathematically incorrect answer.
Our second research question involved the item structure of the fraction comparison
task. We first approached this question by conducting a CFA and hypothesized a 4-factor
model, based on the test design considering two crossed dimensions (congruency and the
presence/absence of a common component). However, the CFA suggested that a 3-factor
model was better. Our first conclusion was that the two factors comprising incongruent items
were redundant because they exhibited a correlation of .999. This suggests that the cognitive
processes engaged by students in solving items of these two types were practically the same,
regardless of the presence of a common component. While this pattern may be explained by
the use of naive component comparison, a different picture emerges for congruent items: they
show a correlation below .80, much lower than that between incongruent items.
Still, the CFA exhibited convergence problems even for the 3-factor model. We thus
explored other possible sources of this problem, leading us to focus on a subsample of
students who showed a pattern of answers extremely similar to naive component comparison,
being systematically correct in congruent items and incorrect in incongruent ones. We
hypothesized that these students’ behavior was affecting model convergence, and decided to
exclude them from the CFA. We did so by computing a congruency index, and discarded
participants with indices above a given threshold. In Appendix A, we present the CFA results
for models with different values of this threshold. All models failed to converge until
sufficient extreme values of the congruency index were discarded. In this successful model,
which considered 191 students, we confirmed the aforementioned pattern: the factors
corresponding to incongruent items had an extremely high correlation, in contrast to those
corresponding to congruent items.
It is important to note that, while this process of excluding a subset of participants
allowed us to achieve model convergence, it also poses a limitation to our conclusions. From
the perspective of sample size, the final number of students considered (n = 191) is somewhat
low according to usual CFA recommendations, but still acceptable (e.g., Koran, 2016). More
importantly, we note that by excluding the students whose patterns of answers were the most
aligned with naive component comparison, we also altered the population of students that our
results may generalize to. Our CFA results are likely to generalize to populations of students
who use a diversity of strategies and do not rely only on naive component comparison.
Interestingly, in the restricted sample our main conclusions remained similar: congruent items
without common components exhibited lower discrimination indices than items of the other
types, and the factors corresponding to the two incongruent types showed extremely high
correlations, suggesting that they reflect a single underlying factor. As for the exclusion
criterion, we note that it was chosen with a theoretically-based approach rather than a data-
driven one, so it is unlikely that our results are an artifact induced by the sample reduction.
We also analyzed the item structure of the task by conducting a data-driven clustering
analysis at the item level. In contrast to CFA, this analysis used all the data. Items got first
clustered according to congruency, and then split, within congruent items, according to the
presence of a common component. This coincides with the factor structure preferred by the
CFA, where incongruent items grouped together while congruent items did not.
Altogether, we conclude from the CFA and the clustering analysis that incongruent
items tend to behave in a cohesive manner. The case for congruent items is different: these
items group less tightly and split according to whether the fractions share or not a common
component.
Our results contrast with those obtained by Van Hoof et al. (2015), who used a more
general rational number knowledge test. A Rasch model of their data showed that their test
had a unidimensional structure, suggesting that all congruent and incongruent items merged
into a single factor. Nonetheless, they use a broader knowledge test that included not only
fraction comparison but also concepts such as rational number density and the effect of
arithmetic operations. The fraction comparison task we analyze here could be approached by
children by using specific strategies, whereas Van Hoof et al.’s (2015) required a much
deeper understanding.
Apart from the methodological limitations described above, a broader theoretical and
methodological question concerns whether item analysis can yield meaningful conclusions
when the sample under study is known to be a mixture of children using different approaches
to fraction comparison. This is the case with our data, which was first analyzed by Gómez
and Dartnell (2019) to uncover six clusters of children with different patterns of reasoning.
Unfortunately, all but one of these clusters were smaller than 100 children, so CFA cannot be
applied in these cases. Nonetheless, we can still perform the item clustering analysis. We,
therefore, replicated Gómez and Dartnell’s (2019) clustering of children and then performed
item clustering within each participant cluster. In Appendix B, we present the results of
clustering children into six groups for comparison with Gómez and Dartnell (2019). We then
also present the item dendrograms corresponding to each participant cluster. Overall, in five
of the six participant clusters we observed that congruent items behave less cohesively than
incongruent items. This conclusion, however, must be taken with caution because four of
these children clusters grouped less than 35 participants.
All things considered; our results speak about the population of students who do not
systematically use naive component comparison to answer a fraction comparison task. This
could mean that they systematically use a single strategy different from this one, or that they
use a diversity of strategies. In this population, we conclude that some congruent items still
show negative discrimination indices. Therefore, negative discrimination is not a spurious
result due to the presence of large numbers of students using naive component comparison.
However, the limited number of items in the test prevents us from analyzing in more depth
why an item has negative discrimination or not. Further research with school children and a
broader set of items are needed to understand the cognitive processes involved in their
reasoning during fraction comparison.
Acknowledgments. The research leading to these results was supported by funding from the
Chilean Research and Development Agency (ANID), grants Fondecyt 1160188, PIA/Basal
FB0003, and Milenio/NCS2021_014.
Appendix A
subsets of them showing non-extreme congruency indices. Almost all models exhibited
problems in the fitted covariance matrix of latent variables (the matrix failed to be positive
definite). The only model that did not have this problem was the one with 3 factors and
participants with a congruency index of at most +8.
Systematically, the 4-factor models showed extremely high correlations between the
latent variables corresponding to the two types of incongruent items (all rs > .985).
Sample 4 factor statistics 3 factor statistics
All participants SRMR = .070 SRMR = .071
(N = 403) RMSEA = .029 RMSEA = .030
CFI = .999 CFI = .999
TLI = .999 TLI = .999
Mc = .901 Mc = .896
χ² (246, N = 403) = 329.9, p χ² (249, N = 403) = 337.2, p
= .0003 = .0002
χ²/df = 1.34 χ²/df = 1.35
Only participants with SRMR = .095 SRMR = .097
congruency index ≤ +11 RMSEA = .045 RMSEA = .046
(n = 285) CFI = .997 CFI = .997
TLI = .997 TLI = .997
Mc = .780 Mc = .772
χ² (246, N = 285) = 387.0, p χ² (249, N = 285) = 396.0, p
< .0001 < .0001
χ²/df = 1.57 χ²/df = 1.59
Only participants with SRMR = .122 SRMR = .123
congruency index ≤ +10 RMSEA = .066 RMSEA = .066
(n = 235) CFI = .987 CFI = .987
TLI = .986 TLI = .986
Mc = .588 Mc = .580
χ² (246, N = 235) = 494.8, p χ² (249, N = 235) = 503.8, p
< .0001 < .0001
χ²/df = 2.01 χ²/df = 2.02
Only participants with SRMR = .124 SRMR = .126
congruency index ≤ +9 RMSEA = .063 RMSEA = .064
(n = 208) CFI = .980 CFI = .980
TLI = .978 TLI = .977
Mc = .617 Mc = .604
χ² (246, N = 208) = 446.0, p χ² (249, N = 208) = 457.9, p
< .0001 < .0001
χ²/df = 1.81 χ²/df = 1.84
Only participants with SRMR = .131 SRMR = .133
congruency index ≤ +8 RMSEA = .059 RMSEA = .061
(n = 191) CFI = .972 CFI = .970
TLI = .969 TLI = .967
Mc = .652 Mc = .633
χ² (246, N = 191) = 408.7, p χ² (249, N = 191) = 422.6, p
< .0001 < .0001
χ²/df = 1.66 χ²/df = 1.70
Appendix B
(2019) did. Clustering was done using the k-means algorithm with six clusters and ran 1000
times with random starting centers. Cluster centers were not expected to be exactly equal to
those of Gómez and Dartnell (2019) because we only considered the subset of children with
complete data. Nonetheless, there was still a high similarity both in terms of relative cluster
sizes and cluster centers.
For each participant cluster (A-F), we computed its item dendrogram (Fig. 3).
Dendrograms of clusters A and E exhibit a clear first branching based on congruency:
congruent items on one branch and incongruent items on the other. In both cases, we observe
that subsequent branching splits first the subset of congruent items. Item dendrograms for
clusters B and D start by splitting some or all of the congruent items without common
components from the rest. The dendrogram of cluster C separates items in a way that seems
unrelated either to common components or to congruency. Finally, the first split in the
dendrogram of cluster F separates almost all congruent items from the rest, and subsequent
branching further splits the set of congruent items. In summary, five of these six dendrograms
pack incongruent items more tightly than congruent items.
With a common component Without common
components
Cluster n (%) Congruent Incongruent Congruent Incongruent
A 216 (54%) 94% 3% 98% 2%
B 60 (15%) 94% 94% 71% 94%
C 33 (8%) 78% 66% 55% 60%
D 34 (8%) 89% 95% 17% 96%
E 30 (7%) 21% 86% 15% 92%
F 30 (7%) 46% 21% 59% 24%
Note: Cluster names were chosen so that cluster centers matched with those of Gómez and
Dartnell (2019).
A B
C D
E F
cluster (A-F).