Running head: TRANSPARENT REPORTING OF REACTION TIME DATA PRE-
PROCESSING
We Don‚Äôt Know What You Did Last Summer. On the Importance of Transparent
Reporting of Reaction Time Data Pre-processing
1 2,a 3,a 4,a
Hannah Dorothea Loenneker , Erin M. Buchanan , Ana Martinovici , Maximilian A. Primbs ,
5 6 4 7
Mahmoud Medhat Elsherif , Bradley J. Baker , Leonie A. Dudda , Du≈°ica Filipoviƒá ƒêurƒëeviƒá ,
7 4 8 9 10
Ksenija Mi≈°iƒá , Hannah K. Peetz , Jan Philipp R√∂er , Lars Schulze , Lisa Wagner , Julia
11 12,b 13,b,c
Katharina Wolska , Corinna K√ºhrt , & Ekaterina Pronizius
Diagnostics and Cognitive Neuropsychology, T√ºbingen University, T√ºbingen, Germany
Analytics, Harrisburg University of Science and Technology, Harrisburg, USA
Department of Marketing Management, Rotterdam School of Management, Erasmus
University, Rotterdam, Netherlands
Radboud University, Nijmegen, Netherlands
Department of Psychology, University of Birmingham, Birmingham, UK
Temple University, Philadelphia, USA
University of Belgrade, Belgrade, Serbia
Witten/Herdecke University, Witten, Germany
Freie Universit√§t Berlin, Berlin, Germany
Jacobs Center for Productive Youth Development, University of Zurich, Zurich, Switzerland
Manchester Metropolitan University, Manchester, UK
Faculty of Psychology, Technische Universit√§t Dresden, Dresden, Germany
Department of Cognition, Emotion, and Methods in Psychology, Faculty of Psychology,
University of Vienna, Vienna, Austria
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Author note
These authors contributed equally and are listed in alphabetical order.
Shared last authors.
Correspondence concerning this article should be addressed to E. Pronizius, University
of Vienna, Liebiggasse 5, 1010 Vienna, Austria. E-mail: ekaterina.pronizius@univie.ac.at
Funding
CK was partially funded by the German Research Foundation (Deutsche
publication was funded by Open Access Publishing Fund of the University of Vienna. The
funders had no role in study design, data collection and analysis, decision to publish, or
preparation of the manuscript.
Authors information
‚àí Hannah Dorothea Loenneker hannah-dorothea.loenneker@uni-tuebingen.de,
‚àí Mahmoud Medhat Elsherif mahmoud.medhat.elsherif@gmail.com,
‚àí Leonie A. Dudda Leonie.dudda2@ru.nl.
5044-5428.
‚àí Ksenija Mi≈°iƒá ksenija.misic@f.bg.ac.rs.
196X.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Abstract
In behavioral, cognitive, and social sciences, reaction time measures are an important
source of information. However, analyses on reaction time data are affected by researchers‚Äô
analytical choices and the order in which these choices are applied. The results of a systematic
literature review, presented in this paper, revealed that the justification for and order in which
analytical choices are conducted are rarely reported, leading to difficulty in reproducing results
and interpreting mixed findings. To address this methodological shortcoming, we created a
checklist on reporting reaction time pre-processing to make these decisions more explicit,
improve transparency, and thus, promote best practices within the field. The importance of the
pre-processing checklist was additionally supported by an expert consensus survey and a
multiverse analysis. Consequently, we appeal for maximal transparency on all methods applied
and offer a checklist to improve replicability and reproducibility of studies that use reaction time
measures.
Keywords. Checklist, Open Scholarship, pre-processing, Reaction Time, Transparency
Word count: 10,108
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
1. Introduction
Chronometric methods, such as reaction time (RT) measurements, are used as a proxy to
understand cognitive processes underlying behavior (e.g., Baayen & Milin, 2010). In the
behavioral, cognitive, and social sciences, RTs are used to depict the consecutive processes from
stimulus input (e.g., visual or auditory) to any output response (e.g., written, speech, or motor,
Baayen & Milin, 2010). Given factors such as participant inattention or technical malfunction, it
is common among behavioral scientists to pre-process RT data because those affected RTs do not
depict the underlying process of interest. Once the influence of these random and extraneous
processes has been addressed, behavioral scientists believe that only relevant factors remain to
drive the RT pattern, such as the effect of experimental manipulation or group differences. A
wide range of methods has been suggested to reduce the data collected to observations that are
representative of the underlying cognitive process (see the following for methodological
examples: Andr√©, 2022; Baayen & Milin, 2010; Ratcliff, 1993; Tukey, 1962; Van Selst &
Jolicoeur, 1994), but so far there is no consensus on how to report the actions taken to prepare for
statistical analysis. These pre-processing methods lead to different samples of items, participants,
and observations, and correspondingly, different skewness of data distributions, measures of
central tendency, and linear relations between RT and the independent variable [e.g., groups or
conditions; Ulrich and Miller (1994)]. Therefore, pre-processing actions can alter conclusions
from hypothesis testing, as shown in different proportions of significant tests in empirical studies
(e.g., Andr√©, 2022; Mor√≠s Fern√°ndez & Vadillo, 2020) or in increased false positive rates in
simulated data (e.g., Andr√©, 2022; Berger & Kiefer, 2021; Mor√≠s Fern√°ndez & Vadillo, 2020).
Pre-processing not only changes effect sizes, but also the reliability with which constructs are
measured (Parsons, 2022). Therefore, this article provides a checklist on reporting RT pre-
processing actions, informed by discrepancies between a literature review on an exemplary
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
cognitive phenomenon and an expert consensus survey, a multiverse analysis, as well as personal
scientific experiences.
The characteristics of RT data which make pre-processing necessary can be categorized
according to different procedural levels: 1) artefacts outside of the researcher‚Äôs control (e.g.,
technical malfunction, attrition, missing data, Mor√≠s Fern√°ndez & Vadillo, 2020; Woods et al.,
2021; Woods et al., 2023), 2) data points exceeding pre-defined criteria (e.g., based on
psychophysiological considerations, very fast RTs/anticipations and very slow RTs/omissions
cannot depict the process of interest, Luce, 1986; Pain & Hibbs, 2007), and 3) observations
deviating from the empirical individual or group average reaction pattern (e.g., RTs exceeding 2
median absolute deviations from the sample median). Transparency regarding which pre-
processing has been used, in which order, and based on which rationale is utterly important,
because the researchers‚Äô degrees of freedom caused by the many available methods can lead to
different conclusions (Leys et al., 2013), as they result in different samples of included
participants and observations (see below for results from the multiverse analysis conducted in this
project).
Consequently, the effect of pre-processing decisions remaining unnoticed weakens
inferences from the body of scientific evidence. Analytical choices made by one or a few
researchers can be seen as different but equally rational, leading to conflicting empirical results
and therefore, false positives and negatives in the literature (e.g., Andr√©, 2022; Berger & Kiefer,
2021). Therefore, as a first step, awareness needs to be raised for the effect of choosing certain
pre-processing method(s) on the results (see Aguinis et al., 2013; Berger & Kiefer, 2021).
Second, disclosing pre-processing decisions is necessary for reproducing and replicating
published results (Mor√≠s Fern√°ndez & Vadillo, 2020). Reproducing numerical results by using the
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
same data and analysis of the original study is considered a minimum standard for evaluating the
reliability of scientific findings (Nosek et al., 2022; Peng, 2011). Access to the original dataset
used in a study is necessary, yet not sufficient, for assessing reproducibility. Chances of
reproducing all the numerical results reported in a study increase as authors share information
about pre-processing actions, code, information about the software they used, and adopt good
computing practices (Wilson et al., 2014, 2017).
Given the increase in popularity of pre-registrations and registered reports (Christensen et
al., 2020; Hardwicke et al., 2022), scientists have improved their ability to thoroughly relate their
research questions to pre-defined analyses. However, as RT pre-processing takes place prior to
data analysis and hypothesis testing, these decisions need to be reported with the same rigor and
openness. Although there are guidelines on how to deal with outliers and missing data (Aguinis et
al., 2013; Ratcliff, 1993; Woods et al., 2021; Woods et al., 2023), the field of psychological
science lacks consensus on how to report the pre-processing of RT data. As a result, guidelines
providing best practice examples of the latter are needed to further increase transparency.
Therefore, we quantified the frequency of reporting pre-processing actions and assessed their
completeness and transparency in the literature by conducting a systematic literature review on an
exemplary cognitive phenomenon (the Simon effect), which is summarized in the following
section.
2. Systematic Literature Search on Pre-processing Reports
To get an overview of how pre-processing is generally reported, we conducted a
systematic literature review on the Simon effect (i.e., a cognitive phenomenon stemming from the
difference between congruent and incongruent trials, Craft & Simon, 1970). For details on
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
methods of the literature search and a meta-analysis see Supplementary Materials A. The
Simon effect has been chosen as an example because it has been repeatedly replicated since its
introduction in 1970 (Cesp√≥n et al., 2020; Craft & Simon, 1970) and is representative of the
classical behavioral experiment including a within-subject factor with two levels (comparing
congruent and incongruent trials). The authors are not aware of a study investigating the
influence of different pre-processing pipelines on effect size estimates in the Simon task, but
there is at least one study (Mor√≠s Fern√°ndez & Vadillo, 2020) showing it for the Stroop task.
Mor√≠s Fern√°ndez and Vadillo (2020) showed that as the set of pre-processing pipelines grew, the
proportion of false positives also increased successively. It is important to note that only when a
single pre-processing pipeline was applied, the proportion of false positives equaled to the Œ±-level
of the t-test, 0.05. There were no specific pre-processing pipelines that had the strongest
influence, and it was shown that it is not even necessary to conduct all possible pre-processing
pipelines every time: Simply considering them is already sufficient to increase the false-positive
rate. The Simon task shares conceptual similarities with the Stroop task, and the Simon effect is
we found a large variety in the number of RT pre-processing actions that were reported. Only five
of 55 articles explicitly stated the order in which the pre-processing actions were applied, 32
articles indicated the rationale for the basis for choosing the respective method(s) applied, 16
reported how many or if any participants were excluded, and 21 documented how many data
points were excluded through these actions.
Looking at the six articles not reporting any RT pre-processing, it remains unclear whether
the authors did not mention data pre-processing they performed or whether they did not conduct
any pre-processing. It is self-evident that this literature search does not allow us to make claims
on discrepancies between pre-processing actions being conducted and pre-processing actions
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
being reported, as we cannot back-trace what has been done with the data in these studies
retrospectively. However, observing that we cannot infer if any or which pre-processing actions
have been applied to the data is an alarming fact for the endeavor of reproducible and robust
research, which is why this article proceeds with the development of a checklist supporting
complete reporting of pre-processing actions.
literature search on the Simon effect. Note: Pre-processing actions include exclusion of error
trials, exclusion of participants, handling of missing data, exclusion of fillers, exclusion of trials,
outlier trimming with a fixed minimum, outlier trimming with a fixed maximum, relative outlier
trimming based on measures of central tendency and variability, and data transformation. The
number of reported actions per article can range from 0 to 9.
3. Development of a Checklist
Based on our literature review on the exemplary Simon effect, we observed that
researchers rarely report the complete set of pre-processing actions for RTs. We assume that
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
these results cannot be interpreted as an exception, but rather as a case study representative of the
field of behavioral sciences. To address this shortcoming, the present paper proposes a checklist
to facilitate the accurate and complete reporting of pre-processing choices for the analysis of RTs.
literature (Kerr et al., 2017; Primbs, Holland, et al., 2022) and our own literature search (see
expert-consensus survey confirming the importance and applicability of our suggested pre-
processing checklist. We provide readers with an easy-to-follow checklist on which data pre-
2). Last, we demonstrate the necessity of a reporting checklist by using multiverse analysis to
illustrate the effects of the pre-processing actions on the standardized and raw results of the
Simon Task.
Action Level Reason Examples
Research assistants collected data from participants
who do not match participation criteria (e.g., age,
gender, visual acuity).
External
events Participant reported headache and being unable to
concentrate.
Participant
Fire alarm went off. Building was evacuated.
Minimum number of correct trials for a participant to
Data exclusion
Outlier: Fixed
be included.
Mean accuracy was assessed in each group and those
Outlier: Data-
participants performing two standard deviations below
dependent
the group mean were excluded.
External Sticky keys were activated for the first few trials. The
events participant proceeded normally after deactivation.
Trial
Trials faster than 100ms were excluded because it is
Outlier: Fixed
unlikely that participant indeed processed the stimulus
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Action Level Reason Examples
that quickly.
Outlier: Data- Abnormally slow trials are likely to reflect distraction
dependent on part of the participant and are thus excluded.
External There was a mistake in the processing of a stimulus.
events By accident, a stimulus was not shown often enough.
Stimuli which were not correctly recognised above
Stimuli Outlier: Fixed
chance level were excluded.
Outlier: Data- Stimuli which were recognised considerably worse
dependent than other stimuli were excluded.
Log-transformation was employed to make the data
approach the normal distribution.
Data
transformation
Latency-normalisation was employed to account for
between-subject variability in overall reaction times.
D-scores were calculated from the raw IAT data in line
Other scoring
with established procedures (Greenwald et al., 2003).
Data Analyses were conducted on the trial-level data / on the
aggregation participant means.
Note: Order - Outliers were removed before data transformation took place.
4. Expert-consensus Survey
4.1. Method
4.1.1. Participants.
The study was approved by the Institutional Review Board at Harrisburg University of
Science and Technology, USA (20221206). Participants were recruited through social media,
internal listservs (e.g., faculty newsletters), and personal contacts of the researchers. 141 novel
observations were collected through Qualtrics, and 66 responses were analyzed after excluding
participants who did not explicitly consent to the study (n = 2), who did not provide any
information about their use of response time data for research (n = 54), who indicated having no
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
experience with response time data (n = 1), incomplete responses (n = 15), and observations
corresponding to survey previews (n = 3). In order to reduce risks of non-random drop-out (i.e.,
participants concerned about being identified based on their demographic information being more
likely to drop out of the survey), the questions on year of birth and gender were optional. Of 66
participants, 10 did not enter their year of birth and two provided implausible values (the years
1900 and 2020). The remaining participants have an average age of M = 36.20 years (SD = 8.45).
Fifty-eight participants provided information about their gender: 28 selected ‚Äúwoman‚Äù, 26 ‚Äúman‚Äù,
two ‚Äúnon-binary‚Äù, and two ‚Äúprefer not to disclose‚Äù. Participants indicated they completed or were
completing their education in Western Europe (37.9%), Northern America (21.2%), Northern
Europe (10.6%), Western Asia (7.6%), Southern Europe (6.1%), Eastern Europe (4.6%), Latin
America and the Caribbean (3.0%), and other or multiple regions (6.1%).
Materials and Procedure. The survey in text and Qualtrics import format can be found on
consenting to complete the study, participants were shown three main study sections:
demographics, the proposed pre-processing checklist, and a final thoughts section.
Demographics. Participants were asked to explain the types of research they performed
that used RTs, and this information was used to screen participants for the appropriate sample of
researchers using RT data. Participants then indicated their geographical region where they were
completing or completed their higher education, using the 17 United Nation Subregions
classification system ‚ÄúUnited Nations geoscheme‚Äù (2022). In an open text box, participants
indicated the subdiscipline that characterizes their research, followed by indicating their current
role with options (e.g., students, lecturers, professors). Two software questions were included:
participants listed all software they used to measure RTs and analyze RT data. Next, they were
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
asked to indicate the number of years (response options: <1, 1-3, 4-6, 7-9, and 10+) that they
were involved in collecting RT data, analyzing RT data, open science/scholarship, experiment
coding (i.e., writing code to collect RTs), and analysis coding (i.e., writing code to analyze data)
respectively. At the end of the survey, they were asked to indicate their gender and year of birth
for reporting purposes only.
4.1.2. Procedure Checklist Creation.
After answering the demographics questions, participants were asked to think of a project
that used RT data and write down how they would process the data in preparation for analysis.
For the exact wording, see Supplementary Materials B, section ‚ÄúOpen Response Times‚Äù.
Next, participants answered a series of closed and open-ended questions on data
exclusions, data transformations, data processing order, and reproducibility likelihood. For data
exclusions, participants indicated how often they used each of three criteria to eliminate
observations at each of three levels, how often they reported doing so, and how often they
reported the exact number of observations excluded (never, sometimes, about half the time, most
of the time, always). The three exclusion criteria refer to: (1) events outside the researchers‚Äô
control, (2) fixed criteria (i.e., thresholds that are independent from observations in the sample),
and (3) data-dependent criteria (i.e., thresholds relative to observations in the sample). The three
levels refer to: participant, trial, and stimulus. For each data exclusion criterion, participants
indicated how important they thought its usage to be for accuracy in analyses and interpretation
(5-point between not at all important and extremely important). After answering questions about
how often they use data transformations, report transformations, and report the order of pre-
processing actions, participants were asked to indicate from 0 to 100 how likely it would be for
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
another researcher to reproduce their analyses. The participants could also indicate reasons for
(not) reporting pre-processing actions in their manuscripts.
Participants were then shown the proposed checklist in the form available at the time (see
proposed items to ensure feedback on potential areas missed by the proposed checklist. The list
of pre-processing actions and the corresponding checklist were initially drafted by M. P. based on
previous surveys of the literature (Kerr et al., 2017; Primbs, Holland, et al., 2022) and the present
was improved based on survey results and suggestions from the review team.
Section What to report? Examples and suggestions
"The reporting order reflects the data pre-processing order.‚Äù
or
"Only trials followed by a correct response were
incorporated in the reaction time (RT) analyses (‚Ä¶) (please
note that error rates were arcsine transformed prior to the
analysis to approximate normal distribution). (...)
Subsequently, possible decade as well as five break effects
were computed as follows for each participant individually:
Order first, the logarithm (ln) of the averaged response latencies
per experimental number pair (both orders collapsed; e.g.
3:5 and 5_3) was calculated. Then, a logarithmic function
General (‚Ä¶) was fitted to these individual data. (‚Ä¶) Afterwards, the
(‚Ä¶) the residuals were computed by subtracting the
predicted values from the actual logarithm of the RT.
Finally, these residuals were standardized to a mean of 0
and a standard deviation (SD) of 1 (‚Ä¶)." (Domahs et al.,
2010)
Transparency (e.g.,
‚ÄúThe deviation between planned and executed pre-
reporting discrepancies
processing actions has been addressed in section X.‚Äù or
between pre-registered and
"Our confirmatory analyses do not deviate from the pre-
actually used pre-
registered procedure. All datasets and the analysis code are
processing pathways)
available for download on the OSF." (Primbs, Holland et al.,
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Section What to report? Examples and suggestions
2023)
‚ÄúTheoretical or empirical justification for chosen pre-
processing actions has been provided in the respective
Theoretical or empirical
sections of the present manuscript.‚Äù or
justification for chosen
"RTs ‚â§ 100 ms we removed as they reflect implausible
pre-processing actions
cognitive processing of the Go signal (Gabay & Behrmann,
2014 as cited in De Pretto et al., 2021)."
"A group of 16 participants (five women, 11 men, 18‚Äì50
Total number of years of age) participated in this experiment. They all had a
participants collected normal (or corrected-to-normal) vision and gave their
informed consent." (Burle et al., 2014)
"After application of our pre-registered exclusion criteria, a
final sample size of 155 participants remained. Please note
that most excluded participants (n = 102) did not actually
Total number of
complete the experiment ‚Äì they failed the attention check
participants excluded per
presented during the instructions and were directly
reason for exclusion
forwarded to the end of the experiment, skipping all
Participants (participant-level data
experimental trials. The other participants were removed
exclusion)
because they were too slow (3SD from the mean reaction
time; n = 3) or made too many mistakes (n = 2)." (Primbs,
Rinck, et al., 2022)
"As pre-registered, we recruited 100 participants who
fulfilled our inclusion criteria (at least 18 years old, fluent in
Total number of
English) and completed the online study via the research
participants (per condition)
platform Prolific. Of those, 9 fulfilled our pre-registered
included in final analysis
exclusion criteria (...), leaving the data of 91 participants to
be analyzed." (Rinck et al., 2022)
"Eight models (...) were selected from the Radboud Faces
Database on the basis of how well their emotional
expressions were recognized in a validation study (RaFD;
Langner et al., in press). (...) Subsequently, pictures of the
three emotional expressions central to this experiment were
Number of trials (per
selected per model, namely happy, sad, and angry. This
condition, per participant)
resulted in a total of 24 pictures: three expressions x two
ethnicities x four targets (models) per ethnicity. (...) Each
Materials
experimental block consisted of sixteen pictures randomly
displayed five times, resulting in 80 trials per experimental
block." (Bijlstra et al., 2010)
Stimuli-level data "Criteria for item selection were high discriminatory power,
exclusion: how many high convergent validity with openness for experience, as
stimuli were excluded for well as content validity, based on expert judgment." (Mussel
which reason? et al., 2011)
Proportion of trials "For correct RTs, a mean and standard deviation were
Analysis
included in final analysis / calculated for each subject within each SOA and session,
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Section What to report? Examples and suggestions
proportion of trials and any RT greater than 3 SDs above or below the mean for
excluded for a particular that subject during that SOA and session was identified as
reason an outlier. This eliminated 1.7 % of both the lexical decision
and naming RTs." (Hutchison et al., 2013)
"For the minimum threshold, we varied the response time
cut-off from 0ms to 300ms in steps of 50ms, resulting in 7
levels. For the data-based outlier trimming method we
Type of trial-level data
varied the number of median absolute deviations from the
exclusion
median (Leys et al., 2013) from 1 to 3 in steps of 0.5 or
applied no data-based trimming, resulting in 6 levels."
(Primbs, Rinck, et al., 2022)
"Separated repeated measures analyses of variance
(rmANOVAs) were conducted to investigate one-session
Data transformation and two-week training effects on median RTs, arcsine-
square-root-transformed error rates, and inverse efficiency
scores." (Soltanlou et al., 2018)
"Most often the mean RT within each trial type is calculated
Data aggregation (...). Researchers may opt to use the median RT instead. I
included both options." (Parsons, 2022)
"Logarithmic fitting was chosen first because of evidence
for a logarithmically compressed quantity representation."
(Domahs et al., 2010) or
‚ÄúUsing the mean or the median as central tendency statistics
alone may conduce to biases and increase the risk of falsely
rejecting null hypotheses (Mor√≠s Fern√°ndez & Vadillo,
Reason for choosing a
2020; Rousselet & Wilcox, 2020). (...) Among other
specific method
alternative approaches (Ging-Jehli et al., 2021), using a
theoretical distribution to describe and compare the shapes
of different RT distributions has been proposed (Castellanos
et al., 2006; Van Zandt, 2000). The most widely used
theoretical distribution in ADHD research is the ex-
Gaussian distribution.‚Äù (Bella-Fern√°ndez et al., 2023)
4.2. Results
The average number of years of experience performing research that uses RT data was 11
of direct involvement in collecting RT, analyzing RT, open science, experiment coding, and
reported extensive experience with various aspects of an RT study. Specifically, more than half
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
of participants have at least four years of experience setting up experiments, and collecting and
analyzing RT data.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
and analysis coding for RT data, and open science practices. Note: NA responses indicate missing
data.
4.2.1. Checklist Item 1: Exclusions Due to Events Beyond the Researcher‚Äôs Control.
RT studies involve the use of hardware and software that present stimuli to participants
and that record the time it takes for them to perform an action (e.g., press a button to indicate
their response). Hardware or software malfunctions therefore impact the collection of RT data.
For example, the response button can get jammed and, as a result, the recorded RT is longer than
normal, or the RT cannot be recorded at all because the button press is not detected. When such
an event happens during a data collection session, this issue determines how many trials within
that session are impacted. When visual stimuli are presented to participants via a web interface,
their loading time is impacted by the size of the image files (i.e., larger files take longer to be
displayed on screen).
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
While there are various ways of preventing such events, if they do happen, then
researchers can choose to exclude all observations of impacted participants, only those
observations in trials where stimuli took longer to load, or all observations where stimuli with
longer loading times were shown. While most respondents excluded participants due to events
that there are several survey participants who report always excluding observations (participants,
stimuli, or trials) due to events beyond their control, which shows that such events happen often
enough. Of those survey participants who excluded observations at least sometimes, the large
majority indicate that they always report doing so in the paper and that they also include the exact
number of excluded observations.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
participants, stimuli, and trials due to events beyond the researcher‚Äôs control. Note: Responses in
‚ÄòReport excluding‚Äô and ‚ÄòReport the exact‚Ä¶‚Äô are from survey participants who reported excluding
observations at least sometimes.
4.2.2. Checklist Item 2: Exclusions Based on Fixed Criteria.
For certain types of RT studies, it is possible for researchers to set exclusion criteria ahead
of data collection, based on the literature and results in previous studies. For example, researchers
can set a minimum RT based on expectations about processing time for stimuli used in the study,
with the underlying assumption being that responses with an RT below this threshold are either
made by accident (e.g., the participant pressed the button by mistake) or by participants who are
not sufficiently engaged in the task (i.e., participants who are bored or impatient). Similar to
responses for the first checklist item, most respondents used such criteria to exclude participants,
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
stimuli, and trials due to fixed criteria. Note: Responses in ‚ÄòReport excluding‚Äô and ‚ÄòReport the
exact‚Ä¶‚Äô are from survey participants who reported excluding observations at least sometimes.
4.2.3. Checklist Item 3: Exclusions Based on Relative Criteria.
Relative criteria such as the mean and standard deviation or the median and median
absolute deviation were used more frequently to exclude participants, rather than trials or stimuli
they never report doing so. Any such exclusions should be reported in detail if they happen, as
that allows the reader to evaluate how appropriate this action was and how likely it was that it
would impact the validity of the results and conclusions presented in the paper.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
stimuli, and trials due to relative criteria. Note: Responses in ‚ÄòReport excluding‚Äô and ‚ÄòReport the
exact‚Ä¶‚Äô are from survey participants who reported excluding observations at least sometimes.
One reason why researchers might not always report excluding observations is that about
one third of participants who completed our survey perceive those exclusions to be ‚Äònot at all‚Äô or
as well as exclusions based on fixed and relative criteria were deemed equally important for
accuracy in analyses and interpretation. However, one might argue that outliers resulting from
external events such as a crash of the recording system should be classified as a missing
completely at random mechanism, while the question whether an observation exceeds a fixed or
relative cut-off is less trivial, because we cannot truly determine whether the latter stem from a
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
different data generating process than the one targeted with the experimental manipulation.
Perhaps the sample undertaking the survey was very aware of dangers resulting from non-
transparency, which led to participants classifying more outlier exclusion procedures as critical
than other researchers would.
action is for accuracy in analyses and interpretation?‚Äô Note: Each panel represents one of the data
exclusion checklist items.
4.2.4. Checklist Item 4: Data Transformations, Scoring, and Aggregation.
Besides data exclusions, an important aspect to report is the transformation, scoring, or
other aggregation of the data. RT data is often modelled or analyzed with a log, or inverse
transform applied to the responses, as RT data is known to have skewed distribution. Further,
data may be aggregated by creating by-participant or by-item averages for conditions or groups in
the study if multiple trials are used. Last, RT data may be further computed into a study specific
scoring, such as difference scores between conditions in a priming or Simon task study. Each of
these transformations can potentially impact the final results of a study and should be described
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
one of these methods at least sometimes, half the time, most of the time, or always in their
analyses, showing that these practices are common. Nearly all survey participants indicated that
they always report these data transformations, while a few participants indicated they only
transformations were moderately to extremely important for accuracy in analyses and
interpretation.
from survey participants who use data transformations at least Sometimes.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
order are for accuracy in analyses and interpretation‚Äô.
4.2.5. Checklist Item 5: Data Processing Order.
The last checklist item involves ensuring the data processing order is explicitly reported in
the manuscript. Exclusion of outlier participants or trials could be based on raw scale data or
potentially transformed data, and reporting the actions used to reach the final RT data allows for
this information is very or extremely important for analysis interpretation. This result was in stark
contrast with the results from the survey of the literature, which indicated that most manuscripts
do not report sufficient detail to understand the data processing order, as it could only be guessed
from the order of listing actions which does not allow for a robust reproduction (see
Supplementary Materials A).
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
4.2.6. Reproducibility.
reproducing the analyses if a researcher used the proposed checklist (M = 83.24, SD = 14.86).
However, this estimate of reproducibility, which was elicited towards the end of the survey, still
shows substantial variability. One potential explanation is that among survey respondents who
indicated that they exclude observations at least sometimes there is variability in how often they
report the exact number of excluded observations - which is important information when trying to
reproduce the study results using the original data. We find a positive association between the
stated likelihood of reproducing analyses and the frequency of reporting the exact number of
excluded observations (i.e., Pearson‚Äôs correlation of .31, p-value = .02). The relatively high
expectations regarding the likelihood of reproducibility could be biased by the sample's belief
that reporting should have positive effects. Alternatively, it may be due to their firsthand
experiences with the benefits of thorough reporting, given the sample's substantial experience
with open science practices (4-6 years and beyond). Additionally, the sample generally reported
working with coding and/or open-source software to collect RT: e-Prime (10.2%), PsychoPy
(10.2%), Gorilla (3.3%), jsPsych (3.3%), and others (participants could list multiple responses,
and percentages represent total reported out of total options listed). For data analyses, survey
participants listed using R (21.9%), SPSS (7.7%), and JASP (4.4%) most frequently, which allow
researchers to share their analysis code to facilitate reproducibility of results.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
think another researcher could reproduce their analyses.
4.2.7. Attention Check.
Due to technical issues, two attention survey check items were not presented to the
participants:
‚Ä¢ Please mark sometimes for this item.
‚Ä¢ Please mark most of the time for this item.
Therefore, we are unable to provide estimates regarding the participants‚Äô level of
attentiveness. However, we can assume that all included participants responded appropriately,
considering that they answered all the items, including the free text items, and open-ended
responses were checked for appropriateness.
4.2.8. Final Thoughts.
We used the feedback from the expert survey and the commentary from the community
(e.g., reviewers‚Äô recommendations, checklist presentation at Society for the Improvement of
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Psychological Science (SIPS) 2022 and 2023 conferences) to further improve our checklist. The
expert survey responses in full (corrected for spelling errors) can be found in the Supplementary
Materials C. First, it was drawn to our attention that our survey was missing an ‚ÄúNA‚Äù answer,
‚Äúforcing‚Äù some participants to choose ‚Äúnever exclude‚Äù as a response. In fact, some participants
may have never encountered situations where they needed to exclude observations (e.g., due to
events beyond their control). As a consequence, we added an NA option to our checklist. Second,
based on the provided feedback, we added three general categories to the checklist, which are not
section-specific: ‚Äúorder of the pre-processing actions‚Äù, ‚Äútransparency about planned and executed
pre-processing actions‚Äù, and ‚Äútheoretical or empirical justification for chosen pre-processing
actions‚Äù. For each of the three general categories, we provided a hypothetical example, such as
‚Ä¢ ‚ÄúThe reporting order reflects the data pre-processing order‚Äù,
‚Ä¢ ‚ÄúThe deviation between planned and executed pre-processing actions has been addressed
in section X‚Äù, and
‚Ä¢ ‚ÄúTheoretical or empirical justification for chosen pre-processing actions has been
provided in section Y‚Äù.
4.3. Discussion
Comparing our findings from the expert-consensus survey to those from the systematic
literature search shows a contradiction between survey respondents claiming to frequently report
all pre-processing actions and the number of excluded observations, and the apparent lack of
reporting in the papers on the Simon effect that we have reviewed. A generous explanation of this
contradiction could be that research practices and journal guidelines have significantly changed
in the past couple of years in the direction of complete and transparent reporting of RT pre-
processing. The results of our literature review provide very little, if any, supporting evidence for
this explanation. Of the 13 papers published between 2017 and 2022, only 6 report the exact
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
number of excluded observations for various criteria. Further, only 5 of all reviewed articles
explicitly reported the order of the pre-processing actions and these were published in 1995,
2005, 2017, 2018, and 2020. Even if journal guidelines were to require transparent reporting, we
argue that there are important benefits to be gained from having a set of reporting guidelines that
researchers can follow. Another reason might be the sample of researchers answering the survey
and of those who conducted the original experiments included in the literature search. First of all,
there might be differences in the levels of experience with RT pre-processing, as well as
conventions in different working groups. Second, the sample of survey respondents might be
selective in the sense that they voluntarily participated in the survey because they regard
transparent reporting of RT pre-processing as an important topic.
5. Multiverse Analysis of Pre-processing Actions in the Simon Effect
transparency and reproducibility; however, we also demonstrate how each of the pre-processing
actions may influence the results. We performed a multiverse analysis (Steegen et al., 2016) on
available data from the Simon effect (Zwaan et al., 2018) using the checklist as a guide for the
choice of pathways. Multiverse analyses allow a researcher to examine the influence of pre-
processing, aggregation, and other choices on the results of the study, often to determine the
robustness or sensitivity of a specific result. In our analysis, we examine how the application of
the steps reported in our guidelines, as well as the order in which they are applied influence the
size of calculated effect, power, and choice of raw or standardized effect size.
5.1. Method
5.1.1. Data.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
In this study, participants were recruited to complete classic cognitive psychology tasks including
a replication of Craft and Simon (1970)‚Äôs task examining spatial compatibility. Participants
completed two waves of the study to examine the influence of repeated participation and previous
knowledge of the experiment on final results. In our multiverse analysis, we used the first wave
of the data collection to most closely mimic the original Simon task. Participants were shown
stimuli (i.e., red square, blue square, yellow circle, green circle) on the left or right side of the
screen and asked to respond to the presentation as quickly and accurately as possible. Each
stimulus was tied to a specific response using the left or right side of the keyboard. Stimuli were
classified as congruent (e.g., stimulus response matched the side of the screen presented - press
the left key and appeared on the left side of the screen) or incongruent (e.g., stimulus response
was the opposite side of the stimulus presentation). The Simon effect occurs when the congruent
responses are faster than the incongruent responses. We used the same stimulus condition (i.e.,
they saw the same stimuli for waves 1 and 2) for multiverse analysis which originally resulted in
an effect size of ùëë = 1.30 . Each participant completed 92 trials for analysis, ùëõ = 46 each in
ùëç ùë°ùëüùëñùëéùëôùë†
the congruent and incongruent conditions.
5.1.2. Pathways.
from Zwaan et al. (2018). Tables 3 and 4 summarize the different pathways.
While the data online indicates it is the raw data, the original sample size tested was larger (total N across
conditions = 172). The published sample was then reduced to N = 160 to create counterbalanced groups across all
study conditions using the following rules: 1) participants with less than 80% accuracy were excluded, 2) participants
with less than 10 percent accuracy in memory tasks were excluded, 3) participants with mean reaction times greater
than the mean plus 3 standard deviations for their group were excluded, and 4) the last participants to complete the
study were excluded to create equally balanced groups after exclusions 1-3 were applied.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Using this example data, stimuli exclusions were unlikely, but these exclusions were regarded as
reasonable examples one might choose for their own data. All other rules were generated based
on the data or other choices that a researcher might make for reaction time data.
Number of Exclusions. Pathways were generated using none of the above exclusions,
only one exclusion, two exclusions, three exclusions, and four exclusions. We used up to four
exclusions as this value represents the most that researchers generally mention in their results
(see expert survey results), four exclusions allowed us to examine the impact of order on the
results, and using up to four exclusions, but not up to nine exclusions, kept the number of results
to 3610.
Order. The orders were created by analyzing every combination of possible order that did
not repeat a specific data exclusion. Therefore, no exclusion was used twice in one pathway (e.g.,
1-2-1 was not allowed); however, different orders of different exclusions could be used (e.g., 1-2-
3, 3-2-1, 1-3-2 were all allowed).
Data Transformations. Analyses were conducted on both raw and log-transformed
reaction times. Note that the transformation always occurred after the other data exclusions.
Data Aggregation. Data were aggregated based on the two analysis pathways chosen for
this multiverse analysis. For t-test analysis, data were first averaged by participant and condition,
then averaged by condition during the calculation of paired samples t-tests. For multilevel model
analyses, data were not aggregated. The nlme (Pinheiro et al., 2022) package was used to analyze
the model using the condition (i.e., congruent, incongruent) to predict raw or log-transformed
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
reaction times. The random-intercept of participant was included to control for correlated error of
repeated measurement of the same person.
Standardized Effect Size. One issue with reporting in repeated measures analyses is the
lack of transparency indicating which effect size calculation was used. Given the raw data, we
were able to determine the original study calculated standardized effect size using:
ùëë =
ùëÜùê∑
where ùëÄ represents the mean of the difference scores between incongruent and
congruent conditions, and ùëÜùê∑ represents the standard deviation of the difference scores. This
effect size tends to be upwardly biased due to the reduction in variance by subtracting conditions
(Dunlap et al., 1996; Lakens, 2013), and therefore, we also present results using ùëë (Cumming,
ùëéùë£
2012) which is calculated by:
ùëÄ ‚àí ùëÄ
1 2
ùëë =
ùëéùë£
ùëÜùê∑ + ùëÜùê∑
1 2
where M and SD represents the mean/standard deviation from each condition. Note that
standardized effect size calculations only occur on aggregated data.
Raw Score Effect Size. The mean difference between congruent and incongruent
conditions was used for the raw score effect size for t-test analysis aggregated data. The b
coefficient from the multilevel model analysis was used to calculate the raw score effect size on
the non-aggregated data. We included these two effect size pathways, even though not necessary
in our reporting checklist, to show if the choice of other suggested reporting guidelines
differentially impacts what a researcher might present as the final ‚Äúresult‚Äù in the study.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Manipulated
Options Criteria
path
To mimic data loss from random participant events or
Participant external
collecting data from the incorrect population, we randomly
events
removed 3% of participants.
Participants were excluded if they did not have 80 of the 92
Participant outlier:
total possible trials. The trial minimum included both correct
Fixed
and incorrectly answered trials.
Participants were excluded if they did not achieve at least 85%
Participant outlier: accuracy on only included trials, and this criterion was selected
Data by examining a histogram to find the break in accuracy
between high and low performers.
We randomly removed 3% of trials to mimic computer issues
Trial external events
or other events that might affect trials individually.
Exclusion Trials were excluded that did not meet a minimum reaction
Trial outlier: Fixed
time of 250 ms.
Trials were excluded that were longer than the overall sample
Trial outlier: Data mean reaction time plus two times the standard deviation of all
reaction times.
Given the small number of stimuli each participant
Stimuli external encountered (i.e., 2 per participant, 4 overall), we randomly
events excluded 3% of trials to mimic random removal due to external
events.
Stimuli that do not achieve at least 50% correct will be
Stimuli outlier: Fixed
excluded, as this value represents chance performance.
Stimuli outlier: Data Stimuli that do not achieve at least the overall mean accuracy
- deviations for stimuli plus two standard devaitions of stimuli accuracy.
Number of
0, 1, 2, 3, 4 options No exclusions are applied, one exclusion applied, etc.
exclusions
All combinations
Order of Participant external-participant outlier, Participant outlier-
with no repeating
exclusions participant external, etc.
exclusions
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Manipulated path Options Performed actions
Data No transformation or
Log transformation.
transformation transformed reaction times
Aggregated or no Data were first averaged by participant, then averaged
Data aggregation
aggregation by condition when aggregated.
Standardized Effect size with standard deviation of the differences
Aggregated data
effect size (dz), effect size with average standard deviation (dav).
Mean difference between congruent versus
Raw score effect Aggregated data or non-
incongruent, congruent versus incongruent coefficient
size aggregated data
from multilevel model.
5.2. Exploratory Questions
1) Does the order of data exclusions impact the results of the analysis?
To answer this question, we will present all results for the two calculations of standardized
effect sizes (Question 3) and raw effect sizes (Question 4). If order of processing is not important,
we would expect to find the same effect size for processing orders that included the same applied
exclusions.
2) How do differences in exclusions applied and analysis choice influence power?
The original effect size found for this study (i.e., ùëë = 1.30) indicates that likely all effects
will be significant using ùõº < .05. Therefore, we will comment on the effect of exclusions on
power by displaying the overall sample size (for aggregated data and t-tests) and overall trials
included (for raw data and multilevel models) to denote differences in change in sample size used
to calculate degrees of freedom for the statistical test.
3) What is the impact of the data exclusions, transformations, and choice of effect size on
the standardized effect sizes?
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
In this question, we will compare the standardized effect sizes ùëë and ùëë for the impact
ùëç ùëéùë£
of data exclusion and transformation choice on the final reported effect size. We expected that ùëë
would be upwardly biased in comparison to ùëë , but it was unclear how processing and
ùëéùë£
transformation would impact these effects.
4) What is the impact of the data exclusions, transformations, and analysis choice on the
raw effect size?
For the last question, we will visualize raw effect size (incongruent - congruent) by
exclusions and transformation for each analysis type to determine the impact of each on the final
raw score effect sizes in the same manner as Question 3.
5.3. Results
5.3.1. Question 1 - Order of Exclusions.
For this question, we first excluded analysis pathways that did not use exclusions or only
used one exclusion. We then matched pathways that included the same exclusions with different
orders (total pathways = 3610; total matches = 246). Within each matching set of exclusions, the
results of the effects were subtracted (i.e., exclusion 1 order 1 minus exclusion 1 order 2,
separately for standardized and unstandardized effects), and the absolute value was taken. This
displays the results. If the order of processing combinations did not affect the results, the results
should show zero differences for combinations. Standardized effect sizes indicated that the
difference in processing order could be up to ùëë = 0.20 or ùëë = 0.14 (ùëÄ = 0.02, ùëÜùê∑ = 0.02;
ùëç ùëéùë£ ùëë ùëë
ùëç ùëç
ùëÄ = 0.01, ùëÜùê∑ = 0.01). For unstandardized effect sizes, the log RT could be up to 0.01 (i.e.,
ùëë ùëë
ùëéùë£ ùëéùë£
1.01 ms) while raw reaction times could be 4.49 ms different (ùëÄ = 0.00, ùëÜùê∑ = 0.00; ùëÄ =
ùëôùëúùëî ùëôùëúùëî ùëüùëéùë§
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
0.55, ùëÜùê∑ = 0.52). Given these results are not zero, the order of pre-processing actions appears
ùëüùëéùë§
to influence the final results.
by examining the difference scores for the same exclusion criteria in different orders. These
difference scores are arranged from smallest to largest to demonstrate a sensitivity plot of the
effect of order. If order did not affect the results, all dots will align at zero. However, we find that
different orders create different effect sizes, and thus, the difference scores are not zero. The first
row represents standardized effect sizes, and the second row represents unstandardized effect
sizes. The left side represents a log-score transformation on RT, and the right side indicates no
transformation of RT. In the first row, the black dots indicate ùëë effect sizes and gray dots
indicate ùëë effect sizes. In the second row, the black dots indicate mean differences calculated
ùëéùë£
from t-tests, and the gray dots indicate mean differences calculated from multilevel models.
We found the same pattern of effect size differences across standardized and raw score
effect sizes, wherein the order of processing can create non-zero differences between different
particular exclusion was the reason for these differences. For example, if the exclusion of poorly
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
performing participants was the main improvement in effect size ‚Äî because of reduced error and
noise in performance ‚Äî we might expect to see this exclusion have a higher standardized effect
largest differences appear to occur with more processing actions, as there are more opportunities
to remove participant/trial/stimuli outliers. However, within each number of processing actions,
we do not seem to find a consistent pattern of one exclusion that creates the largest or least
differences. Therefore, the order of processing does change results, but it cannot be necessarily
attributed to a single exclusion type suggested in our checklist.
Each panel represents the number of exclusions applied on the data before effect size calculation.
P = Participant, T = Trial, S = Stimuli.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
5.3.2. Question 2 - Power.
All statistical tests indicated a significant non-zero effect using ùõº < .05. The number of
participants and trials can/did decrease with increasing number of exclusions; thus, resulting in
lower power via degrees of freedom when more exclusions were applied. However, exclusions
are often applied to reduce the amount of error and noise in the study, and as shown above, these
inclusions may increase power/effect size when applied due to the reduction in the error term.
With this in mind we will further investigate the impact of pre-processing steps on the power of
the study. Therefore, we demonstrate the differences in sample size in aggregated analyses (t-test)
graphs, we demonstrate the pairwise-combinations of exclusions to explore the effect of
exclusion pairs on the data. The heatmap cells represent the maximum change across all
combinations for that step (i.e., when I apply X exclusion and then Y exclusion, what is the
change in the number of trials/participants between those two pre-processing actions?).
In participant level analyses, the exclusions related to participants will have the strongest
impact on power, given that these exclusions will remove an entire participant from the data.
Participant level exclusions have the largest impact on power when trial-level exclusions are first
applied because in this scenario, trials are excluded for fixed or data focused reasons, and then
participants do not have enough data to be included in the study or become an outlier for other
reasons. The reverse processing order (i.e., excluding participants and then trials) does not affect
the data in the same way. For trial level analyses, we see a similar pattern that participant level
exclusions have a larger impact on power ‚Äî generally because removing an entire participant
means the removal of all their trials. However, the trial level and stimuli level exclusions also
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
show changes in the number of trials across pairwise combinations, but again, we find that the
order of exclusion does not show the same exclusion numbers (i.e., 1-2 versus 2-1 exclusions do
colored in the same pattern). Overall, trial level analyses are likely to have more power, due to
larger degrees of freedom for the focal statistical test, but it is important to note that the
exclusions show different effects on the overall power based on their application and order.
order and pairwise-combination of exclusions. The panels represent the number of exclusions
present. The x-axis represents the previous exclusion applied, while the y-axis represents the next
exclusion to be applied. The heat color represents the change in number of participants for that
combination. For example, one tile may show that after participant external exclusions were used,
then participant outlier fixed may show a change of up to 10 participants. These cells represent
the maximum change across all combinations of pathways. P = Participant, T = Trial, S =
Stimuli. All zero values have been excluded.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
order and pairwise-combination of exclusions. The panels represent the number of exclusions
present. The x-axis represents the previous exclusion applied, while the y-axis represents the next
exclusion to be applied. The heat color represents the maximum change seen in number of trials
for that combination. P = Participant, T = Trial, S = Stimuli. All zero values have been excluded.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
5.3.3. Question 3 - Standardized Effect Size.
ùëç ùëéùë£
transformation (row 2). These effect sizes are arranged from smallest to largest across the x-axis
to demonstrate a sensitivity plot of the range of possible effects found.
transformation. First, it may have a small (i.e., r < .20), but non-zero relationship with sample
size as the correlation between sample size and log RT ùëë was ùëü = ‚àí.04, 95% CI [‚àí.08, ‚àí.01]
with corresponding correlations for each of the other effect sizes: log RT ùëë ùëü = ‚àí.14, 95% CI
ùëéùë£
[‚àí.17, ‚àí.11], raw RT ùëë ùëü = ‚àí.04, 95% CI [‚àí.07, .00], and raw RT ùëë ùëü = ‚àí.11, 95% CI
ùëç ùëéùë£
[‚àí.14, ‚àí.08]. The results indicate that ùëë may be more influenced by final sample size;
ùëéùë£
however, the patterns are the same for both data aggregation methods. Second, the difference in
effect size bias is clear ‚Äî ùëë returns an effect size that is double or more the size of ùëë on the
ùëç ùëéùë£
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
same data. They appear to show the same pattern of effects when the pre-processing actions are
applied, but the effect is more pronounced in ùëë .
5.3.4. Question 4 - Raw Effect Size.
appear to affect the final raw score effect size; however, the size of the transformed effect size
difference = 1.08 - 1.12 ms is smaller than the overall raw score difference: between 30 - 50 ms.
The effect does vary by pre-processing scenario with larger confidence intervals in various
scenarios due to sample or trial level size.
RTs. These effect sizes are arranged from smallest to largest across the x-axis to demonstrate a
sensitivity plot of the range of possible effects found.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
5.4. Discussion
The multiverse results portray that all facets of the pre-processing choices and checklist
items proposed in this manuscript likely have impacts on the result presented. First, the order and
number of pre-processing actions changed effect size results, with larger impacts on the
traditional choice for effect size, ùëë over the now recommended choice ùëë . The number of
ùëç ùëéùë£
participants or trials will affect the power of the study, and the choice of pre-processing action
and order also impacted these results. Finally, data transformation and aggregation choices
influenced the results of the study. Therefore, we conclude that all aspects of the checklist are
likely necessary for sufficient understanding of the results presented in a manuscript.
We would like to stress the fact that we could not use raw data, as the original authors
excluded some participants without enough documentation for us to simulate the missing
participants. These circumstances are another example for our results from the literature search
showing why a checklist like ours can be helpful for understanding the methods of a study and re-
using available data. We assume that the multiverse analysis with actual raw data should have
yielded larger effect size differences between the pipelines, as the data would have included more
noise, while already conveying differences of 0.14 to 0.20 in the current set-up. Of course, the
Simon effect is a relatively stable and large effect in a straightforward two-by-two design, so that
smaller and less reliable cognitive effects might be even more prone to changes in reaction time
preprocessing (as indicated by large false positive rates identified by Berger & Kiefer, 2021, and
Mor√≠s Fern√°ndez & Vadillo, 2020). However, we used the Simon effect only as a well-known
example of cognitive processing, so that we have an empirical basis for our suggestions about
reporting RT preprocessing. It will be instructive to see the preprocessing actions and their order
as manipulated in the current study applied to other effects and research domains.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
6. Recommendations and Conclusion
Data processing is a multi-action process - from the initial data cleaning, to outlier
detection and variable transformation. For each action, there are many possible options used in
the scientific literature, and these data pre-processing choices can systematically affect statistical
outcomes and theoretical conclusions (see present multiverse analysis and Kerr et al., 2017;
Primbs, Holland, et al., 2022). Therefore, it is important to document each of these actions and
transparently report all pre-processing decisions. To that end, the present research provides an
necessity of reporting all forms of data pre-processing as the choice of order, aggregation, and
effect size type influenced the final size of raw/standardized effects and power. Our checklist
offers concrete guidance on what to report and where to report it in order to facilitate the accurate
reporting of pre-processing decisions. This checklist does not give advice on how to pre-process
reaction time data, but on how to report these actions, thus, our recommendations can be
generalized to any kind of experimental reaction time paradigm.
As a result of the present research, we provide the scientific community with two available
versions of the checklist: a short and a detailed one. Both can be found in the Supplementary
Materials D and E as well as downloaded as pdf documents from our OSF project page [longer
how to use the checklist, without the need to read the entire manuscript. We foresee that our
checklist will be a living document, that integrates within itself changes of a scientific mindset,
and most importantly used by researchers. Note, it is not a goal of our checklist to tell which pre-
processing actions should be applied (if any), as responses to the survey clearly indicated a large
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
variety of approaches being applied, each with a legitimate rationale. The heart of the checklist is
the easy-to-grasp transparency of RT data pre-processing.
Sharing the code is the gold standard of open-data practices and enables access to exact
analysis pipelines. However, for several reasons, access to the code is not itself enough to provide
the public with the information they can use. On the one hand, some researchers use non-open-
source programs with expensive licences, thus making the shared code useless for the researchers
who cannot afford such software. On the other hand, researchers use different programming
languages for statistical analyses, with few researchers being fluent in multiple programming
languages. Consequently, literacy reading and understanding code is not self-evident.
Additionally, going through code from an unknown researcher can be time-consuming.
Therefore, we deem our checklist to improve inclusiveness by ensuring that all relevant
information is provided as a verbal (programming language/ software-agnostic) description using
common methodological wording.
We envision that journals should require the checklist to be uploaded as a Supplementary
Material to the published manuscript so that it does not take up valuable space in the main text
(comparable to policies of medical journals). This will ensure sufficient reporting by authors and
enable reviewers to quickly get an overview on how the data has been processed. Furthermore, it
will be easier for a potential data editor to double-check the correctness and appropriateness of
the code if the authors verbally formulate their intention. The headers we suggested for the
different actions could then be used in any shared code, as well to structure the code, and make
code easier to relate the respective pre-processing action. Therefore, we provide an additional file
with checkboxes on OSF which can be downloaded and filled in by the authors.
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
We also see advantages in the educational domain. First of all, supervisors can give their
students guidance in writing theses and conducting their own experiments by introducing
common pre-processing approaches and reflect on their applicability to the respective research
question. Second, the checklist allows for researchers who are new to the field to faster
summarize best practice of RT pre-processing in their specific domain.
For cases where no or only some pre-processing has been applied, it does not make sense
to upload an empty checklist. Therefore, we would like to recommend the following sentences in
the analysis section of the manuscript:
‚Ä¢ In our analysis, we have used unpre-processed (raw) data, where no pre-processing was
done.
‚Ä¢ If the authors conducted only parts of the pipelines, they should explicitly state: First, we
excluded erroneous data, then RT > 2000 ms or < 150 ms. No other pre-processing
actions were performed.
In addition, the use of checklists is widespread in other research areas such as health
research, leading to development of entire frameworks such as the EQUATOR framework
(Altman et al., 2008). Their main aim is to increase transparency and standardize the reporting of
various aspects of papers. This transparency is also especially important when it comes to
reaction times. The necessity for data transformations due to a typical experiment producing
skewed distributions to meet statistical assumptions, and lack of reporting on outlier exclusion
may lead to a variety of outcomes, as shown in our multiverse analysis. Therefore, clear
information on how the data were processed prior to statistical analyses seems crucial. In a way,
this transparency can be achieved by sharing analysis codes, in addition to data. However,
analysis codes can vary in programming languages, styles of coding, and clarity of the comments
available to the secondary user. Also, analysis codes are not subject to mandatory peer review.
Bearing that in mind, standardization attempts currently need to focus on the main paper text and
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
journal published supplementary information. An important point that needs to be made in
addition to being available for use and improvement, checklists need to be actively promoted by
both authors and editorial boards (Altman et al., 2008). Finally, this checklist may be used in
order to structure parts of methods or results sections of a paper, or to be included as authors
instructions or recommended by reviewers. In other words, all parties in the publishing process
may have some use from this checklist. As an example, the ARRIVE 2.0 guidelines (Percie du
Sert et al., 2020) are recommended to be used with the intent to facilitate joint effort to increase
reporting transparency; however, they are not intended to be a replacement for journal
requirements.
Our research aligns with recent demands for greater transparency in psychological
research (Wicherts et al., 2016). Wicherts and colleagues (2016) provide researchers with a list of
decisions that allow for researchers‚Äô degrees of freedom including data pre-processing decisions
and argue for the importance of pre-registration. Our checklist extends this work as a detailed
account of all planned pre-processing actions should be part of a good pre-registration. As such,
our checklist facilitates the evaluation of the severity of a test (Lakens, 2019). Multiverse
approaches that include analyses based on multiple pre-processing pathways (Steegen et al.,
2016) are not exempt from this transparency and should also be pre-registered (Primbs, Rinck, et
al., 2022). However, the complete and accurate reporting of data pre-processing decisions is also
important for a different reason: differences in data pre-processing decisions have been shown to
considerably influence results and conclusions (Bastiaansen et al., 2020; Botvinik-Nezer et al.,
2020; Kerr et al., 2017; Primbs, Holland, et al., 2022; Silberzahn et al., 2018). Ultimately, our
checklist allows researchers to gauge whether different results may be due to differences in data
pre-processing decisions, which is crucial for designing replication projects (Bokhove, 2022).
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
actions within the text of a paper, we recognize that it can be difficult to include very detailed
information while complying with word/page limits and that too much detail can sometimes
negatively impact the readability of a paper. Therefore, we strongly recommend that researchers
share the code and data needed to reproduce the numerical results presented in their paper. This
information makes it more likely that pre-processing actions, and the sequence in which they are
implemented, are completely and transparently reported.
Our checklist also facilitates novel meta-scientific endeavors: If researchers report all
decisions they make, this information allows for the development of updated and improved
checklists, enables the creation of inventories of common practices, and facilitates comparisons
between different tasks and research fields. Overall, the present research contributes to the
scientific literature by providing a checklist for the complete and accurate reporting of reaction-
time-based experiments which is not only accessible and easily implementable, but also achieved
through interaction among the scientific community members.
Transparency and Openness Promotion (TOP)
In the present paper, we reported how we determined our sample size, all data exclusions
(if any), all data inclusion/exclusion criteria, whether inclusion/exclusion criteria were
established prior to data analysis, all manipulations, and all measures in the study (Simmons et
al., 2012). Preregistration: no part of the study procedures and analyses was pre-registered prior
to the research being conducted.
Study materials and corresponding outputs can be found on the project‚Äôs OSF page:
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
CRediT Author Statement
Hannah Dorothea Loenneker: Conceptualization, Methodology, Validation, Formal
analysis, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing,
Visualization, Supervision, Project administration. Erin M. Buchanan: Conceptualization,
Methodology, Validation, Formal analysis, Investigation, Data Curation, Writing - Original
Draft, Writing - Review & Editing, Visualization, Project administration. Ana Martinovici:
Conceptualization, Methodology, Validation, Formal analysis, Investigation, Data Curation,
Writing - Original Draft, Writing - Review & Editing, Visualization, Project administration.
Maximilian A. Primbs: Conceptualization, Methodology, Validation, Investigation, Writing -
Original Draft, Writing - Review & Editing, Project administration. Mahmoud Medhat
Elsherif: Conceptualization, Methodology, Validation, Investigation, Writing - Review &
Editing, Project administration. Bradley J. Baker: Validation, Investigation. Leonie A. Dudda:
Validation, Investigation. Du≈°ica Filipoviƒá ƒêurƒëeviƒá: Validation, Investigation, Writing -
Review & Editing. Ksenija Mi≈°iƒá: Validation, Investigation, Writing - Review & Editing.
Hannah K. Peetz: Validation, Investigation, Writing - Review & Editing. Jan Philipp R√∂er:
Validation, Investigation, Writing - Review & Editing. Lars Schulze: Validation, Investigation.
Lisa Wagner: Validation, Investigation. Julia Katharina Wolska: Conceptualization,
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
Methodology, Validation, Investigation, Writing - Review & Editing. Corinna K√ºhrt:
Conceptualization, Methodology, Validation, Investigation, Writing - Review & Editing, Project
administration. Ekaterina Pronizius: Conceptualization, Methodology, Validation,
Investigation, Writing - Review & Editing, Visualization, Project administration, Funding
acquisition.
Acknowledgements
This project is the result of a collaboration started at SIPS 2021 in the following
unconference and hackathon: Cipora, K., Loenneker, H.D. (2021, June). (Too) many shades of
reaction time data pre-processing [Unconference session]. Society for the Improvement of
Psychological Science (SIPS) 2021 meeting, virtual conference, originally Padua: Italy.
time data pre-processing [Hackathon]. Society for the Improvement of Psychological Science
the attendees of both sessions.
Moreover, we would like to thank Gijsbert Bijlstra for comments on earlier versions of the
checklist.
We also would like to express our gratitude to the following open-source resources, which
greatly improved the project workflow: GitHub (github, 2020), R Markdown: The Definitive
Guide (Xie et al., 2018), papaja (Aust et al., 2022), ggplot2 (Wickham, Chang, et al., 2022),
flextable (Gohel et al., 2021), officedown (Gohel & Ross, 2022), rio (Chan et al., 2021), dplyr
(Wickham, Fran√ßois, et al., 2022), tidyverse (Wickham & RStudio, 2022), Cairo (Urbanek &
Horner, 2022), here (M√ºller & Bryan, 2020), gmodels (Warnes et al., 2022), metafor
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING
(Pedersen, 2020), RColorBrewer (Neuwirth, 2022), latex2exp (Meschiari, 2022), and psych
(Revelle, 2022).
TRANSPARENT REPORTING OF REACTION TIME DATA PRE-PROCESSING