A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 1
A Fair Lexical Decision Task for Monolingual and Multilingual Spanish-speakers
1 1 1 1
Julian M. Siebert , Mia Fuentes-Jimenez , Wanying Anya Ma , Carrie Townley-Flores , Ana
1 1 1,2
Saavedra , The ROAR Developer Consortium , and Jason D. Yeatman
Graduate School of Education, Stanford University
Division of Developmental-Behavioral Pediatrics, Stanford University School of Medicine
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 2
Author Note
We would like to thank the school districts, including Redwood City School District,
families, and students that made this research possible through a research practice partnership
model. We would also like to thank Sendy Caffarra for help developing and reviewing items. This
work was funded by NICHD R01HD095861, the Stanford-Sequoia K-12 Research Collaborative,
the Advanced Educational Research and Development Fund, Stanford Impact Labs, and
Neuroscience:Translate grants to JDY.
Correspondence concerning this article should be addressed to Julian M. Siebert,
Graduate School of Education, Stanford University, 485 Lasuen Mall, Stanford, CA 94305, Email:
jms312@stanford.edu
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 3
Abstract
This study describes the development and validation of ROAR Palabra, a novel Spanish lexical
decision task designed for use with both Spanish-speaking children and Spanish-English
bilinguals. This self-administered task requires students to decide whether a string of letters
presented on the screen is a real word in Spanish. While there is evidence that scores on English
lexical decision tasks are highly predictive of performance on conventional (time- and
resource-intensive) word reading assessments in English (Yeatman et al., 2021), we explore
whether this holds in Spanish, which has a much more transparent orthography. The specific goals
are (i) to create a linguistically fair task and an item-response theory model for it and (ii) to
evaluate whether such task can serve as a reliable proxy for conventional word reading measures,
offering a quick and easy-to-administer tool for assessing reading skills across linguistic and
cultural contexts. Results demonstrated strong correlations between performance on ROAR
Palabra and standardized word reading assessments such as the Woodcock-Mu√±oz Bater√≠a IV,
suggesting its effectiveness as a substitute measure. Notably, the task was sensitive to differences
in language proficiency across both monolingual and multilingual groups, reflecting expected
developmental and environmental influences. While not designed for the comparisons between
monolingual and multilingual populations, the findings underscore the potential of this task as a
versatile and culturally adaptable tool for reading assessments in different Spanish-speaking and
bilingual contexts.
Keywords: multilingualism, Spanish, lexical decision task, reading assessment
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 4
A Fair Lexical Decision Task for Monolingual and Multilingual Spanish-speakers
Reading proficiency is a cornerstone of academic achievement and cognitive development,
influencing individuals‚Äô ability to engage with and comprehend written information across various
contexts. Therefore, learning to read is one of the main goals in early elementary school education
(Catts, 2021). Efficient and equitable assessment of reading skills is essential for the early
identification of struggling students so that instruction can be tailored to each student‚Äôs unique
needs. However, traditional reading assessments are often time-consuming, resource-intensive,
and may lack cultural and linguistic adaptability, particularly when applied to diverse populations
such as multilingual students (Solano-Flores, 2016)‚Äîa population that is understudied and often
misconceptualised (Bialystok, 2017; Cummins, 2000; Grosjean, 2008). In response to these
challenges, there is a growing need for innovative assessment tools that are both reliable and
scalable, capable of functioning effectively across different linguistic settings.
In English, lexical decision tasks (LDTs), which have a long tradition in cognitive science
research (Balota et al., 2007), have been found to be efficient and reliable predictors of reading
performance, correlating strongly with traditional assessments of, for example, word reading
(Yeatman et al., 2021). In this study, we extend this approach to Spanish: We describe the
development of ROAR Palabra, a novel self-administered Spanish LDT and investigate its
relationship to traditional proctored assessment of Spanish word reading. Importantly, the task is
designed around the linguistic diversity of both monolingual Spanish speakers in Latin America
and Spanish-English bilinguals in the United States (US).
Reading in Transparent Versus Opaque Orthographies
Orthographic transparency refers to the level of consistency in the graphemes-phoneme
correspondence in a language‚Äôs writing system. A language‚Äôs orthographic transparency
influences the cognitive processes involved in word recognition and reading fluency and, therefore,
is a crucial consideration when developing any reading assessment. Spanish is characterized by a
highly transparent (shallow) orthography, where most letters or combinations of letters reliably
represents specific sounds. In contrast, opaque orthographies like English exhibit numerous
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 5
irregular spellings and inconsistent phoneme-grapheme mappings, which makes the process of
reading‚Äîand learning to read‚Äî more complicated and thus extends the length and amount of
instruction required to achieve mastery (Seymour et al., 2003; Ziegler & Goswami, 2005).
The transparency of the Spanish language facilitates the ease of acquisition of
foundational reading skills. Research indicates that Spanish-speaking children typically develop
phonological skills more rapidly than same-aged peers learning to read in less transparent
languages (Ziegler et al., 2009). This accelerated acquisition of letter-sound correspondence and
phonological awareness, in turn, allows for an earlier focal shift toward decoding skills and
reading fluency (Aguasvivas et al., 2020).
Decoding skills allow readers to translate written text into spoken language based on
acquired letter-sound correspondence. Over time, decoding becomes automatized allowing for
rapid and accurate word recognition. Automated word-level decoding skills facilitate the reading
of individual words and are precursors to sentence-level reading efficiency and comprehension
(Ehri, 2005; Perfetti, 1985). The relatively straightforward syllable structure of Roman languages,
characterized by predominantly open syllables (CV-CV) and limited consonant clusters, facilitates
more efficient grapheme-to-phoneme mapping and thus enhances the ease of decoding for readers
(Seymour et al., 2003). Therefore, in transparent orthographies where decoding is relatively
straightforward, word recognition (alongside reading fluency) becomes one of the primary early
indicators of reading proficiency.
Lexical Decision Tasks
LDTs require participants to determine whether a string of letters constitutes a real word
or a pseudoword, a process that necessitates both decoding skills and lexical retrieval. LDTs are
particularly effective in assessing word decoding and are widely used in psycholinguistic research
to study word recognition and lexical access (Balota et al., 2006; Katz et al., 2012; Keuleers &
Brysbaert, 2011). The literature largely agrees on the assumption that the underlying visual word
recognition processes of a two-alternative forced-choice (2AFC) design in LDTs mirror the
cognitive processes at play during other word recognition tasks, such as single-word reading out
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 6
loud (Balota et al., 2006; Seidenberg & McClelland, 1989).
By measuring the accuracy and/or speed of responses on LDTs, such tasks can provide
valuable insights into a student‚Äôs reading development. The simplicity and efficiency of the task
(easy and short administration) offers a quick and cost-effective means of gauging reading
proficiency that can serve as a proxy for more comprehensive, individually-administered
assessments. LDTs‚Äô utility in predicting performance on traditional reading measures has been
well-documented, particularly in languages with complex orthographies like English. Yeatman et
al. (2021) show that students‚Äô scores on an English LDT are highly correlated (r = .94) with their
scores on the Woodcock-Johnson Letter-word Identification subtask.
LDTs also offer a number of practical advantages over conventional word reading tasks,
such as the Woodcock-Mu√±oz Letter-word Identification subtest (Woodcock et al., 2019). For one,
their easy 2AFC design allows for objective, automatic, and immediate scoring without the need
for verbalization. Because each item takes less than a second, the task can be completed within a
few minutes and is amenable to computer adaptive testing (Ma et al., 2023). Last, the silent nature
of a LDT allows for completion in a large group setting (e.g., classroom), which translates to less
loss of instructional time and lower demands on resources.
The application of LDTs in languages with transparent orthographies presents unique
opportunities and challenges. In Spanish, the ease of grapheme-phoneme correspondence may
enhance the utility of LDTs in assessing word recognition and reading fluency, due to the
relatively low decoding demand (Seymour et al., 2003). However, the higher transparency also
means that LDTs must be carefully designed to differentiate between varying levels of lexical
access and processing speed among different proficiency levels (Vega-Mendoza et al., 2015).
Generally, research is sparse on the use of LDTs in Spanish (Aguasvivas et al., 2020), particularly
for assessment of multilingual learners.
Multilingualism
More than half of the global population is believed to be multilingual (Grosjean, 2010). In
the US, about 10% of K-12 students speak a language other than English as their first language; in
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 7
California this holds true for about 20% of the population (California Department of Education,
2023; National Center on Improving Literacy, 2023). The development of most educational
assessments, however, continues to largely operate from within a monolingual English mindset.
Decisions based on inappropriate assessment choice or interpretation of results can have drastic
consequences and may result in multilingual students‚Äô educational needs going unmet (Umansky,
2016). In this paper, we aim to shift this paradigm toward a more careful consideration of
multilingual individuals in assessment development to ensure fairly comparable outcomes
(Faulkner-Bond & Soland, 2020; Solano-Flores, 2023).
Reading Development in Multlingual Individuals
Multilingual individuals are a heterogeneous population with different levels of language
proficiency and reading skills across their languages. They exhibit unique developmental
trajectories that reflect differences in the amount and sequence of language acquistion, exposure,
formal and informal learning environments, as well as sociocultural context (Solano-Flores, 2016;
Surrain & Luk, 2019). Dur√°n et al. (2024), for example, report that multilingual Spanish-English
bilinguals with different levels of English proficiency show different levels of growth on various
foundational reading skills in kindergarten and first grade, when assessed in English. Especially
students with high levels of proficiency in their different languages (balanced multilinguals) are
able to tap into all of their languages‚Äô linguistic resources, which benefits their metalinguistic
skills (e.g., phonological awareness) cross-linguistic knowledge transfer (Barac et al., 2014).
The effect of concurrent or sequential exposure to multiple languages and linguistic
environments can allow for cross-linguistic transfer, where skills developed in one language
influence the acquisition and proficiency of another (Cummins, 2000). In the context of reading,
individuals may also transfer phonological awareness and decoding strategies from their dominant
language to their second language, enhancing their reading development in both. The nature and
extent of this transfer can vary depending on factors such as language similarity, proficiency levels,
and the context of language use. For Spanish-English bilinguals, the transparent orthography of
Spanish may facilitate the transfer of decoding skills to English, while the less transparent English
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 8
orthography may, in turn, influence reading strategies employed in Spanish (Bialystok, 2017).
Assessing Multilingual Individuals
Understanding the dynamics of reading development is essential for developing
assessments that accurately reflect the reading abilities of multilingual individuals. The variability
in developmental trajectories of multilingual readers mean there is a need for assessment tools that
are sensitive to these learning differences and can provide equitable measures of reading
proficiency across both monolingual and multilingual populations. Unfortunately, most readily
available reading assessments were designed with monolingual English populations in mind‚Äîas
well as in the calibration and norming samples. These assessments do not adequately account for
the heterogeneity and complexities of the multilingual experience, therefore often underestimating
multilingual individuals‚Äô true linguistic abilities (Bialystok, 2001; Luk & Bialystok, 2013;
Solano-Flores et al., 2009; Solano-Flores & Hakuta, 2017).
Linguistically fair assessment means for a measure to produce equally valid and accurate
results for test-takers with different linguistic backgrounds (e.g., first languages, different levels of
proficiency of the same language, etc.), but equal levels of the latent trait of interest. In other
words, a linguistically fair measures for use with mono- and multilingual individuals must not be
biased in favor or against those test-takers that are multilingual. This becomes a difficult endeavor
when the trait to be assessed is a language-related construct, such as in the case of an LDT.
For LDTs, when used with multilingual populations, this means that they must account for
cross-linguistic influences, cultural influences, and varying degrees of language dominance to
ensure accurate assessment. Thus far, Spanish LDTs were successfully used with Spanish-English
bilinguals in a sample of tertiary students with high and low English proficiency (Fairclough,
2011). Moreover, Aguasvivas et al. (2020), using LDTs as a measure of Spanish vocabulary, also
found no statistically significant changes between monolingual and bilingual tertiary students. We
are not aware of any study that examined this in primary and secondary school students or young
multilinguals.
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 9
Resarch Questions and Aims
1. The first goal is to develop a reliable Spanish lexical decision task. Specifically, we aim to
build the ROAR Palabra, a self-administered Spanish lexical decision task use with both
monolingual students in Colombia and multilingual students in the United States.
2. Second, we investigate the efficacy of such a task for use as a proxy for Spanish single word
reading skills, as measured by conventional, resource-intensive, proctored assessments.
Methods
Participants
Our sample (N = 6448) comprises students from two locations: We recruited a mostly
monolingual Spanish-speaking sample from Bogot√°, Colombia, (n = 5602), as well as a sample of
Spanish-English multilingual students from across the United States (US), mostly from California
(n = 846). The Colombian sample comprises students in grades 1 to 11 at two public schools and
one concession schools located in Bogot√°, Colombia. Concession schools (colegios en concesi√≥n)
were first launched in Bogot√° in 2000; the government contracts private operators to run these
schools. Concession school students, on overage, receive higher scores on national standardized
tests (pruebas Saber) relative to students in other public schools. Our sample comes from a
schools located in two different low-income neighborhood of Bogot√°.
Measures
ROAR Palabra
ROAR Palabra is a silent Spanish lexical decision task, which requires test-takers to decide
whether an item flashing on the screen for 350 ms is a real Spanish word versus a made up word
(pseudoword) and to respond via pressing a button on the keyboard/touchscreen. There is no limit
on the response time but the item is only presented for 350ms. It is an online assessment
instrument, developed to accurately measure students‚Äô word reading ability in a time- and
cost-efficient manner, doing away with the necessity for one-on-one assessments by trained
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 10
United States Sub-sample‚Äôs Demographic Characteristics.
1 1 1
Characteristic Gr 1 N = 327 Gr 2 N = 330 Gr 3+ N = 184
Location (within US)
California 327 (100%) 330 (100%) 49 (27%)
Other 0 (0%) 0 (0%) 135 (73%)
Gender
Female 159 (51%) 129 (40%) 0 (NA%)
Male 150 (49%) 194 (60%) 0 (NA%)
Unknown 18 7 184
English proficiency designation
English Learner 202 (65%) 211 (66%) 0 (NA%)
English-only 73 (24%) 75 (23%) 0 (NA%)
English-proficient 34 (11%) 36 (11%) 0 (NA%)
Unknown 18 8 184
Free or reduced-price lunch eligibility
Eligible 218 (70%) 235 (72%) 0 (0%)
Not eligible 94 (30%) 91 (28%) 2 (100%)
Unknown 15 4 182
n (%)
assessment experts. It is modeled on ROAR-Word (Yeatman et al., 2021), which has shown to
have high internal consistency reliability (r = .95) and scores on which highly correlate with
Woodcock-Johnson letter-word identification scores (r = .94).
ROAR Palabra is explicitly not a translation of ROAR-Word‚Äîas a simple translation does
not create equivalent versions of the same test (Solano-Flores et al., 2009). In contrast to many
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 11
other non-English measures, we started the development process from a Spanish perspective: We
created an initial list of stimuli by prompting ChatGPT to produce a list of Spanish words that are
(i) frequent, (ii) known to pre- and middle-schoolers, (iii) known to Spanish-speakers across the
Americas, (iv) and occurring in all the varieties of Spanish spoken there.
We then then used the Wuggy algorithm (Keuleers & Brysbaert, 2010) to create matching,
word-like pseudowords‚Äìstimuli conforming to Spanish orthographic rules and matching the real
word list in terms of word length, letter-transition frequencies, and orthographic neighbourhood
size. Five speakers of various versions and dialects of Spanish (from Colombia, Ecuador, Mexico,
Spain, and United States) then independently reviewed both the real and pseudowords. Items
flagged as problematic due to, for example, low frequency of occurrence or inappropriate slang
meanings in any one of the versions of Spanish were removed. With the Spanish-English bilingual
context in the US in mind, we also removed items that were real words in English.
This process resulted in an initial item bank with 378 items (that is 189 real words and 189
matched pseudowords). To keep administration time reasonable, we selected 70 core items that
were hypothesized to span a broad difficulty range (35 real and 35 pseudowords). We refer to the
remaining 308 items as the extended corpus. Every test-taker responded to all core items, as well
as random sample of additional (extended-corpus) items selected from the larger item pool.
Woodcock-Mu√±oz Bater√≠a IV
To assess the degree to which performance on the silent, self-administered ROAR Palabra
can function as a proxy for conventional, individually administered word- and nonword reading,
we used two subtests of the WM (Woodcock et al., 2019)‚Äîa Spanish parallel of the
Woodcock-Johnson IV (Schrank et al., 2014):
‚Ä¢ The identificaci√≥n de letras y palabras (letter-word identification; WM-LWID) test,
measuring students oral reading ability by having them read out aloud increasingly difficult
words.
‚Ä¢ The an√°lisis de palabras (word attack; WM-WA) test, requiring students to read
increasingly complex nonsense word out aloud, thereby tapping into their phonics and
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 12
decoding skills.
Both tasks are scored for pronunciation accuracy by trained test-administrators following
the guidelines of the scoring manual. The age-standardised scores of both the WM-LWI and
WM-WA can be aggregated to provide a basic reading skills (WM-BRS) composite score.
Procedures
In both settings, we worked closely with school partners in conducting this study. Parents
or guardians provided passive consent and students assented before completing ROAR Palabra or
any of the criterion measures. All protocols were approved by the Institutional Review Board at
Stanford University.
In Colombia, we trained a group of twenty field assistants and a field coordinator for the
administration of both ROAR Palabra and as well as both WM subtests. All students at
participating schools took ROAR Palabra in the computer rooms of the school, unless their
parents had opted out. Then, we randomly selected approximately 25% of those who had
completed ROAR Palabra (balanced across grade levels) to also complete the WM-LWI and
WM-WA. For this, students were taken to a dedicated space in the school library or a
multi-purpose room and completed the task on a laptop with a proctor‚Äìeither in-person or on a
laptop with headphones, connected to a proctor via a video-conferencing software. Scores
obtained this way were double-scored by experienced WM administrators.
In the US, exact study procedures varied by school district. In most instances, schools
made laptops or tablets available to students and tested whole classrooms at a time. Research
coordinators provided training and (on-site) support before and during the administration periods.
Analysis
In addressing the first aim of the study‚Äìbuilding the first version of ROAR Palabra‚Äìwe
undertook several steps to obtain a final item-response theory (IRT) model. Prior to doing model
building, we (i) filtered responses based on students‚Äô median response times (< 450 ms) and
correctness rate (< 65%) to exclude random guessers, (ii) excluded items with low (< .10)
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 13
point-biserial correlations to ROAR Palabra (core corpus only) totals and WM-LWID totals, and
(iii) excluded items with suboptimal item in- and out-fit (< .60 or > 1.40) within the core corpus.
We applied these criteria separately to both the US and Colombian sub-sample, so that
characteristics of both populations are represented in the final item selection.
Using the responses retained after excluding rapid guessers and with those core items
surviving the point-biserial correlation exclusion criteria, we iteratively fit a 1PL model and
excluded all items with poor fit until we obtained a stable model. We fit a 1PL model of the form
(ùúÉ ‚àí ùëè )
exp
ùëÉ( ùëã |ùúÉ) . + . ,
= 1 = 0 5 0 5 (1)
+ (ùúÉ ‚àí ùëè )
1 exp
ùëÉ( ùëã |ùúÉ) ùëñ ùëè
where = 1 is the probability of a correct response given item ‚Äôs difficulty level, , which
ùëñ ùëñ
measured on the same scale as the respondent‚Äôs ability, . Given the 2AFC task design, we
imposed a .50 lower bound on the probability of a correct response (guessing parameter). We
used the mirt package (Chalmers, 2012) for R (R Core Team, 2018) for all IRT analyses and
calculated theta scores using the default EAP estimator.
We then fit a two-parameter (2PL) model, in order to be able to evaluate item
discrimination parameters. While this model is not used for final theta estimation, items‚Äô
discrimination parameters indicate how effectively an item differentiates between respondents
with varying levels of the latent trait being measured. This provides important information about
the item‚Äôs quality and usefulness in the final measure.
Next, we fit another (final) 1PL model using Equation 1. This time, we included all those
items in the entire (core and extended) item bank that survived the exclusion criteria outlined
above. We fixed the scale using the item parameters obtained in the 1PL model for the
core-corpus items and only estimated item parameters for the extended-corpus items. Again, we
fit a 2PL model for the purpose of obtaining item discrimination parameters.
Following this, we evaluated the reliability of the final (1PL) model. Given that ROAR
Palabra is a fixed-length task scored using a 1PL model, the appropriate reliability metric is
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 14
empirical reliability ( ), estimated using Equation 2.
ùë•ùë•‚Ä≤
(cid:154)
ùëâ ùê¥ùëÖ(ùúÉÀÜ)
ùúå ,
ÀÜ = (2)
ùë•ùë•‚Ä≤
(cid:154) c
ùëâ ùê¥ùëÖ(ùúÉÀÜ) + ùëÜùê∏ (ùúÉÀÜ)
Then, we assessed the final model‚Äôs parameter invariance‚Äîthat is, we checked whether item
difficulty and discrimination parameters are significantly different in the two sub-samples. To do
so, we compared item parameters from a jointly calibrated 1PL model to parameters obtained
from 1PL models fit separately for each sub-sample, as well as parameters from the two separately
calibrated models. Because the two sub-samples cover very different grade ranges, we conducted
the parameter invariance analysis on a separate set of models using only data from respondents in
the overlapping grade range.
In a next step, we assessed ROAR Palabra‚Äôs criterion validity. For this we used the ROAR
Palabra theta scores obtained using the final 1PL model (and expected a-posteriori [EAP]
estimation) the Colombian students‚Äô raw scores on the WM-LWID and WM-WA. We correlated
students‚Äô observed WM scores with predicted WM scores obtained from a generalized additive
model with a smooth function on ROAR Palabra theta scores. Finally, given the sub-samples
different grade ranges, we repeated all analysis steps using only the overlapping grades as a
sensitivity analysis in the appendix.
Results
Item Responses
For the 70 core items, we have 5602 observations per item for the Colombia sub-sample
and 846 per item for the US sub-sample. For the extended corpus items, while we observe
sufficiently large numbers for the purpose of item calibration for the Colombia sub-sample, the
response counts from the US are too small to reliably calibrate an IRT model to the US
sub-sample. Therefore, we refrain from comparing performance on the extended corpus and
restrict our detailed analyses and item parameter estimation to the core corpus. Afterwards, we
refit the model with the extended-corpus items while holding the core items‚Äô parameters fixed, so
that the extended-corpus corpus items are calibrated to the same measurement scale that was
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 15
defined based on the detailed analysis of the core item bank.
Sample Performance and Median Response Times
percentage of correct responses), disagreggated by grade. Barely any of the students performing
above chance exhibit median response times < 450 ms. At the same time, students with extremely
fast response times (<450ms) perform around the chance level, which is likely indicative of rapid
guessing. The bimodality of the raw score distribution can be explained by the large grade range,
which includes both children still learning how to read words, as well as high schoolers who
and 2) that are represented in both samples; the overall patterns are very similar and no difference
based on study location is observed.
Median Response Time as a Function of Raw (Proportion Correct) Score on ROAR Palabra.
.25 .50 .65 .75 1.00
ROAR Palabra Proportion Correct
]sm[
emiT
enopseR
naideM
K 2 4 6 8 10 12 Colombia
Grade Location
1 3 5 7 9 11 United States
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 16
Item Properties
For the initial descriptive analysis, item difficulty is calculated as the proportion of all
sub-samples, item difficulty follows a bimodal distribution with real words (right peak) being
easier than pseudowords (left peak). The shift in positions of the distributions indicates that, on
average, items are easier for the Colombian students (which cover a much larger grade range). The
very high correlation between item difficulty in the two sub-samples (r = 0.93) indicates that the
relative positions of items on item difficulty distribution are very similar in both sub-samples.
Point-biserial correlations between student responses to a given ROAR Palabra item and
their raw ROAR Palabra score (proportion correct) are indicators of the degree to which an
individual item taps into the same construct that is measured by the overall scale. Panel B of
sub-sample, all correlations are > .20, within the US sub-sample, the majority is, too.
Further, point-biserial correlations between students‚Äô responses to individual ROAR
Palabra items and their WM-LWID raw score indicate to what extent each item is related to the
correlations. While some real words show very low (< .10) point-biserial correlations, the
majority of items fall into an acceptable range.
Overall correlations of item parameters between the two study locations show similar
repetitions of the analyses described here for the subs-sample of students in grades 1 and 2 only.
IRT Model Building
We started the model building process with the 70 items in the core corpus (35 real words
and 35 pseudowords), because we had sufficiently large numbers of observations in both contexts.
For the extended-corpus items, of which test-takers only saw random selection of 30 out of the
308 items, response counts in the US sub-sample were low. Therefore, we decided to add those
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 17
ROAR Palabra Item Properties with Item DiÔ¨Äiculty (Proportion Correct) Distribution in Panel A,
Item-total (Point-biserial) Correlations in Panel B, and Item-WM-LWID (Point-biserial)
Correlations in Panel C (Colombian Subsample Only), for Core Items.
0.7
r = 0.93***
0.6
0.5
0.4
0.6 0.7 0.8 0.9
Item Difficulty in Colombia
setatS
detinU
eht
ni
ytluciffiD
metI
0.4
0.3
r = 0.8***
0.2
Stimulus Type
0.1
pseudo
real
0.30 0.35 0.40 0.45 0.50 0.55
Item‚àítotal Correlations in Colombia
setatS
detinU
eht
ni
snoitalerroC
latot‚àímetI
Stimulus Type
pseudo
real
10.0
7.5
5.0
2.5
0.0
0.0 0.1 0.2 0.3 0.4
Point‚àíbiserial Correlations with Woodcock‚àíMu√±oz Letter‚àíword Identification Raw Scores
smetI
fo
.oN
items at a later stage, after the calibration of a measurement model based on the core corpus. Prior
to the estimation of an IRT model, we carried out four item selection steps as follows:
Participant Exclusion Criteria: Median Response Time
As a first data cleaning step prior to calibrating an IRT model, we excluded data from
participants whose response behaviour was indicative of random guessing or clicking through the
task without a serious attempt at the task. This was operationalized as a median response time <
450 ms and a score of < 65 % correct. This resulted in an exclusion of 6.95 % (n = 448) of
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 18
participants in total, 5.73 % (n = 321) of Colombian participants and 15.01 % (n = 127) of
participants from the US. Item counts are not affected by this exclusion criterion.
Item Exclusion Criteria
Criterion 1: Item-total Correlations. As a first step toward ensuring we are measuring a
single and coherent construct, we proceeded to eliminate all those items that exhibited
point-biserial correlations with the total task score (proportion of correct responses) of less than
.10‚Äîa very lenient threshold. In other words, this means removing those items whose response
patterns are unrelated to the overall proportion correct scores. To account for both contexts, we
applied this criterion separately to the Colombian and the US sub-sample. This resulted in the
exclusion of 3% of items (2 items; 0 real words and 2 pseudowords) based on data from the US.
The pseudowords excluded were cumpleapos, estudionte. No items had to be excluded based on
data from Colombia.
Criterion 3: Item-WM Correlations (Colombia only). Next, we computed correlations
to WM-LWID raw scores and had planned to exclud all items that exhibited point-biserial
correlations < .10. No items were flagged in this stage.
Criterion 4: Item-fit. Next, we iteratively fit a 1PL model and assessed item fit. A lenient
range for good item in-fit and out-fit parameters is .60‚Äì1.40. Only 2 items fell outside this range.
Core Model
After applying the four above criteria, the resultant core item pool contains a total of 66
items (33 real words and 33 pseudowords). While implemented using a 1PL model, we also fit a
2PL model to obtain item discrimination information. The item discrimination value ( ) indicates
the steepness of the slope of the item characteristic curve and is an indicator of how well that item
discriminates between two respondents whose ability levels are around the item‚Äôs difficulty.
ùõº ùõº < .
Typically, the range of the parameter is from 0 to 3, with an 50 indicating less productive
measurement. None of the items fall below that threshold. Moreover, as was the case with the raw
score, the distributions of theta scores show differences between the US and Colombian
sub-samples, though these disappear when filter to only draw on the earlier grades.
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 19
Final Model
Before adding the items from the extended corpus to the core measurement model, we
subjected them to the same four criteria we used to prune the core item pool. We then fixed the
core items‚Äô parameters and refit a 1PL model with guessing parameter to estimate the new
(extended-corpus) items‚Äô difficulty parameters. We also reran the 2PL model to obtain item
discrimination estimates, this time only estimating the new items‚Äô discrimination parameters.
resultant distribution of item difficulty and discrimination parameters; Panel B shows the test
information curve and the associated standard error; and panel C shows the distribution of theta
scores by location for grades 1 and 2 (which can be compared fairly, as they are covered in both
shows the theta score distribution for the entire grade distribution.
Reliability. We estimated empirical reliability for the final model (n = 6000), comprising
both core and extended-corpus items, using Equation 2.. Overall reliability is high, with
ùúå ùúå
= 0.938, as are reliability estimates for Colombia ( = 0.936, n = 5281) and the United
ùë•ùë•‚Ä≤ ùë•ùë•‚Ä≤
ùë•ùë•‚Ä≤
drawing on both the Colombian and US data.
ROAR Palabra Empirical Reliability by Grade (Colombia and US).
All Gr 1 Gr 2 Gr 3 Gr 4 Gr 5 Gr 6 Gr 7 Gr 8 Gr 9 Gr 10 Gr 11
n 5996 645 800 532 520 580 616 544 436 467 431 425
r 0.938 0.705 0.877 0.918 0.919 0.910 0.890 0.860 0.847 0.866 0.797 0.839
Parameter Invariance. Next, we assessed parameter invariance. To account for the
difference in grade ranges, we fit another set of 1PL models using only data from respondents in
those grades represented in both samples (grades 1 and 2). We compared item parameters of a
jointly calibrated IRT model to parameters for separately calibrated models, as well as
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 20
Summary of Final ROAR Palabra Item-response Theory Model, Showing the Bivariate
Distribution of Item DiÔ¨Äiculty and Discrimination (Panel A), the Test Information Curve With the
Associated Standard Error (SE; Panel B), and the Distribution of Theta Scores for the
Overlapping Grade Range (Grades 1 and 2) by Location (Panel C).
puedardo
rovo
intecigente
selotaesdtourdcuiteosrioo
pombre
agum amerillo
prdoejessliozramiento
hgumrfo√°ecnsoztaliacrote
venrreuqm√∫alimnoa
aplander
wiegpoepqiuteenvtoe
pel√°cula
te c c a √©r c √° luz u voai i s nd mt oo eerno
4 quezco
suapreinocor√≥n
baqarse
ets hp u erm r obo i a un o b iq dla ruo rilo cominar
p la lu p d √°ha nodmulpoita ri l bro
juco
puj e ues grgf u rai i b c i o r desayunar
autaocbiggygaosusitcad ecifh√≠cael
feladoreplescdoidertido
d m pn imn euegg o t t a le a bu r l uf ad ej i √± ri ho l so no si a nu ea et c eae cn s oe rc c o √± r o e a l n la te refigelante
mobrila
q t a v z s a a v a o pi c n n t a f d n a ae z gt m√∫ o cf o pl √≠ go c n m i s s i p lo o c cc a e r e e qi m u a i c a c n a c l s d u d a ai s t c ae d b tc id i e i p i r e m i c os a d i √≠ sa rr e a o n s o oda b d d n o a ca √°e t r a a r nn o r r t r m s e i p a a e s a a s t d o n a o s ta ta l√° d n o
pen clea tzo v√≠r egjrao
ecsoqruaijm√°nal√≥n
3 rrire l √≠g r o r af aq c b bdu r √≥ s e u po l oyi ne n ca s i adl m r a d r cbi hs z ds h s e po o l j a j e o e u a e h aa r u √° l l f sca a s o b en a rr a c ca a c d c n c i vc d i l√≥o l icaa d√°d t eo dr o lid a on a e coa r r o rr and e √≠ den te as graci√≥n
ctroumtabsero cainsttaencetesante
y√°tle o el r xa e i sr z cl√©u i a alg n ni t ao re r ro
re b rr ah e tfueo d rr li or asc e e v ro s l √≠ r a or os oa p uv be ial e u asf r a m cc t c o uio b irtn a riaai r srmohecxo√°mgoinnao a mrmijg√° u c d hrjr s aeo nuu al r √≥ ati c o r as p c e ta i a vz r icllaarsu√°llico
mnunpeeb m omfemr ot epeat onen √©t√≥ cr qh o nuoiccrimogua√±naa
di b b o uc n jao itc rc ou in a a dernoflaevtoeecsiotinrqaudionamiento
estudiar vcierv√°oxido esc√©ptero
helavdeos pa t e i b d lou om traar e sidu rr os o t b a rbd ap eo √±a dob a el
l√° m ju p co e i a zc np gl r c u e t a rr o c a a o m c o ef b a o f i√± n f i√° er iu e p pc e a s c s d s i r l oa r ne i t e o e at l afd a o a i n r m b d co r√± d i ip ln c o o o e ri a c s e √≠c r s os i a r l so a eru r c e n√∫ ne e b tp a a n io an b r z e rb o tds a o a r o s a p o m rd gm oa sl e ae se r e d nb√∫ d a n r u d c ec a c t l i a a ir l h s d i o c l d d b t o o ba u o o a s i ve c n o ir i bg s zro √© oe n r√≠ t a rico
2 bu p b s p e e c a m b n l nt s u a r z e r c f o a p i c r c c c a a m √± ar c a a l a b o c c l d o o √© c a m o r p r a n em o a p f m m l m l a a i o a a o a l t r r f z g d e a b br e o √≥ l g f l o oa p l v a √∫ r u t √± s o c p c p o i p i √© d p a a a m u r t r d h e i a i p c a b i √° r i e s b len fr f s c i n √≠ t n √° p v z e a s r a t i a p d a o l t q a a j e a u o √∫ g r l e b r l v o o h u a e sa l a l a r a t p zo r s r s e i a o j i √≥ i ol e i j ag c a√≠ f p c n t r m i b o l m i a u p e d o q j e be l l cu a t i u s as e r n r o l n √≥ s v l s a l o h i c a e a c i s l mh b l n a m o u r e n t o d e l p b d d o m c a r e d r e a e e r t a u j et e r s a o a t d i p t n o u b m i o e tv p l r o u m e d a a o d g r r e e o c r d s e an o a o √≠ l i e f i n l u b pe c b l o r e a i na l c r i e a v a sila o d s √≠ b l e o r e c r a l √≥ i l ja r al b a p at a a i r√± n e f p n i √≠ o h e e a p e p n c n r t r o i l na a √≠ r c r np r e r r z a c a u a l v √± o r e c i t a v ra i n o t a i me c td u r a t t s o o e n e o e c t n r d n a ci e e j r a v r i n a c il a ra o l r a n r a s n o d c n o t p u i t a r ell oa l u e t a o e e n i a l s s r t o n a t c o t g r e s a r √± r i p e e e b s c t l e e d t di n q e n a e e e √≥ i e d n o √≠i s o un n c c o l u l t u t u j o n n l i o e t r o i e ra t a n t sra√≥ i t a j t o t r o r a l po a √© or e r t o o i uno r c e m m ya s b ra r ar a √∫i egb r a e c s l n s l p i sai f e g o n n l g a ta a i u g e u e o t t √© l o j o nj l i a a n n xa e o t rr s c a oe x r t i a a a i √© r d d c r n m o a eq d z nu i c x a c a√© ti o a t o a g ej l u l l i o r it n sr a u f √° e m c √≥ t r √© si r a n m p o oi c r m a r n o e n o ic q o ue
juga√°rrbooslo mochila
pez
dormitorio
amfefiroguota
sopsailla ajumgaor librer√≠a ut√≠metro
sala
queso cazar
chasqu√≠deo
‚àí1.0 ‚àí0.5 0.0 0.5 1.0
Item Difficulty
noitanimircsiD
metI
A 1.2
0.9
Stimulus Type 0.6
a pseudo
0.3
a real
0 0.0
‚àí4 ‚àí2 0 2
ROAR Palabra Theta Score
noitamrofnI
Standard
Error
Metric Information SE
0.4
0.3
0.2
0.1
0.0
‚àí4 ‚àí2 0 2
ROAR Palabra Theta Score
ytisneD
Location
Colombia
United States
resultant correlations. Both sub-samples‚Äô item parameters are very highly correlated with those
obtained from a joint calibration. The correlation between separately calibrated US and
Colombian parameters, though somewhat lower, still suggests that parameters are similar in both
contexts. Here, the lexicality effect, with pseudowords being easier for the Colombian subsample
warrants further investigation.
Validity Evidence
These analyses draw on a sub-set of the Colombian sub-sample. To assess whether ROAR
Palabra scores can be used of indicators of word reading performance, we correlated students‚Äô
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 21
Parameter Invariance Analysis for 1-Parameter Logistic Model (Grades 1 and 2 Only) in the
Form of Correlations Between Jointly and Separately Calibrated Item Parameters for Colombian
Sub-sample (Panel A) and United States Sub-sample (Panel B), as Well as Between the Two
Separately Calibrated Models (Panel C).
r = 0.96***
1 2 3 4 5
Item Difficulty From Joint Calibration
ylnO
aibmoloC
rof
ytluciffiD
metI
Stimulus Type
pseudo
real
r = 0.91***
1 2 3 4 5
Item Difficulty From Joint Calibration
ylnO
setatS
detinU
rof
ytluciffiD
metI
Stimulus Type
pseudo
real
2 r = 0.77***
2 4 6
Item Difficulty From Colombia Only
ylnO
setatS
detinU
rof
ytluciffiD
metI
Stimulus Type
pseudo
real
ROAR Palabra theta scores (obtained from the final model) with their scores on the
respectively).
Cross-sectional growth patterns can provide additional validity evidence. As children
progress through the grades, their score on lexical decision tasks is reasonable expected to
increase, given the additional reading instruction and vocabulary expansion. Therefore, a good
lexical decision task should produce higher scores for students in higher grades. Panel C in
Discussion
This study investigated the feasibility of fairly using the same Spanish lexical decision task
(LDT) with both monolingual Spanish-speaking and Spanish-English bilingual students, as well
as such a task‚Äôs utility as a proxy for traditional, proctored word reading assessments. Specifically,
we (i) successfully developed ROAR Palabra as a linguistically fair Spanish LDT with very
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 22
Validity Evidence for ROAR Palabra By Means of Correlations Between ROAR Palabra Theta
Scores and Woodcock-Mu√±oz Basic Reading Skills (Panel A), Letter-word Identification (Panel B),
and Word Attack (Panel C) Raw Scores, as well as Cross-sectional Growth Across Grades on
ROAR Palabra (Panel D) For the Colombian Sub-sample.
r = 0.79
n = 798
‚àí5.0 ‚àí2.5 0.0 2.5
ROAR Palabra (Theta)
sllikS
gnidaeR
cisaB
MW
r = 0.78
n = 866
‚àí5.0 ‚àí2.5 0.0 2.5
ROAR Palabra (Theta)
DI
drow‚àíretteL
MW
r = 0.77
n = 798
‚àí5.0 ‚àí2.5 0.0 2.5
ROAR Palabra (Theta)
kcattA
droW
MW
2.5
0.0
‚àí2.5
‚àí5.0
1 2 3 4 5 6 7 8 9 10 11
Grade
erocS
arbalaP
RAOR
Grade 1 2 3 4 5 6 7 8 9 10 11
similar item parameters and score distributions among US and Colombian first- and
second-graders and (ii) showed its moderate to high correlations with the Woodcock-Mu√±oz
Bater√≠a IV Letter-word Identification and Word Attack subtasks. Additionally, we found that‚Äìfor
both mono- and multilinguals‚Äìitem difficulty is affected by lexicality.
Lexicality Affects Item DiÔ¨Äiculty
suggest the presence of a lexicality effect. Items cluster closely together and the cluster of
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 23
real-word items is less difficult and less discriminating than the cluster of pseudoword items. This
means that correctly responding to real-word items (i.e., recognising known words) is easier than
correctly identifying pseudowords as such. Moreover, this tells us that pseudowords are more
useful in telling apart high- from low-performers.
We are confident that this difference is largely based on lexicality. Given that we
effectively controlled for length, phonotactic constraints, and orthographic neighbourhood size
when constructing the pseudoword-items by using the Wuggy algorithm (Keuleers & Brysbaert,
2010), these stimuli characteristics are similarly distributed within the real-word and pseudoword
item groups. This lexicality effect holds true for both the mono- and the multilingual sample and
is consistent across grades.
This clustering of items is not observed in other (less transparent) languages. In a very
similar English task, Yeatman et al. (2021), for example, did not observe this pronounced
difference in item difficulty and discrimination between real words and pseudowords.
This lexicality effect likely generalizes to other languages with similarly transparent
orthographies, but further research is needed to confirm this. In addition to that, this finding could
be corroborated if this pattern holds even with a larger set of real-word items that are less
common, longer, or more difficult due to some other item features.
Decoding vs. Vocabulary
One possible explanation for the presence of a lexicality effect in this Spanish LDT, but
not in a comparable English task, is the lower decoding demand in Spanish (Ziegler & Goswami,
2005). Given that the LDT task design sees the stimuli only appear briefly, efficient decoding is
necessary in order to, in a second step, make a lexicality decision. In Spanish, due to its
transparent orthography, the ability to correctly decode both words and pseudowords develops
much faster than other reading skills, so that Spanish readers can successfully decode text before
they can comprehend it (L√≥pez-Escribano et al., 2013).
This would suggest that Spanish LDTs load less on efficient decoding skills, but are more
directly affected by vocabulary size‚Äìor the ability to recognise a known word. Indeed, others have
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 24
also used Spanish LDTs to assess vocabulary size (Aguasvivas et al., 2020). Our findings of lower
correlations between ROAR Palabra scores and WM-LWI and WM-WA scores (both of which
assess decoding) compared to parallel English analyses (Yeatman et al., 2021) supports this
hypothesis.
Linguistically Fair Assessment?
A linguistically fair assessment instrument produces equally accurate results for test-takers
who have different linguistic backgrounds but equal levels of mastery of the target construct. In
the case of LDTs, this begs the question as to whether we would expect all multilinguals in a
certain developmental stage‚Äìregardless of amount and context of language experience‚Äìto have
achieved the same level of mastery of Spanish lexical decisions. Here, developmental stage is
operationalised as grade level. Thus, this study suggests that, when developed carefully, the same
Spanish LDT may be used with both monolingual and multilingual Spanish-speaking
kindergarteners and first-graders‚Äìat least in Colombia and the US. We presented evidence
showing that ROAR Palabra item parameters are not statistically significantly different in the two
contexts and that final model theta scores for grades 1 and 2 are almost identically distributed.
This is in line with Aguasvivas et al. (2020), who found no statistically significant
differences in vocabulary size assessed via LDT between monolingual and bilingual
Spanish-speaking adults. In contrast, Izura et al. (2014) showed large difference between first-
and second-language speakers of Spanish in favour of the former on the LexTALE-Esp, a similar
task requiring the correct identification of presented stimuli as words or pseudowords. One
possible explanation for this difference might be the different age groups for the sample; our
sample of students in grades 1 and 2 is likely to largely comprise beginning decoders and that
differences might only start manifesting later.
Finally, while we establish parameter invariance and obtain very similar score
distributions for the grade range compared, more analyses Nonetheless, the present findings
strongly suggest that ROAR Palabra is suitable for use in both populations represented in the
sample. Caution is to be exerted, however, when comparing students from different backgrounds,
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 25
especially when extrapolating the to grades 3 and higher.
Next Steps and Limitations
Our data is currently imbalanced, in favour of the Colombian context. Therefore, we are
currently collecting both ROAR Palabra and WM-LWID data from a more sizeable sample of
Spanish-English bilinguals in higher grades in the US to further corroborate our claims. This will
allow us to also provide criterion validity evidence of for the US context. Even though the
parameter invariance analysis suggested that item parameters are sufficiently similar in both the
US and Colombian sub-samples, this will also need to be verified with the larger sample.
Particularly for the items in the extended corpus, our observation counts in the US sub-sample are
too low to inform item pruning. Additionally, the suggested lexicality effect warrantes further
investigation.
Furthermore, we plan to create new items‚Äîparticularly more difficult real words‚Äîin
order to increase the size of the item bank. A larger item bank with more well-performing items is
necessary for an efficient use of a CAT algorithm. Though unlikely, once we will have collected
more (US) data on the extended-corpus items, as well as on a set of additional items, we will have
to revisit the IRT model and decide whether a re-calibration is warranted.
Additional analyses of interest relate to the features of individual items. Which features
(length, cognates, age of acquisition, etc.), in addition to stimulus type, make items easier or more
difficult? Analysing the characteristics of well-performing items will also help facilitate
longer-term item bank development and inform other scholars developing Spanish reading
measures.
Other important questions that need answering pertain to the generalisability of these
findings. Do these findings hold true across other Spanish-speaking populations, or other school
types or socioeconomic strata? It would also be particularly insightful to disaggregate the US data
by instructional model, in order to check for effect of students‚Äô language of instruction on ROAR
Palabra scores and to reflect the notion that multilinguals are a heterogeneous population
(Solano-Flores et al., 2009; Solano-Flores & Hakuta, 2017).
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 26
Conclusion
We can conclude that ROAR Palabra is a reliable task that can be used both with first- and
second-graders in monolingual Spanish-speaking settings, as well as in multilingual settings, such
as with Spanish-speaking English learners in the US. The task shows moderate to strong
correlations with the WM-LWI and WM-WA, which underpins its potential as an efficient proxy
for time- and cost-intensive proctored reading assessments.
A FAIR SPANISH LDT FOR MONO- AND MULTILINGUALS 27