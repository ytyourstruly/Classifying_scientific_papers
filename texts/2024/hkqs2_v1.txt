Public and scientific climate action messaging on
Twitter
Daniel Lundgaard1*, Kristian Steensen Nielsen1, Trisha Harjani2, Jon Roozenbeek2,3, Wilhelm
Hofmann4,5, Sander van der Linden2, James K He2, & Kimberly A Nicholas6
1. Department of Management, Society and Communication, Copenhagen Business School,
Frederiksberg, Denmark.
2. Department of Psychology, University of Cambridge, Cambridge, United Kingdom.
3. Department of War Studies, King’s College London, London, United Kingdom.
4. Department of Psychology, Ruhr-University Bochum, Germany
5. German Center for Mental Health (DZPG), partner site Bochum-Marburg
6. Lund University Centre for Sustainability Studies, Lund, Sweden
* Corresponding author
Email: dlu.msc@cbs.dk
Abstract
Ambitious and legitimate climate action requires extensive public discussion, with social media
serving as an important forum. This study analyzes over 6.4 million English tweets on ‘climate
action’ from 2021, posted by the general public (1.25 million users; two-thirds of tweets) and over
3,000 climate scientists working on climate change on the then-dominant climate discussion
platform, Twitter. We examined the themes, emotional and moral sentiment, and tweet
effectiveness measured by retweets. Our findings reveal a disconnect between frequently used
communication strategies and their effectiveness. While tweeting about climate change mitigation
and climate politics using positive language was common, it was only weakly associated with
retweet counts. Conversely, the most effective strategies—tweeting about fossil fuels and using
negative (including negative moralization) language—were infrequent. Our study contributes to the
understanding of how social media platforms can be used to effectively communicate about climate
action strategies and policies.
Introduction
Limiting global climate change requires ambitious and legitimate climate action. This includes
personal action (e.g., reducing air travel and meat consumption1), corporate action (e.g., more
sustainable business practices2), and political action (e.g., subsidizing renewable energy production
and taxing carbon-intensive goods and services3,4). The climate actions promoted by the public,
scientists, non-governmental organizations, and corporations shape the content and process of the
political climate agenda and its outcomes5.
Social media has become an increasingly important forum for sharing and shaping opinions
on societal topics6, with about 60% of the world population active on social media7. Traditional and
social media overwhelmingly focus on current events, sports, and pop culture, with climate
mentioned far less often. For example, in 2021, there were 23 times more tweets about COVID-19
than climate change, despite climate tweets returning to their pre-pandemic average and a
substantial decline in COVID-related tweets in the second year of the pandemic11. Nonetheless,
social media platforms, such as Twitter (known as X since April 2023), Facebook, and Reddit, are
used by citizens, scientists, policymakers, and corporations alike to express their opinions and
promote or counteract specific climate actions8,9. For example, science communicators and fossil
fuel companies have a presence on many social media platforms, discussing the merits of different
climate actions, such as eliminating fossil fuel subsidies or promoting recycling10.
While studies have analyzed developments in the climate change discourse8,14, few studies
have investigated how scientists and the public communicate about climate action on social media
and how these communications differ. This includes examining the types of climate actions
discussed and whether certain themes are more effective than others in reaching a wider audience.
Furthermore, emotional dynamics are known to influence the diffusion of social media content15,16,
and studies have found that moral outrage can drive engagement17–19. However, these aspects have
been under-explored in comparing communication by scientists and the public. Recent studies also
highlighted the importance of positivity over negativity (or the benefits of spreading “ingroup love”
rather than “outgroup hate”) in driving engagement online20,21.
In this article, we investigate the communication strategies for climate action on Twitter
employed by the general public and scientists working on climate change (hereafter referred to as
‘climate scientists’) and evaluate their effectiveness. We have three overarching research objectives.
First, we examine the themes that the general public and climate scientists engage with in their
climate action communications and how these themes evolve throughout the year. Second, we
investigate the extent to which the general public and climate scientists use emotional and moral
language to communicate about climate action. Finally, we analyze the effectiveness of the different
themes and emotional and moral communication strategies in reaching a large audience, as
measured by the log-transformed frequency of retweets.
To address these objectives, we draw on two large datasets from Twitter. The first dataset (N
= 4,237,424, shared by 1,250,910 unique users) includes all English tweets posted in 2021
containing either the phrase “climate action,” the hashtag #climateaction, or a link with “climate
action” in the title. The second dataset (N = 2,250,973) includes all English tweets posted in 2021
from the 3,023 climate scientists on the “Scientists who do climate”-list created by Katherine
Hayhoe22. The data was collected retrospectively in April 2022 directly through the Twitter API
using the academic research access providing full archive search options (see Methods for details).
Themes in tweets about climate action
To understand the data, we conducted a quantitative content analysis23, examining the percentage of
original tweets, retweets, and quotes; the median retweets, likes, and quotes received; the median
overview). We found that climate scientists shared a significantly higher proportion of original
tweets, while the general public predominantly retweets content shared by others. Tweets from the
general public were retweeted at a much higher rate, whereas tweets from climate scientists
received more likes. Analyzing the frequency of terms used in shared tweets in relation to climate
action revealed that the general public discussed change, the climate crisis, and the global nature of
climate change, while climate scientists focused more on research and people (Supplementary Fig.
1).
We next investigated the themes in tweets about climate action using the topic modeling
method Latent Dirichlet Allocation (LDA)24. This analysis yielded 47 topics for the public and 40
for climate scientists (see Methods). We manually combined these topics to identify ten overarching
details). Both the public and climate scientists frequently tweeted about politics in general (Politics
and society) and about climate solutions and mitigation strategies (Climate action and mitigation).
The general public tweeted about the impact of climate change (Impact and urgency of climate
change) more than twice as often as climate scientists. Similarly, politics specifically related to
climate action (Climate politics, 11.3% of tweets) and specific events like COP 26 (Events and
summits, 12.9% of tweets) emerged as central themes within the general public tweets, while these
two themes were about three times less common in tweets shared by climate scientists. The use of
fossil fuels (Fossil fuels) was one of the smallest themes in both datasets but represented a greater
share of the general public’s tweets (6.6% vs. 1.9%). Unsurprisingly, we identified a tendency for
climate scientists to focus more on scientific reports and research (Research and reports, 10.9% vs.
3.7% in the general public tweets) and academic life more broadly (Academia, 12.9% vs. 0%).
Finally, we observed a tendency for climate scientists to more commonly use Twitter to interact
directly with others (Personal interaction, 18.8% of tweets), which included sharing words of
support or positive encouragement to specific colleagues on the platform–something that was not a
theme in the general public tweets (Supplementary Fig. 2).
modeling in both the general public (N = 4,237,424) and climate scientists’ (N = 2,250,973) climate
action tweets, with illustrative topic examples.
Themes General public Climate scientists
Politics and society: non-climate issues, e.g., “Canadian
21.2% 15.9%
elections”, “Social justice during covid”, “Tax budget allocation”
Impact and urgency of climate change: e.g., “Long-term
climate impacts”, “Adapting to extreme weather”, and “Ice 20.5% 8.9%
melting/Sea levels rising”
Climate action and mitigation: e.g., “Green energy transition”,
18.4% 16.6%
“Reducing emissions”, “Energy sources”
Events and summits: e.g., “NYC climate week” and “Glasgow
12.9% 3.4%
COP summit” (COP26)
Climate politics: e.g., “UN response to climate change”,
11.3% 3.8%
“climate policy and adaptation”
Fossil fuels: e.g., “Stop/end fossil fuels”; industry, e.g., “Fossil
6.6% 1.9%
fuel disinformation”, “Exxon misleading the public”.
Pop culture: Entertainment and the media environment, e.g.,
“Blackpink at COP” (Record-breaking Korean pop group), “BTS 5.4% 6.9%
talking about covid” (Biggest Korean pop group in the world)
Research and reports: e.g., “IPCC report”, “Climate modeling,” 3.7 % 10.9%
and “Scientific grounds for resistance/activism”
Personal interaction: e.g., “Positive encouragement” and
0.0% 18.8%
“Words of support” directed at specific individuals
Academia: University and research life, e.g., “DEI in research”
0.0% 12.9%
and “Academic recruitment”
Throughout 2021, we found a strong seasonality in tweets from the general public (Fig. 1A).
This especially coincided with COP 26 (held the first two weeks of November in 2021), with
steadily increasing numbers of tweets about climate action and mitigation, climate politics, politics
and society, and events and summits. However, the climate action debate seemed to saturate after
the COP, evidenced by a sharp decline in the overall number of tweets relating to climate action.
We also observed a peak in tweets related to pop culture in February and September 2021,
corresponding with popular climate-action tweets and public activities by music groups BTS and
Blackpink.
Examining the timeline of climate scientists’ tweets, we observed a more consistent pattern
with the different themes appearing at a more constant rate. For example, tweets related to politics
and society and climate action and mitigation appeared comparatively frequently throughout the
year, with politics declining from a high in January 2021 and climate action reaching a gradual peak
during the COP (Fig. 1B). Finally, during summer in the Northern hemisphere, there was a greater
emphasis on the impact and urgency of climate change based on current examples of forest fires,
record temperatures, and extreme weather in general.
Fig. 1. The prevalence of climate action tweets in 2021 by theme. a, Climate action tweets from
seasonality aligned with the COP events. b, Climate action tweets from climate scientists (N =
a moderate increase in climate action and mitigation themes around the COP in November.
Emotional and moral language in tweets
To investigate the use of emotional language, we first classified tweets as emotionally positive,
negative, and neutral using VADER (see Methods). We found that both the general public and
climate scientists extensively used emotional language, but climate scientists were more often
neutral, and the public was more often negative (Fig. 2A). The general public often used emotional
language in tweets about climate action (only 11.6% of tweets were emotionally neutral) and used
more positive than negative language (53.6% vs. 34.7% negative) (Fig. 2A). By comparison,
climate scientists were more likely to avoid emotional language (22.7% of tweets were neutral) but
also used more positive language about twice as often as negative language (52.9% vs. 22.8%
negative).
We next analyzed the tendency to use moral language in tweets based on the Moral
Foundations Theory25 (see Methods). The majority of climate action tweets by the public used
moralized language (61.3%), whereas the majority of climate scientists’ tweets did not (only 34.9%
did so) (Fig. 2B). Within the subset of tweets containing moralized language, most of the moralized
language emphasized moral virtues (80.8% for the public, 73.8% for climate scientists), that is,
upholding or endorsing a moral (e.g., “helping” or “protecting”) rather than moral vices (e.g.,
“damaging” or “harming”) (Fig. 2C). For both groups, the most common form of moralized
language was associated with the moral dimension care (38.5% of moralized tweets for the general
public and 37% for climate scientists; Fig. 2D), which comprises virtuous words such as “helping”
and “protecting” but also vices such as “exploit” or “endangers.” Loyalty (e.g., “unity” or
“collectively;” 23.1% and 19.3% of moralized tweets, respectively) and authority (e.g.,
“authorizing” or “govern”) were also common moralization approaches for both groups,
representing 17.9% and 18.4% of moralized tweets, respectively. Climate scientists used sanctity
(e.g., “sacred” or “desecration”; 8.5% vs. 14.8%) moralization almost twice as often as the general
public (Fig. 2D).
Fig. 2. Percentage of emotional and moral language in tweets. a, Distribution of emotional
tweets from the general public (N = 4,237,424 tweets) and from climate scientists (N = 2,250,973
tweets). b, Distribution of moralized language in tweets from the general public (N = 4,237,424
tweets) and from climate scientists (N = 2,250,973 tweets). c, Distribution of language emphasizing
moral virtues and moral vices in tweets containing moralized language by the general public (N =
2,597,102) and climate scientists (N = 786,396). d, Distribution of language associated with each of
the five moral dimensions in tweets containing moralized language by the general public (N =
2,597,102) and climate scientists (N = 786,396).
Effectiveness of tweets
Themes. We fitted linear mixed-effects models to analyze the effectiveness of lexical features on
tweet effectiveness (see Methods). These analyses revealed that the most effective (indicated by
retweet count) theme from the general public tweets were tweets with popular culture references
(Pop culture, b = 1.711, SE = 0.012, P < .001; Fig. 3), despite appearing in only 5.4% of the tweets.
Tweets relating to the use of fossil fuels were also effective (Fossil fuels, b = 0.519, SE = 0.008, P <
.001) yet appeared infrequently (6.6% of the tweets). More frequently appearing themes, such as
tweets about the impact of climate change (20.5%) and solutions and climate change mitigation
(18.4%), were negatively correlated with effectiveness (Impact and urgency, b = -0.45, SE = 0.007,
P < .001, and Climate action and mitigation, b = -0.32, SE = 0.007, P < .001). Furthermore, tweets
about politics and society more broadly were positively correlated with effectiveness (Politics and
society, b = 0.383, SE = 0.007, P < .001), while tweets specifically concerned with climate politics
were negatively correlated with effectiveness (Climate politics, b = -0.491, SE = 0.007, P < .001).
This indicates that even within the subset of the Twitter conversation focused on “climate action”,
discussing broader politics was more effective in ensuring retweets compared to climate politics
specifically (Fig. 3; see Supplementary Fig. 3-7 for an overview of the regression results).
The most effective themes for climate scientists were broader politics and society (Politics
and society, b = 0.903, SE = 0.01, P <.001) and the use of fossil fuels (Fossil fuels, b = 0.408, SE =
0.012, P < .001), despite the fact that climate scientists less frequently tweeted about fossil fuel use
(1.9% of the tweets). For climate scientists, discussing popular culture was negatively associated
with retweets (Pop culture, b = -0.23, SE = 0.008, P < .001), while tweeting about the impact of
climate change was positively associated with retweets (Impact and urgency, b = 0.248, SE = 0.008,
P < .001). Furthermore, while climate scientists often used Twitter to interact directly with others
(18.4% of climate scientists’ tweets), doing so was strongly negatively associated with retweets
(Personal interaction, b = -0.736, SE = 0.007, P < .001), suggesting that this common form of
climate scientists’ tweets is of little broader interest.
Fig. 3. Climate action themes and their associations with retweet count (log-transformed). The
analysis is based on each tweet representing one of the ten themes derived from the results of the
Emotional language. For the general public, using both positive emotional (b = 0.224, SE =
0.013, P < .001) and negative emotional language (b = 1.499, SE = 0.014, P < .001) was strongly
and positively associated with effectiveness (Fig. 4A). The substantially larger effect sizes for
negativity are interesting, considering the overall tendency within the general public data to use
more positive language (Fig. 2A). Climate scientists’ tweets using negative language were also
more effective (b = 1.036, SE = 0.016, P < .001), but the effect size was smaller compared to the
general public. Interestingly, for the climate scientists, positive emotional language was strongly
negatively associated with effectiveness (b = -1.602, SE = 0.010, P < .001) (Fig. 4A). This is
particularly interesting considering that climate scientists had twice as many positive as negative
tweets (Fig. 2A), implying many of their tweets were not as effective in generating retweets.
Moral language. We first analyzed the effectiveness of using moralized language on
retweet count. Although both the general public and climate scientists tended to use language
associated with moral virtues in their tweets (Fig. 2B), language related to moral vices (i.e., words
linked to morally negative concepts) was more effective for both groups, with the largest effect
when climate scientists used such negatively moralized language (b = 2.289, SE = 0.015, P < .001).
However, language related to moral virtues was also positively associated with retweets (b = 1.404,
SE = 0.009, P < .001) (Fig. 4B). For the general public, the effect sizes were smaller, but language
related to moral vices was more than twice as effective as language associated with moral virtue (b
= 0.973, SE = 0.011, P < .001 vs. b = 0.413, 0.005, SE = 0.005, P < .001, respectively) (Fig. 4B).
Disaggregating the moralized language represented in the five dimensions of moral
foundations theory25, we found that for the general public, moralized language associated with
fairness (b = 0.46, SE = 0.005, P < .001), loyalty (B = 0.302, SE = 0.004, P < .001) and care (b =
0.292, SE = 0.003, P < .001) were positively associated with effectiveness. By contrast, authority
and sanctity were negatively associated with effectiveness (b = -0.129, SE = 0.005, P < .001 and b =
-0.076, SE = 0.006, P < .001, respectively) (Fig. 4C). In other words, tweeting specifically about the
role of authorities (i.e., using words such as “authorizing” or “regulating”) or sanctity (i.e., using
words such as “desecrating” or “exterminating”) reduced the effectiveness of tweets for the general
public. For the climate scientists, all moral dimensions were positively associated with retweets,
with authority appearing as the strongest association (b = 0.923, SE = 0.008, P < .001), followed by
fairness (b = 0.746, SE = 0.01, P < .001), loyalty (b = 0.658, SE = 0.008, P < .001), then care (b =
0.544, SE = 0.005, P < .001) and sanctity (b = 0.539, SE = 0.008, P < .001) (Fig. 4C).
Fig. 4. Impact of tweet characteristics on retweet count (log-transformed). a, Effectiveness
(i.e., association with retweet count) of positive and negative sentiment in non-neutral tweets shared
by the general public (N = 3,742,682) and climate scientists (N = 1,740,351). b, Effectiveness of
language associated with morally positive concepts from the five moral foundations (moral virtues)
and language associated with morally negative concepts from the five moral foundations (moral
vices) in moralizing tweets shared by the general public (N = 2,597,102) and climate scientists (N =
786,396). c, Effectiveness of the five moral foundations in moralizing tweets shared by the general
public (N = 2,597,102) and climate scientists (N = 786,396).
Discussion
We investigated the themes and language (sentiment and moralization) in over 6.4 million tweets
about climate action from the general public and climate scientists. We also assessed the
effectiveness of different types of communication strategies in terms of their effectiveness at
spreading widely (as measured through retweet counts). Both climate scientists and the public
frequently tweeted about themes that were likely to be retweeted, but these themes appeared to have
little direct relevance for climate action, such as non-climate-related politics and pop culture.
Focusing on more climate-relevant themes, both the public and climate scientists frequently tweeted
about climate action, though mostly in general and not specific terms (e.g., “Global action on
climate needed now” or the simple “Demand climate action”) and occasionally about climate
politics, but such tweets were less likely to be retweeted. The climate-relevant themes most likely to
be retweeted were tweets about climate impacts from climate scientists (8.9% of tweets) and about
fossil fuels from both groups. However, fossil fuels were rarely discussed by either group; only
6.6% of the public’s tweets and 1.9% of climate scientists’ tweets. Overall, for tweets about climate
action, we found very little content on specific, and particularly on effective, actions. That is,
climate action tweets tended to convey that “someone should do something” without being clear on
who should do what. We also found a disconnect between the themes that climate scientists and the
public tweet about on Twitter and the themes that are likely to find significant uptake or
engagement in the form of retweets.
In line with previous research, using emotional language had a strong effect on the
likelihood of retweets15, with negative tweets much more likely to be shared26,27. For the general
public, it was seven times less effective to use positive than negative sentiment, but they used
positive language about 33% more often. Climate scientists used positive language about twice as
often as negative language, but their negative tweets were much more likely to be retweeted, while
their positive tweets were even more unlikely to be retweeted. Our findings align with previous
work supporting the negativity bias on social media, whereby environment-related tweets with
negative emotions are shared more often27,28. However, although studies have found correlations
between online discussions and real-world actions29, the link is complex. Negative messages can
make participants less inclined to take pro-environmental action (direct effect) but more likely to
report a higher intention to act (indirect effect)28.
Using moralized language was effective as a means of achieving increased engagement,
especially for scientists15,30,31. For both groups, language related to negative moral concepts (moral
vices) such as unfairness was more effective in terms of driving engagement, despite positive moral
language (moral virtues) appearing more than three times as often as language related to negative
moral concepts. For the general public, tweeting about moral issues of loyalty/disloyalty and
fairness/unfairness, and for climate scientists, about authority/subversion were the most effective
strategies when it came to generating engagement. Whereas these findings regarding effectiveness
appear to contradict the popular notion that scientists should not moralize in scientific
communication, it is too early to derive practical recommendations, as the present results do not
allow us to disentangle the identified positive effects (e.g., message amplification) from potentially
negative downstream effects of moralized language (e.g., polarization) on various social media
audiences. However, our findings align with the modern position that (climate) science cannot be
value-free32. The use of value-laden language, such as climate scientists highlighting unfairness
within the dire collective social dilemma of climate change, suggests that recognizing and
leveraging these moral dimensions may be an authentic and effective approach to science
communication.
We note several limitations to this study. These include the potential difference between
climate action and the general climate discourse, the application of moralized language to different
groups, and issues with the platform Twitter. Repke et al.11 provided important context for the
overall climate change discourse on Twitter in 2021, while we only focused on the climate action
discourse. They found the majority of climate discourse was climate contrarians (16% of all
“climate change” tweets), climate politics (16%), climate impacts (9%), solutions such as tree
planting projects, philanthropist pledges, or celebrity awareness-raising (8%), followed by causes,
which tended to blame capitalism and corporate greed, as well as inequality and fossil malfeasance
(5.2%). We see an overlap between our tweets on climate action and Repke et al.’s11 tweets on
climate change, in the themes of politics, impacts, and solutions, with less focus in our dataset on
contrarians (who are not tweeting about “climate action”). While the Repke et al.11 dataset
identified primarily political and ideological causes, our dataset focused on the factual cause of
anthropogenic warming: fossil fuels. Furthermore, we did not assess the impact of moralized
language about groups (e.g., ingroup solidarity or outgroup hostility), which recent research has
identified as important drivers of Twitter engagement in the case of US politics19 and the Russian-
Ukrainian war20.
Our focus on Twitter has both strengths and weaknesses. Twitter was the dominant social
media platform for climate discourse in 2021, when we collected our data, where large numbers of
both climate scientists and the public were consistently active33. At that time, the open API allowed
researchers to freely collect very large samples, offering unique opportunities for big data analysis
of social media communications. A drawback for all social media analysis is the potential that
algorithmic amplification can distort results34,35,36, such as pushing negative content to a wider
audience that subsequently receives more retweets. There is also the potential for differential
engagement patterns depending on the platform under observation37, 38. The landscape for climate
communication on Twitter has fundamentally changed since Elon Musk’s acquisition of Twitter,
concluded in October 2022, and subsequently rebranded as X in 2023. One study showed that over
the course of Musk’s takeover of Twitter, people started using the platform much less and reported
increased negative experiences19. Another study found that half of environmentally focused users
stopped using the platform within six months of the Musk takeover37. As of November 2024, the
founder of the climate scientists on Twitter list, Katharine Hayhoe, called climate Twitter a “ghost
town,” while Science magazine reported the “gradual migration” away from X had turned into a
“stampede” as the platform increasingly amplified misinformation39. Thus, we view our results as
an important snapshot in time, which are worth exploring on emerging social media platforms
where the climate debate seems to be shifting, such as Bluesky.
Overall, we conclude that social media posts about climate action had surprisingly little
focus on informing meaningful climate action, with the majority of tweets including “climate
action” in fact focusing on climate impacts (as a motivation for the urgency of acting), or climate
events and reports, as well as unrelated politics, pop culture, and personal and professional topics.
The label seems to be used as a marker for supporting climate action, without indicating what those
actions should be. Ultimately, effective tweets used negative and moralizing language, and
moralizing was especially effective for climate scientists. Focusing on fossil fuels was effective for
both climate scientists and the public. Indeed, as the most urgent and effective climate action is to
stop burning fossil fuels, posting more about fossil fuels as a climate action seems an effective way
forward on social media.
Methods
Twitter data, including tweets and user information, was collected using the official Twitter API for
academic research40, which was available until policy changes in February 2023. For the general
public, we collected English-language tweets from Twitter using the keyword “climate action”
posted from January 1, 2021, to January 1, 2022. The search term includes tweets containing
“climate action” in the text, tweets using the hashtag “#climateaction”, and tweets sharing a link
with “climate action” in the title. For the climate scientists, we collected all tweets shared by
Twitter users on the “scientists who do climate” list compiled by Prof. Katharine Hayhoe22 posted
from January 1, 2021, to January 1, 2022. In total, this produced 4,237,424 general public tweets
shared by 1,50,920 unique users and 2,250,973 climate scientist tweets shared by 3,023 unique
disproportionately dominated the climate action discourse on Twitter in 2021, as they represented
less than 1% of all tweeters on the topic but over a third of all tweets.
Topic modeling
We trained topic models for both datasets independently. For this, we used an implementation of
the Latent Dirichlet Allocation (LDA)20 provided by the Mallet package for R41. LDA is an
unsupervised learning method that identifies latent text patterns in a large and often unstructured
corpus of text (such as millions of tweets)20,23. Because the LDA is an unsupervised machine
learning technique, it is less subject to the biases inherent in supervised methods where a researcher
imposes a set of predefined topics on the data42.
Identifying the number of topics (K) is one of the most challenging questions in topic
modeling44. One of the most common ways of identifying K is to evaluate the semantic validity of
the topics44,45. Semantic validity is an assessment evaluating if the topics identify coherent groups of
Supplementary materials for a complete overview). We combined the semantic validity check with
sensitivity analysis44 – identifying K through an iterative process with varying K – as the usefulness
of an unsupervised model is a matter of whether the latent patterns it uncovers are interesting and
coherent43. Ultimately, it was decided that a K-value of 47 (i.e., 47 topics) produced the most
internally coherent yet distinctive set of topics for the general public data, and a K-value of 40
additional details). Three authors independently reviewed the top 15 terms for each of the 87 total
topics and suggested a topic name, which was then discussed and a final name agreed upon by
consensus.
Identifying themes. Due to the relatively high number of topics, we then sought to identify
themes comprising multiple topics to compare the two datasets. The identification of themes was
the result of three authors reviewing the results from the topic modeling with respectively 40 and 47
topics including the names identified in the earlier step, and then each suggesting potential themes
and assigning topics to those themes independently. In a second step, the same three authors then
collectively reviewed the theme suggestions for each of the two datasets and arrived at a set of ten
themes that, to a satisfying degree, covered the topics identified for both datasets. (See
Sentiment analysis
Valence Aware Dictionary and sEntiment Reasoner (VADER) is a rule-based model for sentiment
analysis of social media text46. VADER is comparable in structure to previous dictionaries such as
the Linguistic Inquiry Word Count (LIWC). However, it is more attuned to microblog-like contexts
such as Twitter. VADER relies on five heuristics or generalizable rules that tend to be
conventionally present when expressing or conveying sentiment intensity: (1) the use of an
exclamation point to express intensity, (2) capitalization, (3) degree modifiers (i.e. the use of an
adverb such as “very”, “extremely”, “marginally” etc.), (4) contrastive conjunction (for example,
“the food is great but the service is terrible”), and (5) examining the tri-gram preceding a
“sentiment-laden lexical feature”, which may alter the sentiment of the text (for example, in the
phrase “The food here isn’t really all that great,” the sentiment-laden lexical feature is “isn’t really
all that great”, and the three words preceding it are “The food here”). In the case where a contrastive
conjunction is observed, weight is given to the statement following the conjunction. Thus, in the
example above “the service is terrible” outweighs “the food is great”. VADER relies on over 7,500
scored lexical features validated by independent human raters.
The model assigns text with a compound score between -1 (extremely negative), to 1
(extremely positive). This score is calculated as the sum of the rating on each “lexical feature”
(rated between -4 to 4), which is then normalized to be between -1 and 1. Hutto and Gilbert38 have
provided the following guideline thresholds for classifying sentences a compound score: greater
than or equal to 0.05 is assigned a positive sentiment; a compound score between -0.05 and 0.05 is
assigned a neutral sentiment; a compound score less than -0.05 is assigned a negative sentiment.
Moralization
The Moral Foundations Dictionary 2.0 (MFD) is a dictionary developed based on Moral
Foundations Theory (MFT) which identifies five core dimensions, or “foundations” of morality:
care, fairness, authority, loyalty and sanctity25. The MFD 2.0 dictionary47 was built in collaboration
with the developers of MFT categorizing prototypical moralized words under each foundation.
Within each foundation, words are further classified as representing a “virtue” or a “vice” of a
specific foundation. A “virtue” signifies an endorsement of the foundation, and a “vice” signifies a
violation of the foundation. For example, the care foundation is concerned with the avoidance of
suffering. Hence, the dictionary lists contain words such as kindness or empathy under the “care
virtue” foundation and words such as suffer or cruel under the “care vice” foundation. In the MFD
2.0, each foundation’s vice or virtue dictionary contains 210 words, on average.
To infer the moral language present in each tweet, the model calculates the mean rate of words
belonging to each foundation. For example, on average, each tweet from the ‘general public’ dataset
contains 0.28 words associated with endorsing the loyalty foundation. Similarly, each tweet is also
assigned an aggregate ‘virtue’ and ‘vice’ score across all foundations. The ‘virtue’ score is
calculated as the mean rate of words from the “virtue” dictionaries of all foundations, while the
‘vice’ score is calculated as the mean rate of words from the “vice” dictionaries across all
foundations.
Effectiveness
Linear mixed-effects models were used to analyze the effect of three types of lexical features on
tweet effectiveness, operationalized here as retweets. For all models: (1) we use the R package lmer
function, (2) the dependent variable is the log-transformed retweet count, and (3) the model allows
for random effects at the Twitter user level. The latter consideration was due to the hierarchical
structure of the Twitter data. Since the data is at the tweet level, a single user may appear multiple
times. Therefore, we account for within-user variation in the model.
To analyze the effectiveness of the different dimensions, we computed five models: model 1
regresses the score of each tweet on 10 identified themes on retweet counts, model 2 regresses the
sentiment scores of each tweet (positive and negative) on retweet counts, model 3 regresses the
aggregate virtue and vice score across all moral foundations on retweet counts, i.e., whether a tweet
violates a moral foundation (vice) or upholds the moral foundations (virtue), model 4 regresses the
presence of each of the five moral foundations on retweet counts. An additional model 5 regresses
all of the lexical features (theme, sentiment, moral foundations) to test the effectiveness of the
variables in a complete model (see details on how each lexical feature was calculated above). Each
of these five models was run for both the public and climate scientists’ datasets and the combined
dataset (See Fig. 3-6, Supplementary materials), resulting in a total of nine models.
Acknowledgments
This work was supported by the Carlsberg Foundation, grant number CF22-1059 (to
K.S.N).
Author contributions
D.L., K.S.N, and K.A.N. designed the research with support from W.H. and J.R.. D.L.
analyzed the data with support from K.S.N., W.H., K.A.N., T.H., and J.H.. D.L., K.S.N.,
J.R., and K.A.N. wrote the original draft. W.H., T.H., and S.L. reviewed and edited the
manuscript.
Competing interests
The authors declare no competing interests