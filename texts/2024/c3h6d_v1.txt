HEART RATE AND ATTENTION IN SPEECH PROCESSING 1
Evaluating the Feasibility of Using Heart Rate to Measure Auditory Attention Allocation
During Spoken Language Processing
1,2 3,4 1,2
Wenfu Bao , Alejandro Pérez , and Monika Molnar
Department of Speech-Language Pathology, University of Toronto
Rehabilitation Sciences Institute, University of Toronto
Department of Psychology, Neuroscience & Behaviour, McMaster University
University Institute of Neuroscience (IUNE), University of La Laguna
Author Note
We have no known conflict of interest to disclose. This work was supported by the
Natural Sciences and Engineering Research Council of Canada awarded to M. M. (RGPIN-2019-
06523).
Correspondence concerning this article should be addressed to Wenfu Bao, Department
of Speech-Language Pathology, University of Toronto, 500 University Avenue, Toronto,
Ontario, M5G 1V7, Canada. Email: wenfu.bao@mail.utoronto.ca.
HEART RATE AND ATTENTION IN SPEECH PROCESSING 2
Abstract
A deceleration in heart rate (HR), along with a lengthening of the time between heartbeats, has
been associated with attentional engagement. Here, we investigated the feasibility of using
changes in HR to estimate attentional engagement during spoken language processing. Prior
fMRI evidence suggests that speech processing of an unknown language is more cognitively
demanding; we thus designed an experiment to replicate this finding through HR. In an active
listening task, we measured cardiac responses in 66 native English speakers (34 monolingual, 32
simultaneous bilingual), who listened to spoken passages in two conditions: one in a familiar
language and the other unfamiliar. Results demonstrated significant condition effects on
participants’ BPM (beats per minute) and IBI (interbeat interval). Listening to an unfamiliar
language induced significantly lower BPM and a trend of longer IBI, particularly during the first
five out of nine trials. Our finding aligns with previous neuroimaging evidence that processing
an unfamiliar language demands more attention than a familiar one. Our analysis also revealed
that the effects were independent of participants’ bilingual experience or language and cognitive
abilities. Overall, our results indicate that HR measurement has a potential in psycholinguistic
and cognitive research.
Keywords: heart rate, attention allocation, spoken language processing, BPM, IBI
HEART RATE AND ATTENTION IN SPEECH PROCESSING 3
Evaluating the Feasibility of Using Heart Rate to Measure Auditory Attention Allocation
During Spoken Language Processing
1. Introduction
Heart rate (HR) describes the rhythm of the heart and is usually quantified as the number
of heartbeats within a time unit (e.g., beats per minute or BPM). It can be derived from an
electrocardiogram (ECG), which also provides metrics such as the interbeat interval (IBI)—the
temporal space between successive heartbeats measured in milliseconds. As a physiological
measure, HR has been considered to indicate cognitive engagement, specifically related to
attention allocation (Keene et al., 2017; Lang, 2009; Ravaja, 2004; Richards & Casey, 1991),
which can be conceptualized as the amount of cognitive resources allocated during a specific
task (Anderson, 2020; Basil, 1994; Kahneman, 1973). Counterintuitively, HR decelerates as
attentional demands increase. This has been observed across visual and auditory domains (e.g.,
Coles & Duncan-Johnson, 1975; Lacey, 1972; Sroufe, 1971), and various evidence suggests that
decreased BPM along with increased IBI is associated with greater attention allocation (e.g.,
Adamowicz & Gibson, 1972; Backs & Seljos, 1994; Keene et al., 2017; Lang, 2009).
Nevertheless, the extent to which HR measurement reflects attention allocation during
linguistic auditory processing, particularly in response to spoken language, remains uncertain. A
large amount of auditory research has characterized HR deceleration as an orienting response,
especially to non-linguistic signals over a short period of time (i.e., around six seconds; Keene et
al., 2017); much less work addresses cardiac responses to spoken language. Among the few
studies with linguistic input, a common implication is that cardiac deceleration is modulated by
the stimulus property: stronger deceleration is associated with aversive or demanding stimuli.
For instance, Ilves and Surakka (2012) presented participants with spoken words that were
HEART RATE AND ATTENTION IN SPEECH PROCESSING 4
emotionally charged. They found that HR deceleration five seconds from the stimulus offset was
largest to the negative words compared to neutral and positive ones. Focusing on processing
demands, Francis et al. (2016) tested spoken sentences (duration: about three seconds) that were
masked by noise or two-talker babble. Results revealed that listening to these masked samples
induced significantly greater HR decrease than did unmasked speech, which linked more
demanding tasks to stronger cardiac reactivity. With more prolonged spoken passages (duration:
50–60 seconds), Guntupalli et al. (2006) played stuttered and fluent speech to fluent listeners.
When analyzing a 30-second period amid the stimulus presentation, they observed a significant
cardiac deceleration during stuttered samples relative to the fluent ones as the former possibly
required more attention.
Interestingly, cardiac responses to speech stimuli have been studied more with infants
than with adults. For example, Trehub and Curran (1979) examined the relationship between
speech complexity and habituation rate in 4.5–5.5-month-olds, while infants were listening to
speech over a longer period (i.e., 30–40 seconds per trial). They found that infants’ HR response
habituated (i.e., decreased) most rapidly to a synthetic syllable that remained constant across
trials, and least rapidly to speech syllables that changed from trial to trial. It seemed that the
more complex the stimulus, the longer the HR deceleration; as such, the longer the infant would
attend (Trehub & Curran, 1979). With 9-month-old infants, Santesso et al. (2007) recorded
cardiac responses to maternal infant-directed speech (IDS) expressing comfort, surprise, or fear,
and observed HR deceleration to all speech samples independent of affective content. They
speculated that IDS regardless of valence was attentionally engaging, which confirmed HR
deceleration in response to attentional load (Lacey, 1967; Santesso et al., 2007). Endevelt-
Shapira et al. (2024) found that among 3-month-old infants, more IDS exposure at home was
HEART RATE AND ATTENTION IN SPEECH PROCESSING 5
associated with lower infant HR during mother-infant free play interactions in the laboratory.
They proposed that experience with IDS may tune infants’ attention to speech (Endevelt-Shapira
et al., 2024). Overall, infant literature suggests that HR deceleration during speech processing or
interactions is related to increased attention capacities.
HR may reflect cognitive processes as there is a direct relationship between the heart and
the brain. The heart-brain connection can be approached from a one-way perspective that focuses
on the heart’s responses to the brain’s commands (McCraty, 2015). For example, the brain
directly controls the heart through the sympathetic and parasympathetic branches of the
autonomic nervous system, in which the sympathetic innervation increases HR during the “fight
or flight response,” whereas the parasympathetic activation decreases HR to rest and digest
(McCarty, 2016; Silvani et al., 2016). Yet, the heart-brain communication has also been
suggested to be bidirectional, with the heart sending even more information to the brain than the
other way around (McCraty, 2015). Specifically, the heart has a complex neural network, known
as the intrinsic cardiac nervous system (ICNS), which can impact cognitive functions
independently of the cranial brain (Armour, 2004; McCraty, 2015; Schandry & Montoya, 1996).
While the exact nature of the influence of ICNS is unclear, measuring HR can shed light on how
the entire human nervous system is at play in cognitive processes, including spoken language
processing.
The overarching goal of this study was to investigate the feasibility of using HR to
measure auditory attention allocation during spoken language processing. Specifically, we had
two aims. The first aim was to test whether different degrees of auditory attention allocation can
be indicated in cardiac responses to a familiar vs. unfamiliar language. To do so, we considered
prior research that employed other methods to gauge attention allocated in speech processing.
HEART RATE AND ATTENTION IN SPEECH PROCESSING 6
Functional magnetic resonance imaging (fMRI) studies have shown that an unfamiliar language
is more cognitively demanding to process than a familiar one (i.e., native speech, e.g., Cotosck et
al., 2021; Hernández et al., 2019). For instance, in an auditory word-monitoring task, Cotosck et
al. (2021) instructed Brazilian young adults to respond to a target word while listening to a
continuous story (duration: 30 seconds) in either their native language Portuguese or an
unfamiliar language Japanese. Domain-general brain networks associated with greater cognitive
challenge were activated only in unfamiliar language processing, which involved more demands
for sustained attention and working memory. Similar results were obtained in Hernández et al.
(2019) with Spanish monolinguals, who strongly recruited brain areas indexing cognitive effort
when watching movie clips dubbed in unknown Dutch than in their native language.
Built on these fMRI findings (Cotosck et al., 2021; Hernández et al., 2019), we adapted
the task paradigm for measuring HR responses. Specifically, we designed an active listening
experiment, where native English speakers attentively listened to spoken passages in two
separate blocks corresponding to the two experimental conditions: one in a familiar language
(English) and one in an unfamiliar language (Hebrew). Distinct from Cotosck et al. (2021) who
alternated between native and unknown language conditions in consecutive trials, we minimized
the resulting language discrimination effects by arranging a number of trials with each condition
so that each language can be processed in relative isolation. Further, instead of presenting target
words before the speech stream as in Cotosck et al. (2021), our paradigm recorded cardiac
responses to natural speech in a “free listening” scenario, which required sustained attention
throughout the trial without participants knowing and attending to certain specific word or sound
a priori. In the light of the fMRI evidence (Cotosck et al., 2021; Hernández et al., 2019) and the
correlations between attention and HR (Keene et al., 2017), we hypothesized that participants
HEART RATE AND ATTENTION IN SPEECH PROCESSING 7
would have lower BPM and longer IBI when attending to the unfamiliar language than the
familiar one.
Considering the feasibility nature of our study, the second aim was to determine a
suitable number of trials to record the potential effects of language familiarity on HR. Therefore,
we conducted an exploratory analysis of participants’ HR data, by both halving each condition
and analyzing it altogether. As we were interested in cardiac responses over a longer period, we
took a similar approach to Guntupalli et al. (2006) by averaging HR metrics in around 30-
second-long trials within each condition. Moreover, instead of testing a homogeneous sample (as
in Cotosck et al., 2021 and Hernández et al., 2019), we recruited linguistically diverse
individuals and accounted for their variations in cognitive and linguistic abilities. Specifically,
we tested native English speakers who were either monolingual or simultaneous bilingual (i.e.,
learned both languages before the age of three), while controlling their age and socioeconomic
status (SES) background. As bilingualism may enhance attentional control (Bialystok & Craik,
2022), we anticipated an interaction between experimental conditions and language groups
together with bilingual experience related measures (e.g., English exposure, length of stay in an
English-speaking environment, the number of languages known). For example, since bilinguals
have less English exposure than monolinguals, it is possible that the bilingual group would
allocate more attention when processing the familiar language, as reflected by lower BPM and
longer IBI. In addition, we predicted that participants’ cognitive and language abilities may
modulate their cardiac responses. Yet, given mixed findings about the relationships between HR
changes and cognitive/linguistic abilities (e.g., Backs & Seljos, 1994; Ginty et al., 2011a, 2011b;
Wright et al., 2005), we did not have a more directional prediction.
In sum, our study sought to address whether HR can reliably index auditory attention
HEART RATE AND ATTENTION IN SPEECH PROCESSING 8
allocated during spoken language processing in adults. Answering this question can be
potentially relevant to a wide range of fields, considering that HR is a cost- and time-efficient
physiological measure of attention. As an alternative tool to understand cognitive and language
processing, HR has many methodological advantages. Apart from being non-invasive, it is
relatively inexpensive and easy to acquire data. It also has strong compatibility with additional
psychophysiological techniques, such as eye-tracking and electroencephalography (EEG).
Especially given the recent advancement of wearable devices, there have been increasingly more
methods to measure HR accurately in laboratory-based settings (Fuller et al., 2020).
2. Methods
2.1 Participants
Seventy-three young adults participated in this study at the University of Toronto,
Ontario, Canada. They were mostly undergraduate or graduate students, except eight were
university alumni or staff. Participants were eligible if they were: (i) aged between 18–25 years
and born in or immigrated to Canada before the age of three; (ii) a native English speaker who
was either monolingual or simultaneous bilingual without Hebrew (i.e., the unfamiliar language
in the experiment) knowledge; (iii) having normal hearing and normal or corrected-to-normal
vision. An individual’s language profile was evaluated using the adapted Language Experience
and Proficiency Questionnaire (LEAP-Q; Marian et al., 2007), and a diverse heritage language
provided written informed consent and were compensated a small amount of money for
participation. The project was reviewed and approved by the University of Toronto Research
Ethics Board.
HEART RATE AND ATTENTION IN SPEECH PROCESSING 9
Sixty-six participants (34 monolingual, 32 bilingual) were included in the final analysis,
as seven were excluded due to missing data in > 50% of trials caused by technical error. Despite
no a priori power analysis being conducted, this sample size was comparable to previous
characteristics. As expected, compared to monolinguals, bilinguals had significantly less English
exposure (p < 0.001) along with shorter length of residence in an English-speaking family (p <
0.01) and knew more languages (p < 0.001). Note that some monolinguals may know another
language, but we only included those who had < 30% passive exposure to it. In addition, the two
groups showed comparable SES background (scored by combining highest level of education
and family or personal income), language abilities (measured via Clinical Evaluation of
Language Fundamentals Fifth Edition or CELF-5; Wiig et al., 2013), and cognitive skills
(measured via Test of Nonverbal Intelligence Fourth Edition or TONI-4; Brown et al., 2010).
Total Monolingual Bilingual
Sample size 66 34 32
Sex distribution (M:F) 30:36 14:20 16:16
Age (in years) 20.71 (2.19) 20.94 (2.01) 20.47 (2.37)
English exposure *** 0.84 (0.18) 0.97 (0.06) 0.7 (0.15)
Years spent in an English-speaking family ** 18.42 (6.3) 20.79 (2.03) 15.91 (8.13)
Years spent in an English-speaking
18.55 (3.05) 18.71 (2.7) 18.38 (3.42)
school/workplace
Number of languages known *** 1.96 (0.99) 1.27 (0.57) 2.69 (0.82)
SES score 2.65 (1.26) 2.88 (1.37) 2.41 (1.1)
CELF-5 score 10.96 (1.62) 10.68 (1.74) 11.25 (1.46)
HEART RATE AND ATTENTION IN SPEECH PROCESSING 10
TONI-4 score 105.53 (8.67) 104.24 (7.76) 106.91 (9.47)
Note. Values from the age row are means (Standard Deviation or SD). Asterisks indicate significant
group comparisons from t-tests (***p < 0.001, **p < 0.01).
2.2 Task and stimuli
1(A), the task started with presenting participants with a spoken passage auditorily for about 30
seconds. After a brief interval, there was a question asking participants whether they had heard a
word within the passage (e.g., “Did you hear the word ‘coffee’?”). Then, participants were given
ten seconds to make a response by pressing a key on the keyboard. To familiarize participants
with the task, there were two practice trials (one played in English and the other in Hebrew) prior
to the following 18 study trials. These study trials were presented in two separate blocks, each
comprising nine randomized trials for an individual condition (i.e., familiar language vs.
unfamiliar language). The presentation order of the two blocks (or conditions) was also
randomized across participants.
The speech stimuli consisted of 20 spoken passages (10 English, 10 Hebrew; mean
duration: 29.92 seconds, range 28.52–30.76 seconds) that were city descriptions excerpted from
travel guides. They were recorded by three highly proficient Hebrew-English female bilinguals,
and then processed (e.g., reducing noise, normalizing intensity) in Adobe Audition. Two passages
were used in the practice trials, and the rest all in the study trials. Given the shortest passage
duration, we used 28 seconds as the stimulus offset across trials in data analysis. Further,
comprehension questions were recorded by another female native English speaker.
2.3 Procedure and apparatus
HEART RATE AND ATTENTION IN SPEECH PROCESSING 11
Prior to the experiment, participants filled out an online language and SES background
questionnaire (adapted from the LEAP-Q; Marian et al., 2007). The language section assessed the
participant’s acquisition history, current exposure, and proficiency level of each language they
knew; the SES section assessed their highest level of education and family or personal income
(dependent on whether they received financial support from family).
The experiment was programmed with the SR Research Experiment Builder software
(v2.3.1) and conducted in a sound attenuated room. Participants sat comfortably in front of a 24-
inch DELL monitor and spent about 15 minutes on the active listening task. At a sampling rate of
1,000 Hz, participants’ ECG was measured from three electrodes that were connected to the
actiCHamp Plus amplifier (Brain Products, Germany) via a BIP2AUX adapter (Brain Products,
Germany). In terms of electrode placement, a wrist configuration was used in the first ten
participants. Following Pavlov et al. (2022), the active electrode was placed on the right wrist,
the negative on the left wrist, and the ground on the left inner forearm. However, this wrist
configuration was found nonoptimal because the motor response of key press may compromise
participants. According to the Einthoven’s triangle, the active electrode was placed on the lower
chest, the negative electrode on the right shoulder, and the ground on the left shoulder. With this
new configuration, we achieved less data loss (i.e., fewer trials with undetected heartbeats). Note
that irrespective of the skin area, this change in electrode placement would not affect the two
basic cardiac metrics that we analyzed (i.e., BPM and IBI).
After the experiment, participants were administered CELF-5 and TONI-4 to measure
their language and cognitive abilities, which took about 45 minutes together. Specifically, three
subtests of CELF-5 that focus on expressive language abilities were used: Recalling Sentence,
HEART RATE AND ATTENTION IN SPEECH PROCESSING 12
Sentence Assembly, and Formulated Sentence. Raw subtest scores were converted to scaled
scores based on the participant’s age and averaged to obtain an overall CELF-5 score. Similarly,
raw TONI-4 scores were converted to index scores.
listened to a passage, spoken in a familiar or unfamiliar language, for around 30 seconds. After a
brief interval, they were asked whether they had heard a word in the passage. Then, they had 10
seconds to respond by pressing a key on the keyboard. B: Recording setup. During the task,
participants listened to the passages via headphones, while their HR was recorded through three
electrodes (ground, active, and negative) placed on the chest. Electrodes were connected to the
amplifier via an adapter; ECG was shown on the control computer.
2.4 Statistical modeling and analysis
HR measures were extracted using the EEG-Beats Matlab toolbox (Thanapaisal et al.,
2020), which is an automated algorithm that detects heartbeats and calculates IBI from ECG
signals recorded with an EEG sensor. To test whether the number of trials (or the amount of time
spent listening to the stimuli) would interact with the results, we visualized HR data by trial and
observed more apparent differences between conditions in the first five study trials than the last
HEART RATE AND ATTENTION IN SPEECH PROCESSING 13
four (see Supplementary Figures 1 and 2 for BPM and IBI, respectively). As a result, we
conducted three sets of exploratory analyses by focusing on: the first half of each condition (i.e.,
the first five trials, or 140-second listening time), the second half of each condition (i.e., the last
four trials, or 112-second listening time), and the entire condition (i.e., all nine trials, or 252-
second listening time). Specifically, we built linear mixed-effects models using the lme4 package
(v1.1-35.1; Bates et al., 2015). The outcome variables included BPM and IBI, measured while
the spoken passages were presented (0 to 28s, when stimuli terminated at the earliest) and
averaged across trials. For fixed effects, the predictors consisted of Condition (categorical:
familiar language vs. unfamiliar language), Language Group (categorical: monolingual vs.
Exposure, Years Spent in an English-Speaking Family, and Number of Languages Known, and
three control variables: TONI-4 score, CELF-5 score, and SES score. For random effects, a
random intercept for Subject was included and a random slope for Condition was tested.
In a stepwise forward fashion, fixed and random effects were fitted separately. Estimated
p-values for model fitting and comparison were obtained using the lmerTest package (v3.1-3;
Kuznetsova et al., 2017). To guard against Type I error, following Luke (2017), significance of
the fixed effects was evaluated by comparing models that were fitted using the restricted
maximum likelihood (REML) method and that derived p-values using the Satterthwaite
approximation via the anova function. Specifically, based on the null model with a random
intercept for participants, a predictor was included only when the resulting p-value was smaller
than 0.05 and its associated Akaike Information Criterion (AIC) value was smaller; otherwise,
the simpler model was favored (Matuschek et al., 2017).
3. Results
HEART RATE AND ATTENTION IN SPEECH PROCESSING 14
3.1 BPM
We first analyzed BPM in the first half of each condition, which contained 660 data
points in total (66 participants x 5 trials x 2 conditions). While fitting linear mixed-effects
models, model comparisons suggested a significant effect of Condition (p < 0.001) but other
suggests that unfamiliar language processing requires more attention.
halves of each condition, as well as the entire condition. P-values were obtained from model
comparisons.
First half of Second half of
Entire condition
each condition each condition
Condition p < 0.001 p = 0.579 p = 0.001
Language Group p = 0.575 p = 0.745 p = 0.647
English Exposure p = 0.491 p = 0.579 p = 0.528
Years Spent in an English-Speaking Family p = 0.716 p = 0.771 p = 0.739
Number of Languages Known p = 0.467 p = 0.692 p = 0.56
TONI-4 Score p = 0.8 p = 0.991 p = 0.884
CELF-5 Score p = 0.211 p = 0.158 p = 0.185
SES Score p = 0.64 p = 0.694 p = 0.662
Language Group * Condition p = 0.637 NA p = 0.766
English Exposure * Condition p = 0.113 NA p = 0.253
Years Spent in an English-Speaking Family *
p = 0.677 NA p = 0.62
Condition
Note. An asterisk indicates the interaction between two variables. If there were no significant condition
HEART RATE AND ATTENTION IN SPEECH PROCESSING 15
effects, its interaction with other variables was not further analyzed (hence NA).
Then we analyzed BPM in the second half of each condition, consisting of 528 data
points in total (66 participants x 4 trials x 2 conditions). However, linear mixed-effects model
continued to analyze the entire condition, which contained 1,188 data points (66 participants x 9
trials x 2 conditions). Similarly, there was a significant effect of Condition (p < 0.001), with
that the unfamiliar language condition had fewer heartbeats than the familiar language condition
that processing an unknown language is more attention demanding, which is particularly
pronounced in the first five trials.
axis in the right panel) when analyzing the first half of each condition or block (i.e., the initial
five trials, or 140-second listening time). The length of the box within the violin plot indicates
HEART RATE AND ATTENTION IN SPEECH PROCESSING 16
the interquartile range. Red horizontal lines indicate mean values across trials. In the left panel,
model summary suggested that the unfamiliar language condition had significantly lower BPM
than the familiar language condition (p = 0.039). In the right panel, model summary suggested a
trend that the unfamiliar language condition had longer IBIs than the familiar language condition
(p = 0.073).
3.2 IBI
Likewise, IBI in the first half of the two conditions consisted of 660 data points. Model
Consistent with the BPM results above, this finding also suggests that unfamiliar language
processing tends to require more attention.
of each condition, as well as the entire condition. P-values were obtained from model
comparisons.
First half of Second half of
Entire condition
each condition each condition
Condition p = 0.001 p = 0.455 p = 0.003
Language Group p = 0.515 p = 0.737 p = 0.606
English Exposure p = 0.482 p = 0.621 p = 0.539
Years Spent in an English-Speaking Family p = 0.578 p = 0.773 p = 0.659
Number of Languages Known p = 0.387 p = 0.608 p = 0.475
TONI-4 Score p = 0.992 p = 0.835 p = 0.932
CELF-5 Score p = 0.266 p = 0.165 p = 0.215
SES Score p = 0.701 p = 0.765 p = 0.727
Language Group * Condition p = 0.656 NA p = 0.64
HEART RATE AND ATTENTION IN SPEECH PROCESSING 17
English Exposure * Condition p = 0.103 NA p = 0.226
Years Spent in an English-Speaking Family *
p = 0.65 NA p = 0.09
Condition
Note. An asterisk indicates the interaction between two variables. If there were no significant condition
effects, its interaction with other variables was not further analyzed (hence NA).
When analyzing IBI in the second half of each condition, we found none of the predictors
too. According to the optimal model summary, the unfamiliar language condition had longer
IBIs than the familiar language condition, but their difference was not significant (p = 0.125; see
first five trials across BPM and IBI.
4. Discussion
In this study, we investigated whether it is feasible to measure auditory attention
allocation via HR responses in spoken language processing, among native English-speaking
adults from a linguistically diverse sample. Through an active listening task, we presented
monolinguals and simultaneous bilinguals with spoken passages in two separate blocks that
corresponded to the two experimental conditions: one in a familiar language and one unfamiliar.
To explore a feasible number of trials for recording HR effects, we both halved each condition
and considered its entirety when analyzing various factors on participants’ BPM and IBI.
Particularly during the first five out of the nine trials, we observed significant condition effects
on both metrics. Specifically, the unfamiliar language condition induced significantly lower
BPM than the familiar language condition, and the former also tended to elicit longer IBI. This
result suggests that unfamiliar language processing potentially requires more attention. It also
HEART RATE AND ATTENTION IN SPEECH PROCESSING 18
provides novel physiological evidence to previous neuroimaging data (e.g., Cotosck et al., 2021;
Hernández et al., 2019) that processing an unknown language is more cognitively demanding.
Critically, our study examined participants’ cardiac responses within the context of
spoken language processing, which sets itself apart from previous research in several aspects.
First, different from prior studies that primarily used non-linguistic stimuli, we were interested in
naturalistic speech. Our results have confirmed the relationship between cardiac deceleration and
stimulus properties: more demanding stimuli lead to stronger deceleration (also see Guntupalli et
al., 2006). Second, we addressed individual difference effects by evaluating participants’
linguistic experience (e.g., degree of bilingualism). However, we did not observe differences in
cardiac responses between monolinguals and bilinguals, nor did we find interactions between
experimental conditions and language groups or bilingual variables. This could be partially
attributed to the linguistic and age profiles of our participants: monolinguals and simultaneous
bilinguals with comparable English proficiency; young adults who were mostly university
students. In addition, we investigated individual variations in cognitive and linguistic abilities,
measured by tests of nonverbal intelligence and English expressive language, respectively. Yet,
no relations were found between participants’ cardiac responses and these ability measures.
Although different from the results of Ginty et al. (2011a, 2011b) reporting a positive association
between BPM and cognitive ability in a substantial sample (from hundreds to thousands of
participants) across age cohorts, our finding is consistent with the results of Backs and Seljos
(1994) and Wright et al. (2005) who observed no association between BPM or IBI and cognitive
functions. Though it should be noted that these studies, including ours, employed different tests
or tasks to assess cognitive ability, analyzed different HR parameters, and tested different
samples.
HEART RATE AND ATTENTION IN SPEECH PROCESSING 19
Nevertheless, our results are subject to certain limitations. First, our bilingual sample
consisted of simultaneous bilingual young adults, who had similar age of English acquisition and
proficiency as their monolingual counterparts; thus, our findings might not be generalized to all
bilinguals. Future research could test (i) other types of bilinguals, such as those learned English
later in the childhood or in adulthood, as they clearly contrast with monolingual peers in terms of
English proficiency; (ii) participants from other age groups, such as children or older adults who
presumably have lower attentional control abilities than young adults, in that it is more likely to
show cognitive differences between language groups in those populations (Bialystok & Craik,
2022). Moreover, our sample size may be too limited to observe effects related to bilingual
experience and individual differences in cognitive/linguistic abilities. A natural progression of
this work is to recruit a much larger sample with at least hundreds of participants (e.g., Ginty et
al., 2011b) to better understand the relationship between participant characteristics and cardiac
responses. Another limitation could lie within our experimental design. For example, through
exploratory analyses, we discovered that five trials (about 140 seconds) might be more sensitive
to our experimental condition than more trials (e.g., nine trials, or about 252 seconds) (also see
Guntupalli et al., 2006). Yet, more research is required to validate and improve our design details
(e.g., length of stimuli, number of trials and conditions) and to further determine the extent to
which HR can reliably index attention in speech processing. Future work could also develop
approaches for time-course analyses that may capture the cardiac dynamics during attention
allocation.
5. Conclusions
The present study evaluated the feasibility of using HR to measure auditory attention
allocation during adult spoken language processing, while experimental conditions were
HEART RATE AND ATTENTION IN SPEECH PROCESSING 20
manipulated by varying language familiarity. Results indicated significant conditions effects on
participants’ BPM and IBI, which were particularly pronounced at the beginning of each
condition. Significantly lower BPM (i.e., cardiac deceleration) along with a trend of longer IBI
was observed when participants actively listened to an unfamiliar language relative to listening
to a language that they already knew. This finding demonstrates that speech processing of an
unknown language requires more attention, and it confirms earlier neuroimaging evidence that
unfamiliar speech processing is more cognitively demanding. Therefore, HR appears to be a
valuable method to measure cognitive resources allocated during language processing, and it
deserves more consideration in exploring further questions in linguistic and cognitive research.
Despite its exploratory nature, our study has provided fresh insight into the application of HR in
speech processing from a physiological perspective—focusing on the nervous system beyond the
brain.
Supplementary material and data availability: The supplementary material, open data, and