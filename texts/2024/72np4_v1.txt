Eye gaze during route learning in a virtual task
1,2 3 1,2 1,2
Elizabeth H. Hall , Martha R. Forloines , John M. Henderson , Joy J. Geng
Center for Mind and Brain, University of California, Davis
267 Cousteau Place 267, Davis, CA 95618 USA
Department of Psychology, University of California, Davis
1 Shields Ave, Davis, CA 95616 USA
Department of Neurology, University of California, Davis
4800 Y Street, Sacramento, CA 95817 USA
*Address correspondence to:
Elizabeth H. Hall, ehhall@ucdavis.edu
Joy J. Geng, jgeng@ucdavis.edu
Abstract
Looking behavior during route learning is uniquely attuned to information relevant for
navigation, which unfolds continuously over time. To better understand how visual information
is used to learn routes, we tracked eye-position in participants on a guided walk through a
simulated urban environment. The route was presented twice to participants in either a natural
sequential order or a randomly scrambled order. After viewing the route, participants were
presented with images of the intersections and asked to report the correct direction to continue
travel along the route. Predictably, the sequential group performed much better on memory for
the route than the scrambled group. Analyses of the eye-data revealed reliable differences
between groups during route learning but no differences during scene viewing in the memory test
trials. Specifically, while both groups looked at objects such as people and cars during route
learning, looking at the path ahead was highly predictive of being in the sequential group,
whereas looking at buildings ahead was predictive of being in the scrambled group. The results
suggest that route learning involves encoding, and anticipating, information about the path to-be-
traveled, perhaps reflecting formation of a temporally continuous representation of the route. In
contrast, those in the scrambled group appeared to rely on building landmarks to attempt to
create a map of the route, albeit unsuccessfully.
Keywords: route learning, navigation, predictive looking, eye movements, eye tracking
Introduction
Route learning is a ubiquitous skill among animals (Brown & Cook, 2006; Kelly &
Gibson, 2007), but the information used to learn and remember routes varies across species. For
example, birds can use an internal magnetic compass (Able & Able, 1990), ants have been
shown to count steps (Wittlinger, Wehner, & Wolf, 2006), and honeybees use landmarks to
guide navigation (Cartwright & Collett, 1982). In humans, route learning is based on spatial
representations in the hippocampus that are built using a network of brain regions receiving
multisensory inputs while they traverse the route (Maguire, et al., 2003; 2006; Moser, et al.,
2008; Schedlbauer, et al., 2014; Stachenfeld, et al., 2017; van Asselen, et al., 2006). These
studies have found that the spatial maps are built from a combination of allocentric and
egocentric information (Newcombe, 2018; Zhang, et al., 2012). Despite a rich literature on the
representation of spatial maps learned from navigation, relatively little is known about what
information is sampled through attention and eye-movements during route learning. Eye gaze
during route learning is important because fixations indicate what information is selected to build
mental representations of a route.
Detailed models of how eye gaze is controlled during free viewing and during task have
been developed within the scene viewing literature (Torralba, Olivia, Castlehano, & Henderson,
2006). Gaze allocation during free viewing of natural scenes is known to be biased towards
objects that inform the viewer about the context and gist of the scene (Peacock, Hayes, &
Henderson, 2019; Neider & Zelinsky, 2006; Resnick, O’Regan, & Clark, 1997), are visually
salient (Itti, Koch & Nieber, 1998), or have meaning (Henderson & Hayes, 2017; Hwang, Wang,
& Pomplun, 2011). However, when a task is introduced, looking behaviors tend to be driven by
specific information related to the goal. For example, when searching for a trashcan, participants
look at the bottom third of an office scene containing floorspace rather than the middle or top
portions (Castelhano & Krzys, 2020; Pereria and Castelhano, 2019; Torralba, Oliva, Castelhano,
& Henderson, 2006; see also Malcolm & Henderson, 2010). Likewise, search for a teddy bear in
a bedroom begins with eye-movements towards the bed whereas search for a computer in the
same room begins with looks to a desk (Võ, Boettcher, and Draschkow, 2019). These studies
demonstrate that learned expectations guide observer looking behaviors.
Similarly, task requirements drive gaze behavior in real world tasks such as object sorting
(Triesch, et al., 2003), sandwich making (Ballard, Hayhoe, & Pelz, 1995), driving (Shinoda,
Hayhoe, & Shrivastava, 2001), or walking (Turano, Geruschat, & Baker, 2003; for a review see
Henderson, 2017). In these tasks, the participant is an active agent within a continuous
environment and results show that eye-movements not only seek out task-relevant information
but do so in an anticipatory manner. For example, when making a sandwich, individuals look at
the peanut butter before actually reaching for the jar (Hayhoe, Shrivastava, Mruczek, & Pelz,
2003). When driving a car, drivers look toward the tangent of the curve to predict and prepare for
the upcoming turn (Land & Lee, 1994). Cricket batsmen look to where the ball will bounce just
prior to its arrival rather than tracking the ball from the bowler’s release (Land & McLeod,
2000). Finally, walkers told to turn left at a specific room will focus gaze largely on the left side
of the hallway, especially at doors, until the correct turn is detected. Together, these results
indicate that in real-world tasks, eye gaze reflects anticipatory information sampling: observers
selectively look at information that is necessary for active behavior in the next moment of time.
The pattern of looking during natural walking also depends on physical and cognitive
demands. For example, if the terrain is rocky, walkers look at the next immediate foothold,
indicating a need to extract more precise information to modify the current step; alternatively, if
the terrain is smooth, participants look farther ahead on the path or at environmental
surroundings (Hayhoe & Matthis, 2018). When learning a new route through a cityscape,
participants look selectively at landmarks located at intersections where turns are made. For
example, on a righthand turn, walkers trying to learn the route focus on the landmark on their
right side leading up to, and throughout, the turn. They appear to do this to encode the landmark
and its navigationally relevant position for future recall of the route. Participants that walked the
route without the intention of future recall, did not look at landmarks at intersections differently
based on whether a turn was made there or not (Wenczel, Hepperle & von Stülpnagel, 2017).
Landmarks are known to be particularly important for route learning; humans have been
shown to use landmarks in Morris Water Mazes (Bullens, et al., 2010), can triangulate precise
goal locations using only landmarks (Forloines, Bodily, & Sturz, 2015), and at their simplest can
aid in learning and recalling new environments (Spiers & Maguire, 2004; for a review of the
non-human animal literature on landmark learning see Brown & Cook, 2006; Kelly & Gibson,
2007) but there is still much unknown about how gaze can drive attention to gather information
that contributes to building memories for new routes. In this study, we asked what features of the
environment are looked at when learning a new route. To answer these questions, we asked
participants to view a route with the intention of encoding it for later recall by following a
“guide” through a virtual city via a series of screenshot images taken from Grand Theft Auto V
(Rockstar Games), an immersive 3D third-person perspective video game. We split participants
into two groups that differed in how the images were presented; either in natural temporal order
or in a scrambled order. Both groups viewed the route twice to provide two chances for learning.
We queried their knowledge of the direction to proceed along the route and their confidence in
that response by presenting a single image taken from each intersection.
We hypothesized that when provided with the temporal context of a continuous walk,
participants would encode more information about the spatial structure of the route, over and
above just the landmarks in a static scene, in order to create a spatial map. Consistent with the
findings from Hayhoe et al., (2003) and Land et al., (2000; 2004), we expected the ordered group
to look more at the path ahead, particularly on the second walk, in anticipation of where to go
next. Further, when provided with temporal context, we expected there to be more looks to
irrelevant, yet salient, features of the environment such as people and cars along the route. We
predicted that if participants are not provided with temporal context (i.e., view the route in a
scrambled order) they will employ an alternative learning strategy, relying on landmarks such as
the buildings directly ahead of the route to serve as the main source of information to build a
representation of the route (Wenczel, et al., 2017). Finally, we predicted that there would be
differences in the participants provided with temporal context based on the memory requirements
and location along the route.
Method
Participants. Sixty undergraduate participants (mean age = 20.25, SD = 1.74, 16 males)
from UC Davis volunteered for this study. Participants were compensated with course credit for
their participation. All participants provided informed consent in accordance with UC Davis’
Institutional Review Board.
Stimuli and Apparatus. Participants were seated in a sound attenuated and dimly lit room
in front of a 27-in Asus LCD monitor (2560 x 1440 pixels) with a refresh rate of 60 Hz.
Participants’ eyes were tracked throughout the entire experiment using an EyeLink 1000 Desktop
mount eye-tracker sampling at 500 Hz (SR Research, Mississuagua, Ontario, Canada). All
participants completed 13-point eye-tracking calibration, and participants’ eyes were recalibrated
between repetitions if needed. Recalibration was performed for all participants prior to the final
consisted of showing participants two repetitions of the route either in natural temporal order or
in a scrambled order followed by a memory test of the route.
Three identical routes were recorded from a path through the virtual city center from
Grand Theft Auto V (Rockstar Games). The path followed a guide (the player in the center
portion of the screen) through the city and crossed 10 intersections consisting of 3 right hand
turns, 3 left hand turns, and 4 straightaways. For 7 of these intersections, there were 3 potential
directions to travel (straight, left, and right) and for the remaining 3 intersections there were only
2 potential directions to travel (2 with only right and left, and 1 with only straight and right). The
only difference between the 3 routes was the time of day of the recordings. This ensured
participants would experience the route similarly to how it would be experienced in real life with
varying unstable features such as cars, bystanders, shadows, or sun-glares during each walk. As
such, participants could not rely on these inconsistent features to recall the route. Once the routes
were recorded, screenshots were taken at 1s intervals for use in the task from the first (569
images) and second (557 images) repetitions, while the third route screenshots were used for the
test images (10 images). The slight difference in the number of images in each repetition was due
to slightly different paths taken along the route (e.g., avoiding a bystander along the route), but
the overall route walked was identical. Each image was presented to participants for 1s with no
interstimulus interval for the most seamless image presentation possible. While viewing the two
repetitions, participants were not required to respond. If participants’ fixations moved off the
screen (e.g., they closed their eyes, looked away from the computer, etc.) the images paused until
panel for the trial progression of each group).
Memory test images were taken from a third time of day in the game and were composed
of 10 images that aligned most closely to the turning point of each intersection prior to any
Test images were presented to all participants in a random order and were shown for up to 2
minutes or until participants responded with which direction the route proceeded from that
location (straightaway, turn left, or turn right). Following each image participants were asked to
rate their confidence in the direction they chose. The confidence rating was on a sliding scale and
ranged from “not at all” to “very”.
Procedure. Participants were randomly split into two groups of 30, an ordered group and
a scrambled group. The ordered group viewed the images in a natural temporal order while the
participants were shown the route twice and were told to learn the route as best they could. For
each repetition, participants were shown the on-screen instructions presented below. The bolded
portion was added for the scrambled group and was not included for the ordered group. “You
will be shown images taken from a route through a virtual city, but the images will be
scrambled. Please pay close attention to the images you are shown because you will be asked
questions about the route at the conclusion of the experiment.”
Between repetitions, participants were told to close their eyes and rest for a minute. If
needed, their eyes were recalibrated. The instructions were shown again, and participants viewed
the second repetition. Following the second repetition, participants were again given a break and
recalibration was completed. They were then shown the following instructions. “Now you will
be shown a series of images. Each of these images is of an intersection from the route you just
learned. You will have to report which direction you should travel at each intersection to
continue along the route. Even though the images you saw were out of order, please try your
best to decide which direction you should go. Please press the arrow keys to input which
direction is the correct direction to continue along the route. Press LEFT if you should turn left.
Press RIGHT if you should turn right. Press UP if you should go straight. Following this, you
will be asked about your confidence in your answer. Use the ARROW KEYS to move on the
scale and ENTER to input your rating. You will only have 2 minutes to answer each question, so
please answer quickly, but try to be as accurate as possible.” The bolded portion was added for
the scrambled group and was not included for the ordered group. Participants made route
decisions for each test image and rated their confidence in their answer on a sliding scale rating
from 1 (not confident) – 10 (confident). Once they completed these test questions the experiment
was finished.
Data Analysis. The behavioral measure of recall via accuracy and confidence rating was
collected during the test trials. As the two repetitions involved only passive viewing, there was
no behavioral measure collected during the route learning portions of the experiment.
The eye data was the measure of interest during route learning in both groups and was
collected continuously throughout the experiment. The primary analyses of interest were based
on the proportion of fixation duration to each object of interest as a function of group. Fixations
shorter than 100ms were removed from the analyses as noise. Each image during route learning
was only presented for 1s with an average of 2.32 fixations per participant per image (SD = 1.18,
range = 1 to 10, median = 2).
Areas of interest (AOIs) were drawn for the 5 images prior to and the 5 images following
the intersection and for each test image, resulting in 11 images per intersection (110 intersection
of 10 sequential non-intersection images (30 images total) were taken from portions along the
route that contained no visible intersections. The non-intersection data was used as a comparison
to determine how gaze differs when participants are in vs. out of an intersection. AOIs were
determined a priori to be potentially relevant and were bounded by rectangles fitted to the height
and width of each object. The AOIs were created around the following in all images: the guide,
the building ahead, building signs, the path ahead, the path behind, the incorrect path, people,
and building signs were drawn over each object as wholly as possible, with a new AOI for each
object. The “path ahead” was defined as the area encompassing the to-be-walked path by the
guide. The “incorrect path” was defined similarly to the path ahead but covered the area the
guide would travel if they were to go along any of the alternate paths. The “buildings ahead”
were defined as the building directly in front of the guide. Fixations to areas not included in the
AOIs consisted of trees, the sky, distant buildings, etc. and were classified as “other”.
Each of the AOIs contained varying levels of information for the participants. For
example, the buildings ahead could help to unite the spatial representation and act as visual
landmarks during the recall test. Alternately, the vehicles and people varied across repetition and
as such carried little information value. For the majority of the intersection and non-intersection
images, all AOIs were included, however two intersections had no building signs, one
intersection had no incorrect path or building ahead. Of note, the non-intersection images did not
contain an incorrect path as there was only one direction the guide could potentially travel. For
the intersection memory test images, there were two intersections without a building ahead, two
intersections without other people, one without a building ahead or vehicles, and one without
building signs or a path behind.
Results
Behavioral Results. First, in order to determine if the ordered group indeed learned the
route better than the scrambled group, we compared performance on the memory test between
groups. We expected that performance would be much higher for the ordered group compared to
the scrambled group. A one-way ANOVA was conducted on the average accuracy during test
trials with group (ordered, scrambled) as a factor. Results revealed a main effect of group F (1,
58) = 88.845, p < .001, η = .605. Post hoc comparisons of group showed that the ordered group
(M = .84, SD = .125) chose the correct direction to go more often than the scrambled group (M =
group were performing to a level greater than chance, separate one-sample t-tests were
conducted. Of note, chance was based on the total number possible directions at that location.
For 7 of the 10 intersections, chance was set at .33, however, for three intersections chance was
.5 as there was no third path to choose. For the ordered group, all turns (ps < .05) except
intersection 9 had greater than chance performance (p > .05). The scrambled group only
performed above chance on 4 intersections (intersection, 4, 6, 7, 10). Of note, these intersections
on which the scrambled group performed above chance were all straightaways. To test if
participants in the scrambled group had a straightaway response bias (i.e., guessing
“straightaway” when they did not know), we completed a one-sample t-test comparing the
proportion of responses to each direction against chance (.33). There were more responses to the
straightaway direction than would be expected by chance alone t (9) = 3.264, p < .01. The
responses to the left and right direction were not greater than chance (ps > .05). Taken together,
the behavioral results from test trials confirm that participants in the ordered group were able to
learn the route well with only two walk repetitions, and the scrambled group was not.
A one-way ANOVA was conducted on the average confidence rating during test trials
with group (ordered, scrambled) as factors. Results revealed a marginal but significant main
effect of group F (1, 58) = 4.028, p < .05, η = .065. The average rating of the ordered group (M
= 7.767, SD = 1.412) was greater than that of the scrambled group (M = 7.05, SD = 1.354), p <
ratings being high regardless of performance accuracy. This is not unexpected given participants’
tendency to overestimate their performance in testing situations (Foster, Was, Dunlosky, &
Isaacson, 2017; Händel & Dresel, 2018; Rahnev, et al., 2020).
AOI Fixation Results. Fixations for each trial were transformed into the proportion of
total time spent fixating on each AOI, and then averaged across the 11 images surrounding the
intersection to create the average proportion of time spent fixating on each AOI. This duration
metric was used for all of the following fixation analyses. Our primary goal was to test our
hypothesis that the ordered group would show anticipatory gaze toward the path ahead (similar
to and Hayhoe et al., 2003; Land et al., 2000; 2004), particularly more so on the second
repetition when participants might have already learned parts of the route, and that the scrambled
group would attend more to landmarks (e.g., the building ahead, and building signs). In addition,
we tested for group differences in looking time to people and vehicles, which are known to draw
attention during free-viewing of static images. To do this, separate linear mixed effects
ANOVAs with subjects as a random effect were conducted on the proportion of fixation duration
on each of these select AOIs (building ahead, building signs, path ahead, people, vehicles), using
First, to test our main hypothesis that the ordered group would spend a greater proportion
of fixation time on the path ahead, we conducted a linear mixed effects ANOVA which revealed
a significant main effect of group F (1, 57.45) = 25.713, p < .001, but no effect of repetition F (1,
55.75) = 1.938, p > .05. There was however, an interaction between group and repetition F (1,
55.75) = 11.303, p < .01. As predicted, the ordered group fixated on the path ahead to a greater
extent than the scrambled group, t (57.454) = 5.071, p < .001. The interaction was driven by the
ordered group fixating on the path ahead more than the scrambled group in the second repetition
t (55.751) = -3.362, p < .01. This difference in each group’s fixations on the path ahead can be
was fixating the path ahead as a function of having learned the route (e.g., more fixations in the
second repetition than the first) or if they were fixating on the route as a function of learning the
route (e.g., no difference in fixations across repetitions) we separated the ordered group and
conducted a paired samples t-test on fixations by repetition in the ordered group only which
showed a difference between the first and second repetition t (13383) = -3.909, p < .001. In order
to gauge the strength of this difference, we conducted a Bayesian paired samples t-test. The
analysis revealed a strong difference between the repetitions with a BF of 20.185. The
proportion of fixation duration was shorter during the first repetition (M = .011, SD = .061, CI
[.010 - .012]) than in the second repetition (M = .014, SD = .069, CI [.013 - .015]). In order to
determine if these differences were simply due to the participant attending to the guide’s body
motion cues (i.e., physically turning) during the second half of the turn following pivot point, we
conducted a similar set of t-tests on the fixations during the first five images leading up to the
tangent and the five images following the tangent. The paired samples t-test revealed a
significant difference in these two image sets, t (9197) = -2.580, p < .05, however the Bayesian
paired samples t-test revealed this difference was not reliable with a BF of .327, suggesting this
result should be interpreted with caution. Fixations during the first five images (M = .009, SD =
.056, CI [.008 - .010]) were not reliably different from the second five images (M = .011, SD =
.063, CI [.010 - .013]). Combined these results suggests that the anticipatory gaze in the ordered
group is a function of learning the route as much as it is a function of having learned the route.
We found differences in fixations once the participants knew which direction the path would
continue (learned: the second repetition), but showed no reliable differences based on where in
the intersection they were positioned (learning: first half vs second half).
To test our hypothesis that the scrambled group would show greater fixation time on the
landmarks (i.e., buildings and their features), we conducted a linear mixed ANOVA on fixations
to the building ahead which revealed a significant main effect of group F (1, 57.91) = 18.61, p <
.01 and of repetition F (1, 58.09) = 5.753, p < .05. There was an interaction between group and
repetition F (1, 58.09) = 4.72, p < .05. As predicted, the scrambled group fixated more on the
building ahead than the ordered group, t (57.913) = -4.314, p < .001. There were more fixations
during the second repetition compared to the first t (58.093) = -2.398, p < .05. The interaction
was driven by an increase in the scrambled group’s fixations on the building ahead in the second
repetition compared with no change across repetitions in the ordered group. Both groups had
similar fixation times on building signs F (1, 58.09) = .079, p > .05 and showed no differences in
these fixation patterns across repetitions F (1, 58.78) = .07, p > .05. To determine if there were
differences in how the groups fixated on the unstable and navigationally irrelevant environmental
features (i.e., people and vehicles), we conducted a linear mixed ANOVA on the fixations to
people which revealed a main effect of group F (1, 57.91) = 12.815, p < .001, and of repetition F
(1, 57.57) = 163.431, p < .001. There was an interaction between group and repetition F (1,
57.57) = 26.925, p < .001. The ordered group fixated on people to a greater extent than the
scrambled group, t (57.906) = 3.58, p < .001. There were more fixations to people in the second
repetition than the first t (57.575) = -12.784, p < .001. The interaction was driven by a greater
increase in fixations to people in the ordered group in the second repetition than the scrambled
group, t (57.575) = -5.189, p < .001. When looking at fixations to vehicles, there was a main
effect of group F (1, 58.02) = 8.363, p < .01, and of repetition F (1, 56.07) = 88.318, p < .001.
There was no interaction F (1, 56.07) = 3.881, p > .05.
Classifier results. The previous results focused on AOIs for which we had a priori
hypotheses for differences between groups. Next, we explored looking behavior to other AOIs in
order to better evaluate the relative contribution of each type of information on route learning in
the two groups. We trained a classifier to decode from the pattern of fixations whether a
participant had studied the image in the scrambled or ordered condition. A linear support vector
a given trial. As participants had only 1s to study an image, they often fixated on only a few of
the nine AOIs. This led to a large, but sparse dataset, with only 110,098 non-zero feature values
out of 551,120 feature values, or 19.98% (SD = .48) of all feature values. To address this we
used techniques standard to text classification which also deals with datasets with many zero
feature values; namely we normalized each trial independently of other trials so that its norm
equaled one, and we employed a liblinear solver in our support vector machine as it is optimized
for the classification of large, sparse datasets (Fan et al. 2008). The classifier was iteratively
trained and tested on all trials using leave-one-out cross validation and accuracy was estimated as
the proportion of correctly classified trials.
The classifier was able to determine whether the trial was viewed in the ordered or
scrambled condition at an accuracy level significantly above-chance (.56 correct; binomial test, p
< .001). A linear classifier creates a hyperplane that uses support vectors to maximize the
distance between the two conditions and the coefficients obtained from the model represent the
vector coordinates orthogonal to the hyperplane. The direction of the coefficients obtained from
the model indicates the predicted group, and the magnitude of the coefficients in relation to each
other can be used to determine the importance of fixation duration to each AOI when classifying
viewing group. As the classifier was iteratively trained and tested on all trials using leave-one-
out cross validation, coefficient magnitudes are averaged across all trials. Higher fixation time on
path ahead (M = 1.352, SD = .0004), path incorrect (M = 1.0, SD = .0007), people (M = 0.705,
SD = .0003), vehicle (M = .618, SD = .0003), guide (M = .293, SD = .0001) and path behind (M
= .222, SD = .0029) led the model to classify the trial as one viewed in the ordered condition,
while higher fixation time on building sign (M = -.004, SD = .0003), other (M = -.182, SD =
.0003), and building ahead (M = -.998, SD = .0003) led the model to classify the trial as viewed
in the scrambled condition.
We also trained and tested the model separately on trials by repetition. Trials in which the
images were studied in the first repetition (.56 correct; binomial test, p < .001), and second
repetition (.58 correct; binomial test, p < .001) were both classified significantly above-chance.
However, there was a difference in classification accuracy by repetition, with trials from the
second repetition classified at a significantly higher rate (Welch’s t-test: t (55105.33) = 8.734, p
< .001). There was also a range of classification accuracy across images. The image with the
highest average classification accuracy was classified at an average .72 accuracy (binomial test,
p < .001), significantly above chance. The image with the lowest average classification accuracy
across trials was classified significantly below chance (.41 correct, binomial test, p < .01),
suggesting that the pattern of fixation time to AOIs in that image for the ordered group was more
similar to that of the scrambled group and therefore more likely to be misclassified.
Discussion
We sought to determine how gaze varies as a function of route learning in the presence or
absence of temporal context. The behavioral results indicated that the route was well learned in
only two repetitions with temporal context but was challenging without it. However, as both
groups were actively trying to learn the route, albeit with one group at a disadvantage, we
analyzed gaze patterns to determine how looking was different across groups to get an idea of
what people determined as navigationally important throughout various sections of the route.
The primary prediction we tested was that the ordered group would use temporal context
to anticipate the path ahead. Consistent with this prediction, the ordered group showed
anticipatory gaze toward the path and the scrambled group focused more on the most reliable
landmarks (i.e., the buildings ahead of them). We hypothesize that these looking patterns reflect
different strategies used by each group to stitch together a spatial representation of the route.
Additionally, we also found that the ordered group looked more at navigationally irrelevant
objects (e.g., people and vehicles), perhaps because tracking the route was less demanding,
allowing participants to explore other objects of general interest. That is, because the ordered
group could use temporal context to anticipate the route, they may have freed cognitive resources
to explore interesting, but unstable objects regardless of their usefulness. These results suggest
that individuals use temporal order in route learning to create and test predictions about where to
go next along the route. Presumably, when the prediction is not confirmed, an error signal is
generated that updates their route representation (Bestmann et al., 2008; Blakemore et al., 1998;
Franklin & Wolpert, 2011; den Ouden, Kok, & de Lange, 2012). The use of error signals to
update representations is ubiquitous in human learning (Bar, 2009; Friston, 2005; Hawkins,
2004) and suggests route learning employs similar mechanisms.
Next, to explore the relative contribution of information from the a priori AOIs compared
to other possible sources of information, we entered all of the object AOIs into an SVM classifier
to determine what information was most important for each group in building a spatial
representation of the route . The AOIs that were most heavily weighted in the classification were
the path ahead for the ordered group and the building ahead for the scrambled group. Of note, the
path ahead and building ahead, were also statistically significant in the group comparison, but
they were not actually fixated for very long (approximately 5% of the total time). This finding
indicates that although these were important sources of information in differentiating between
route learning in the two groups, the absolute amount of time spent sampling this information
was rather small. This suggests that a lot of time was spent looking at objects that did not
specifically contribute to route learning in the two groups. In this study, one of those objects was
We also considered fixations to the regions outside of the preassigned AOIs (termed “other” in the classifier
analyses). These regions contained trees, the sky, distant buildings, and the areas between the preassigned AOIs. A
one-way ANOVA revealed a main effect of group F (1, 556) = 88.854, p < .001, η = .132, and repetition F (1, 556)
= 24.090, p < .001, η = .036. The scrambled group spent a greater proportion of time fixating on other areas
compared to the ordered group (p < .001, Cohen’s d = -.779). There was a greater proportion of fixations to other
tukey
areas in the second repetition than the first (p < .001, Cohen’s d = -.385). There were no interactions.
tukey
the guide. Although a large proportion of time was spent looking at the guide, the guide provided
very little information about the path and did not predict learning in either group. Similarly, there
was substantial looking at other objects such as passerbys, vehicles, other objects (deHaas et al.,
2019), as might be expected from static scene viewing, but these objects were not as predictive
of route learning.
Overall, the results suggest that scene viewing during a route learning is influenced by
many factors, but only a small subset of information is relevant to route learning, per se.
Specifically, we found key differences in looking behavior during route learning when temporal
order was present versus absent such that the path ahead was looked at more frequently by the
ordered group and differentiated the ordered from the scrambled group. In contrast, the
scrambled group relied more on landmarks such as buildings ahead, but they nevertheless
performed poorly on the memory task. This suggested that route learning is indicated by
anticipatory looking towards the expected path, even when this anticipatory looking involves
relatively little absolute time.
Acknowledgements: We would like to thank Michelle Occhipinti for completing the data
collection and Bo-Yeong Won, Philip Witkowski, Xinger Yu, and Zhiheng Zhou for their
thoughtful comments and guidance. This research was supported by NIH T32AG050061 to MRF
and NIH MH113855 to JJG.