Measurement and prediction of
individual and group differences in the
digital environment
Michal Kosinski
michal@michalkosinski.com
Department of Psychology
University of Cambridge
This dissertation is submitted for the degree of
Doctor of Philosophy
Downing College
I respectfully dedicate this thesis to John Rust who was so much more than a supervisor.
Declaration
I hereby declare that the contents of this dissertation have not been submitted in whole or in
part for consideration for any other degree or qualification in this, or any other University.
This dissertation is the result of my own work and includes nothing which is the outcome of
work done in collaboration, except where specifically indicated in the text. This dissertation
contains about 53,000 words including appendices, bibliography, footnotes, and equations
and has about 55 figures and tables.
Michal Kosinski
Acknowledgements
This work would not have been possible without my supervisor David Good and my two
advisers, John Rust and Alan Blackwell. I would like to thank them for their intellectual
support, trust and patience with my quickly changing research interests.
Thank you to David Stillwell for being an amazing friend, wise advisor and generous
collaborator.
Thank you also to Thore Graepel, my supervisor at Microsoft Research, without whose
advice, intellectual challenges and company over the chess board this work would not be
possible. My greatest respects also go to my senior colleagues at Microsoft Research, and
especially to Yoram Bachrach, Pushmeet Kohli, Filip Radlinski and Gjergji Kasneci for
being amazing role models and an infinite source of knowledge and inspiration, much of
which is included in this work.
This work was generously supported by a research grant from Boeing and accompanied
by inexhaustible amounts of care, love and attention from Deborah Radasch and William
Krechel.
Kalifa Damani, Vesselin Popov, Sandra Matz and Poppy Noor spent a considerable
amount of time patiently reviewing this work and offered critical advice going far beyond
the issues of language and style.
Finally, I would like to thank my friends and family, and especially Slawek and Dorota
Knas whose care, support and encouragement enabled me to focus on my studies.
Abstract
Differences in personality affect the online as well as the offline behaviour of individuals
and groups. I examine this behaviour as captured by social network profiles, performance
in tasks, and preferences for content, such as websites and brands. Results show that there
are psychologically meaningful links between users’ online behaviour and their personality,
intelligence and other traits. By exploring these links, it is demonstrated that it is possible
to accurately predict individual and group psycho-demographic profiles based on samples
of online behaviour.
The internet also presents a magnificent opportunity to study the ways in which individ-
ual differences at both the group and the individual level manifest themselves in different
outcomes. This was explored by examining the aggregate intelligence of a group of people
operating within a crowdsourcing environment and the manner in which this measure could
be affected by different incentives. I design and deploy an intelligence questionnaire aimed
at crowds working together in the online environment and examine the relationship between
the intelligence of the crowd and the intelligence of individual members. The performance
of the crowd working within the context of a crowdsourcing platform is found to be signifi-
cantly higher than that of crowds working within a laboratory environment, and also of that
of an average individual in the general population. Moreover, the results indicate that the
crowd and individual intelligence has a non-linear relationship with the rewards offered to
crowd members, with both too high and too low rewards negatively affecting performance.
This work suggests that human behaviour in the online environment is driven by the
same set of psychological factors as is the case in the traditional offline world. However,
the degree to which human behaviour is nowadays digitally recorded creates significant
opportunities as well as challenges for researchers in particular and humanity in general.
Contents
Contents xi
List of Figures xv
List of Tables xix
1 General introduction 1
1.1 Outline of this Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Social research in the digital environment 5
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Advantages of Facebook as a research platform . . . . . . . . . . . . . . . 6
2.2.1 Access to large and diverse samples . . . . . . . . . . . . . . . . . 6
2.2.2 Robust records of actual behaviour in a natural environment . . . . 8
2.2.3 Openness to share personal information . . . . . . . . . . . . . . . 9
2.2.4 Tracking participants across studies in a privacy preserving manner 10
2.2.5 Convenience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 Using Facebook Graph data in research . . . . . . . . . . . . . . . . . . . 11
2.3.1 Validity of Facebook Graph data . . . . . . . . . . . . . . . . . . . 14
2.3.2 Facebook social networks . . . . . . . . . . . . . . . . . . . . . . 14
2.4 How to effectively research in the digital environment . . . . . . . . . . . . 15
2.4.1 Digital is an age of observation and secondary data . . . . . . . . . 16
2.4.2 Motivate participants properly . . . . . . . . . . . . . . . . . . . . 18
2.4.3 Avoid requesting self-reports of information that can be accessed
otherwise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.4.4 Do not force participants to respond . . . . . . . . . . . . . . . . . 19
2.4.5 Do not prevent anyone from taking part . . . . . . . . . . . . . . . 20
2.4.6 Acquire appropriate skills and experience . . . . . . . . . . . . . . 20
xii Contents
2.4.7 Go viral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.5 Limitations of Facebook research . . . . . . . . . . . . . . . . . . . . . . . 21
2.5.1 Hidden rules of the Facebook environment . . . . . . . . . . . . . 21
2.5.2 Application maintenance . . . . . . . . . . . . . . . . . . . . . . . 22
2.5.3 Changing the past . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5.4 Evolution of cultural norms . . . . . . . . . . . . . . . . . . . . . 23
2.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3 myPersonality Project: Collection and validation of a large online sample 25
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.2 Threats to the validity of online samples . . . . . . . . . . . . . . . . . . . 28
3.2.1 Most widespread threats to the validity of online samples . . . . . . 28
3.2.2 Mitigating risks to data validity . . . . . . . . . . . . . . . . . . . 29
3.3 Validation of the myPersonality sample . . . . . . . . . . . . . . . . . . . 30
3.3.1 Study 1: Cross-validation of age and gender . . . . . . . . . . . . . 31
3.3.2 Study 2: Discriminant validity and reliability of the IPIP FFM Per-
sonality Inventory scales . . . . . . . . . . . . . . . . . . . . . . . 32
3.3.3 Study 3: Internal consistency . . . . . . . . . . . . . . . . . . . . . 33
3.3.4 Study 4: Missing responses and Consecutive Identical Response
Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4 Personality and behaviour in the digital environment 41
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.1 Five Factor Model . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.2 Sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.2.3 Facebook profile features . . . . . . . . . . . . . . . . . . . . . . . 43
4.2.4 Website preferences . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.3 Study 1: Personality and website choices . . . . . . . . . . . . . . . . . . . 46
4.3.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.3.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.4 Study 2: Personality and Facebook profile features . . . . . . . . . . . . . 51
4.4.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Contents xiii
4.6 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5 Using digital footprints to infer psycho-demographic profiles 61
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes . . . 63
5.2.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.2.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.3 Predicting personality with Facebook profile features . . . . . . . . . . . . 80
5.3.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.3.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6 Crowd IQ: Modelling crowd ability using Item Response Theory 85
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.2 Item Response Theory: Modelling the relationship between performance
and ability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
6.2.1 Estimating ability . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.2.2 Computerised Adaptive Testing . . . . . . . . . . . . . . . . . . . 90
6.3 Modelling the performance of crowds using Item Response Theory . . . . . 92
6.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
6.3.2 Crowd Item Response Function . . . . . . . . . . . . . . . . . . . 93
6.3.3 Weighted Majority Voting . . . . . . . . . . . . . . . . . . . . . . 94
6.3.4 Performance and Ability Curves . . . . . . . . . . . . . . . . . . . 96
6.4 Crowd ability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.5.1 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.5.2 Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.5.3 Obtaining crowd and individual ability scores . . . . . . . . . . . . 104
6.5.4 Z appropriateness index . . . . . . . . . . . . . . . . . . . . . . . 105
6.6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.6.1 Appropriateness of measuring ability in the crowdsourcing context . 107
6.6.2 Weighting the votes . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.6.3 Ability of the crowd and its members . . . . . . . . . . . . . . . . 111
6.6.4 Size and ability of the crowd . . . . . . . . . . . . . . . . . . . . . 113
6.6.5 Crowd members’ marginal contributions to crowd ability . . . . . . 116
6.6.6 Crowdsourcing conditions and the ability of the crowd . . . . . . . 117
xiv Contents
6.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.7.1 Future research . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7 General Discussion 127
7.1 Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.1.1 Public data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.1.2 Secondary data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.1.3 Human Subjects Research . . . . . . . . . . . . . . . . . . . . . . 132
7.2 Digital exposure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.3 Labour market of the future . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.4 New personality and ability models . . . . . . . . . . . . . . . . . . . . . . 140
7.5 Future research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
Appendix A List of myPersonality collaborators and publications 143
A.1 Publications based on data collected by myPersonality project . . . . . . . 143
A.2 List of researchers registered to use myPersonality database . . . . . . . . . 146
Appendix B The reliability of the IPIP FFM Personality Inventory 151
Appendix C Goldberg’s Psychometric Antonyms 153
Appendix D Estimating the probability of a crowd arriving at a correct solution
working under Majority Voting in practice 155
Appendix E Order of individual members’ weights 159
Appendix F Royalty-free intelligence questions 161
References 163
List of Figures
2.1 An example of a Facebook consent dialog box shown after offering access
to one’s Facebook data to a third party. . . . . . . . . . . . . . . . . . . . . 9
3.1 Screen capture presenting myPersonality implementation of a Five Factor
Model personality inventory. . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 Frequencies of the age difference (in years) between myPersonality data and
Facebook profile information . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.3 Frequency distributions of GPA and JIR internal consistency coefficients. . 34
4.1 Audience personality profile for deviantart.com website . . . . . . . . . . . 49
4.2 Median number of Likes for users characterised by different levels of Open-
ness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.3 Median number of Likes for users characterised by different levels of Con-
scientiousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4 Median number of groups joined per month by users characterised by dif-
ferent levels of Conscientiousness . . . . . . . . . . . . . . . . . . . . . . 57
4.5 Median of status updates posted per month by users characterised by differ-
ent levels of Extraversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.6 Median of friends added per month by users characterised by different levels
of Extraversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.7 Median number of Likes for users characterised by different levels of Neu-
roticism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.1 Summary of the approach used to infer psycho-demographic profiles from
Facebook Likes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.2 Prediction accuracy as a function of the number of SVD components used . 67
5.3 Fraction of variance explained as a function of the number of SVD compo-
nents for the dimensionality reduction of the User-Like matrix. . . . . . . . 68
xvi List of Figures
5.4 Example of an ROC curve . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.5 Prediction accuracy of regression for numeric attributes and traits . . . . . . 70
5.6 Prediction accuracy of classification for dichotomous/dichotomised attributes
expressed by the AUC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.7 Accuracy of selected predictions as a function of the number of available
Likes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.8 Average levels of five personality traits and age of the users associated with
selected Likes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.9 Relative popularity of selected Likes among selected categories of respondents 80
5.10 Screenshot showing a part of YouAreWhatYouLike application . . . . . . . 83
6.1 An example of an Item Response Function. . . . . . . . . . . . . . . . . . 90
6.2 True score prediction accuracy offered by two popular next item selection
criteria: MFI and Urry’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.3 Weights of three simulated coalitions across the difficulty range for a single
task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.4 Crowd and individual performance curves for two crowds . . . . . . . . . . 97
6.5 Crowd performance and the estimate of crowd ability for crowds with equal
average ability but differing spread of agents’ abilities . . . . . . . . . . . . 98
6.6 Sample Human Intelligence Task as used in this study . . . . . . . . . . . . 101
6.7 Relationship between individual IQ scores in the lab sample estimated us-
ing Item Response Theory Three Parameter Logistic Model and standard
scoring model from SPM manual . . . . . . . . . . . . . . . . . . . . . . . 106
6.8 Comparing the performance of the WMV aggregation based on actual and
manually set IRF parameters and using the lab sample . . . . . . . . . . . . 111
6.9 The intelligence score of crowds of n = 5 expressed on the standard intelli-
gence scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.10 Scatter plots illustrating the relationship between the crowd ability and the
average and maximum ability of crowd members . . . . . . . . . . . . . . 113
6.11 Crowd intelligence scores and number of individual solutions to each of the
tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
6.12 Plots illustrating the relationship between the size of the crowd, the expected
intelligence score, and marginal intelligence score . . . . . . . . . . . . . . 115
6.13 Contribution of the agent towards crowd general intelligence . . . . . . . . 116
6.14 The crowd IQ, and crowd IQ per minute as functions of payment levels . . 119
List of Figures xvii
6.15 Crowd intelligence as a function of agents’ reputation scores . . . . . . . . 120
6.16 Crowd intelligence and IQ points per minute in the “rejection” and “no re-
jection” conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.1 Dendrogram illustrating the structure of music tastes and its relationship to
the personality trait of Openness . . . . . . . . . . . . . . . . . . . . . . . 141
List of Tables
3.1 Correlations between internal consistency measures and personality scores . 35
3.2 Frequencies of missing responses in the myPersonality implementation of
100-item IPIP FFM Personality Inventory protocols. . . . . . . . . . . . . . 37
3.3 The frequencies of the maximum CIRS length . . . . . . . . . . . . . . . . 38
4.1 Summary of Facebook features used . . . . . . . . . . . . . . . . . . . . . 44
4.2 Distribution of the psycho-demographic traits among users and website au-
diences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
4.3 Pearson product-moment correlation between website audience personality
profiles estimated using both datasets. . . . . . . . . . . . . . . . . . . . . 48
4.4 Pearson product-moment correlation between personality profiles estimated
using both datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.5 Websites characterised by the extreme values of aggregate audience person-
ality profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.6 Similarities between aggregate website personality profiles . . . . . . . . . 51
4.7 Spearman’s rank correlations between personality and Facebook activities . 54
5.1 Likes characterised by the most extreme average levels for each of the nu-
meric variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2 The accuracy of predicting personality, Satisfaction with Life, Intelligence,
and age using Facebook profile features. . . . . . . . . . . . . . . . . . . . 82
6.1 Individual intelligence distribution across the AMT batches. . . . . . . . . 103
6.2 The mean and standard deviations of the Z appropriateness indices . . . . 108
B.1 Scale reliabilities in the myPersonality implementation of the 100-item and
the 300-item long IPIP FFM Personality Inventories . . . . . . . . . . . . . 152
xx List of Tables
C.1 Goldberg’s Psychometric Antonyms in the 100-item IPIP FFM Personality
Inventory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Chapter 1
General introduction
Human behaviour and the weather might be two most important aspects of the human en-
vironment, greatly affecting our well-being and chances of survival. Effectively, for all of
recorded history, people have strived to predict both the behaviour of their friends and foes
and the weather.
The ancient methods of predicting weather relied on observing local patterns of events,
such as wind direction, temperature, patterns of cloud, or the colour of the sunset. While
ancients made some accurate observations related to the mechanics of the weather system,
perhaps best represented by Aristotle’s treatise “Meteorologica” (circa 340 BC), in great
part their findings were incorrect. Moreover, there was little or no progress in meteorology;
for 2 millennia nothing significant was added to Aristotle’s findings [70].
With the benefit of current knowledge, it is easy to see why meteorology has eluded
humanity for such a long time. The weather is not a local phenomenon but an intricate
global system that is difficult to understand by sampling it in a certain spot at a certain
time. This, however, did not become apparent until the introduction of the telegraph in
th
the 19 century, which heralded the start of the age of modern meteorology and weather
forecasting [70].
The way in which a new system-wide perspective offered by the telegraph changed me-
th
teorology in the 19 century offers insights into what is currently happening in the social
sciences. Until relatively recently, social sciences were to a large extent based on case
studies and pattern-spotting akin to methods used in pre-modern meteorology and weather
lore. This is best illustrated by psychoanalysis, a systematised set of theories about hu-
man behaviour originally popularised by Sigmund Freud [74]. While psychoanalysis had
th
a tremendous impact on psychological science in the 20 century and is still applied in
2 General introduction
therapy, it is now widely accepted to be a pseudo-science (e.g. [67, 181, 184, 227]) and its
therapeutic techniques have been shown to be ineffective (e.g. [108, 183]). The rejection of
psychoanalysis and other psychological lores has been propelled by new research methods
and technologies, offering perspectives and insights that were unfathomable before.
There are two phenomena that offer a brand new perspective in the social sciences. The
first is the ability to examine the neural system’s activity offered by recent advances in
neuroscience. Scientists can directly observe and explain the actual mechanisms driving
behaviour, cognition, emotions, or motivations. A good example is provided by neuroimag-
ing techniques, such as Functional Magnetic Resonance Imaging, used for the analysis of
brain activity with unprecedented accuracy. By associating certain behaviours or thoughts
with patterns of brain activity, it is now possible to decode people’s thoughts, their dreams
and even to predict their intentions [216]. However, while neuroscience will most likely
revolutionise current understanding of human psychology, it is, as of today, in many ways
limited; for example, its methods are expensive, inconvenient, and usually require equip-
ping participants with, or putting them into, unwieldy apparatuses. Moreover, in order to
understand humans, the measurement of what is happening inside the neural system has to
be supplemented with observations of external behaviour and social interactions.
The shortcomings of neuroscience are addressed by the second phenomenon revolution-
ising social sciences: an ability to observe the behaviour of groups and individuals at an
unparalleled scale. A growing proportion of human activities, such as social interactions,
entertainment, shopping and gathering information, are now mediated by digital services
and devices. Such digitally mediated behaviours can easily be recorded and analysed, fu-
elling the emergence of computational social science [148]. The availability of Big Social
Data is changing the paradigm in the social sciences, as it undergoes a transition from
small-scale experiments and observation studies to large-scale projects based on thousands
or millions of individuals (e.g. [12, 231]). Observing or experimenting with samples of
this size enables scientists to overcome the problem of “noise” typical for studies involving
humans and detect regularities that might not be apparent in smaller samples [148]. Much
like the introduction of the telegraph made it apparent that the weather is a global, and not
a local, system, Big Social Data offers unprecedented insights into the dynamics and organ-
isation of individual behaviour and social systems, with the potential to radically improve
our understanding of human psychology. Big Social Data could, for example, increase our
understanding of the intricate interactions between personality, life events, social environ-
In fact, at the time of writing psychoanalytic therapy was widely offered by the UK’s National Health Ser-
1.1 Outline of this Thesis 3
ment, culture and myriad other factors driving individuals’ subjective well-being [58]. Im-
portantly, the same technological advances that led to the emergence of computational social
science are also transforming the human environment, posing new opportunities and threats
to individuals and societies, and opening new research areas for social sciences. Rapid
changes in the technology and environment surrounding humans further drives the need to
re-examine the relevance of established social science theories and use modern technology
to develop new ones.
The digital environment and Big Social Data have immense theoretical and practical
potential for sciences and have already transformed many fields, including physics and bi-
ology. However, social scientists are relatively slow in embracing Big Social Data [148],
increasingly ceding the data-driven human-subject research to computer scientists and engi-
neers, who often lack the appropriate theoretical background and ethical standards [34, 96].
This thesis aims to contribute towards addressing this issue by demonstrating the advan-
tages of data-driven social science research in three ways. First, it argues that Big Social
Data collected through online social networks is as reliable and valid as that collected in
the offline environment and that it can be used as a powerful new research tool. Second,
it demonstrates that human behaviour in the digital environment is not psychologically di-
vorced from offline behaviour as it is driven by well-established psychological traits. Third,
it demonstrates and critically evaluates the capabilities of the online “crowd” collectively
solving tasks. The following section describes this in more detail and forms an outline of
the structure of this thesis.
1.1 Outline of this Thesis
There has been a remarkable shift towards more social Internet usage in recent years, with
people supplementing their offline social networks with a robust range of online tools. Per-
haps the most remarkable and certainly most popular online social networking environment
is Facebook which is increasingly used as a powerful social science research tool. Whether
researchers aim at conducting experiments, collecting information about people in a natural
environment, or recruiting participants for external studies, the Facebook environment offers
a number of advantages over traditional offline and online research settings. Such advan-
tages, but also challenges, related to conducting social science research in this environment
are explored in detail in Chapter 2. Next, Chapter 3 introduces a sample of 3.5 million
psycho-demographic profiles collected on Facebook and explores the validity of Facebook
profile variables and psychometric scores collected in an unsupervised online environment.
4 General introduction
Results confirm that data collected on Facebook is of comparable quality, if not better, than
in carefully controlled offline samples.
Given that Facebook and other online environments affect the daily lives of a large frac-
tion of the global population by shaping the ways in which they access information, socialise
and communicate, those platforms are certainly worth investigating. As the extent to which
online behaviour is driven by established psychological phenomena, Chapter 4 investigates
how users’ personality is manifested in their Facebook social networking activities and web-
site preferences. Results based on a sample of over a third of a million users show psycho-
logically meaningful links between users’ personalities, their website preferences and their
activity on Facebook.
The meaningful relationship between online behaviour and personality supports the va-
lidity and utility of personality theories in modelling human behaviour in the rapidly chang-
ing technological environment. It also implies that personality, and other individual traits,
can be automatically predicted based on the samples of online behaviour, as illustrated by
highly accurate prediction models based on the records of online behavioural residues pre-
sented in Chapter 5.
Social networking and entertainment are not the only activities that are increasingly
mediated or accompanied by digital technologies. Online collaborative platforms, com-
munication software, project management suites and knowledge repositories constitute an
increasingly important part of companies, institutions and the economies. A number of
collaborative projects, such as Wikipedia or Amazon Mechanical Turk, stem beyond the
institutional boundaries and, arguably, may constitute an early form of a massive collabora-
tion and knowledge creation environments that could one day become a major platform for
human economic activities. Chapter 6 studies the mechanism translating individual ability,
such as general intelligence, into an aggregate performance of a crowd collectively solving
tasks in an online setting. This chapter introduces a novel formalisation of the relationship
between individual ability and crowd performance and proposes ways of optimising crowd
performance by weighting individuals’ input. A number of experiments based on empirical
and simulated samples is performed to illustrate the properties of the crowd performance
and its relationship with factors including crowd size, the distribution of crowd members’
abilities, and the conditions of the task.
Finally, Chapter 7 discusses several consequences of the findings reported in this thesis,
including the risks to well-being and security of individuals that stem from the predictability
of individual psycho-demographic profiles and other ethical issues related to conducting
studies in the online environment.
Chapter 2
Social research in the digital
environment
This chapter focuses on the unique challenges as well as opportunities of social science
research based in the digital environment, with a particular focus on Facebook.
2.1 Introduction
A growing proportion of human activities, such as social interactions, entertainment, shop-
ping and gathering information, now takes place on the Internet, mediated by digital services
and devices. In recent years, there has been a remarkable shift towards more social use of
the Internet with a diminishing concern for absolute anonymity. Interactions between people
hiding behind nicknames, email addresses or avatars are increasingly replaced by interac-
tions based on real identities and connections that largely mirror offline social links. This is
illustrated by Facebook, by far the most popular online social networking platform and one
of the three most popular websites in the world.
Facebook has become a natural environment to its 1.2 billion monthly users that spend
an average of six-and-a-half hours per month using this platform [69]. It is estimated that, in
many developed countries, more than 50% of the total population has a Facebook account. It
In 2012 and 2013, Facebook.com, Google.com and YouTube.com exchanged their positions as the world’s
Facebook allows its users to maintain a personal profile, connect and communicate with their friends and
interact using a variety of functions, applications and games. Users can publish and comment on content such
as photos, videos, links or status updates. Also, Facebook provides its users with personalised news-feed
containing selected content and activities of users’ friends and brands or institutions that they are connected
with.
6 Social research in the digital environment
has a wide global reach, with more than 80% of its users being outside of the United States,
and has been made available in over 70 different languages [69, 209]. Facebook reports that
their average user has around 130 friends, posts around 90 pieces of content per month, and
belongs to approximately 80 varying community pages, groups and events [69].
Facebook affects the daily lives of a large fraction of the global population by shaping
the ways in which they access information, socialise and communicate. It is therefore a
phenomenon that merits investigation in its own right. The benefits offered by Facebook,
such as its capacity to strengthen and expand social networks [245], are accompanied by
challenges, for example to people’s privacy [113] and well-being [143]. Facebook and other
digital environments have become so integral an aspect of peoples’ lives and identities [65]
that it is increasingly impossible to fully understand human behaviour without studying
those.
The majority of studies mentioning Facebook use it as the object of study [242], but this
misses a remarkable opportunity to use Facebook as a powerful environment for conducting
social science research. Whether researchers aim at conducting experiments, collecting
information about people in a natural environment, or recruiting participants for external
studies, the Facebook environment offers a number of advantages over traditional offline
and online research settings.
2.2 Advantages of Facebook as a research platform
2.2.1 Access to large and diverse samples
The size and representativeness of samples are two of the main issues in social science
research. Few, if any, research methods collect truly representative samples; for instance,
random-digit calling surveys, typically considered to be highly representative, are plagued
by very low rates of participation and are limited only to those available on the phone, ex-
cluding people that are homeless, phone-less, hold high-level executive positions or work
at night-shifts [46]. A large proportion of studies in the social sciences is based on rela-
tively small and disproportionately Western, Educated, Industrialized, Rich and Democratic
(WEIRD; [99]) samples. For psychological research, for instance, the number of studies
using undergraduate samples was estimated to range within 80% and 85% [38]. Given that
most studies aim to generalise results from the examined sample to the population as a
whole, these limitations pose a considerable threat to the validity of empirical findings.
While some groups including old, homeless, deprived of electricity, illiterate, and those
2.2 Advantages of Facebook as a research platform 7
ideologically opposed to online social networking, are heavily under-represented or absent
from Facebook, it might still be the world’s largest and most diverse participant pool. Be-
yond the mere availability of large samples, the demographics of the online population are
expected to converge with those of the general population as the Internet continues to ex-
pand [193].
The inexpensive and convenient access to a large number of users allows researchers
at any stage of their career (and with any degree of funding) to collect samples of a size
and representativeness that in the past could only be achieved in well-funded projects. A
good illustrations of the sample sizes achievable is provided by a recent social network
study conducted by an internal research team at Facebook on a sample of over 721 mil-
lion users [12, 231]. Another illustration of the research potential offered by Facebook is
provided by the myPersonality project, which collected detailed profiles of over 3.5 million
individual users [221].
While the relative ease with which large samples can be recruited is characteristic for
internet-based research methods in general, Facebook offers two particular features that
further help to address the issues of sample size and representativeness.
First, the Facebook advertising platform allows researchers to target users by very finely
defined psycho-demographic groups. With only a few clicks, for example, the link to a study
can be advertised to 40-50 year old, single, native speaking, lesbian women, who enjoy
sports, have completed higher education and live in London. These targeting mechanisms
can be particularly useful to communicate directly with people who may be stigmatised in
the offline world or hesitant to meet face-to-face with researchers, such as gay men and
lesbian women or older people.
Second, the sheer number of participants that can be recruited on Facebook, and their
willingness to donate their data to research (as discussed in Section 2.2.3) can often alle-
viate the issue of sample bias: given a large enough sample, researchers can increase its
representativeness by weighting particular subgroups or by randomly removing individuals
belonging to the overrepresented groups. For example, even though respondents in the 55+
age group constituted only 1.7% of myPersonality users, this group nevertheless contains
several thousand participants, allowing for a large and still representative group of partici-
pants to be sub-sampled [221].
At the time of writing, the Facebook Advertising platform reported 7,800 individuals that matched such a
profile, and these could be reached with an advertising budget of under 50 US dollars.
8 Social research in the digital environment
2.2.2 Robust records of actual behaviour in a natural environment
Social scientists often rely on self-reported behaviour and opinions or on observing par-
ticipants in the artificial environment of an experimental setting [23, 75]. These research
methods are costly, inefficient and often criticised for yielding low external validity [213].
The validity of findings may be threatened further by misrepresentation [145] and unin-
tentional cognitive biases such as the consistency motif [121], implicit theories about the
covariation of behaviours and outcomes [182], or the availability heuristic [230].
Facebook, in contrast, allows observing the actual and natural behaviour of a large num-
ber of people. Users’ diverse activities on Facebook, such as socialising, communicating or
expressing preferences, leave an enormous mount of concrete and reliable records—or be-
havioural residues [90, 93].
Many of the behaviours easily observable on Facebook would be difficult or impossible
to observe in artificial settings or to reliably assess with the help of self-reports. Studying
personal preferences, for instance, can be significantly more convenient and accurate than
in traditional research settings. As the average Facebook user is connected with over 100
community pages, groups and events, valuable insights into their hobbies, preferences and
interests can be gained through only a few lines of code. By contrast, self-reports related
to preferences and activities such as these are not only time-consuming to gather but are
also likely to be more biased and less accurate. Furthermore, egocentric social networks are
usually composed of hundreds of agents and tens of thousands of connections between them.
It is unlikely that even the most motivated participant would have enough time, attention
and knowledge to manually recreate their own egocentric network structure in a traditional
self-report setting. Facebook profiles contains egocentric network structure (described in
more detail in Section 2.3.2) that can easily be extracted and analysed. Finally, Facebook
offers a chance to study natural language at a scale difficult to achieve in the offline contexts
(e.g. [211])
Apart from observing users’ activities, researchers can design controlled testing envi-
ronments. The recently published Colonel Blotto game [133] is a good example of such
an environment, in that case designed to collected behavioural data through a Facebook
application.
Finally, researchers can use Facebook to access actual records of participant past be-
haviour. This allows one to avoid the biases in participant behaviour or preferences intro-
duced by involvement in the study.
Facebook users can to some extent alter the records of their past behaviour as discussed in Section 2.5.3.
It is possible that some individuals may want to modify their behavioural residue before joining the study.
2.2 Advantages of Facebook as a research platform 9
2.2.3 Openness to share personal information
Traditionally, social scientists were cautious not to require too much information from their
participants, who in turn were cautious not to give away too much. It is uncommon to ask
participants to produce an extensive list of social and family connections, including per-
sonal details and pictures of their peers, or to recount discussions and messages exchanged
over the last decade. It is also unlikely that any of the participants would be able, let alone
willing, to provide the researchers with such information. Yet, the majority of social net-
work users appear to be comfortable sharing such information with researchers in the online
context [63]. For example, over 45% of 7.5 million myPersonality users decided to opt-in
to voluntarily share with researchers their personality and intelligence scores along with
the contents of their profiles and Facebook activity [221]. Such unprecedented openness to
share personal information constitutes an important feature of the Facebook ecosystem and
a fascinating topic for future research.
Fig. 2.1 An example of a Facebook consent dialog box shown after offering access to one’s
Facebook data to a third party.
There are several potential sources of individuals’ willingness to share their personal
information with researchers. First, the Facebook environment is specifically designed to
facilitate the dissemination of personal information. This means that requesting access to
personal information fits well with the theme of the platform and might be perceived as
10 Social research in the digital environment
being less intrusive than the same request made in other contexts. Second, many applica-
tions (including Facebook itself) require users to surrender their personal information. It is
therefore possible that users have embraced the fact that their personal information can, and
perhaps even should, be shared with third parties. Third, Facebook offers a way to share per-
sonal information with relative ease. Whereas explicitly providing information (e.g. writing
down a list of friends) gives users a chance to contemplate their actions, sharing troves
of personal records with only a few mouse-clicks might be far less disturbing psycholog-
ically. Fourth, traditional research often involves personal interaction with the researcher
in an unnatural research setting. This may boost respondents’ anxiety and self-awareness,
decreasing the chances of them being honest and behaving naturally [217]. Fifth, Facebook
privacy settings enable users to revoke or limit access to data after it has been granted. The
possibility to change one’s mind might make it easier to commit to sharing the data with
researchers in the first instance. Finally, despite the very clear and concise consent forms
users simply do not realise how much information they actually share.
Importantly, users’ apparent willingness to share their personal information does not
grant researchers an automatic licence to record and use it. Section 7.1 discusses several
ethical aspects of Facebook research, including the appropriateness of recording user-related
data.
2.2.4 Tracking participants across studies in a privacy preserving man-
ner
Many traditional studies (both offline and online) were hindered by the inability to distin-
guish between participants and track them over time - for instance to prevent them from
participating multiple times [26]. Connecting with users’ Facebook accounts enables re-
searchers to identify and track their participants without necessarily revealing and recording
their identity. Unique identities allow for the combination of scores across experiments,
while preserving participants’ privacy. This not only enhances the potential for future re-
search but also saves participants from having to respond to the same set of questions (e.g.
basic demographic facts) multiple times in related studies. Moreover, it implies that the
drawback of repeated submissions in online research [26] can be made into an opportunity
for longitudinal studies.
Facebook Connect functionality enables websites and applications to identify users without accessing any
user-related information apart from user’s identification number.
2.3 Using Facebook Graph data in research 11
Facebook’s advert targeting capabilities also enable researchers to adequately protect
participants’ privacy, as it offers the possibility of reaching users on the basis of well-defined
demographic profiles. These target groups can be presented with different links inviting
them to the same study. By simply recording the variant of link which was used by an
anonymous participant to reach the survey or experiment, researchers can tag their responses
with the target group’s profile without at any point revealing participants’ identities.
Furthermore, Facebook facilitates an ongoing communication with participants. By con-
necting with users’ accounts, researchers do not only get access to their data, but can broad-
cast messages or communicate with particular participants. This makes it possible to update
participants on their scores or the results of a study and perhaps to invite them to contribute
to other projects.
2.2.5 Convenience
Facebook allows collecting participant-related data in a way that is convenient and com-
fortable for both participants and researchers. While participants have traditionally been
required to fill in long forms and surveys, Facebook users can give access to the data stored
on their accounts with one click. Furthermore, studies based on Facebook and other online
environments usually enable people to participate in a time and place of their own choos-
ing, resulting in higher motivation and honesty when compared with traditional laboratory
studies [193].
On the other side, researchers can integrate their online studies with Facebook by simply
adding a few additional fragments of code. The Facebook Developers website, for instance,
provides numerous case-studies, robust documentation, and several code examples ready to
be copied and pasted into the relevant research design. Desirable features of any modern
behavioural study include (1) a Facebook Connect button to identify the user and potentially
request access to his or her Facebook data, (2) a Facebook Like button, enabling users to
stay in touch with news and updates about the research project, and (3) “Invite Friends” and
“Post to the Wall” buttons that help participants share their results and information about
the study with friends.
2.3 Using Facebook Graph data in research
Nearly every action performed on Facebook is recorded and, with users’ permission, a large
part of these records can be accessed by third parties. This repository includes a wide range
12 Social research in the digital environment
of demographic variables, social network data and extensive records of user’s activities
and interactions over time (at the time of writing it was possible to record users’ activities
since the time they joined Facebook). Importantly, the information about users’ friends and
content that they interact with is also available.
Facebook stores data in the form of a graph connecting Facebook users with the objects,
content and other users they interact with. Joining a certain fan group, for instance, creates
a membership connection between the user and the group, while setting political views
to Liberal creates a preference connection between the user and this political preference.
Facebook graph allows one to distinguish between different types of connection. Users can,
for example, be connected with each other using a typical friendship connection or by a
follow connection, in which a following party receives selected updates about the followed
user. Similarly, users’ interaction with books or movies on Facebook can take the form of
watched/read, rate (which contains users’ opinion about the object) or want to watch/read
connections.
Researchers can use the Facebook Graph Application Programming Interface (Graph
API; [68]) to automatically download user-related data where consent has been obtained
and the ethical risks assessed. Graph API can be accessed using a wide range of software
tools in order to supplement online questionnaires or other web-based research applications
with user-related information. At the time of writing, Facebook Graph contained data that
could be categorized as follows:
1. Demographic profile including unique id of the user, full name, profile picture, age,
gender, relationship status, romantic interests, geographical location, place of origin,
work and education history, biography, link to personal website and time zone. This
also contains political and religious views, general interests, and lists of favourite
music, movies, TV shows, books, quotes and sport disciplines.
2. User generated content encompassing photos, videos, status updates, comments on
other people’s content or pages, links and notes published by users or their friends.
3. Social network structure containing connections with other Facebook users. Those
connections can be of multiple types, including the friendship and following connec-
tions described above, but also family connections (e.g., spouse, parents or children).
Importantly, the social network structure contains information about the connections
between a user’s friends that allows one to analyse egocentric network structures. See
also Section 2.3.2.
2.3 Using Facebook Graph data in research 13
4. User activities including users’ Likes, group memberships, events, installed applica-
tions and tags in the photos or posts.
5. Background Information about other users and Facebook Graph objects, containing
an enormous database of readily available facts to enrich existing data. For example,
researchers can access additional information about the objects their participants have
Liked (e.g. description of the Liked artist), groups they have joined (e.g. list of other
members), or their friends’ basic demographic profiles.
Additionally, a large proportion of Facebook data is public and can be recorded without
users’ consent. For example, [175] studied public social network structures, while [11, 89]
used print-outs of public Facebook profiles in their experiments.
The data made available through Facebook Graph serves as more than merely an exten-
sion of a demographic profile. Pictures, events, wall posts and Likes are in fact behavioural
residues [89] conveniently recorded information about users’ behaviour on Facebook, on
other parts of the web and in real life. Examples of these insights into parallel environments
include the list of real-world events attended and the links created by clicking a Like button
on other websites. Another interesting connection between Facebook data and the offline
environment is provided by pictures published by users and often accompanied by the list
and position of users present on the picture (tags) and the place where it was taken. Effec-
tively, the information that can be extracted from these pictures reaches far beyond users’
behaviour on Facebook. Moreover, thoughts, feelings and opinions of people in the picture
as well as their friends are often expressed in the corresponding description, comments and
number of Likes received by a given picture.
The data collected from Facebook profiles can be enriched by processing or combining
it with other databases to obtain additional insights. The extraction of additional variables
can include simple procedures, such as counting the number of people on the picture or esti-
mating the importance of the content by counting the number of Likes and comments. More
complicated analyses are also possible, such as studying the relative location of users in the
picture or detecting faces of users that for some reasons were not tagged by others. Finally,
Facebook status updates can be tagged with their emotional valence using Pennebaker’s
LIWC dictionary [176], or users’ individual traits can be inferred using models such as
Apply Magic Sauce API that translates of Facebook Likes into personality estimates.
14 Social research in the digital environment
2.3.1 Validity of Facebook Graph data
Regardless of whether data is collected offline or online, the use of participants as infor-
mants is associated with multiple factors that might limit the validity of results. Apart from
the risk of misrepresentation and cognitive biases, the accuracy of information may be re-
duced by participants’ playful behaviour, their unwillingness to share personal information
and by technical problems [123]. Whereas entering data into paper-and-pencil or online
forms is not only tiresome but also prone to accidental errors, Facebook profile information
is recorded over a long period of time, reducing the risk of unintentional errors. By reduc-
ing the outlined risks, acquiring data from Facebook Graph API might in fact be the most
reliable and robust way of collecting data in the context of social science research.
Validity of the Facebook profile data is increased by the fact that it is not a network of
virtual avatars, but a tool supporting real relationships based mainly on offline networking
activities [11, 31, 65, 124]. Idealised or otherwise imprecise profile information is difficult
to maintain in a network of friends that can easily challenge false assertions. The reliability
of the Facebook profile as a mirror on reality is further bolstered by the fact that its contents
- wall posts, pictures, comments and more - are often contributed by a user’s friends rather
than by the user. In fact, there is a wealth of evidence that, while some degree of self-
enhancement is likely to occur (e.g. see Section 3.3.1), Facebook profiles contain valid
and accurate information about their owners [11, 236], and reliably represent their owners’
actual personality traits [89], narcissism [39] and motivation for Facebook use [246].
Moreover, as Facebook users have full control over the amount of information they share
with the researcher, the problem of deliberately faking responses is minimised. Rejecting a
request to share profile data or avoiding its publication on one’s profile in the first place is
much simpler than going to great lengths to invalidate one’s data or create a fake profile for
the purpose of participating in a study. Additionally, the wealth of user-related information
and digital footprints allows for the detection of fake or invalid profiles. While it may be
easy to set up a fake profile and fill it with basic information, it would take years of constant
attention to enrich said profile with convincing content, interactions and friendships.
2.3.2 Facebook social networks
As it was mentioned before, Facebook social links are rarely entirely virtual, and usually
represent or complement offline relationships [31]. The platform’s written and unwritten
rules of etiquette discourage users from interacting with any people with whom they are not
also acquainted outside of Facebook. However, the network structure and social networking
2.4 How to effectively research in the digital environment 15
activities stored on one’s Facebook profile differ from their offline equivalents. Appreciat-
ing and understanding this distinction is essential for the selection of appropriate research
designs and data processing methods. This is also important in securing a valid interpre-
tation of results in the context of social network structures. Several considerations arise in
this respect.
First, Facebook social networks represent an archive of users’ networking activities, as
opposed to a snapshot of their active connections. Users tend to add friends or acquaintances
they know from the past (e.g. primary school friends) and do not usually remove connec-
tions that become inactive with time. On the one hand, this mixture of past and current
relationships provides researchers with the possibility to analyse social networks that are no
longer limited by respondents’ recollection of them. On the other, this combination of his-
toric and active friendships might blur the researcher’s perspective on the current intensity
of a given user’s relationships.
Second, Facebook social networks are to some extent shaped by Facebook mechanisms.
As discussed in more detail in Section 2.5.1, Facebook provides users with suggestions re-
lated to other people they may like to connect with, potentially influencing the shape of
the social networks. Also, Facebook mechanisms choose the content to be presented on
users’ news-feeds, potentially affecting the intensity of interactions between the users. Such
recommendations introduce a difficult to control and potentially significant extrinsic fac-
tor in networking behaviour. Consequently, Facebook may facilitate connections between
people who would not connect otherwise or inflate the rate of interactions among existing
connections.
Finally, Facebook friendship networks may lack important real-life actors. Some might
not have a Facebook account and others might intentionally not be added as a friend in
order to conceal intimate relationships or for other reasons. It is therefore possible that
users’ social networks exclude influential individual connections or entire sub-groups of the
real-life networks. People who purposefully do not add family members on Facebook are
an obvious example of this.
2.4 How to effectively research in the digital environment
Recent technological developments and the increasing pervasiveness of digital devices ob-
serving and recording human activities have introduced great research opportunities. Rec-
ognizing the benefits of the digital environment, researchers have produced internet-based
versions of traditional surveys (e.g. [221]), designed online games mirroring offline exper-
16 Social research in the digital environment
imental designs (e.g. [133]) and developed applications monitoring participants’ behaviour
akin to a traditional diary approach (e.g. [179]). While translating established and well-
understood study designs to the digital environment may bring many advantages (e.g. in-
expensively reaching large samples), to fully benefit from the opportunities of the digital
environment researchers need to (1) develop a profound understanding of the new technol-
ogy, (2) re-examine and perhaps even abandon many of the established research tenets and
practices, and (3) become more creative in designing new methods for data collection.
2.4.1 Digital is an age of observation and secondary data
Neatly designed and skilfully executed controlled experiments constitute a cornerstone of
progress in modern social science. The main advantages of controlled experiments include
the ability to focus on a limited number of factors affecting one outcome, as well as design-
ing situations that are difficult to observe in nature (e.g. due to the rarity or randomness of
their occurrences). While some attempts have been made to design virtual reality environ-
ments to perform controlled experiments [22], such endeavours retain most of the limitations
associated with their offline predecessors. Rather than inviting participants to enter artifi-
cial environments, researchers should take advantage of well-populated virtual realities that
were created for other purposes. Administrators of games or other digital environments, for
example, might prove to be valuable collaborators. By integrating experimental manipula-
tions into existing applications, they can bring the process of data collection to a setting that
is more natural and comfortable for users than artificial research environments [15].
The growing number of digital devices recording human behaviour implies that obser-
vational studies and natural experiments provide an increasingly powerful alternative to
controlled experiments. The latter are often expensive, requiring a considerable amount
of time and effort from both experimenters and participants, whereas observational studies
and natural experiments can be conducted using data that has already been collected at a
massive scale. Moreover, digitally-empowered observational studies can be based on much
larger and diverse samples than controlled experiments. Studying the influence of multi-
ple predictor and outcome variables on large and diverse samples encompassing different
environments greatly decreases the need to control for confounding context factors. The
risk of obtaining random and/or non-generalisable effects is also reduced. Finally, observa-
tional studies in the digital environment can have an ongoing character, with models being
manually or automatically updated as new data becomes available.
Similar reasoning can be applied to self-report measures, which were originally designed
2.4 How to effectively research in the digital environment 17
to circumvent the difficulties of observing actual behaviour. While relatively practical and
efficient, self-reports are not without serious drawbacks, including response biases, delib-
erate or accidental misrepresentation and a number of cognitive biases. The ability to
observe behaviour at an unprecedented scale and in a cost-efficient manner renders self-
reports increasingly obsolete. Rather than asking participants’ to report on their behaviour
or preferences, Facebook allows their actual behaviours and preferences to be recorded.
Similarly, rather than requesting individuals to assess the extent to which they agree with
the statement: “I make friends easily”, researchers could, for example, measure the size and
growth of participants’ Facebook social networks, analyse the emotional valence of friends’
comments on their activities, and count the number of times they were tagged in other peo-
ple’s pictures. While such an approach requires greater skill and effort from the researcher,
it is likely to produce far more robust results than traditional self-reported measures.
Finally, it is unlikely that the variables of interest to the digital researcher are not already
being monitored by an existing service or applications. Rather than designing new moni-
toring tools or looking for ways to motivate participants, researchers should instead focus
on locating potential sources of secondary data. Apart from Facebook Graph API, there are
numerous other sources of extensive records of behavioural residues. Prominent examples
of such sources include Netflix, sharing the movie rental records of its consumers [169],
AOL releasing search records [97], and Nokia making detailed user logs available from a
large sample of mobile phone users [146].
There are several advantages of relying on secondary data. First, such samples are usu-
ally large, are already collected and require no or very limited financial investment. Second,
using such data allows many of the ethical problems hampering the social sciences to be
minimised (this is discussed in greater detail in Section 7.1). Third, results based on data
collected in natural rather than artificial research environments are more likely to be gener-
alisable to other contexts and populations.
Given the fact that responses to questionnaires reflect retrospective accounts of perceptions rather than ob-
servations of actual behaviour, they are prone to various biases. The process of generating responses to ques-
tionnaire items requires respondents to engage in a considerable number of higher-order cognitive processes,
including the procedural stages of comprehension, retrieval, judgement, response selection and response re-
porting [228]. Accounting for the complexity of this often unconscious and automatic process as well as the
limited capacity of the human working memory [13], the ultimate response product is prone to human error.
Among the biases most likely to interfere with the response process are: (1) The human tendency for socially
desirable answers [55], (2) the need for consistent response patterns, or consistency motif [121], (3) implicit
theories about the covariation of traits, behaviours and outcomes [178], (4) leniency bias, or the tendency to
alter the ratings of persons well-known and/or liked [95], and (5) dispositional as well as situational affective
mood components of cognitive assessments [32].
18 Social research in the digital environment
2.4.2 Motivate participants properly
Having well-motivated participants is a key factor in obtaining valid and accurate results.
Unmotivated and careless participants are likely to ignore experimenters’ instructions, an-
swer randomly, skip or misread the test items, or answer in the wrong areas of the form [145].
Motivation of participants in social science research is typically based on financial incen-
tives (e.g. payments or lotteries) or coercion (e.g. course credits). Unfortunately, neither
of these approaches provides encouragement to be attentive and respond honestly. Whereas
this might be of limited importance in the offline environment and in some research designs
(e.g. in experiments), poor motivation is particularly problematic in the online environment;
decreased control over the research setting, lack of physical contact between participants
and researchers and allowing participants to simultaneously engage in other activities, such
as instant messaging [91, 123], create additional barriers to communication. In fact, almost
40% of children aged 8 to 18 reported engaging in multiple tasks “most of the time” while
using the internet, while only 14% claimed to have “never” done so [199]. Also, based on
the analysis of 68 online surveys [157], an average drop-out rate of 16% has been reported,
with a considerable range from 0% to 73%.
Consequently, research in the online environment demands that researchers develop new
ways of incentivising participants. Designing studies interesting enough for the participants
to be willing to contribute their time, data and attention, might result in the data of unparal-
leled quality, and greatly reduce the cost of recruitment. This is, fortunately, supported by
opportunities to significantly improve and expand upon existing study designs using various
tools native to the digital environment. One way to encourage Internet users to participate
in research voluntarily is to focus on their experience and enjoyment. Digital experiments,
for instance, can be designed as online games [15]. By boosting participants’ motivation
while simultaneously increasing the ecological validity of the results, online gaming envi-
ronments offer a promising alternative to conventional research designs [133]. Another
approach to motivating participants is to provide instant feedback on their responses, data,
or behaviour within an experiment. For example, while participants in a pilot of a new psy-
chometric measure cannot receive feedback on their scores, researchers can mix the piloted
measure with an established measure, allowing not only for the integration of personalised
feedback but also providing additional research data. This demonstrates that even those
studies with no inherent design for feedback could be expanded to include elements that
make the assessment experience more engaging for the individual and more informative for
the researcher.
Considering the higher drop-out rate associated with the online environment [157], re-
2.4 How to effectively research in the digital environment 19
searchers should attempt to keep their participants engaged from the very beginning of the
study and collect the most crucial information as early as possible. A good example is
provided by myPersonality project [221], where users could receive feedback on their per-
sonality after answering as few as 20 questions, ensuring that the immediate barrier to par-
ticipation was low. Participants who enjoyed this short questionnaire (or were otherwise
motivated) could continue to answer more questions or quit and begin another assessment.
2.4.3 Avoid requesting self-reports of information that can be accessed
otherwise
In traditional research settings, it is common to require participants to answer a range of
demographic questions, usually at the beginning of a study. However, such an early request
to share personal information may be discouraging to some. It has already been said that
the online research environment offers numerous mechanisms by which this information
can be recorded automatically, for example from a social network profile (e.g. Facebook or
Google+) or by sending out customised links to different demographic groups (e.g. using
Facebook Advertising Targeting). This not only saves participants’ time and gives them
control over their data, but it also prevents typing errors and offers researchers the possibil-
ity to stay connected afterwards. I would in fact go further and suggest that any personal
information should be requested at the end rather than at the beginning of the study. Having
acquainted themselves with the design of the study and the feedback it offers, participants
might be more willing to provide honest responses. It would also be desirable to somehow
incentivise participants to provide accurate personal information. This could be achieved by
offering feedback that is highly personalised and explaining that the enjoyment and level of
insight to be derived from participation in the study will greatly increase if personal infor-
mation is accurate, responses are honest and behaviour is natural.
2.4.4 Do not force participants to respond
Even though dishonest or otherwise invalid responses are of greater detriment to the research
value of results than missing data, it is common practice to try to force participants to answer
all of the questions. In many studies, participants cannot progress to the next page or submit
their results before every question has been answered. By evoking feelings of frustration,
apathy or defiance, such an approach is likely to trigger deliberate selection of incorrect
responses by participants, thereby decreasing the validity of the results. I therefore suggest
that respondents ought to be allowed to skip any parts of the experiment. While missing
20 Social research in the digital environment
values continue to pose a challenge in traditional research settings, where the number of
participants is inherently limited, the possibility of analysing large sample sizes effectively
overrides this problem in the digital environment. It is also worth noting the growing number
of statistical methods designed to impute missing data [206] or to compute results based on
incomplete protocols (e.g. Item Response Theory [16]).
2.4.5 Do not prevent anyone from taking part
Scientific studies are often aimed at a specific population and usually consider repeated sub-
missions as a considerable threat to the validity of results. Highly attractive online studies
often attract repeated submissions or participants outside the target group, but it is submitted
that researchers should avoid preventing anyone from participating. Preventative measures
may encourage circumvention, creating an additional and unnecessary risk of collecting in-
valid data that may be difficult to filter out; participants may attempt to lie about their gender
or age, or use multiple computers to access a study multiple times, in order to simply pass
through the access limits. All of these factors lead to the conclusion that it is more efficient
to remove undesirable records during analysis of the results than to prevent participation in
the first place.
2.4.6 Acquire appropriate skills and experience
As digital environment significantly affects the lives of a great fraction of humanity, it is
increasingly impossible to fully understand social and psychological processes and study
humans without considering the role of this crucial factor. Effectively, social scientists
should not only target the digital environment in their work, but also take advantage of the
research opportunities it offers. However, in order to efficiently research digital environment
or use it as a research tool, social scientists need to acquire a wide range of new skills and
methods.
First, social scientists and practitioners are advised to maximise their exposure to and
understanding of the digital environment. Scientist should avoid designing studies and in-
terpreting results obtained in an unknown environment. Consequently, it is recommended
that researchers personally experience the platforms they intend to study or intend to use in
their study. Familiarizing oneself with Facebook and other digital platforms can to some
extent be achieved by studying literature, but this is no substitute for personal experience.
Second, the digital environment offers access to data of unprecedented size and diver-
sity. The sheer volume of this data allows for - and in some cases demands - the application
2.5 Limitations of Facebook research 21
of a wide range of analyses that have previously been under-represented in the social sci-
ences. Examples include Latent Semantic Analysis, Bayesian Graphical Models and Sparse
Matrix Factorisation approaches. In order to tap the full potential offered by digital records
of behaviours and preferences, social scientists need to learn how to use modern large-scale
data analyses such as these. This requires a profound understanding of scientific program-
ming languages like R [189] or Python [233] as well as the skills to build and maintain large
databases using MySQL, Microsoft SQL or other database engines well suited for handling
large amounts of data.
2.4.7 Go viral
The Facebook platform is designed to encourage virality of information and there are a
number of native features that researchers can use to enhance the success of their application
or survey.
Nevertheless, convenient access to millions of participants and the ability to promote the
study through social networks does not guarantee immediate success. It is also necessary to
have a practical understanding of the platform’s policies and the cultural norms created by
Facebook’s users. Successful applications make virality an integral part of the application
functionality rather than a clumsy add-on. For example, myPersonality users were able
to share their results with friends by creating a wall post and to invite other users to rate
their personality. This helped in building a database of cross-ratings whilst simultaneously
increasing the sample size (users who accepted an invitation to cross-rate their friends would
also ordinarily take one of the surveys themselves). Virality-by-design should therefore be
the goal of any ambitious study run in the digital environment.
2.5 Limitations of Facebook research
Conducting research or data collection on Facebook has a number of serious limitations.
While some are endemic, others can be tackled or ameliorated.
2.5.1 Hidden rules of the Facebook environment
Perhaps the biggest concern related to the validity of digital footprints collected through
Facebook relates to the unknown and changing mechanics of the platform itself. These
mechanisms greatly determine users’ exposure to different types of content and could in-
troduce severe biases to the data collected. It is of the utmost importance that researchers
22 Social research in the digital environment
keep abreast of changes in the platform’s functionality and in user experience, to the extent
to which that is possible, and account in their analyses for changes introduced by Facebook.
There are several features that ought to be accounted for.
First, Facebook users’ experience, including the selection of news-feed stories, adver-
tisements, and even friendship suggestions, is highly personalised. As users are more
likely to interact with content and people suggested to them by Facebook, their behaviour
is driven not only by their intrinsic goals and motivations, but also, to some extent, by the
mechanisms constantly adjusting their exposure to content. For example, research results
describing the patterns of content spread across the social networks (e.g. [2]) might be de-
scribing a real social network phenomenon, but could be also attributable to Facebook’s
content recommendation engine.
Second, many profile entries, such as job position or political views, are collected using
open text input fields. Such fields are often equipped with an auto-suggestion mechanism,
suggesting potential entry values based on the letters typed in by the user. This feature
makes platforms more user-friendly and the database more elegant, but it could also affect
and bias user choices. For example, a user might have intended to identify himself as being
a Social Psychologist, but after typing 3 letters in the relevant field and being presented with
the platform’s suggestion of Social Scientist, she might settle on the latter option. Effec-
tively, the changes in the frequency of one or the other entry could be interpreted as a real
world phenomenon (i.e. a decrease in people’s self-identification as social psychologists).
However, in this example, it would simply be the expression of a technological feature.
Third, Facebook constantly experiments with its interface. New features are introduced
for only subsets of its users in order to test their popularity and performance, selectively
affecting the behaviour of those users. Participants’ experience with certain types of content
or their engagement with a given application may therefore differ within the same sam-
ple, emphasising the fact that the hidden mechanisms of the Facebook environment do not
always operate in a homogeneous fashion.
2.5.2 Application maintenance
Regular maintenance may also be required to ensure compliance with official usage poli-
cies and to respond to any changes in behavioural norms that might render an application
The rules driving the personalization process are unknown, but, most likely, it is based on what the
Facebook recommendation engine infers to be most engaging for a given user. Typical recommendation
systems are based on the records of previous behaviour of a given user (e.g. type of content the user had
interacted with previously), user’s profile data (e.g. current location or relationship status), and the behaviour
and preferences of similar people or users’ friends.
2.5 Limitations of Facebook research 23
undesirable.
Since the rules and policies of Facebook change relatively often, a successful application
with thousands of users may become obsolete next month, when an algorithm is changed
or a link redirected. A great majority of applications that had millions of users in 2007 are
no longer in existence. It is therefore paramount that research applications scale well in
this dynamic environment. Researchers need to be prepared for sudden spikes in activity,
making user support an ongoing concern.
For example, in myPersonality’s experience, there was one private support request per
day for every 5,000 daily users, on average. However, maintaining the application Facebook
page, posting news about new features and responding to users’ public comments further
increases the commitment.
2.5.3 Changing the past
While it is impossible to change the past, it is certainly possible for users to alter the records
of their past behaviour. This can impact upon the validity of the data being studied, posing
additional challenges in cases where researchers are granted access to only a snapshot of
a participant’s activity. While any biases introduced by these alterations are likely to be
less significant than those associated with self-reporting, their potential impact ought to be
carefully considered when drawing conclusions about individual profiles.
2.5.4 Evolution of cultural norms
More significant than alterations in individual records is the rapid evolution of the Face-
book platform itself. Since its inception in 2004, the introduction of new features has been
accompanied by changes in user behaviour and accordingly in the unwritten etiquette, or
cultural norms, that govern online interactions.
When analysing digital footprints collected on Facebook, researchers should always in-
vestigate whether global trends observed in the data (e.g. users posting more pictures) stem
from changes in the behaviour of users, or from changes in the platform. The new timeline,
the Facebook Like button, Facebook places and the launch of Facebook Graph Search have
undoubtedly had profound effects on behaviour and on the structure of social networks. As
a consequence, it is questionable whether studies conducted on Facebook as early as 2005
can be directly compared to those conducted in 2011 [242].
24 Social research in the digital environment
2.6 Conclusions
Taking account of the fact that Facebook shapes the lives and social interactions of more than
17% of the world’s population, it appears increasingly impossible to fully understand hu-
man behaviour without studying its dynamics and implications. However, as shown in this
Chapter, Facebook is more than just another interesting topic for social science research.
The combination of advantages generally associated with internet-based research methods
and the idiosyncrasies of the Facebook environment provides researchers with a highly ef-
fective and sophisticated research instrument. By offering an inexpensive and time-effective
way to reach large and diverse samples while at the same time allowing for the collection
of reliable records of behaviour in a natural environment, Facebook makes it possible for
researchers to collect data with an external validity that could otherwise be achieved only
in well-funded projects. Likewise, the integration of enjoyable study designs and privacy-
protecting mechanisms to track participants over a given period of time is likely to enhance
their experience and trust. Taken together, Facebook offers a highly convenient and unique
way of both conducting and participating in social science research.
In order to tap the full potential of these possibilities, however, it will be necessary for
social scientists to develop a profound understanding of the Facebook environment, develop
innovative study designs and employ advanced methods of data analysis. Responding to the
relative scarcity of the literature on this topic, we have offered some advice on how to ethi-
cally conduct social science research on Facebook. We stress that while the methodologies
prevalent in social science research have evolved over many decades and have been shown
to be of great value in traditional settings, the tremendous technological and social progress
observed in the digital environment now demands that the academic community rethinks
or even discards some of the long-standing methods. Perhaps most crucially, we believe
that providing participants with non-financial motivation to participate in the study should
become an important part of the design. Such an approach would not only solve numerous
ethical issues but is also most likely to result in data of the highest quality and quantity.
Beyond that, we suggest that the social sciences ought to derive greater benefit from the
unprecedented amounts of secondary data being collected by commercial organisations and
other third parties.
Chapter 3
myPersonality Project: Collection and
validation of a large online sample
Dataset described in this sample was collected in collaboration with David Stillwell. How-
ever, the results and text presented in this chapter were produced solely by Michal Kosinski.
This chapter describes the collection process and validity checks of the myPersonality
sample, used in the studies presented in Chapters 4 and 5. The following sections briefly
introduce the myPersonality application and the sample it produced, discuss the potential
threats to the validity of its data, describe the measures that mitigate those risks, and provide
results suggesting that the myPersonality sample contains valid personal data and highly
reliable psychometric scores.
Importantly, the collection, preprocessing, maintenance and sharing of the datasets col-
lected by myPersonality application constituted a significant fraction of author’s academic
work during the course of his PhD.
3.1 Introduction
myPersonality application [221] offered a set of established personality and ability measures
to Facebook users. It was first published in June 2007 and continued collecting the data until
January 2012, in which period over 7.5 million people have completed at least one of the
A new version of myPersonality is currently being designed and programmed.
26 myPersonality Project: Collection and validation of a large online sample
ity participants received personalised feedback on their scores and could opt-in to donate
the information stored on their Facebook profile data for research purposes. Over 45% of
myPersonality participants volunteered to donate their data.
Fig. 3.1 Screen capture presenting myPersonality implementation of a Five Factor Model
personality inventory.
myPersonality did not actively recruit participants, but spread by the digital equivalent of
the word of mouth - participants sharing their results with friends, which in turn encouraged
them to use the application. Such an approach to recruiting participants would be signifi-
cantly more difficult nowadays, as Facebook limits the exposure of application messages in
an attempt to encourage application owners to spend on paid advertising. The popularity of
myPersonality was certainly boosted by its novelty - to my knowledge it was the first appli-
cation of this type to appear on Facebook. Similar applications published in the following
years, often more advanced in terms of technology and design, attracted much less interest
among Facebook users.
Recognizing the generosity of the participants that have donated their data for research
purposes, the authors of myPersonality decided to share its databases with the academic
community. In 2010 the myPersonality project website ( ) was
At the time of writing, applications could still let participants post content, such as their personality scores,
on their walls, but such content is unlikely to appear on their friends’ news-feeds if the application owner does
not use Facebook marketing tools.
3.1 Introduction 27
established offering to its registered collaborators dozens of cleaned, preprocessed and
anonymised databases, knowledge base, and examples of code useful in analysis. The
database contains over 3.5 million individual profiles including a wide range of psycho-
demographic variables and extensive records of digital footprints. It holds scores and item-
level data for over 25 psychological scales ranging from the 300-item long International
Personality Item Pool (IPIP) Five Factor Model (FFM) Personality Inventory [84], through
Centre for Epidemiological Study Depression Scale [190] and an adaptive proxy of Raven
Standard Progressive Matrices [192], to personality cross-ratings. The scores on the most
popular questionnaire, IPIP FFM Personality Inventory available in different lengths (from
20 to 300 item long), are available for more than 3.1 million unique participants.
Facebook profile information stored in myPersonality databases includes records of
wide range of variables, such as detailed demographic profile, friendship networks, status
updates, Likes and group membership. Number of databases were heavily pre-processed
prior to publication; for example, the database of nearly 1 million participant-Likes com-
ponent scores (described in [140]), Linguistic Inquiry and Word Count (LIWC) tags for
12 mn status updates of over 88,000 participants, egocentric social network properties of
over 50,000 participants (described in [138]), and 3.5 million friendship triads composed of
myPersonality participants. The full and up-to-date list of available datasets is available at
myPersonality project’s website [221].
The age and gender distribution of the myPersonality sample is wider than in most of
the traditional samples. According to [91], 71% of participants in 510 paper-and-pencil
samples from 156 articles published in the Journal of Personality and Social Psychology
(JPSP) during 2002 were female. Gender bias in the myPersonality sample was significantly
weaker (55% females) and comparable with an online dataset collected by [91]. The average
age of the myPersonality sample at the time of collection was about 24 years old, with 42%
of participants younger than 21 years. This is similar to the average age in the paper-and-
pencil samples published in JPSP and the online sample reported by [91] (23 and 24 years
old respectively).
myPersonality project and its participants inspired and enabled a wide range of research
projects, apart from this thesis. At the time of writing, nearly 150 researchers from over
60 academic institutions, ranging from Microsoft Research to Harvard University, were ac-
tive collaborators of the myPersonality project. Those, and earlier collaborations resulted
in 33 peer-reviewed publications to date, covering a wide range of topics, such as person-
ality [139, 197], social networks [138, 185–187], privacy [188], language [211], predicting
individual traits [98, 140], computer science [24], happiness [237], music [196], and delayed
28 myPersonality Project: Collection and validation of a large online sample
discounting [156]. For a full list of collaborators and myPersonality related publication see
Appendix A.
3.2 Threats to the validity of online samples
Even a well-controlled offline study can produce some invalid individual protocols due to
random responding—a strategy in which responses are made without regard to item con-
tent [145], or deliberate misrepresentation. As discussed in Chapter 2 the risks to validity
of the data can be exacerbated in the online environment. Obviously, the protocols that are
partially or wholly produced in this way have reduced or no validity at all [53]. This section
explores the potential threats the validity of the data collected online, including linguistic
incompetence, inattentiveness, or deliberate misrepresentation, and describes the ways in
which such threats were mitigated in case of the myPersonality project.
3.2.1 Most widespread threats to the validity of online samples
Carelessness and inattentiveness. In most research designs, inattentive and/or careless
participants cannot produce a valid protocol. Such participants may skip or misread the
instructions or the questions, answer in the wrong areas of the answer sheet or respond
randomly [145]. Internet studies have several features that may lead to higher levels of inat-
tentiveness and carelessness. First, low barriers to access the study, ease of responding and
instant feedback may encourage the participants to rush through the study without paying as
much attention [123]. Second, researchers have little or no control over the circumstances
in which the study is being accessed, so it is possible that some participants simultane-
ously engage in other activities, such as instant messaging. Third, the lack of face-to-face
contact increases the psychological distance between the researcher and the participants,
which may decrease the feeling of accountability in participants, especially if the study is
anonymous [91].
Misrepresentation. Another threat to the validity of the data stems from a deliberate
and conscious attempts to manipulate one’s behaviour or responses. There are two main
forms of misrepresentation, “faking good” and “faking bad” [145]; those employing the
former misrepresentation strategy attempt to appear as better adjusted, more attractive and
competent than they are perceived in everyday life, while those using the latter strategy try
to exaggerate their incompetence or maladjustment. The numbers of the protocols invali-
dated by deliberate misrepresentation may be elevated in web-based studes, since the lack
3.2 Threats to the validity of online samples 29
of personal contact, the psychological distance and feeling of anonymity may encourage
participants to use misrepresentation strategies or even construct completely fake identi-
ties [44].
Linguistic incompetence. Web-based studies can be easily accessed by participants
from different backgrounds and countries. It is a great advantage, but it also elevates the risk
of obtaining data invalidated by participants’ linguistic incompetence. Non-native speakers
or those who use different variety of a given language may misunderstand instructions or
test questions [123]. Furthermore, even language competent participants can interpret a
question or statement in an idiosyncratic way [122].
3.2.2 Mitigating risks to data validity
myPersonality included several features aimed at maximizing the validity of the data. The
following paragraphs briefly review those that may have had the largest impact on the quality
and the size of the resulting sample.
In stark contrast to many other social science studies, the myPersonality sample is
entirely composed of volunteers. Participants were neither motivated by credit points or
money, as commonly studied undergraduate psychology students are, nor by the pressure of
the researcher collecting his sample. Effectively, they were mainly concerned with receiv-
ing accurate feedback on their scores, a goal that coincided with the objectives of the study
— obtaining accurate scores on psychometric scales. This approach arguably decreases
the number of unmotivated and/or careless participants that may answer randomly, skip, or
misread the test items [145], so increasing the validity of the data.
As random responding has much more adverse effect on the validity of the sample than
missing answers, participants were not forced to take all of the questionnaires and could skip
any of the questions. Also, to minimise random responding related to exhaustion, partici-
pants could adjust the number of items they were willing to take when responding to longer
questionnaires, such as 100-item IPIP FFM Personality Inventory. Another approach was
tested in the case of myPersonality application’s longest measure: 300-item version of the
IPIP FFM Personality Inventory. To ascertain that participants are appropriately motivated
to spend up to one hour filling this questionnaire, they were required, before getting access,
to fill a battery of uninteresting marketing surveys or make a donation towards the cost of
hosting the project (an equivalent of US 4$). Results presented in this work confirm that
such approach results in highly valid response protocols.
The questionnaires were available to all participants and could be accessed as many
30 myPersonality Project: Collection and validation of a large online sample
times as desired. This not only decreased the risk of participants responding dishonestly or
using fake profiles to access the application, but resulted in large amounts of potentially in-
teresting data. For example, IPIP FFM Personality Inventory was retaken over 1.15 million
times, and over 5 thousand participants took this questionnaire more than 10 times. Such
retest information can be used in longitudinal studies, or to examine psychometric proper-
ties of the questionnaire, such as test-retest reliability, misrepresentation (manifesting in a
low correlation between tests at time one and time two), and much more.
3.3 Validation of the myPersonality sample
The most direct way of ascertaining the quality of the data provided by the participants is
to cross-validate it with other sources of participant-related information. Reported gender,
for example, can be compared with the information on one’s Facebook profile, or validated
against the profile picture. Scores on psychometric tools, such as personality questionnaires,
can be validated against another, confident sources of information, including ratings from
participant’s peers, self reports of behaviour and life events, or by retesting a participant
with another psychometric tool aimed at the same traits [37]. Most practical and easily
implementable approaches to establishing the validity of psychometric scores rely on the
internal properties of the response data, such as discriminant validity, internal consistency
of the scales, number of missing responses, and the length of response strings.
Age, gender, and 100-item IPIP FFM Personality Inventory were chosen to investigate
the validity of the myPersonality data. IPIP FFM Personality Inventory, is an open-source
representation of the FFM construct of the Costa and McCrae’s employed in NEO-PI-
R [52]. IPIP FFM Personality Inventory scores were shown to correlate highly with the
corresponding NEO-PI-R domain scores, with correlation coefficients ranging from r = .88
to r = .93 [83]. The IPIP FFM Personality Inventory is widely used in both traditional
and online personality assessment and has proven to be useful and reliable in the research
(e.g. [37, 49]). Moreover, IPIP domain scales have been shown to outperform the matching
NEO-PI-R constructs as predictors of a number of self-reported behavioural indices [84].
The results presented in this chapter are based on a snapshot of the myPersonality
database made in January 2009, at the time of the first publication of the myPersonality
project website [221]. Database consists of 182,922 individual response patterns to the 100-
item IPIP FFM Personality Inventory, 28,628 profiles containing two independent records
of gender, and 18,321 profiles containing two independent records of date of birth.
3.3 Validation of the myPersonality sample 31
3.3.1 Study 1: Cross-validation of age and gender
Several questionnaires published on the myPersonality application included questions re-
lated to participants’ age and gender [221]. Such self-reported information can be compared
against the data stored on participants’ Facebook profiles, providing a proxy for the validity
of both sources of data. Note that while neither of the sources can be treated as a “ground-
truth“ data, potential inconsistencies provide a lower-bound estimate of the invalidity.
Results
In a subset of 28,628 participants where both Facebook profile and self-reported information
about gender was available, it did not match in 356 (1.2%) of cases. Sixty two participants
self-reported their gender as female but listed as males on Facebook, while 294 participants
did the opposite.
Age difference
ycneuqerF
−15 −10 −5 0 5 10 15 20
Fig. 3.2 Frequencies of the age difference (in years) between myPersonality data and Face-
book profile information. Only cases in which age values did not match are presented.
Values above 0 indicate that age entered in Facebook profile was lower than age reported to
myPersonality. Note that extreme values are omitted from this plot for clarity; age difference
ranged from -65 to +94 years.
32 myPersonality Project: Collection and validation of a large online sample
The date of birth differed between the Facebook profile and self-reported questionnaire
in 5% of cases (948 out of 18,321 records). The distribution of the differences between those
their age to be lower than what was stated on their Facebook profiles, and 701 participants
did the opposite. In 294 cases the difference in age was lower than 1 year.
Those results indicate that the agreement between Facebook profile and self-reported
data is relatively high. This suggests that both sources, Facebook profile and voluntary
self-reports, provide highly valid information. While it is possible that some participants
provided matching but invalid values on Facebook profile and within myPersonality appli-
cation, this is relatively unlikely as those who did not want to reveal their age or gender
could simply skip relevant questions or remove/hide given information from their Facebook
profiles.
Lower, when compared with gender, validity of age variable may be driven by two
phenomena. First, at the time of writing the minimum age required to create a Facebook
account was 13 years. Hence, some of the cases in which participants told myPersonality to
be younger than indicted on their Facebook profiles, might be caused by those participants
reporting higher age during Facebook registration to circumvent the age limit. On the other
hand, some of the participants that told myPersonality to be older than reported on their
Facebook profiles, might have knowingly under-reported their age on a Facebook profile,
for instance to increase their appeal as potential romantic partners. Note that as presented on
profile) the age difference fell within 6 years, difference unlikely to be detected by their
peers based solely on the profile picture.
3.3.2 Study 2: Discriminant validity and reliability of the IPIP FFM
Personality Inventory scales
The overall validity of responses in a sample can be examined by estimating the reliabilities
of the scales and discriminant validity of the instrument [119]. The reliability of the scale is
a measure of its consistency across all participants; reliable scales are characterised by high
correlations between responses to items, indicating that such responses are driven by an
underlying latent trait. Unusually low values of scale reliability achieved in a given sample,
may be an indication of high rates of random or dishonest responses. This study uses one
of the most popular classical measures of reliability: Cronbach’s alpha coefficient (α; [54]),
representing an average correlation between pairs of scores obtained by splitting the scale
3.3 Validation of the myPersonality sample 33
in half in all possible ways. The higher the alpha, the more reliable the scale, suggesting
high validity of the individual responses.
Discriminant validity tests whether scales that are supposed to be unrelated are, in fact,
unrelated. FFM model, for example, assumes that its five scales should be relatively un-
correlated. Strong relationships between such scales might indicate that participants’ re-
sponses were aimed, for example, at achieving a socially desirable profile. Discriminant
validity is usually expressed as an average absolute Pearson product-moment correlation
between scales of an instrument (µ ); the lower the value of such correlation, the higher
|r|
the discriminant validity.
Results
Discriminant validity of the myPersonality implementation of the IPIP FFM Personality
Inventory equals µ = .16 and it is better than one reported for comparable paper-and-pencil
|r|
samples (µ = .20; [120]). Also, individual scales’ reliability of the IPIP FFM Personality
|r|
Inventory are on average better than those reported for the standardization sample used to
develop the psychometric properties of this tool [83]. In myPersonality implementation of
the 100-item version of the questionnaire the average reliability (µ ) of five domain scales
equalled µ = .91, compared with µ = .89 reported for the standardization sample. In the
α α
300-item version, the average reliability of the 30 facet scales in myPersonality sample was
µ = .84, compared with µ = .80 reported for the standardization samples. The full list
α α
of scale reliabilities is presented in Appendix B.
3.3.3 Study 3: Internal consistency
Section 3.3.2 focuses on the properties of the entire sample. However, the consistency of
responses can be also measured on the level of an individual participant to detect those who
are responding randomly [145]. There are several approaches to measuring response con-
sistency in psychometric measures. The semantic antonym method [85] relies on the fact
that the responses to items that are semantically opposite (e.g.: “I dislike myself” and “I
am very pleased with myself”) are expected to be negatively correlated. Similar approach,
Goldberg’s Psychometric Antonym (GPA; [123]) relies on pairs of items that have the high-
est negative correlation but are not necessarily semantically opposite. GPA coefficient is
reversed so the higher values indicate more consistent protocols.
Fisher transformation was used to average the reliability coefficients.
34 myPersonality Project: Collection and validation of a large online sample
Another inconsistency measurement method, Jackson’s Individual Reliability Coeffi-
cient (JIR; [116]), relies on correlating split-half scores across multiple scales within a single
protocol, using Pearson product-moment correlation corrected for decreased length by the
Spearman-Brown formula. Jackson [116] proposed that individual protocols with JIR score
below .3 ought to be categorised as inattentive, careless, uncooperative or linguistically in-
competent. This cut-off point was later endorsed by [123]. More sophisticated approach is
offered by the Z coefficient [61] discussed in detail in Chapter 6.
Two consistency measures were estimated for myPersonality implementation of the 100-
item IPIP FFM Personality Inventory: GPA [123] and JIR [116] coefficients. GPA was
based on the 15 psychometric antonyms, listed in Appendix C, identified based on the inter-
the rules of choosing the psychometric antonyms were relaxed by including antonyms com-
ing from the same scale. JIR and GPA values estimated for the myPersonality sample are
compared with the values reported by [123] for a similar online sample.
Results
skewed shape of both frequency graphs indicates that the majority of individual protocols
were highly consistent.
Fig. 3.3 Frequency distributions of GPA and JIR internal consistency coefficients.
Internal consistency of myPersonality sample was lower than one reported by [123]. The
3.3 Validation of the myPersonality sample 35
average JIR value for myPersonality sample equalled µ = .73 (σ = .38), compared with
JIR
µ = .83 reported by [123]. Moreover, the minimum acceptable JIR value of .3 proposed
JIR
by [116] and used by [123] was not met by 10.3% protocols, much higher value than .2%
in [123]. The average value of the GPA coefficient in myPersonality sample was µ =
GPA
.61 (σ = .26), higher than µ = .47 reported by [123]. About 3% of myPersonality
GPA
protocols fell below the minimum GPA value (.03) proposed by [123].
Relatively low internal consistency values on the individual level are surprising in the
context of high reliabilities of the scales reported in Section 3.3.2. There are several potential
sources of decreased internal consistency that could explain those results. JIR coefficient
could have been negatively affected by the fact that results presented here are based on
shorter and hence less reliable scales than in [123]. Also, lower GPA score might be to
some extent attributed to the relaxed rules employed to choose the psychometric antonyms
in this study.
It is likely, however, that lower GPA and JIR levels in myPersonality sample are not an
indication of its lower validity. Some findings suggest that inconsistent protocols may be a
product of conscious and attentive responding and are driven by an underlying psychologi-
cal trait. Two independent studies found that out of five personality scores, only Agreeable-
ness exhibited an appreciable reduction in convergent validity for the sets of inconsistent
responses [53, 145]. Also, GPA was shown to correlate with Neuroticism and Openness and
have a distribution close to normal, which is characteristic for individual differences [123].
personality scores.
Personality Score
O C E A N GPA
GPA .21 .17 .33 .11 -.22 1
JIR .23 .11 .04 .22 -.30 .31
Note: All of the correlation coefficients pre-
sented above are significant at p < .001 level.
To further investigate this issue, the correlation between internal consistency measures
internal consistency measures and personality traits (r = .33 between GPA and Extraversion
and r = −.30 between JIR and Neuroticism) suggest that internal consistency might be
Note that GPA coefficient is reversed. Effectively, higher values, indicating higher negative correlation
between psychometric antonyms, are desirable.
36 myPersonality Project: Collection and validation of a large online sample
regarded as one of the personality traits. This was consistent with the previous studies on
the subject [53, 123, 180]. Effectively, the lower consistency of the myPersonality sample
could be to some extent driven by differences in personality scores distributions between
the sample.
3.3.4 Study 4: Missing responses and Consecutive Identical Response
Strings
Individual protocols beset by high numbers of missing responses and long Consecutive Iden-
tical Response Strings (CIRS) produced by continually selecting the same response option
in subsequent items, are of questionable validity. While it is possible that some honestly
responding participants indeed “Strongly Agree” with or cannot answer to 20 consecutive
questionnaire items, it is relatively unlikely. Effectively, excessively long CIRS or miss-
ing response strings are usually considered a sign of rush, distraction, tiredness, linguistic
incompetence, carelessness or inattentiveness. Such phenomena are more likely to occur
in the longer questionnaires and towards the end, when some of the participants lose their
focus [166].
This section compares the number of missing responses and the maximum length of
CIRS between myPersonality implementation of the 100-item IPIP FFM Personality In-
ventory and the values reported for similar questionnaires administered online and offline.
Normative data for missing responses used in this study is provided by [123], who reported
the average of 1.2% missing responses in the online implementation of the 300-item IPIP
FFM Personality Inventory and .1% - .5% in several paper-and-pencil samples. He proposed
to treat protocols as invalid if they have more than 10 missing responses, which included
2.9% of his online sample.
Normative data for CIRS is based on a paper-and-pencil sample of responses to the 240-
item long NEO-PI-R questionnaire, composed respondents believed to be highly coopera-
tive and attentive (n = 983; [53]). In this sample no one used the same response option more
than 6, 9, 10, 14, 9 consecutive times (“strongly disagree”, “disagree”, “neutral”, “agree”,
“strongly agree” respectively). This cut-off points applied by [123] to an online sample of
300-item IPIP FFM Personality Inventory and paper-and-pencil sample of NEO-PI-R pro-
tocols resulted in removing 6.3% and .9% records respectively.
While the myPersonality version of the questionnaire was shorter than one used by [53]
and [123], more conservative (i.e. lower) cut-off points could be considered in practice
(which, as results suggest, is also enabled by low average CIRS and missing response strings
3.3 Validation of the myPersonality sample 37
values in myPersonality sample). In this work, however, values proposed previously are
used for the sake of comparison.
Results
item IPIP FFM Personality Inventory protocols.
# missing Frequency Percent Cumulative
Percent
0 143387 78.4% 78.4%
1 29084 15.9% 94.3%
2 6678 3.7% 97.9%
3 1938 1.1% 99.0%
4 741 .4% 99.4%
5 338 .2% 99.6%
6 177 .1% 99.7%
7 112 .1% 99.7%
8 72 0% 99.8%
9 50 0% 99.8%
10 45 0% 99.8%
11 29 0% 99.9%
12 26 0% 99.9%
13 18 0% 99.9%
14 20 0% 99.9%
15 17 0% 99.9%
16 20 0% 99.9%
>17 170 .1% 100%
Sum: 182922 100%
of missing responses in a protocol was .34% which is comparable to the .1% - .5% missing
responses reported for a set of comparable paper-and-pencil inventories and nearly four
times smaller than 1.2% reported for the comparable online sample [123]. Also, only .16%
of cases had more than 10 missing responses, much lower value than 2.9% reported for
an online sample by [123]. Low numbers of missing responses suggest that myPersonality
participants were highly motivated and attentive.
show that the average maximum CIRS in the individual records varied from µ = 2.08
CIRS
The number of missing responses in the individual protocol did not correlate with any of the personality
scores.
38 myPersonality Project: Collection and validation of a large online sample
myPersonality implementation of the 100-item long IPIP FFM Personality Inventory.
Response category
1 2 3 4 5
Very Moderately Neither Moderately Very
inaccurate inaccurate accurate accurate
Mean 2,08 2,50 2,43 3,10 2,39
SD 1,55 0,98 1,83 1,28 2,19
SRIC
tsegnol
ehT
0 1638 950 1501 703 1273
1 54814 19019 32914 6930 37835
2 71905 79051 79593 47773 71864
3 43164 61568 43817 74564 52242
4 8419 17812 16297 34362 12551
5 2267 3354 5286 11880 4252
6 417 831 1939 4212 1357
7 156 241 780 1645 933
8 42 63 359 539 268
9 20 16 142 175 111
10 17 6 77 64 44
11 7 3 51 23 43
12 5 0 31 10 16
13 3 0 20 9 9
14 3 1 9 7 6
15 2 1 7 3 7
16 1 0 8 0 3
>16 42 6,0 91 23 108
Sum: 182922 182922 182922 182922 182922
Note: The maximum response lengths found by [53] and endorsed by [123] are in boldface
and followed by horizontal lines.
3.4 Conclusions 39
(σ = 1.55) for response option (“Very inaccurate”) to µ = 3.1 (σ = 1.28) for response
CIRS
option 4 (“Moderately accurate”). Only .3% records in myPersonality sample fall below the
CIRS length cut-off points suggested by [51] and endorsed by [123]. This value is much
lower than 6.3% reported in the online sample by [123] and comparable with .9% reported
by the same author for the paper-and-pencil NEO-PI-R protocols.
3.4 Conclusions
The results presented here suggest that the validity of the myPersonality sample is relatively
high. Study 1 found the inconsistency between self-reported and Facebook profile values of
age and gender to be 5% and 1.2% respectively. While this constitutes the lower bound limit
for the validity of the data (i.e. at least such fraction of records is invalid), it is unlikely that
the true fraction of invalid records is much higher. Study 2 showed that the discriminant va-
lidity of the myPersonality implementation of the 100-item IPIP FFM Personality Inventory
were higher than the average discriminant validity in a large sample of studies published in
a premier journal. It was also found that the reliability of the scales of the same inventory
were higher than those reported by its authors. Study 3 examined the internal consistency
of individual response patterns and showed that it was lower than in a comparable sample
collected online. This finding is somewhat surprising in the context of the results obtained
in Studies 1, 2 and 4 suggesting an above average validity of the myPersonality sample.
Potential explanation of this phenomenon could lie in differing lengths of compared scales,
but it certainly deserves further examination. Finally, Study 4 focused on the number of
missing responses and the length of CIRS, showing that myPersonality respondents were
exceptionally unlikely to omit questions or continuously select the same response option.
This suggests that despite the lack of control over the study environment and allowing par-
ticipants to skip questions, their response protocols were comparatively complete and valid.
The findings of this chapter suggest that the myPersonality sample contains valid per-
sonal data and highly reliable psychometric scores. This is further confirmed by the psy-
chologically meaningful results obtained in various analyses based on the myPersonality
sample, presented in Chapters 4 and 5. Relatively high validity of the myPersonality sample
achieved despite the lack of control over the test-taking environment and participants cul-
tural background and language skills, indicates that participants were highly motivated to
provide honest and meaningful responses.
Numerous questions remain unanswered and should be addressed in future research.
Analyses focused on age, gender and FFM personality scores could be expanded to include
40 myPersonality Project: Collection and validation of a large online sample
other variables collected by the myPersonality application. Also, large sample of responses
collected by the myPersonality project could be used to improve the original IPIP FFM Per-
sonality Inventory scales (see also [37]), build Item Response Theory models, and perhaps
even a computerised adaptive personality inventory. Finally, possible reasons for misrepre-
senting age and gender could be explored in more detail.
Chapter 4
Personality and behaviour in the digital
environment
Fragments of this chapter were previously published in [8, 138, 139].
This chapter examines how personality is manifested in users’ behaviour in the digital
environment as reflected by the websites they browse and their Facebook profile features.
Results based on a sample of over a third of a million users show that there are psychologi-
cally meaningful links between users’ personalities, their website preferences and Facebook
activity.
4.1 Introduction
Decades of psychological research suggest that individuals’ behaviour and preferences can
be accurately explained by psychological constructs called personality traits [3]. This is
valuable in practice, as it implies that knowledge of an individual’s personality enables pre-
diction of both behaviour and preferences across different contexts and environments. Re-
search has shown that personality is correlated with many aspects of life, including job suc-
cess [19, 126, 225], attractiveness [42], drug use [198], marital satisfaction [130], infidelity
[172], and happiness [173]. Also, researchers have shown that individuals can identify other
people’s personality traits by examining their living spaces [90] or music collections [195].
Following the shift of human interactions, socializing and communication activities towards
online platforms, researchers have noted that personality-related behavioural residues are
not restricted to the offline environment and showed that personality is related to keyboard
42 Personality and behaviour in the digital environment
and mouse use [131], contents of personal websites [159, 234], and Facebook Likes [140].
This chapter examines whether personality is reflected in human behaviour in the digital
environment, represented by people’s website choices and their Facebook profile features.
First, the personality model and datasets used in this chapter are introduced. Second, exam-
ples of the associations between personality and website choice are presented and discussed.
Third, the relationship between Facebook profile features and personality is explored in de-
tail. Finally, this chapter is concluded with a discussion of the significance of the results.
4.2 Methods
4.2.1 Five Factor Model
Five Factor Model (FFM) of personality used here represents the most widespread and gen-
erally accepted model of personality [52, 82, 203, 229]. FFM was shown to subsume most
known personality traits and it is claimed to represent the basic structure underlying the
variations in human behaviour and preferences, providing a nomenclature and a concep-
tual framework that unifies much of the research findings in the psychology of individual
differences. FFM includes the following traits:
Openness to experience (or Openness) is related to imagination, creativity, curiosity,
tolerance, political liberalism, and appreciation for culture. People scoring high on Open-
ness like change, appreciate new and unusual ideas, and have a good sense of aesthetics.
Conscientiousness measures the preference for an organised approach to life in contrast
to a spontaneous one. Conscientious people are more likely to be well organised, reliable,
and consistent. They enjoy planning, seek achievements, and pursue long-term goals. Non-
conscientious individuals are generally more easy-going, spontaneous, and creative. They
tend to be more tolerant and less bound by rules and plans.
Extraversion measures a tendency to seek stimulation in the external world, the com-
pany of others, and to express positive emotions. Extraverts tend to be more outgoing,
friendly, and socially active. They are usually energetic and talkative; they do not mind
being at the centre of attention and make new friends more easily. Introverts are more likely
to be solitary or reserved and seek environments characterised by lower levels of external
stimulation.
Agreeableness relates to a focus on maintaining positive social relations, being friendly,
compassionate, and cooperative. Agreeable people tend to trust others and adapt to their
needs. Disagreeable people are more focused on themselves, less likely to compromise, and
4.2 Methods 43
may be less gullible. They also tend to be less bound by social expectations and conventions
and are more assertive.
Emotional Stability (reversely referred to as Neuroticism) measures the tendency to
experience mood swings and emotions, such as guilt, anger, anxiety, and depression. Emo-
tionally unstable (Neurotic) people are more likely to experience stress and nervousness,
whereas emotionally stable people (low Neuroticism) tend to be calmer and self-confident.
4.2.2 Sample
The sub-sample of the myPersonality dataset (see Chapter 3) used in both studies presented
in this chapter is large and diverse, consisting of over 350, 000 US Facebook users. To the
best of my knowledge, this constitutes the largest ever collected sample linking psycholog-
ical traits to the behaviour in digital environment. Users’ personality was measured using
the 100-item long International Personality Item Pool (IPIP) Five Factor Model (FFM) Per-
sonality Inventory discussed in more detail in Chapter 3. Users’ website preferences were
recorded using their website-related Facebook Likes and a questionnaire specifically de-
signed for this study. Facebook profile features (i.e. the behavioural residues of Facebook
activity) analysed here include: the size and density of the users’ Facebook friendship net-
works, the number of Facebook Groups and Likes that a user has connected with, the number
of photos and status updates uploaded by the user, the number of times the user was tagged
on photographs uploaded to Facebook, and the number of events attended by the user.
4.2.3 Facebook profile features
Facebook profile features used in Section 4.4 were obtained for more than 354,000 US
Facebook users, who had used Facebook for at least 24 months before the data was recorded.
The total number of friends, events, status updates, photos, photo tags and groups that users
belongs to accrue on their profiles over time. In order to account for this process, before
analysis, those variables were divided by the number of months since the user was active
on Facebook, estimated by looking for an earliest sign of users’ activity in the sample—for
example users’ first status update, photo tag, uploaded photo, or attended event (the date on
which given Facebook account was created was not available). Interestingly, the number of
Likes did not significantly depend on the amount of time since joining Facebook and hence
the total number was used.
The density of the friendship network largely relates to its size, which is a well known
property of social networks. Therefore, a simple linear regression model was built explain-
44 Personality and behaviour in the digital environment
ing log-transformed density by the log-transformed network size, and it was used to remove
the effect of the network size on density. The residual present in this model, which is an
equivalent of the density not explained by the sheer size of the network, was used as a
measure of an individual user’s network density.
Many Facebook users had incomplete profile information or their privacy settings did not
allow for accessing some parts of their profile. Consequently, not all of the features were
available for all of the users, but at least 9, 000 data points per feature and over 100, 000 data
points for many of the features were available. The frequencies of Facebook features used
of users for which data on given feature was available, median value, and thresholds of the
first and third quartiles
Label Details n Median Q Q
1 3
Friends Average number of friends 62,780 295 182 488
Network Density Density of the friendship 28,835 .03 .014 .05
network
Groups Groups joined 97,622 15 7 32
Likes Number of Facebook Likes 109,837 82 33 196
Photos Number of photos uploaded 110,540 10 4 29
Statuses Number of status updates 71,912 99 38 195
posted
Photo Tags Number of times others 315,169 20 4 83
tagged user in photos
Events Number of events attended 8,977 10 4 32
Age User’s age 198,959 23 20 28
4.2.4 Website preferences
Users’ website preferences used in Section 4.3 were collected using two independent ap-
proaches.
Self-reported website preferences
Self-reported website preferences were collected using the standard approach applied in
personality research. In the Website Preference Questionnaire (WPQ) designed for this
study, users were asked for the frequency with which they visit 23 websites on a five point
scale (from never to regularly). The websites included in the WPQ were selected to be
4.2 Methods 45
potentially informative about a visitor’s personality. For instance, it was assumed (and later
confirmed by the results) that songlyrics.com, an online library of song lyrics, would be
attractive to outgoing and sociable people or, in other words, people characterised by high
levels of Extraversion. Moreover, the websites were selected to be neither too popular nor
too obscure. Extremely popular websites attract visitors of all personality types and thus are
not informative. On the other hand, obscure websites do not attract a reasonable fraction of
users and thus are not discriminative.
The WPQ was offered in May 2010 to myPersonality respondents who had previously
taken the IPIP FFM Personality Inventory. Responses to the WPQ questionnaires were
collected from 10,897 individual users. On average, respondents reported that they had
visited three of the websites in the questionnaire at least rarely (σ = 1.9). The maximum
number of websites endorsed by a respondent was 13, while around 4% of the participants
did not visit any of the websites included in the questionnaire.
Liked websites dataset
Independent records of users’ website preferences were obtained using the Facebook Like
feature. Facebook users can annotate a website as Liked, in order to recommend it to their
friends and receive updates or news regarding the website publishers’ activities. Users can
like a website by clicking the Like button directly on the website (an increasing number of
websites offer such functionality) or by joining a website’s fan page directly on Facebook.
In contrast to the responses to a questionnaire, the individual records of Liked websites
are not influenced by the data collection context, nor limited to a small number of alterna-
tives. However, Liked websites are visible to a user’s social circle and thus might be used
strategically to convey a desired impression. Also, it should be noted that there is no mea-
sure of the degree to which users spend time on each website. Some users may choose to
like a website simply to promote it to friends, without actually spending much time on it.
Conversely, certain websites, such as technical documentation, might be less likely to be
promoted with Likes regardless of the time a user spends on them.
The Liked dataset contains about 153,000 individual US Facebook users and nearly
75,000 unique website-related Likes selected by at least 20 distinct users. Users that filled
in the WPQ questionnaire were removed from this sample to ensure the full independence
of the results.
46 Personality and behaviour in the digital environment
4.3 Study 1: Personality and website choices
4.3.1 Related work
A number of studies have analysed the relationships between online preferences, browsing
behaviour and demographic characteristics of website audiences, including age, gender,
occupation and education levels, income, and race. Most of these studies (e.g. [14, 28, 110,
167, 240]) are based on explicit profile data which is typically collected during the sign up
process for an online service.
Another approach relies on implicit profile data, or user characteristics that are inferred
rather than known. Typically, Internet Protocol addresses are translated into users’ loca-
tion which, combined with census information, can be used to infer characteristics, such as
education, income, race, etc. (e.g. [239, 240]).
Studies listed above focused on demographic properties of individuals. To the best of
my knowledge, no attempts have been made to relate personality of Internet users to their
web browsing behaviour. However, the psychological literature provides some examples
of the relationship between personality and other aspects of the users’ behaviour in an on-
line setting. For example, [159] and [234] assessed personality using the contents of per-
sonal websites, [78] studied the accuracy of personality judgements based on emails, [10]
showed that there is some valid personality related information even in users’ email ad-
dresses, while [140] (see also Chapter 5) studied the relationship between individual traits
and the Facebook Likes.
Estimating and validating website audience personality profiles
Website audience profiles were obtained by computing the mean personality scores as well
as age and gender of all users who reported to visit (WPQ dataset) or liked (the Liked
website audience profiles and n = 153, 838 individual users based on the Liked dataset.
The differences between the aggregated and individual levels of traits offer interesting in-
sights into the relationship between individual traits and propensity to like websites. While
women constituted 61% of the sample, the audience of an average website was predom-
inantly female (71%), indicating that women tended to like more websites than men did.
Similarly, young age, low Conscientiousness, low Agreeableness, and high Openness were
4.3 Study 1: Personality and website choices 47
based on the Likes dataset.
Aggregated profiles Individual users
Min Max Mean SD Min Max Mean SD
Gender .00 1.00 .71 .16 61% females
Age 15.71 51.10 21.24 4.14 13.00 65.00 24.36 8.76
Openness -1.00 1.22 .12 .26 -4.05 2.19 .00 1.00
Conscientiousness -1.21 .92 -.26 .23 -3.26 1.83 .00 1.00
Extraversion -1.29 1.02 -.03 .21 -3.13 2.11 .00 1.00
Agreeableness -1.25 .91 -.10 .20 -3.61 2.24 .00 1.00
Neuroticism -1.18 1.14 .03 .22 -2.85 2.46 .00 1.00
Number of Users 20 22,643 214.69 671.17 –
Number of Likes – 1 2,877 105 244
n = 74, 993 n = 153, 838
websites users
Note: personality traits were standardised to ensure a zero mean (µ = 0) and unit standard
deviation (σ = 1). Average website audience gender ratio of .71 indicates that websites
attracted more women on average (females were coded as 1 and males as 0).
positively related to the number of liked websites. To preserve the clarity of the further
results presented here and to allow for meaningful comparisons between audience profiles
to be made, the aggregated values were re-scaled to a mean of zero in both the Lab and the
Liked datasets. For instance, the aggregated values of Openness in the Liked dataset were
decreased by its mean value (µ = .12).
The validity of the website preference data was examined by comparing the website
that website audience personality traits were highly similar between the samples, supporting
the validity of both datasets. For instance, the average Conscientiousness of the website
audience in the Liked dataset correlated highly (r = .83) with the average Conscientiousness
estimated using the WPQ dataset. Moreover, the stability of the personality profiles (as
opposed to individual traits) across the samples was examined. This analysis was conducted
using Pearson product-moment correlation between personality profiles of the 14 websites
high similarity of audience profiles between the samples. The examples of website audience
All mentioned differences are significant at p < .001 level. The relationship between individual traits and
liking behaviour is further explored in Section 4.4.
Note that there were only 23 websites in the WPQ sample
48 Personality and behaviour in the digital environment
relation between website audience per- tion between personality profiles estimated
sonality profiles estimated using both using both datasets.
datasets.
domain r
Personality Trait r deviantart.com .98
Openness .77 tumblr.com .78
Conscientiousness .83 etsy.com .98
Extraversion .57 nba.com .94
Agreeableness .78 snagajob.com .85
Neuroticism .83 job.com .77
# common websites 23 gamefaqs.com .59
digg.com .86
ancestry.com .64
myxer.com .92
pandora.com .84
barnesandnoble.com .34
foodnetwork.com .83
Average correlation .83
Note: Correlation coefficients
were averaged using Fisher’s
z transformation.
4.3.2 Results
This section uses website audience personality profiles to show that users’ websites choices
are significantly and meaningfully related to their individual personality profiles. Website
audience profiles were obtained by computing the mean personality scores as well as age
and gender of all users who reported to visit (WPQ dataset) or liked (Liked dataset) a given
website.
An example of a website audience personality profile, of deviantART.com, is presented
tends to be liberal and artistic rather than conservative and traditional (i.e. with high Open-
ness), spontaneous and flexible rather than well organised (i.e. with low Conscientiousness),
shy and reserved rather than outgoing and active (i.e,. with low Extraversion), and emo-
tional rather than calm and relaxed (i.e. with high Neuroticism). Both personality theory
and common intuition suggest that those results accurately represent the character of de-
viantART.com users in general – alternative art enthusiasts and artists.
4.3 Study 1: Personality and website choices 49
-0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4
Liked Websites
WPQ
Fig. 4.1 Audience personality profile for deviantart.com website based on Liked and WPQ
datasets. The error bars present 95% confidence intervals.
profiles, based on the Liked dataset.
Trait Websites
ssennepO
citsitrA
larebiL
modcloth.com gateway.com
Conservative
senate.gov newegg.com
boingboing.net fitnessmagazine.com
astrology-online.com ourtoolbar.com
gutenberg.org nhl.com
cafeastrology.com pier1.com
ssensuoitneicsnoC
dezinagrO
lleW
lww.com candystand.com
Spontaneous
ecollege.com crunchyroll.com
ecnext.com allthetests.com
exct.net bestuff.com
education.com lyricsdepot.com
kodak.com letmewatchthis.com
noisrevartxE
evitcA
gniogtuO
clubzone.com lyricsty.com
Shy
Reserved
ideeli.com fanfiction.net
thanksmucho.com behindthename.com
discoveryeducation.com newworldencyclopedia.org
list-manage.com personalitypage.com
trails.com gaia-online.com
50 Personality and behaviour in the digital environment
Trait Websites
ssenelbaeergA
evitarepooC
abebooks.com localtribune.org
Competitive
socialsecurity.gov funnyjunk.com
myrecipes.com marvel.com
bluemountain.com allthetests.com
serialssolutions.com personalitypage.com
ecollege.com supercheats.com
ytilibatS
lanoitomE
citorueN
cineplex.com ncsu.edu
Calm
Relaxed
comparedby.us sheetmusicplus.com
myprofilepimp.com pitt.edu
barbie.com highschoolsports.net
yellowpages.ca myrecipes.com
biglots.com lww.com
senting six websites with highest and lowest aggregate audience scores for each of the per-
sonality traits. For example the most liberal, creative, and open to new experience audiences
(i.e. with high Openness) are especially attracted to (1) modcloth.com, a mod-retro-indie
clothing website, (2) boingboing.net, a blog on media, technology and popular culture, (3)
astrology-online.com and cafeastrology.com, astrology websites, (4) gutenberg.org, a free
e-book repository, (5) failblog.com, containing humorous media content, (6) fineartamer-
ica.com, a fine art website, (7) 911tabs.com, a website specializing in guitar tabs, and (8)
senate.gov, the website of the United States senate (which at the time of data collection had
a majority of Democrat senators).
On the other end of the Openness scale, websites for which the user population is es-
timated to be most conservative and conventional include (1) dealspl.us and newegg.com,
shopping deal websites, (2) a variety of health, fitness, recipe and style websites, such as
fda.gov, mydailymoment.com and fitnessmagazine.com, (3) doctorslounge.com, a website
specializing in health and medical jobs, (4) gateway.com, which sells information technol-
ogy products, (4) nhl.com, the website of the National Ice Hockey League in the United
States, and (5) pier1.com which sells furniture and accessories.
The practical applications of website audience personality profiles might be in the per-
sonalization of the search results and recommending websites of interest to users. Such an
approach could be based on matching users with websites based not on the previous websites
they have visited, but rather based on users’ inferred personality profiles (see Chapter 5).
4.4 Study 2: Personality and Facebook profile features 51
This could significantly increase the robustness of the recommendation and search engines
by expanding their suggestions beyond the content that is directly related to websites that
were previously visited or searched for.
Liked dataset.
Pearson correlation
Domain O C E A N freq SEM 1 2 3 4 5 6
1 deviantART.com .40 -.19 -.42 -.05 .16 3,154 .01-.02 1
2 Tumblr.com .23 -.23 -.16 -.10 .22 639 .03 .89 1
3 Etsy.com .41 .14 -.26 .07 .10 612 .03 .88 .59 1
4 Gaia online.com .33 -.23 -.40 .00 .19 2,076 .02 .99 .91 .82 1
5 Fanboy.com .36 -.27 -.44 -.04 .22 128 .07-.09 .99 .93 .81 1 1
6 RainyMood.com .36 -.22 -.35 .03 .12 236 .06-.07 .99 .88 .84 .99 .98 1
Note: The columns labelled O through N represent the five personality traits, freq indicates
the number of distinct users who liked each website. The column labelled SEM is the
standard error of the mean, which was of similar magnitude for all of the five personality
traits and is hence presented in a single column. Audience overlap between any two of the
websites is lower than 2%.
It is illustrated by presenting an exemplary set of websites that, while not overlapping
gregate personality profiles and their similarity, expressed by Pearson product-moment cor-
relation, for a set of websites including Tumblr.com (a micro blogging platform), etsy.com
(a marketplace of hand-made craft), gaia online.com (advertised as a forum of young open
minded people), fanboy.com (marketed as a website for intellectuals with imagination), and
rainymood.com (providing sounds of rain to visitors). It is clear that those websites are
frequented by audiences with similar mean personality: liberal, introverted, and rather emo-
tional. Notably, the only website in this group that attracts relatively non-spontaneous and
well organised users is etsy.com – a market place of hand-made crafts. Apparently, one
needs a degree of Conscientiousness, in addition to a general arty profile, to be interested in
buying and selling art.
4.4 Study 2: Personality and Facebook profile features
Facebook profiles have become an important source of information used to form impres-
sions about others. For example, people examine other people’s Facebook profiles when
trying to decide whether to start dating them [247], and when assessing job candidates [72].
52 Personality and behaviour in the digital environment
This section explores the relationship between personality and the residues of Facebook
activity—Facebook profile features. It continues and expands the work of [5, 89, 201] re-
garding personality and social network profiles, attempting to overcome some of the limita-
tions of those studies, in particular their relatively small (at most a few hundred participants)
and biased (mostly student) samples. The large sample used in this study is more represen-
tative of the general online population and enables us to make more statistically significant
conclusions.
This section starts with a description of the previous work relevant to this study followed
by the results. The correlations between Facebook profile features and personality obtained
here are compared with those reported by [5, 201].
4.4.1 Related work
Existing work [50, 204, 248] has shown that certain personality traits are correlated with
total internet usage and with the propensity of individuals to use social media and social
networking sites. However, these papers focus on the amount of time spent using these
tools rather than the specific features individuals engage and interact with. These papers
add value by identifying the personality profiles of heavy internet and Facebook users, but
shed little light on the issue of how a person’s Facebook profile reflects their personality.
The existing research has additionally shown that Facebook profiles reflect the actual
personality of their owners rather than an idealised projection of desirable traits [11]. Re-
searchers asked participants to assess the personality of the owners of a set of Facebook
profiles and revealed that they could correctly infer at least some personality traits, con-
cluding that the users did not deliberately misrepresent their personalities on their Facebook
profiles.
Despite the fact that people can judge other people’s personalities based on their Face-
book profiles or web browsing history, it is possible that some of the personality cues are
ignored or misinterpreted. Humans are prone to biases and prejudices which may affect
the accuracy of our judgements. Recent work [66], examining what aspects of the Face-
book profile individuals use to form personality judgements, shows that certain features are
difficult to grasp for people. For example, while the number of Facebook friends is clearly
displayed on the profile, people cannot easily determine features such as the network density
(i.e. whether a user’s friends know each other).
Several earlier papers investigate the relationships between personality traits and Face-
book activity expressed by Facebook profile features. First, a selection of studies closest in
4.4 Study 2: Personality and Facebook profile features 53
nature to this work is described.
Gosling et al. [89] revealed several connections between personality and self-reported
Facebook features; for example, they showed the positive relationship between Extraver-
sion and frequency of Facebook usage and engagement in the site. As in offline contexts,
Extraverts seek out virtual social engagement, leaving behind behavioural residues, such as
friendship connections or picture postings. However their work was based on a relatively
small sample of 157 participants, limiting the reliability and generalizability of their results.
Quercia et al. [187] studied the relationship between Facebook popularity (number of
contacts) and personality traits, showing that Extraversion predicts the number of Facebook
contacts. They also found no statistical evidence for the relationship between popularity
and self-monitoring – a personality trait describing an ability to adapt to new forms of
communication, present oneself in likeable ways, and maintain superficial relationships.
Ross et al. [201] pioneered the study of the association between personality and patterns
of social network usage. The study proposed a number of hypotheses but reported only one
significant correlation - between Extraversion and group membership. A relatively small
(n = 97) and homogeneous sample (i.e. mostly female students studying the same subject
at a single university), and a potentially unreliable approach to collecting data (i.e. partic-
ipants’ self-reports of their Facebook profile features, rather than direct observation) may
have prevented the authors from finding more significant connections and made it difficult
to extrapolate findings to a general population.
In a similar study, Amichai-Hamburger and Vinitzky [5] used actual Facebook profile
information rather than self-reports, although their sample was still small (n = 237) and ho-
mogeneous (i.e. Economics and Business Management students of an Israeli university).
They found several significant correlations, but some of their findings were in contradiction
to those of [201]. For example, they found that Extraversion was positively correlated with
the number of Facebook friends, but uncorrelated with the number of Facebook groups,
whereas [201] found that Extraversion had an effect on group membership, but not on the
number of friends. Additionally, they found that high Neuroticism was positively correlated
with users posting their own photo, but negatively correlated with uploading photos in gen-
eral, while [201] argued that high Neuroticism is negatively correlated with users posting
their own photo.
Following the work of [201] and [5] the present study focused on the relationship be-
tween personality and Facebook use, but based on a much larger sample (350,000 versus 97
and 237 users in [201] and [5] respectively). Similar to [5], actual features of the Facebook
profiles are recorded, instead of relying on potentially unreliable self-reports such as those
54 Personality and behaviour in the digital environment
Facebook profile features.
Psychological Trait Activity s-r
Openness Likes .09
Statuses .07
Groups .07
Conscientiousness Likes -.11
Groups -.09
Extraversion Statuses .12
Friends .18
Events .08
Groups .08
Neuroticism Likes .12
Statuses .09
Note: All reported coefficients are sig-
nificant at p < .01 level. s − r repre-
sents Spearman’s rank correlation co-
efficient.
used in [201].
4.4.2 Results
Personality was correlated with Facebook profile features using Spearman’s rank correla-
tion, which is appropriate for variables characterised by a long tailed distribution. The
t-distribution test showed that all reported correlations are significant at the p < .01 level.
Additionally, the Mann-Whitney-Wilcoxon (MWW) test was used to determine whether
the top and bottom thirds of the population differed significantly in terms of their mean
personality score. MWW showed that all relations were significant at the p < .01 level.
profile features and psychological traits. Those relationships are further explored in Fig-
ures 4.2 to 4.7. The horizontal axis of those Figures represents the standardised psycho-
logical trait score while the vertical axis represents the median of a given Facebook profile
feature (e.g. the median number of Facebook friends for users characterised by a given level
of Extraversion). In order to increase the clarity of the plots, users were grouped by their
standardized trait scores rounded to the nearest half integer (e.g users with a standardised
MWW is also known as the Mann-Whitney U test or the Wilcoxon rank-sum test
4.4 Study 2: Personality and Facebook profile features 55
Openness score between 1.75 and 2.24 were grouped together and represented by a score of
2). The shaded ribbon represents the interquartile range, IQR (also referred to as “middle
fifty”) for the Facebook feature. The IQR is the range of values between the 25% percentile
of the population to the 75% percentile of the population.
Openness. Liberal and open to experience individuals tend to like more items on Face-
the definition of this personality trait. Highly open users do not only choose different Likes
and Groups than conservative ones (as shown in Chapter 5) but are also willing accept a
wider range of objects.
These results confirm the hypotheses of [201] and results presented in [5] suggesting
that individuals high on Openness are more willing to use Facebook as a communication
tool and to use a greater number of features.
Fig. 4.2 Median number of Likes for users characterised by different levels of Openness.
Ribbon represents the interquartile range, or the middle 50 percentiles of the number of
users’ Likes
Conscientiousness. As presented in Figures 4.3 and 4.4, spontaneous (low on Con-
scientiousness) individuals tend to join more groups and like more things. Interestingly,
conscientious individuals do not only join less groups and use the Like feature less fre-
quently, but also are more homogeneous in doing so, as indicated by a significant drop in
conscientious individuals is higher by 40 Likes from the most spontaneous ones. Also, while
56 Personality and behaviour in the digital environment
25% of spontaneous users have more than 210 Likes, the same value for conscientious users
is lower by a third (i.e. 140 Likes).
These results confirm the hypothesis of [201] that Conscientiousness is negatively re-
lated to engaging in Facebook activities (importantly, [201] were not able to confirm this
hypothesis). Furthermore, the results do not support the hypothesis and results presented
in [5] who found that Conscientiousness was positively related to number of friends.
Fig. 4.3 Median number of Likes for users characterised by different levels of Conscien-
tiousness. Ribbon represents the interquartile range, or the middle 50 percentiles of the
number of users’ Likes
Extraversion. The results show that Extraverts are generally more likely to reach out
and interact with others on Facebook. They more actively share what is going on in their
lives or their feelings with other people (and allow other people to respond to these) using
dividuals using Facebook groups. This allows them to exchange information and connect
with individuals outside their immediate friendship circle. Finally, Extraversion relates to
Previous results of [201] show a positive link between Extraversion and group mem-
bership but no relationship with the number of friends, while [5] showed a positive link
between Extraversion and number of friends but no effect in regards to the use of Facebook
groups. The current work suggests that such conflicting results may have stemmed from the
relatively small sample sizes limiting the ability to establish significant relationships.
4.4 Study 2: Personality and Facebook profile features 57
Fig. 4.4 Median number of groups joined per month by users characterised by different
levels of Conscientiousness. Ribbon represents the interquartile range, or the middle 50
percentiles of the number of groups joined per month
Fig. 4.5 Median of status updates posted per month by users characterised by different levels
of Extraversion. Ribbon represents the interquartile range, or the middle 50 percentiles of
the number of status updates posted per month
Agreeableness. Agreeableness does not appear to be significantly correlated with any
58 Personality and behaviour in the digital environment
Fig. 4.6 Median of friends added per month by users characterised by different levels of
Extraversion. Ribbon represents the interquartile range, or the middle 50 percentiles of the
number of friends added per month
of the Facebook profile features studied in this chapter. This result suggests that the rela-
tionship between Agreeableness and the number of friends hypothesised but not confirmed
by both [201] and [5] may in fact be non-existent.
of Facebook Likes, indicating that more emotional users tend to use the Like function more
frequently. While 75% of the most stable users like fewer than approximately 150 Likes,
75% of the most emotional users like more than 220 Likes.
Those results are in agreement with the hypothesis proposed in [201] and [5] suggesting
that Neurotic individuals would be more willing to share personal information on Facebook.
While there were no significant relationships between Neuroticism and number of photos
uploaded by the users, positive relationships was found between Neuroticism and number
of likes, and status updates – serving a similar function.
4.5 Conclusions
This chapter focused on how personality is manifested in users’ behaviour in the digital
environment, as reflected by their Facebook profile features and preferences for websites.
Potential implications of these results range from online advertising and recommender sys-
4.5 Conclusions 59
Fig. 4.7 Median number of Likes for users characterised by different levels of Neuroticism.
Ribbon represents the interquartile range, or the middle 50 percentiles of the number of
users’ Likes
tems, to personalizing the Facebook user interface. Results suggest that new approaches to
marketing and recommendation could be based on segmenting users based on their person-
ality profile. Moreover, the relationship between psychological traits and the behavioural
residues recorded in the digital environment suggest that it is possible to infer psychological
profiles based on the online behaviour. This is further explored in Chapter 5.
Results presented here have three particularly important implications:
Personalization. It is valuable for websites, service providers, and brands to know the
psycho-demographic profiles of their users. Currently, websites personalise their content,
optimise their marketing, and tailor their search results using audience profiles encompass-
ing demographic traits, such as age, gender and income [110]. If websites and other web
services attract audiences with a distinct personality profile, online platforms could greatly
expand their understanding of users and thus improve their services and the user experience.
Inferring psychological profiles. By finding associations between personality, web-
site preferences, and social network profiles, alternative avenues for psychological research
could be opened. Currently, psychological measurement mainly relies on self-report ques-
tionnaires completed by relatively small numbers of participants. Results suggest that per-
sonality could be measured automatically based on records of online behaviour; thus enlarg-
ing the scope of psychological assessment to an unprecedented scale. Such profiles could
60 Personality and behaviour in the digital environment
potentially be of higher quality as they are based on the actual behaviour recorded in the
increasingly natural digital environment rather than on self-reported test answers. More-
over, it is likely that studying vast samples of digitally recorded behaviour will improve
researchers’ existing psychological models or suggest new ones.
Privacy. While it is widely accepted that an individual’s personality can be accurately
assessed using traditional psychometric tools, such as a personality questionnaires, the abil-
ity to automatically infer psychological profiles using digital records of behaviour chal-
lenges users’ privacy expectations. Such inferences deprive individuals of control over what
other parties can learn about them and may breach the trust between users and online service
providers. This is further explored in chapters 5 and 7.
4.6 Limitations
Studies measuring personality are often limited to the lab environment and rely on a small or
moderate population of volunteers to self-report their personal behaviours and preferences
under certain situations. While this study relied on a very large and relatively representative
population of respondents, it was restricted to the US-based and rather WEIRD (Western,
Educated, Industrialized, Rich and Democratic [99]) Facebook users. Although this helped
preventing cultural biases, it also limits the generalizability of the findings. Moreover, the
observations were limited to volunteers who opted-in to participate in the research. While
the demographic structure of the population used in this study matches the general Facebook
population, it is possible that users interested in participating in the study systematically
differed from the general population, for example in their psychological profile. It is hoped
that this self-selection effect is partly mitigated by the scale of the experiments. For broader
discussion of the limitations of the sample see Chapters 2 and 3.
Chapter 5
Using digital footprints to infer
psycho-demographic profiles
Fragments of this chapter were previously published in [138, 140].
The relationship between personality and behaviour in the digital environment presented
in Chapter 4 indicates that it might be possible to predict psychological traits based on
behavioural residues collected in the digital environment. This chapter further explores
this idea and shows that easily accessible digital records of behaviour, Facebook Likes and
Facebook profile features, can be used to automatically and accurately predict a range of
highly sensitive personal attributes. These include: sexual orientation, ethnicity, religious
and political views, personality traits, intelligence, happiness, use of addictive substances,
parental separation, age, and gender.
Studies presented here rely on two sub-samples of the myPersonality dataset. The first
study is based on 58,000 US volunteers who provided their Facebook profile information,
detailed demographic profiles, and the results of several psychometric tests. The proposed
model uses dimensionality reduction for preprocessing the Likes data, which are then en-
tered into logistic/linear regression to predict individual psycho-demographic profiles from
Likes. The model correctly discriminates between homosexual and heterosexual men in
88% of cases, African Americans and Caucasian Americans in 95% of cases, and between
Democrats and Republicans in 85% of cases. For the personality trait “Openness”, predic-
tion accuracy was close to the test-retest accuracy of a short personality questionnaire. Also,
the examples of the associations between individual attributes and Likes are presented.
The second study is based on a sample discussed in Chapter 4 and focuses on predict-
62 Using digital footprints to infer psycho-demographic profiles
ing personality, satisfaction with life, and general intelligence based on Facebook profile
features—such as the total number of friends, events, status updates, photos, photo tags and
membership in groups. A prediction model based on multivariate linear regression is used
and the results show that Facebook profile features can be used to predict individual traits.
While the accuracy achieved in this study might not be satisfactory on an individual level,
it could prove useful in accurately inferring group profiles (e.g. psychological profiles of
website audiences or product users).
This chapter is concluded with a discussion of the results within the context of online
personalization and privacy.
5.1 Introduction
A growing proportion of human activities, such as social interactions, entertainment, shop-
ping, and gathering information, are now mediated by digital services and devices. Such
digitally mediated behaviours can easily be recorded and analysed, fuelling the emergence
of computational social science [148] and new services, such as personalised search engines,
recommender systems [136], and targeted online marketing [48]. However, the widespread
availability of extensive records of individual behaviour, together with the desire to learn
more about customers and citizens, presents serious challenges related to privacy and data
ownership [41, 169].
Predicting individual traits and attributes based on various cues, including samples of
written text [71], answers to a psychometric test [52], or the appearance of spaces people
inhabit [90], has a long history. Human migration to the digital environment renders it pos-
sible to base such predictions on digital records of human behaviour. It has been shown
that age, gender, occupation, education level, and even personality can be predicted from
people’s website browsing logs [28, 79, 110, 139, 167]. Similarly, it has been shown that
personality can be predicted based on the contents of personal websites [159], music col-
lections [194], properties of Facebook or Twitter profiles, such as the number of friends or
the density of friendship networks [80, 139, 186, 187], or language used by their users [81].
Furthermore, location within a Facebook friendship network was shown to be predictive of
sexual orientation [118].
This work demonstrates the degree to which relatively basic digital records of human
behaviour can be used to automatically and accurately estimate a wide range of personal
attributes that people would typically assume to be private. The study is based on variables
describing respondents’ activity on Facebook, and their Facebook Likes.
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 63
5.2 Study 1: Inferring psycho-demographic profiles from
Facebook Likes
Facebook Likes constitute a mechanism used by Facebook users to express their positive as-
sociation with (or “Like”) online content, such as photos, friends’ status updates, Facebook
pages of products, sports, musicians, books, restaurants, or popular websites. Likes repre-
sent a very generic class of digital records, similar to Web search queries, web browsing
histories, and credit card purchases. For example, observing users’ Likes related to music
provides similar information to observing records of songs listened to online, songs and
artists searched for using a Web search engine, or subscriptions to related Twitter channels.
In contrast to these other sources of information, Facebook Likes are unusual in that they are
currently publicly available by default. However, those other digital records are still avail-
able to numerous parties (e.g. governments, developers of Web browsers, search engines,
or Facebook applications), and, hence, similar predictions are unlikely to be limited to the
Facebook environment.
5.2.1 Method
discussed below.
1 2 3
Singular Value
Users’ Facebook Likes Decomposition Prediction Model
55,814 Likes 100 Components
Using Logistic or Linear Regression
(with 10-fold cross validation)
e.g. age=α+β C +…+ β C
1 1 n 100
Predicted variables
Facebook profile: age, gender, politi-
s s
r r
e e
cal and religious views, relationship
s s
U U
status, proxy for sexual orientation,
6 6
6 6
social network size and density
4 4
, ,
8 8
Profile picture: ethnicity
5 5
Survey / test results: BIG5 Personali-
User – Like Matrix
t y, intelligence, satisfaction with life,
User – Components Matrix
(10M User-Like pairs)
substance use, parents together?
Fig. 5.1 Summary of the approach used to infer psycho-demographic profiles from Facebook
Likes.
64 Using digital footprints to infer psycho-demographic profiles
Sample
The study is based on a sample of 58,466 volunteers from the United States, obtained
through the myPersonality Facebook application (see Chapter 3), which included their Face-
book profile information, a list of their Likes (n = 170 Likes per person on average), psycho-
metric test scores, and survey information. Users opted in to provide their data for this study
and gave their consent to have their scores and profile information recorded for analysis.
An important limitation of this sample is that some of the predicted variables are ac-
quired from Facebook profile information. Individuals who openly declare their political
and religious views, relationship status, and sexual orientation on their profile may also like
more revealing things than their non-declaring peers. While this might lead to an overesti-
mate of prediction accuracies for these groups, the model was still able to predict privately
reported information, such as personality or intelligence quotient questionnaire results, and
survey results on addictive substance use.
Variables
Age, gender and the properties of the social network. Age (n = 52, 700; µ = 25.6, and
σ = 10), gender (n = 57, 505, 62% female), and the Facebook social network information
˘ ˘
(n = 17, 601; median size X = 204; interquartile range IQR = 206; median density, X = .03;
IQR = .03) were obtained from users’ Facebook profiles.
Political and religious views. Political (n = 9, 752, 65% Liberal) and religious (18, 833,90%
Christian) views were recorded from the respective fields of users’ Facebook profiles. Both
fields allow users to input text freely (but suggest popular choices). Political views “Demo-
crat”, “Democratic”, or “Democratic Party” were recoded to “Democrat”. “Republican”,
“GOP”, and “Republican Party” were recoded to “Republican”; other entries were ignored.
Religious views “Christian”, “Catholic”, and “Jesus Christ” were recoded to “Christian”.
“Moslem”, “Muslim”, “Islam”, and “Sunni” were recoded to “Muslim”.
Sexual orientation. Sexual orientation (n = 17, 543, 4.3% gay, 2.4% lesbian) was taken
from the “Interested in” section of users’ Facebook profiles; users who listed being inter-
ested in only the opposite gender were labelled as being “heterosexual”, whereas users who
listed only the same gender were labelled as being “homosexual”.
Relationship status. Relationship status was recorded from the “Relationship Status”
profile field, where the options were “Single”, “It’s complicated”, “In an open relationship”,
“In a relationship”, “Engaged”, and “Married”. The latter three options were recoded to “In
a relationship”. Users (n = 46, 027) were evenly split between those who were single and in
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 65
relationship (49% to 51% respectively).
Ethnicity. Labels for ethnicity (n = 7, 000, 73% Caucasian; 14% African American;
13% others) were assigned to users by visual inspection of their profile pictures. This pro-
cedure has the advantage that the data are not explicitly self-reported and, hence, does not
suffer from disclosure bias. However, some users do not include any picture with their pro-
file or use a picture that does not show themselves. To confirm the reliability of the manual
classification procedure, a sub-sample of the data were compared with self-reported ethnic
background from a survey, and there was agreement (r = .98) between the two sets of labels.
Substance use and user’s parents together at age twenty-one years. Both substance
use and whether a user’s parents stayed together or split up before the user was 21 years old
were measured using self-report survey measures on the myPersonality application. These
questions were explicitly labelled as optional. Individuals were asked if they smoked daily,
less than daily, or were non-smokers; less than daily and daily were recoded as “smokers”.
They were also asked if they drank alcohol by offering the choices “weekly or more often”,
“less than once a week”, or “never”; the first two options were recoded as “drinkers”. For
drug use, the options were the same as for drinking; the first two options were recoded as
“drug users”.
The following numbers of users reported to use substances: alcohol (n = 1, 196; 50%
reported to drink), drugs (n = 856; 21% reported to take drugs), and cigarettes (n = 1211;
30% admitted to smoke). In most cases (56%) users’ parents stayed together until they were
21 years old (n = 766)
Five factors of personality. Five-Factor Model (FFM; [52]) personality scores (n =
54, 373) were established using the 20-item International Personality Item Pool (IPIP) FFM
Personality Inventory [84], discussed in more detail in Chapter 3. This test is widely used
in both traditional and online studies and is known to be successful at explaining variability
across individuals.
General intelligence. General Intelligence (n = 1, 350) was measured using Raven’s
Standard Progressive Matrices (SPM; [192]), a multiple choice non-verbal intelligence test
drawing on Spearman’s theory of general ability. SPM is a proven standard intelligence
test used in both research and clinical settings, as well as in high-stake contexts, such as
in military personnel selection and court cases [155]. The SPM test was shortened for the
purpose of this study and contained 20 items only. Note that SPM was used only to compare
between users of this study, and no comparisons with the general population were made.
Satisfaction with life. “Satisfaction with Life” (SWL) (n = 2, 340) was measured us-
ing the SWL Scale [59], a widely used, five-item instrument designed to measure global
66 Using digital footprints to infer psycho-demographic profiles
cognitive judgements of satisfaction with one’s own life.
Facebook Likes. Facebook Likes allow Facebook users to connect with virtually any
object that has an online presence. Likes are one of the most typical and pervasive forms
of digitally recorded behaviour. People can Like quotes, Web sites, press articles, products,
activities, places they visit (or would like to visit), and content such as pictures, movies,
books, and music. Likes span a diverse set of entities, from “Bible” and “Philosophy”
through “Bonfires”, “BMW”, and “cnn.com” to statements such as “I hate myself”. Peo-
ple’s Likes are shared with their friends and can be used as a way of expressing support,
bookmarking, or enhancing online identity, by indicating individual preferences.
More than 9 million unique objects liked by users were recorded, a great majority of
which were associated with one or very few users only. For the purpose of building a
predictive model, Likes associated with fewer than 20 users, as well as users with fewer
than two Likes, were removed from the sample.
Analyses
Users (n = 58, 466) and their Likes (l = 55, 814 unique Liked objects) were represented as
a sparse User-Like matrix, the entries of which were set to 1 if there existed an associa-
tion between a user and a Like and 0 otherwise. The matrix contained roughly 10 million
associations between users and Likes.
Reducing Dimensionality of the User-Like Matrix. To facilitate the predictive analy-
sis, the dimensionality of the User-Like matrix was reduced using Singular Value Decom-
position (SVD; [86]) such that each user is represented by a vector of k component scores.
SVD provides a low-rank approximation to the original matrix, and the approximation qual-
ity increases with the number k.
To choose the optimum number of SVD components to be used in this study, the cross-
validated prediction accuracy as a function of the number of components was examined.
steeply in the beginning but flattens out relatively early (note that the horizontal axis is
not linear). Interestingly, including some of the components abruptly increases prediction
accuracy for certain traits. For example, including component 3 in the model increases
the accuracy of Openness estimates from r = .1 to r = .4. Similarly, component 5 sharply
increases the accuracy achieved in predicting Extraversion. This suggests that particular
components are specifically related to a given attribute.
The first 100 SVD components were used, which explained 28% of the variance in the
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 67
Fig. 5.2 Prediction accuracy in terms of Pearson correlation coefficient as a function of the
number of SVD components used for the two personality traits Openness and Extraversion.
consumption, only 30 top SVD components were used because of the smaller number of
users for which this information was available.
Predictions. SVD components were used to build models that predict users’ individual
traits and preferences. Predictions related to numeric variables, such as age or intelligence,
were calculated using a linear regression model based on the users’ 100 SVD components as
covariates. Dichotomous variables, such as gender, relationship status, and political views
were modelled using logistic regression based on the same SVD components. In both cases
10-fold cross-validation was used to assess the out-of-sample prediction accuracy; the sam-
ple was randomly split into 10 equally sized subsets of users, and predictions for each subset
were calculated based on parameters determined on the remaining users.
Prediction accuracy was measured in two ways. For the numeric variables, such as
age in years, the Pearson product-moment correlation coefficients between the actual and
predicted values across users are reported. For the dichotomous variables, such as gender,
the Area Under the Receiver-Operating Characteristic (ROC) Curve (AUC) reported, which
can be interpreted as the probability of correctly classifying two randomly selected objects:
one of each class (e.g. male and female).
AUC. AUC relates to the ROC curve, which is a plot of true-positive rate (or sensitivity)
versus false-positive rate (or specificity) for detection or classification tasks. Positive cases
are those classified by the model to belong to a target class (e.g. male or “Democrat”). Thus,
true positive cases are the cases that were correctly classified by the model as belonging to a
68 Using digital footprints to infer psycho-demographic profiles
Fig. 5.3 Fraction of variance explained as a function of the number of SVD components for
the dimensionality reduction of the User-Like matrix.
target class, whereas false-positive cases were classified incorrectly as belonging to a target
class.
The true-positive rate is the ratio of the number of true positives to the number of all
cases in the target class, whereas the false-positive rate is the ratio of the number of false
positives to the number of all cases in the background class. The logistic regression model
used in this study to predict dichotomous outcomes assigns a probability of belonging to a
target class to each of the users. To avoid having to select a single threshold for assigning
users to a given target category, an ROC curve can be used to analyse the entire spectrum of
In general, ROC curves for random (or null) models should be close to diagonal, because
the probability of seeing a true positive is not greater than the probability of seeing a false
positive. The more an ROC curve bulges to the upper left, however, the higher the accuracy
of the model, because higher true-positive rates are achieved for a given number of false
positives. The AUC is simply the area below the ROC curve, and it is equal to the probability
that a classifier will rank a randomly chosen positive instance higher than a randomly chosen
negative one.
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 69
Fig. 5.4 Example of an ROC curve, detecting users associated with the Facebook Page
associated with the FailBlog.org website. The AUC for this plot equals AUC = .79.
5.2.2 Results
Dichotomous Variables.
AUC, which is equivalent to the probability of correctly classifying two randomly selected
users one from each class (e.g. male and female). The highest accuracy was achieved for
ethnic origin and gender. African Americans and Caucasian Americans were correctly clas-
sified in 95% of cases, and males and females were correctly classified in 93% of cases,
suggesting that patterns of online behaviour as expressed by Likes significantly differ be-
tween those groups allowing for nearly perfect classification to be made. Christians and
Muslims were correctly classified in 82% of cases, and similar results were achieved for
Democrats and Republicans (85%). Sexual orientation was easier to distinguish among
males (88%) than females (75%), which may suggest a wider behavioural divide (as ob-
served from online behaviour) between hetero- and homosexual males. Good prediction
accuracy was achieved for relationship status and substance use (between 65% and 73%).
The relatively lower accuracy for relationship status may be explained by its temporal vari-
ability compared with other dichotomous variables (e.g. gender or sexual orientation). The
model’s accuracy was lowest (60%) when inferring whether users’ parents stayed together
or separated before users were 21 years old. Although it is known that parental divorce
does have long term effects on young adults’ well-being [168], it is remarkable that this
70 Using digital footprints to infer psycho-demographic profiles
is detectable through their Facebook Likes. Individuals with parents who separated have a
higher probability of liking statements preoccupied with relationships, such as “If I’m with
you then I’m with you I don’t want anybody else”.
Fig. 5.5 Prediction accuracy of regression Fig. 5.6 Prediction accuracy of classifi-
for numeric attributes and traits expressed cation for dichotomous/dichotomised at-
by the Pearson correlation coefficient be- tributes expressed by the AUC.
tween predicted and actual attribute values;
all correlations are significant at the p <
.001 level. The transparent bars indicate
the questionnaire’s baseline accuracy, ex-
pressed in terms of test-retest reliability.
extreme average levels for each of the numeric variables (e.g. personality traits) or most
of personality traits and age of the users associated with selected Likes presented on the
Democrat, Homosexual, Christian, and African American users.
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 71
Linear variables
son product-moment correlation coefficient between the actual and predicted values. The
highest correlation was obtained for age (r = .75), followed by density (r = .52) and size
(r = .47) of the Facebook friendship network. Closely following were the personality traits
of Openness (r = .43), Extraversion (r = .40), and Intelligence (r = .39). The remain-
ing personality traits and SWL were predicted with somewhat lower accuracy (r = .17 to
r = .30).
Psychological traits are examples of latent traits (i.e., traits that cannot be measured di-
rectly). As a consequence, their values can only be measured approximately, for example,
indicate the accuracy of the questionnaires used as expressed by their test-retest reliabili-
ties (Pearson product-moment correlation between the questionnaire scores obtained by the
same respondent at two points in time). The correlation between the predicted and actual
Openness score (r = .43) was very close to the test-retest reliability for Openness (r = .50).
This indicates that for the Openness trait, observation of the user’s Likes is roughly as in-
formative as using their personality test score itself. For the remaining traits, prediction
accuracies correspond to roughly half the questionnaire’s test-retest reliabilities. The rel-
atively lower prediction accuracy for SWL (r = .17) may be attributable to the difficulty
of separating long-term happiness [207] from mood swings, which vary over time. Thus,
although the SWL score includes variability attributable to mood, users’ Likes accrue over
a longer period and, so, may be suitable only for predicting long-term happiness.
Amount of data available and prediction accuracy
This section investigates the relationship between the accuracy of prediction and number
of Likes entered into the model. The results are based on a sub-sample (n = 500) of users
for whom at least 300 Likes were available. A random subset of Likes of a given size
(n = 1, 2, ..., 300) was selected for each user and entered into predictive model. The results
served Likes. It is clear that even a single random Like leads to a non-negligible prediction
accuracy. Increasing the number of Likes boosts the accuracy, but this effect diminishes
with each additional Like available.
72 Using digital footprints to infer psycho-demographic profiles
Fig. 5.7 Accuracy of selected predictions as a function of the number of available Likes.
Accuracy is expressed as AUC (gender) and Pearson product-moment correlation coefficient
(age and Openness). About 50% of the users in this sample had at least 100 Likes and
about 20% had at least 250 Likes. Note that for gender (dichotomous variable) the random
guessing baseline corresponds to an AUC = .50.
variables (e.g. personality traits) or most extreme frequencies of classes (e.g. being a Demo-
crat). Only Likes associated with more than 100 users were used.
Trait Selected most predictive Likes
QI
hgiH
The Godfather Jason Aldean
Low
Mozart Tyler Perry
Thunderstorms Sephora
The Colbert Report Chiq
Morgan Freemans Voice Bret Michaels
The Daily Show Clark Griswold
Lord Of The Rings Bebe
To Kill A Mockingbird I Love Being A Mom
Science Harley Davidson
Curly Fries Lady Antebellum
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 73
Trait Selected most predictive Likes
efiL
htiW
noitcafsitaS
defisitaS
Sarah Palin Hawthorne Heights
Dissatisfied
Glenn Beck Kickass
Proud To Be Christian Atreyu (Metal Band)
Indiana Jones Lamb Of God
Swimming Gorillaz
Jesus Christ Science
Bible Quote Portal
Jesus Stewie Griffin
Being Conservative Killswitch Engage
Pride And Prejudice Ipod
ssennepO
citsitrA
larebiL
Oscar Wilde NASCAR
Conservative
Charles Bukowski Austin Collie
Sylvia Plath Monster-In-Law
Leonardo Da Vinci I don’t read
Bauhaus Justin Moore
Dmt The Spirit Molecule ESPN2
American Gods Farmlandia
John Waters The Bachelor
Plato Oklahoma State University
Leonard Cohen Teen Mom 2
ssensuoitneicsnoC
dezinagrO
lleW
Law Officer Wes Anderson
Spontaneous
National Law Enforcement Low- Bandit Nation
fares.Com
Accounting Omegle
Foursquare Vocaloid
Emergency Medical Services Serial Killer
Sunday Best Screamo
Kaplan University Anime
Glock Inc Vamplets
Mycalendar 2010 Join If Ur Fat
Not Dying
74 Using digital footprints to infer psycho-demographic profiles
Trait Selected most predictive Likes
noisrevartxE
evitcA
gniogtuO
Beerpong RPGs
Shy
Reserved
Michael Jordan Fanfiction.Net
Dancing Programming
Socializing Anime
Chris Tucker Manga
I Feel Better Tan Video Games
Modeling Role Playing Games
Cheerleading Minecraft
Theatre Voltaire
Flip Cup Terry Pratchet
ssenelbaeergA
evitarepooC
Compassion International I Hate Everyone
Competitive
Logan Utah I Hate You
Jon Foreman I Hate Police
Redeeming Love Friedrich Nietzsche
Pornography Harms Timmy South Park
The Book Of Mormon Atheism / Satanism
Circles Of Prayer Prada
Go To Church Sun Tzu
Christianity Julius Caesar
Marianne Williamson Knives
ytilibatS
lanoitomE
citorueN
Sometimes I Hate Myself Business Administration
Calm
Relaxed
Emo Getting Money
Girl Interrupted Parkour
So So Happy Track & Field
The Addams Family Skydiving
Vocaloid Mountain Biking
Sixbillionsecrets.com Soccer
Vampires Everywhere Climbing
Kurt Donald Cobain Physics / Engineering
Dot Dot Curve 48 Laws Of Power
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 75
Trait Selected most predictive Likes
redneG
elameF
Tv Fanatic Modern Warfare 2
Male
Chiq ESPN
Gillette Venus Sportscenter
Shoedazzle Band Of Brothers
Bebe Starcraft
Proud To Be A Mom Deadliest Warrior
Covergirl Dos Equis
Wet Seal Red Vs Blue
Aerie By American Eagle X Games
Mall World Bruce Lee
egA
dlO
Cup Of Joe For A Joe Walt Disney Records
Young
Coffee Party Movement Body By Milk
Dr Mehmet Oz Harperteen
Fixit And Forgetit J Bigga
The Closer Because I Am A Girl
Joyce Meyer Ministries I Hate My Id Photo
Proud To Be A Mom 293 Things To Do In Class When
You Are Bored
Freedomworks Dude Wait What
Small Business Saturday JCP Teen
Fly The American Flag
sdneirF
ynaM
Mojo-Jojo The Dark Knight
Few
Biology In’n’out Burger
Dollar General Hard Rock
Hillary Honey, Where Is My Supersuit
106 & Park Hating ICP
Jennifer Lopez Minecraft
Paid In Full Iron Maiden
Yo Gotti Walking With Your Friend & Ran-
domly Pushing Them Into Some-
one/Something
The Dollar You Are Holding
Could’ve Been In A Stripper’s Butt
Crack
76 Using digital footprints to infer psycho-demographic profiles
Trait Selected most predictive Likes
noigileR
naitsirhC
The Bible I’m A Muslim & I’m Proud
Muslim
Jesus Daily Hadith Of The Day
I’m Proud To Be Christian I Love Islam
God I Need Allah In My Life
Jesus Christ Prophet Muhammad Saw The
Greatest Man In History
Church Remove Group Fuck Islam From
Facebook
The Holy Bible Nancy Ajram
I Love Jesus Moozlum The Movie
Christian Music Desihits.Com
Gospel Music
scitiloP
nacilbupeR
George W Bush Joe Biden
Democrat
John McCain Speaker Nancy Pelosi
Conservative Health Care Reform
Rush Limbaugh The White House
Sean Hannity Democrats
Bill Oreilly Barbara Boxer
Positively Republican Anthony Weiner
Sarah Palin Being Liberal
Ronald Reagan Left Action
Glenn Beck Barack Obama2012
Ted Kennedy
noitatneirO
lauxeS
selaM
lauxesomoH
No H8 Campaign X Games
Heterosexual
Males
Kathy Griffin Nike Basketball
Kurt Hummel Glee Bungie
Human Rights Campaign WWE
Mac Cosmetics Sportsnation
Adam Lambert Wu-Tang Clan
Ellen DeGeneres Foot Locker
Juicy Couture Shaq
Sue Sylvester Glee Bruce Lee
Wicked The Musical Being Confused After Waking Up
From Naps
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 77
Trait Selected most predictive Likes
selameF
lauxesomoH
Girls Who Like Boys Who Like Lipton Brisk
Boys
Heterosexual
Females
Rupauls Drag Race Yahoo
No H8 Campaign Adidas Originals
Gay Marriage Foot Locker
Human Rights Campaign WWE
The L Word Inbox 1 Makes Me Nervous
Sometimes I Just Lay In Bed And Thinking Of Something And
Think About Life Laughing Alone
Not Being Pregnant I Just Realized Immature Spells I’m
Mature
Gay Marriage Did You Get A Haircut No It Grew
Shorter
Tegan And Sara Nike Women
ecaR
naciremA-nacirfA
I Support My President Just Because You Can Reproduce
Doesn’t Mean You Should
White
American
Fantasia I Come From A Town Where A
Traffic Jam Is 4 Cars Behind A
Tractor
Jill Scott Harley Davidson
Next Friday Halloween
Erykah Badu Bret Michaels
Maxwell David Bowie Official
Taraji P Henson ASPCA
Madea Fly The American Flag
Tyga Road Trips
Love And Basketball Bonfires
Predictive power of Likes
Individual traits and attributes can be predicted to a high degree of accuracy based on records
attributes. For example, the best predictors of high intelligence include “Thunderstorms”,
“The Colbert Report”, “Science”, and “Curly Fries”, whereas low intelligence was indi-
78 Using digital footprints to infer psycho-demographic profiles
cated by “Sephora”, “I Love Being A Mom”, “Harley Davidson”, and “Lady Antebellum”.
Good predictors of male homosexuality included “No H8 Campaign”, “Mac Cosmetics”,
and “Wicked The Musical”, whereas strong predictors of male heterosexuality included
“Wu-Tang Clan”, “Shaq”, and “Being Confused After Waking Up From Naps”.
Due to a large number of Likes used in this study, only a fraction of associations between
Likes and psycho-demographic traits can be presented. Nevertheless, meaningful patterns
of associations between Likes and traits are apparent. For example, highly competitive and
assertive individuals (i.e. those of low Agreeableness) use Likes to express their hatred of
others, lack of religious beliefs (“Atheism” and “Satanism”), and preference for famously
competitive historical figures, such as “Julius Caesar” and “Sun Tzu”. On the other ex-
treme, highly agreeable individuals “Go to Church”, belong to “Circles of Prayer”, listen to
Christian rock by “Jon Foreman”, and believe that “Pornography Harms”.
Although some of the Likes clearly relate to their predicted attribute, as in the case of
No H8 Campaign and homosexuality, other pairs are more elusive; there is no obvious con-
nection between “Curly Fries” and high intelligence. While due to a relatively large sample
size, such strong relationship is unlikely to occur randomly, it should not be automatically
concluded that intelligent people prefer curly fries to regular ones. Such connection could
be explained in number of other ways, for example individuals of high intelligence might
have liked Curly Fries on Facebook due to a joke or a meme, circulated within a limited
community (e.g. an audience of a given website). This may reduce the importance of such
insights in the context of the intelligence studies, but does not affect the validity of a predic-
tive approach.
Interestingly, few users were associated with Likes explicitly revealing their attributes.
For example, less than 5% of users labelled as gay were connected with explicitly gay
groups, such as No H8 Campaign, “Being Gay”, “Gay Marriage”, “I love Being Gay”, “We
Didn’t Choose To Be Gay We Were Chosen”. Consequently, predictions rely on less infor-
mative but more popular Likes, such as “Britney Spears” or “Desperate Housewives” (both
the average levels of personality traits and age for several popular Likes. Each Like attracts
users with a different average personality and demographic profile and, thus, can be used
to predict those attributes. For example, users who liked the “Hello Kitty” brand tended to
be high on Openness and low on Conscientiousness, Agreeableness, and Emotional Stabil-
ity. They were also more likely to have Democratic political views and to be of African
American origin, predominantly Christian, and slightly below average age. Similarly, “The
Colbert Report” was related to high Openness, above average Stability and age, and below
5.2 Study 1: Inferring psycho-demographic profiles from Facebook Likes 79
Fig. 5.8 Average levels of five personality traits and age of the users associated with selected
Likes presented on the percentile scale. Error bars signify 95% confidence intervals of the
mean.
average Agreeableness. It was also very indicative of Introversion, in fact only 23% of the
audiences of other Likes had lower average Introversion levels.
ity in four groups: Democrats, Christians, Homosexuals, and African-American individuals.
Note that because Likes differed greatly in popularity (e.g. “Barack Obama” was nearly four
times more popular than “Mitt Romney”), relative popularity was calculated by dividing the
frequencies of associations with a given Like within the studied groups by the respective
frequency in the entire sample. Next, relative popularity was transformed into a percentile
informative about multiple traits. For example, although liking “Barack Obama” is clearly
related to being a Democrat, it is also relatively popular among Christians, African Amer-
icans, and Homosexual individuals. Similarly, “The Colbert Report” is relatively popular
rd th
within Democrats and Homosexual groups (93 and 80 percentile respectively) but rather
th th
unpopular among Christians and African Americans (24 and 35 percentile, respectively).
Importantly, whilst psychometric tools are usually designed to be based on individual
behaviours or skills that have relatively permanent relationship with measured trait (such
as the relationship between Extraversion and enjoying social gatherings), prediction models
80 Using digital footprints to infer psycho-demographic profiles
Fig. 5.9 Relative popularity of selected Likes within groups of Democrat, Homosexual,
Christian, and African American users presented on the percentile scale. Error bars signify
95% confidence intervals of the population proportion.
similar to the one presented in this study are based behaviours and preferences that might
be only temporarily related to the predicted trait. Consequently, such models have to be
regularly updated to account for the dynamic relationships and new phenomena occurring
in the digital environment. Fortunately, many widely employed predictive models can be
easily and automatically updated as new data becomes available.
5.3 Predicting personality with Facebook profile features
The relationship between personality and Facebook profile features discussed in Chapter 4
suggests that the former can be predicted from the latter. This section explores this fur-
ther by building a relatively simple prediction model, continuing and expanding the work
of [81] who attempted to predict personality from Facebook profile information using ma-
chine learning algorithms. They used a very rich set of features, including both Facebook
profile features, such as the ones that are used in this work, but also the words used in status
updates. However, their sample (n = 167) was very small, especially given the number of
features used in prediction (m = 74), which limits the reliability and generalizability of the
results.
5.3 Predicting personality with Facebook profile features 81
5.3.1 Methods
This study is based on a sample discussed in detail in Section 4.2.2. Predictors included
the total number of friends, egocentric network density, number of events attended, number
of posted status updates, number of photos and photo tags, and number of groups mem-
berships. Those predictors were log-transformed in order to normalise their distribution.
Predicted variables included FFM personality traits, satisfaction with life, and Intelligence
discussed in Section 5.2.
The prediction model used here employs a multivariate linear regression with bi-directional
stepwise variable selection based on Akaike Information Criterion (AIC; [40]). AIC selects
the best model from a set of models by minimizing the Kullback-Leibler divergence be-
tween the model and the truth. This “greedy” procedure starts with all variables and keeps
removing (one-by-one) those that are least predictive until no further improvement in the
quality of the model is achieved. Next, it examines whether adding back any of the removed
variables could potentially improve the quality of the model, and does so until no further
improvement is achieved. Application of the AIC procedure implies that the psychological
traits are being predicted with a different subset of Facebook profile features. As the overlap
between the data related to different Facebook features is not complete, each of the traits is
predicted using a sample of a different size.
Accuracy of the model was examined using 10-fold cross validation and Pearson corre-
lation coefficient between the predicted and actual values.
5.3.2 Results
in Facebook profile features, followed by Neuroticism, Conscientiousness, and Openness.
Agreeableness is the hardest trait to predict based on the Facebook profile features and the
simple model used in this study. Best prediction accuracy was achieved for age (r = .5).
When compared with the results based on Facebook Likes (see Section 5.2), it is ap-
parent that the predictive power of Facebook profile features is consistently lower. This
indicates that the individual selection of Likes is more informative about users’ psycholog-
ical traits. This effect is especially strong for Openness, which is relatively well manifested
in the selection of Likes, but very weakly represented in the aggregate features of Facebook
behaviour.
Several non-linear machine learning methods for predicting traits were also applied to
this problem. This included M5 Rules [106], Support Vector Machines with polynomial
82 Using digital footprints to infer psycho-demographic profiles
age using Facebook profile features.
Trait r n Features used in the prediction
Openness .11 18,720 Friends, Groups, Likes, Network Density, Photo Tags,
Photos
Conscientiousness .16 18,720 Friends, Groups, Likes, Network Density, Photo Tags,
Photos
Extraversion .31 16,900 Friends, Groups, Likes, Network Density, Photo Tags,
Statuses
Agreeableness .05 45,565 Friends, Likes, Photo Tags
Neuroticism .23 9,515 Friends, Likes, Photos, Statuses
SWL .33 311 Events, Likes, Network Density, Photo Tags
Intelligence .20 395 Events, Likes, Photo Tags, Photos, Statuses
Age .50 3,826 Events, Friends, Groups, Likes, Network Density,
Photo Tags, Photos
sample size, and Facebook features used in the prediction.
and Radial Basis Functional kernels, and decision stumps (for details on these methods
see [27, 105]). Accuracy provided by those approaches was similar to the ones obtained
using multiple linear regression. This suggests that relatively low prediction accuracy stems
mostly from a high variance in personality traits among people with similar Facebook profile
features.
Relatively weak but predominantly linear relationship between log-transformed Face-
book features and personality, and comparable accuracy achieved using other prediction
relationship between personality and descriptive statistics summarizing Facebook profile
features.
5.4 Conclusions
This work shows that a wide variety of people’s personal attributes, ranging from sexual ori-
entation to intelligence, can be automatically and accurately inferred using their Facebook
Likes. Similarity between Facebook Likes and other widespread kinds of digital records,
such as browsing histories, search queries, or purchase histories suggests that the potential
to reveal users’ attributes is unlikely to be limited to Likes. Moreover, the wide variety of
attributes predicted in this study indicate that, given appropriate training data, it may be
5.4 Conclusions 83
possible to reveal other attributes as well.
Additionally, it was shown that some prediction accuracy can be achieved using Face-
book profile features, such as number of friends or number of status updates posted. While
the accuracy achieved when using Facebook profile features might not be satisfactory on an
individual level, it could prove useful in accurately inferring group profiles (e.g. psycholog-
ical profiles of website audiences or product users).
Fig. 5.10 Screenshot showing a part of YouAreWhatYouLike application (
) that uses Facebook Likes to infer personality profile not only
youarewhatyoulike.com/
of its user but also of all of her/his Facebook friends. Names and the pictures of author’s
friends characterised by extreme levels of Openness were blurred to protect their privacy.
Predicting users’ individual attributes and preferences can be used to improve numerous
products and services. Digital systems and devices (such as online stores or cars) could be
designed to adjust their behaviour to best fit their users’ inferred profiles [170]. For exam-
ple, a car could adjust the parameters of the engine and the music to the personality and
current mood of the driver. Also, the relevance of marketing and product recommendations
could be improved by adding psychological dimensions to current user models. For ex-
ample, online insurance advertisements might emphasise security when facing emotionally
unstable (neurotic) users but stress potential threats when dealing with emotionally stable
ones. Moreover, digital records of behaviour may provide a convenient and reliable way to
84 Using digital footprints to infer psycho-demographic profiles
measure psychological traits. Automated assessment based on large samples of behaviour
may not only be more accurate and less prone to cheating and misrepresentation but may
also permit assessment across time to detect trends. Finally, inference based on observations
of digitally recorded behaviour may open new doors for research in human psychology.
On the other hand, the predictability of individual attributes from digital records of be-
haviour may have considerable negative implications, because it can easily be applied to
large numbers of people without obtaining their individual consent and without them notic-
ing. Commercial companies, governmental institutions, or even one’s Facebook friends
could use software to infer attributes, such as intelligence, sexual orientation, or political
situations in which such predictions, even if incorrect, could pose a threat to an individual’s
well-being, freedom, or even life. Importantly, given the ever-increasing amount of digi-
tal traces people leave behind, it becomes difficult for individuals to control which of their
attributes are being revealed. For example, merely avoiding explicitly homosexual con-
tent may be insufficient to prevent others from discovering one’s sexual orientation. More
detailed discussion of this issue is presented in Chapter 6.
There is a risk that the growing awareness of digital exposure may negatively affect
people’s experience of digital technologies, decrease their trust in online services, or even
completely deter them from using digital technology. It is hoped, however, that the trust
and goodwill amongst parties interacting in the digital environment can be maintained by
providing users with transparency and control over their information, thus leading to an
individually controlled balance between the promises and perils of the Digital Age.
Chapter 6
Crowd IQ: Modelling crowd ability
using Item Response Theory
This chapter explores the relationship between the performance of the crowd and the abil-
ities of its members. I introduce a novel formalisation of this relationship based on the
Condorcet Jury Theorem and Item Response Theory, and propose a Weighted Voting Mech-
anism based on individual agents’ abilities and Item Response Theory parameters of the
tasks. Range of experiments based on empirical and simulated samples are conducted, il-
lustrating the properties of the crowd performance, its dependence on the distribution of
crowd members’ abilities, crowd size, and the conditions of the task. Also, the performance
offered by Weighted Voting Mechanism is compared with an unweighed approach. Ad-
ditionally, I propose that the performance of crowds across a wide range of tasks can be
accurately modelled based on its members general intelligence, and conveniently expressed
on a standardised intelligence scale.
6.1 Introduction
A crowd’s ability to solve tasks has been extensively studied for many years. Perhaps the
best known work in this area, “The Wisdom of Crowds” by James Surowiecki [223], argues
that a diverse crowd of individuals working independently is likely to make better deci-
sions than experts, or any of the crowd members alone. Surowiecki lists four properties
of a wise crowd: (1) diversity of crowd members’ opinions, (2) independence of crowd
members’ opinions, (3) ability of crowd members to specialise and draw on local knowl-
edge (decentralisation), and (4) the aggregation mechanism used to turn private judgements
into a collective decision. Similar conclusions have been reached by many other authors
86 Crowd IQ: Modelling crowd ability using Item Response Theory
working in diverse fields of research, such as social choice theory [212], a research area
in artificial intelligence and multi-agent systems modelling joint decision-making by self-
interested agents, or judgement aggregation [152] dealing with aggregating judgements on
interconnected propositions. The collective ability to solve tasks has been studied in very
diverse crowds, such as insect colonies [43], social mammals [29], robots [165], and human
teams [244].
Recent progress in information technologies has dramatically expanded the potential to
utilise crowds in solving a wide range of problems. Well-known examples of platforms
facilitating this process include Wikipedia, Yahoo! Answers, and various prediction mar-
kets [1, 60, 243]. Crowds that seem to best match the properties of a wise crowd, as proposed
by Surowiecki, can be found on crowdsourcing platforms such as Amazon Mechanical Turk
(AMT), Taskcn, or Crowd-Flower, which are increasingly being used to outsource labour,
collect information, and make decisions.
In the light of the growing importance and popularity of crowdsourcing platforms and
other multi-agent systems, it is of critical importance that we deepen our understanding of
the factors that drive the ability of crowds to solve different types of tasks; such factors
include financial rewards, aggregation methods, or reputation mechanisms. Consequently,
crowdsourcing research and industry should be equipped with a universal and flexible model
describing the underlying mechanisms of the crowd’s performance and a metric to express
the ability of agents and crowds to correctly solve tasks of varying difficulty. This would
allow researchers and practitioners to effectively evaluate factors affecting performance and
reliably compare between agents, crowds and environments.
This study investigates the relationship between the performance of a crowd and the indi-
vidual abilities of its members. It focuses on crowdsourcing platforms and other multi-agent
systems, where crowds of human or artificial agents independently submit their responses
to tasks. Crowd and individual performance across tasks is modelled using Item Response
Theory [16] (IRT), a core concept of modern psychometrics. IRT is based on the assump-
tion that performance across related tasks is determined by an underlying statistical factor,
referred to in this study as ability. It is further shown that the performance of the crowd di-
rectly depends on the distribution of its members’ abilities and an approach to the weighting
of individual solutions is proposed. The feasibility of expressing collective ability as a sin-
gle score on a standardised metric, which could function as an index of crowd performance
across various tasks, is explored in detail. This would enable researchers and practition-
ers to make comparisons between crowds of different sizes as well as between crowds and
individuals.
6.1 Introduction 87
The experiments conducted in this study are aimed at investigating properties that af-
fect crowd performance. These include crowd size, aggregation methods, financial rewards
offered to agents and the degree to which the weighting mechanism relies on both prop-
erties of the task and the abilities of members. A key finding is that the ability advantage
held by an agent in the context of a given crowd drives its marginal contribution to crowd
performance.
Human abilities range from specific (driving performance in a narrowly defined set of
tasks) to general (underlying performance across a wide range of tasks). Decades of re-
search in psychology have shown that most of the specific abilities share common variance.
For example, performance in tasks requiring memory correlates highly with performance in
tasks relying on a wide range of other mental abilities, such as mathematical skill, verbal
fluency, language skills, spatial visualisation, creativity, learning ability, and abstract think-
ing [208, 220]. This common statistical factor, summarising positive correlations among
different mental tasks, is one of the most replicated results in psychology [57] and a foun-
dation of the concept of general intelligence [220].
I propose that the general intelligence score, or intelligence quotient (IQ), can be used
as a powerful metric of individuals’ and crowds’ general ability to solve a wide range of
tasks (this was also previously suggested by [249]). I refer to the crowd’s score on the
general intelligence test as crowd IQ and argue that it provides a convenient proxy for crowd
performance across a wide range of tasks. Modelling general intelligence offers a number
of advantages.
First, general intelligence is the most far-ranging and universal psychological predictor
of performance across various tasks in educational, occupational, and social settings [92,
117, 155, 208]; someone’s ability to solve an intelligence test is therefore likely to be related
to their individual and aggregated performance across many other contexts.
Second, general intelligence is relatively well understood and widely measured using
one of a large number of high-quality tests. Increasingly, attempts are being made to extend
the concept of general intelligence to non-human subjects (e.g. animals or machines) and
develop appropriate tests (e.g. [101, 150]). Consequently, it is not only relatively easy to
obtain or build measures to assess the general intelligence of human agents but, increasingly,
for non-human agents as well. Moreover, general intelligence measures are often designed
to minimise their dependence on agents’ language and cultural background (e.g. [191]),
allowing for a reliable comparison across cultures, geographical areas and educational levels
to be made.
88 Crowd IQ: Modelling crowd ability using Item Response Theory
Third, the standardised character of the IQ scale facilitates the comparisons across
crowds, individuals and the general population. For example, an agent characterised by
an IQ of 130 (or 2σ above the mean), can outperform 98% of individuals in the general
population characterised by the normally distributed IQ scores.
Fourth, general intelligence has been shown to be an excellent predictor of performance
in those tasks which lack a single correct solution or perhaps rely on a large number of
different skills and abilities. Some examples of these tasks might include writing product
reviews, designing software or making strategic decisions [208].
Finally, IQ has been shown to be only weakly affected by knowledge, training, and pre-
vious exposure to similar tasks. Even a low-performing agent with some prior experience in
a specific task might initially outperform a potentially much better agent with no prior expe-
rience. Assessing general intelligence rather than a particular ability would therefore offer
a better indication of an individual’s potential. Such an assessment is particularly useful be-
fore deciding to provide agents with often expensive and time-consuming training. Studies
indicate that the predictive validity of intelligence is greater than that of job experience and
that this validity is not affected by increases in the individual’s experience [57].
6.2 Item Response Theory: Modelling the relationship be-
tween performance and ability
Item Response Theory (IRT), a core concept in modern psychometrics akin to factor anal-
ysis [16], offers a convenient approach to modelling the relationship between ability and
performance. IRT proposes that the probability of solving a task (referred to as an item) cor-
rectly, depends on the ability of an individual, the difficulty of the task, and the strength of
the relationship between the task and the underlying ability (discrimination). This relation-
ship is formalised by the Item Response Function (IRF) and can be further mediated by the
likelihood of solving the task by chance (guessing). The popular three Parameter Logistic
(3PL) IRT model [16] defines the probability of an individual to correctly solve task I as:
1 − c
P (θ |a , b , c ) = c + (6.1)
I I I I I
1 + e
−a (θ −b )
I I
where theta (θ ) represents the ability, a is the discrimination, b is the difficulty, and c is
I I I
the guessing parameter of task I. In the remaining part of this work P (θ |a , b , c ) will be
I I I I
The standard IQ scale is normally distributed, has a mean of µ = 100, and has a standard deviation of
σ = 15.
6.2 Item Response Theory: Modelling the relationship between performance and ability 89
replaced by a shorthand notation (P (θ )) as the parameters a , b , and c are evident from
I I I I
the context.
The IRF’s inflection point is located at its difficulty level. Decreasing the difficulty shifts
the function to the left and thus increases achievable performance across the ability range.
Both ability and the difficulty of a task are conveniently expressed on the standardised and
normally distributed theta scale (µ = 0, σ = 1). The values on the theta scale can be easily
θ θ
interpreted; for example, an ability of t = 0 indicates that a given individual performs better
than half of the population, while an ability t = 2 indicates better performance than 98% of
the population.
The discrimination parameter, indicating the slope of the IRF at the inflection point,
shows the strength of the relationship between ability and the given task—the steeper the
slope, the stronger the relationship. The guessing parameter, or IRF’s lower asymptote,
measures the degree to which a given task can be solved regardless of the level of ability
(e.g. by guessing in a multiple choice test), and may depend on the number of available
response options.
From Equation 6.1 it follows that an individual with an ability equal to the difficulty of
a given task (i.e. t = b ), is expected to solve the said task correctly in more than half of the
j I
1+c
cases (corrected for guessing, i.e. P (t = b ) = I ). Effectively, the difficulty level of a
I j I
task provides information about the fraction of the population that can solve it in more than
half of the cases (corrected for guessing). For example, a task of the difficulty of b = −2
can be solved with a probability of P (θ ) > .75 by 98% of individuals when guessing equals
c = .5.
The IRF parameters are estimated by analysing empirical samples of responses to a
between ability and performance for a given task. It shows, for instance, that at the ability
level of θ = 1 (one standard deviation above the average ability, or higher than 84% of the
general population), the expected probability of solving a given task correctly is equal to
approximately 90%.
6.2.1 Estimating ability
IRT provides a number of ability estimators (or scoring methods) based on an individ-
ual’s performance in a set of tasks of known parameters [21]. A popular estimator, Bayes
Modal [25], estimates the individual score by selecting the maximum value of the posterior
90 Crowd IQ: Modelling crowd ability using Item Response Theory
Fig. 6.1 An example of an Item Response Function.
distribution L of an individual’s u ability, defined as:
∏ ∏
L (θ ) ∝ (µ, σ ) P (θ ) (1 − P (θ )), (6.2)
u I I
I∈C I∈/C
where C is the set of correctly solved tasks, P (θ ) is the probability of solving task I correctly
across θ represented by the IRF, and (µ, σ ) is a prior on θ , which in this case takes a form
of a Gaussian distribution with µ = 0 and σ = 1. A number of other scoring approaches
could also be used. One such example is the Maximum Likelihood approach [153], which
attempts to find the ability θ by maximizing the same expression without the prior.
The IRT framework offers a large number of other models, some highly interesting in
the context of multi-agent systems and crowdsourcing. For example, multidimensional IRT
deals with tasks where performance depends on more than one underlying ability.
6.2.2 Computerised Adaptive Testing
The IRT approach cab be used to estimate scores based on different subsets of tasks, pro-
vided that the latter are part of the same IRT model [16]. This has many powerful practical
applications, including the Computerised Adaptive Testing (CAT) approach (see e.g. [232])
which successively selects the most informative items for a given participant based on their
performance on previous items. CAT has numerous advantages from the perspective of
6.2 Item Response Theory: Modelling the relationship between performance and ability 91
both the test-taker and the test administrator. The test automatically adjusts to the level of
the participant, ensuring that it is not overly difficult, nor too easy, preventing frustration
and boredom. From the administrator perspective, asking items of the correct difficulty
significantly shortens the testing time and increases test accuracy.
Fig. 6.2 True score prediction accuracy offered by two popular next item selection criteria:
MFI and Urry’s. Randomly selecting the next item provides a baseline estimate of the
accuracy. The horizontal axis represents the length of the test, and the vertical axis shows
the correlation between the estimated score and the true score. The results are based on the
simulated response patterns of n = 1, 000 agents with normally distributed abilities (µ = 0,
σ = 1) to 50 items. The items are characterised by normally distributed difficulty parameters
(µ = 0, σ = 1), uniformly distributed discrimination (min = .5, max = 3), and guessing of
c = .125.
Maximum Fisher Information (MFI) and Urry’s approach. Results show CAT’s significant
superiority over selecting items at random. For example, the r = .95 accuracy of the to-
Other well known and easily deployable methods include the Maximum Likelihood Weighted Informa-
tion, the Maximum Posterior Weighted Information, the Maximum Expected Information, and the Minimum
Expected Posterior Variance [232].
92 Crowd IQ: Modelling crowd ability using Item Response Theory
tal score estimate is achieved after 7 items when using MFI, 12 items when using Urry’s
approach, and 18 items when selecting items at random. Effectively, in this case CAT fa-
cilitated the shortening of the 50-item test to 7-12 items without significantly decreasing its
accuracy.
6.3 Modelling the performance of crowds using Item Re-
sponse Theory
One of the advantages of crowdsourcing and other multi-agent platforms is that its per-
formance can be increased by aggregating agents’ solutions into a single crowd solution.
However, while the amount of information available to construct a final solution grows with
the number of individual solutions obtained, so does the cost in terms of time and money.
While in most cases the Majority Voting (MV) approach is used to aggregate individual so-
lutions, other methods can offer better performance. For instance, requesters can manually
select the best response or use a multi-level voting system that is based on requesting agents
to vote on the best solution from among those suggested by other crowds [77]. Additionally,
computer science offers numerous practical and computationally tractable ways of aggre-
gating individual solutions, ranging from machine learning approaches [8, 127, 128, 218]
and adaptive schemes [56, 111] to prediction markets [177]. This work introduces a new
weighting approach based on an individual agent’s ability.
In this study, the formulation of the relationship between individual and crowd perfor-
mance is inspired by the IRT and Condorcet Jury Theorem (CJT) frameworks. CJT provides
theoretical bounds on the probability of a set of agents to reach the correct decision under
MV [7, 151, 161]. CJT was originally developed to model performance in dichotomous
tasks and agents with uniform probabilities of solving the task correctly. Extensions to
the CJT focus on tasks with more than two response options and agents with non-uniform
probabilities of solving the task.
6.3.1 Definitions
In this work the crowd is defined as a set of agents, characterised by individual ability levels
t , ..., t , each of which submits exactly one solution to task I. The size of the crowd is given
1 n
by n, and the number of response options of task I is given by k. In the interest of clarity,
the index of 1 is assigned to the correct response option. The subset of crowd members that
selects response option q is referred to as the coalition and denoted by K(q). The weight of
6.3 Modelling the performance of crowds using Item Response Theory 93
coalition K (discussed in more detail in Section 6.3.3) is denoted by w(K). The probability
of an individual i characterised by ability level t to solve task I correctly is denoted by P (t )
i I i
and is calculated using the IRF (Equation 6.1).
6.3.2 Crowd Item Response Function
In many real-life scenarios the tasks are characterised by one correct solution and a finite
number of incorrect solutions. Examples include voting for the best candidate, categorizing
images and solving tasks from a multiple choice test. Below I show how to estimate the
probability of solving the task correctly in such a context. Note that a dichotomous task
traditionally considered in the CJT is a special case of a multiple choice task (where k = 2).
Let Q be the set of all possible k-tuples of jointly exhaustive but mutually exclusive
coalitions K, where the coalition that selected a correct response (K(1)) has the highest
weight, making it a winning coalition:
(cid:8)
Q := ⟨K(1), K(2), ..., K(k)⟩ :
k (6.3)
(cid:9)
|K(i)| = n & w(K(1)) ≥ w(K(2)) ≥, ..., ≥ w(K(k)) .
i=1
In the event of a tie, where two or more coalitions share the highest weight, one of them
is selected as a winning coalition at random. Effectively, the probability of coalition K(1)
winning the vote in the event of a tie is equal to:
(6.4)
w(K(1))−w(K(i))
i=1
where the denominator represents the number of coalitions K sharing the highest weight.
The probability of a crowd selecting the correct response is equal to the sum of probabilities
of all of the k-tuples of coalitions K divided by the number of winning coalitions:
P (t )
∏ ∏
j=1 i∈K( j) j i
P (t , ..., t ) = (6.5)
I 1 n
w(K(1))−w(K(i))
⟨K(1),K(2),...,K(k)⟩∈Q i=1
where P (t ) represents the probability of an individual characterised by ability t to select
j i i
response option j.
For simplicity, it is assumed that the probabilities of incorrect responses are uniformly
distributed, from which it follows that the probability of selecting any of the (k − 1) incorrect
1−P (t )
I j
responses equals . This allows for the Equation 6.5 to be presented in the following
k−1
94 Crowd IQ: Modelling crowd ability using Item Response Theory
way:
1−P (t )
P (t ) I i
∏ ∏
i∈K(1) I i i∈/K(1)
k−1
P (t , ..., t ) = . (6.6)
I 1 n
w(K(1))−w(K(i))
Q i=1
Equation 6.6, or Crowd IRF, can be used to estimate the probability of a crowd providing
a correct response based on the individual members’ abilities t , ..., t and the IRF parameters
1 n
(a , b , and c ) of a task. The value of a guessing parameter is usually equal or greater
I I I
than . Note that the shape of the Crowd IRF depends on the aggregation mechanism
used. Due to the high number of potential k-tuples of coalitions, Equation 6.6 may prove
to be computationally inefficient. D presents a tractable approach to estimating a crowd’s
probability of solving a given task correctly.
6.3.3 Weighted Majority Voting
The Majority Voting approach weighs the coalitions by their size and simply selects the
most popular solution. However, if the information about agents’ abilities (and thus their
probability of solving the task correctly) is available, far better results can be achieved by
choosing the response with the highest likelihood of being correct (e.g. [144]). This work in-
troduces an ability-based approach to aggregating individual solutions, namely the Weighted
Majority Voting (WMV) approach.
Given coalition K(I), the probability that the solution of their choice I is correct is given
by:
1 − P (t )
I i
∏ ∏
L(K, I) = P (t ) (6.7)
I i
k − 1
i∈K i∈/K
where P (t ), given by the IRF (Equation 6.1), represents the probability that an agent will
I i
solve the task correctly.
Note that the coalition K that maximises L(K, I) also maximizes log(L(K, I) ∗ (k −
1)/(1 − P (t ))). Therefore, I define the weight of a coalition as:
I i
(cid:32) (cid:33)
(cid:18) (cid:19)
k − 1 P (t )(k − 1)
I i
∏ ∑
w(K, I) = log L(K, I) · = log . (6.8)
1 − P (t ) 1 − P (t )
I i I i
i=1 i∈K
The weight of an individual crowd member is thus given by:
(cid:18) (cid:19)
P (t )(k − 1)
I i
w(t , I) = log . (6.9)
1 − P (t )
I i
6.3 Modelling the performance of crowds using Item Response Theory 95
Note that the weight of a coalition is simply a log-transformed product of its agents’
odds ratios of solving the task correctly. This quantification of an individual agent’s weight
(Equation 6.9) is an equivalent of the approach proposed for dichotomous choices by Shap-
ley [214] and Nitzan [171].
Fig. 6.3 Weights of three simulated coalitions across the difficulty range for a single task
characterised by k = 8 response options, discrimination of a = 1, and guessing of c = .125.
The coalitions vary in size and average ability level.
difficulty. Interestingly and perhaps counter-intuitively, the order of coalitions in terms of
coalitions of large size but low average ability can outperform higher-ability, but smaller
coalitions,on easy tasks; the opposite happens for tasks of higher difficulty. In practical
terms, it implies that while seeking solutions for easy tasks, large groups of non-experts
endorsing single solutions ought to be trusted more than small groups of experts. When
96 Crowd IQ: Modelling crowd ability using Item Response Theory
seeking solutions to difficult tasks, on the other hand, small groups of experts endorsing one
of the choices should be preferred over large coalitions of non-experts.
Importantly, the same does not relate to individuals. An agent of higher ability outper-
forms agents of lower ability regardless of the parameters of the task (see E for a formal
proof). In other words, the order of individual agents’ weights does not depend on the task,
nor on other agents.
Several other interesting insights are offered by the weighting approach introduced here.
First, the weight of an agent or a coalition depends solely on coalition members’ abilities and
the task’s IRF parameters, and does not depend on the abilities of other agents or coalitions.
Second, adding an agent to a coalition always increases its weight. Finally, a coalition’s
weight is always greater than the weight of any of its members.
The weighting mechanism introduced here relies on the parameters of the task and the
abilities of the crowd members. An agents’ ability can be estimated in a number of ways, for
example by using a distinct set of pre-screening tasks or by comparing their performance
with agents of known abilities. While the task parameters cannot be easily estimated in
the crowdsourcing environment, the results presented here show that weighting based on
uniform task parameter values (i.e. equal for all tasks) leads to a relatively high aggregation
performance.
6.3.4 Performance and Ability Curves
Equation 6.6, or Crowd IRF, allows the probability of a crowd providing a correct response
to be estimated, based on the individual members’ abilities t , ..., t and the IRF parameters
1 n
(a , b , and c ) of a task. This relationship can be conveniently represented using Crowd
I I I
and Individual Performance Curves, showing the probability of solving a task correctly
(vertical axis) against its difficulty (horizontal axis). Performance Curves can be used to
investigate the relationship between agents’ and crowds’ performance across conditions and
aggregation approaches.
The average agents’ abilities of both crowds is equal, yet the crowds have drastically differ-
ent performance, showing that the distribution of individual abilities can affect the perfor-
mance of the crowd. Additionally, as illustrated by the example of Crowd 2, the unweighted
MV approach offers relatively poor performance in the context of crowds characterised by
a wide distribution of abilities. Crowd 2 working under the MV aggregation is outper-
formed by its smartest member across the entire plotted range of difficulty. Crowd 1, which
6.3 Modelling the performance of crowds using Item Response Theory 97
Fig. 6.4 Crowd and individual performance curves for two crowds. Performance curves
represent the probability of solving the task correctly (vertical axis) against its difficulty
(horizontal axis). Performance of the individual agents (continuous lines), and the crowds
working under MV (pink dashed line) and WMV approaches (green scatter-points) is pre-
sented. Both crowds have n = 3 members, equal average ability levels (µ = 0) and differing
variability of members’ abilities (σ = .38, σ = 3). Individual performance is estimated
1 2
using an inverse of the IRF function (Equation 6.1). Expected performance of the crowds
working under the MV approach is estimated using Equation 6.6. Expected crowd perfor-
mance under the WMV approach is estimated by aggregating individual agents’ simulated
responses (the procedure is repeated n=1,000 times and the average is taken). The tasks are
characterised by varying difficulty, a guessing parameter of c = .125 and discrimination of
a = 1.
has a highly uniform ability distribution, is outperformed by its highest ability individual
(t = .3) on tasks more difficult than b = 1.8, when also working under the MV approach.
By contrast, in both examples, the WMV approach outperforms the crowd’s smartest mem-
ber across the entire range of difficulty.
The performance curves of the individuals are parallel, illustrating the previously dis-
cussed fact that agents of higher ability can consistently (though to a different degree)
outperform agents of lower ability. Importantly, crowd performance curves have differing
shapes and slopes indicating that a given crowd can outperform individuals or other crowds
in some difficulty ranges, but not others.
98 Crowd IQ: Modelling crowd ability using Item Response Theory
Fig. 6.5 Crowd performance and the estimate of crowd ability for crowds with equal average
ability (µ = 0) but differing spread of agents’ abilities (as expressed by varying σ ). The
horizontal axis represents the difficulty of the task. The performance of the crowd working
under the MV (pink dashed line) and WMV approaches (green scatter-points) is expressed
on the left vertical axis. The ability of the crowd at a given difficulty level is represented by
the pink (MV) and green (WMV) continuous lines and expressed on the right vertical axis.
6.4 Crowd ability 99
Differences between the shapes of the crowd and individual performance curves indicate
that crowd ability does not fit the same model as individual ability. This is illustrated by
characterised by equal average level, but differing spread of their members’ abilities. The
ability of the crowd is estimated using an inverse of the IRF function, representing the ability
level that an individual agent would need to have in order to correctly solve a given task with
the same expected probability.
The varying shapes of crowd ability curves indicate that crowds characterised by the
same average agents’ ability can have very different ability profiles , especially when using
MV and when agents’ abilities show high variability (i.e. for σ > 2). In the examples
presented here, the ability levels achieved by MV tend to fall below the levels of the highest
ability individuals, while WMV is at least as high as the ability of the most able member.
Additionally, MV approach results in more fluctuations in crowd ability, indicating that it is
not as robust as the WMV approach.
6.4 Crowd ability
Section 6.3.4 shows that the ability of the crowd cannot be precisely expressed as a location
on a unidimensional ability metric. First, crowd ability changes across the difficulty range
and is not ranked in a fixed way; for example, a given crowd can outperform other crowds
on some tasks but not others. Second, as illustrated by Equation 6.6, crowd performance
across tasks directly depends on the abilities of its members. Crowd ability is therefore a
function on a multidimensional variable composed of the abilities of its members.
However, being able to express a proxy for crowd ability on a unidimensional metric
would be most convenient in providing an index of crowd performance across tasks and
could be used to compare between crowds and agents. It is proposed that a crowd’s score
on a standardised ability test featuring tasks of a wide difficulty range could provide such
a proxy. Interestingly, in the case of crowds of the type examined in this work, the score
on said test is determined by Equation 6.6 and the IRF parameters of the standardised test’s
items. It follows that a proxy for crowd ability can be estimated based solely on its members’
abilities, without having to administer the actual test.
Importantly, in many contexts, including crowdsourcing platforms, crowds are rather
ephemeral in nature as their membership changes across tasks and jobs. However, it is
argued that the ability distribution within the population of potential crowd members, and
Note that an individual agent’s ability is constant across the difficulty scale.
100 Crowd IQ: Modelling crowd ability using Item Response Theory
the processes driving the selection process (e.g. motivation to work on the task or supply
and demand for crowd members), remain relatively stable, resulting in a similar ability
distribution among agents submitting their solutions. Therefore, it is hypothesised that the
ability of different crowds sourced from the same population and working under the same
conditions remains relatively constant across tasks.
This work considers crowds working in the context of a crowdsourcing platform where
agents submit their solutions independently. However, the function mapping the individual
abilities to the performance of crowds working in other contexts, such as teams of agents
collaborating in person to solve tasks is likely to include psychological aspects. For exam-
ple, in teams collaborating face-to-face on solving general intelligence-related tasks, social
interactions were shown to play a more important role than the average and maximum IQ of
the group members [154]. Psychological aspects affecting performance of such crowds may
also include members’ social sensitivity, equality in conversational turn-taking, or the gen-
der ratio within groups [154, 244], which all continue to be poorly understood and highly
variable.
6.5 Methods
6.5.1 Tasks
Performance and ability of crowds and individuals is benchmarked using items from an
established intelligence test - Raven’s Standard Progressive Matrices (SPM) [191, 192].
SPM is a multiple choice non-verbal intelligence test, developed by John C. Raven, which
draws on Spearman’s theory of general ability [220]. Raven’s SPM and its other forms
(e.g. Advanced and Colored Progressive Matrices) are among the most commonly used
intelligence tests, in both research and clinical settings, as well as in high-stake contexts,
such as in military personnel selection and court cases [191]. Due to its non-verbal format,
SPM can be used across ages, cultures, and languages.
The original SPM test consists of 60 tasks designed to measure test takers reasoning
ability, the core element of general intelligence. Each task is a matrix of shapes, with one
element missing and k = 8 possible shapes that can complete the matrix, out of which one
AMT Human Intelligence Task (HIT). Note that while SPM is copyright protected, this
work is accompanied by a royalty-free intelligence test that can be used free of charge in
commercial and non-commercial research (see Section F).
6.5 Methods 101
Fig. 6.6 Sample Human Intelligence Task as used in this study. Note that as the SPM items
are copyright protected, the IQ item presented here is not an actual SPM item, but a similar
one. Option 1 is the correct solution to this item.
The SPM test is used in assessment across a wide range of ability levels and age. Con-
sequently, it contains items ranging from very easy to very difficult. Based on the previous
experience with AMT it was decided to skip the 24 easiest items. Asking items that are
too easy would not be informative and would significantly increase the time and cost of the
experiment. A similar approach is used in traditional intelligence testing, where the easy
tasks are skipped while administering the test to the individuals that are likely to achieve
high scores. This is facilitated by the IRT scoring approach which can be used to estimate
the theta score based on different subsets of items.
The IRF parameters for the SPM items were provided by the authors of the SPM manual
for the UK [192]. The parameters were estimated using the entire standardization sample
and adjusted for the 16-18 year old population. Using this age group as a reference point
provides the most conservative (lowest) estimates of intelligence, as people of this age tend
to achieve the highest performance on the SPM test. Guessing and discrimination param-
102 Crowd IQ: Modelling crowd ability using Item Response Theory
eters have uniform values across tasks (c = .125 and a = 1 respectively). Difficulty of the
items ranged from b = −3.06 to b = 3.00, with an average difficulty of µ = 1.09.
min max b
6.5.2 Samples
Two empirical datasets are used in this study: solutions to the full SPM test collected in the
laboratory setting from the 206 individuals (lab sample) and 1,800 solutions to individual
SPM tasks collected on the AMT platform from 236 unique agents (AMT sample).
Lab Sample
The lab sample was collected during the SPM test’s standardization for the British market in
the year 2006 [192]. Each participant filled an entire SPM test in its original pen-and-paper
format. The sub-sample used in this study consists of 206 individuals from the UK aged
16-18, and is representative of the British population of this age. The individual IQ score
distribution in the lab sample very closely follows the expected distribution of IQ scores
with mean µ = 100.20 and standard deviation σ = 15.10. This is to be expected as this
sample was used to develop the population norms for the UK market used in this study.
AMT Sample
The AMT sample was collected on Amazon Mechanical Turk in January and February 2011.
AMT connects human agents (referred to as workers), interested in selling their labour, with
requesters seeking agents to work on their jobs. Requesters split their jobs into single tasks
of minimal complexity (so-called Human Intelligence Tasks or HITs) and offer rewards to
agents for solving them. Agents can accept the HIT and submit their solutions working
independently. Requesters can influence the subset of the agents involved in their project
by limiting access to their tasks based on the agents’ reputation, location, and experience.
Additionally, they can influence agents’ motivation by adjusting the conditions of HITs
including the size of the reward, bonuses, instructions, layout of the task, and decide whether
to reject incorrect solutions. Both the subset of agents with access to the tasks, and their
conditions were shown to have a significant effect on the quality of resulting work (e.g.
[30, 111, 115, 132, 137, 160, 205])
IQ tasks described in Section 6.5.1 were turned into separate HITs described as “rea-
soning problems.” Importantly, while this approach mirrors the usual AMT practice, it may,
to some extent, affect the individual agents’ ability estimates. The IRT scoring approach
assumes that an individual solves all of the administered items, which might not be the
6.5 Methods 103
case under this experimental design. Effectively, while the individual AMT agents’ ability
levels are reported, the analyses depending on the validity of the individual agents’ ability
estimates are based on the lab sample.
To deter dishonest submissions and spamming, the instructions stated that “... it is very
easy to recognize spammers and we will block and reject them.” Also, the payment was
offered only for correct solutions, greatly reducing the potential benefits of spamming; given
k = 8 responses there is only a one in eight chance of solving the task correctly when
HITs were collected in ten separate experimental conditions (batches) differing in terms
of payment levels ($0.01, $0.05, $0.10, and $0.20), agents’ reputation thresholds (by default
98% and above), and rejection rules (by default the guidelines stated that there are “NO
REJECTIONS for incorrect answers.”).
In each batch five different agents were requested to provide solutions to each of the
HITs. To prevent agents from participating in more than one batch, only one batch of
HITs was available at any given time and agents who submitted HITs in more than one
experimental condition were removed. To minimise the biases related to the weekly or daily
fluctuations in AMT efficiency, batches were published on different dates but on the same
weekday and at the same time of the day. Access to tasks was limited to the US agents
with a decent prior AMT experience (at least 200 AMT HITs submitted). The final database
consisted of 1,800 individual solutions from 236 unique agents (7.6 solutions per agent on
average).
Intelligence
Id Reputation Payment n mean SD min max
1 ≥ 98% $0.01 30 92.4 15.4 46.7 133.6
2 $0.05 33 97.9 13.6 79.9 137.1
3 $0.10 29 96.4 13.3 66.1 130.8
4 $0.20 20 92.0 21.0 46.6 142.0
5 ≥ 98% R $0.10 31 100.0 17.7 74.1 146.8
6 $0.01 32 94.7 14.7 49.2 147.1
7 95%-97% $0.05 23 95.0 16.6 63.4 129.1
8 90%-94% 17 83.1 18.2 46.0 114.0
9 80%-89% 10 74.1 24.6 37.6 125.4
10 < 80% 11 74.4 26.7 36.4 132.8
Note: number of unique agents in a batch is presented in column
n. In batches 5 and 6 (marked with “R”) submitting an incorrect
solution was threatened with rejecting the HIT.
104 Crowd IQ: Modelling crowd ability using Item Response Theory
The average intelligence of agents in most of the batches is between 90 and 100, close to
the average of general population. Strikingly, all batches (including low-reputation ones)
are characterised by extreme range of ability levels. Most of the batches contain individuals
with IQ scores below 60 and above 130, and the most able member has a score of 147
IQ points. Extreme intelligence levels are expected to be encountered very rarely in the
normal population (i.e. 2.3 times in a 100 for IQ of 130, and 1 in 1,000 for IQ of 147),
but are relatively common in the AMT sample. High-ability outliers can be explained by
a population bias (i.e. AMT attracts individuals with high IQ) and self selection bias (i.e.
individuals with lower IQ are less likely to accept IQ HITs). Low-ability outliers are most
likely an effect of spamming (selecting responses at random results in very low IQ scores).
The average intelligence of the agents characterised by the reputation below IQ = 95%
is significantly lower than in the remaining batches; t(49.59) = −3.72, p < .001. However,
low-reputation batches are still characterised by relatively high maximum intelligence.
Simulating responses
Some of the analyses employ simulated response patterns. To obtain the simulated response
patterns, the size and the distribution of the abilities of the crowd (or agents’ pool) is defined.
Next, the IRFs are used to estimate the probability of solving a task correctly for each of
the simulated ability values. Resulting probabilities are used to draw correct or incorrect
solutions. Finally, incorrect solutions are replaced with a random incorrect response option.
6.5.3 Obtaining crowd and individual ability scores
Crowd solutions are based on agents’ solutions aggregated using MV and WMV approaches
described in Section 6.3. Unless stated otherwise, agents’ weights are based on their abilities
estimated using all available IQ tasks (i.e. less or equal to 36 tasks) and fixed IRF parameters
(a = 1, b = 4, c = .125). Effectively, agents’ weights are uniform across the tasks, mirroring
the conditions of the real-life setting, where agents’ abilities can be approximated using
screening tests, but the parameters of the tasks are not known.
Note that in the AMT setting the HITs are assigned to agents in an ongoing fashion and
each task is solved by a different subset of agents. This property is used to verify whether
the hypothesis that the ability of different crowds from the same population and working
This, however, is to some extent imposed by the Bayes Modal scoring approach that assumes a normal
prior distribution. Note that many of the agents submit one or few solutions only, and hence the influence of
the prior on the final ability estimates is relatively large.
6.5 Methods 105
under the same conditions, remains constant across tasks. The lab sample, on the other
hand, contains each agents’ solutions to the entire test, offering an opportunity to measure
crowds’ performance across tasks, and observing an influence of self-selection bias present
in the AMT sample (AMT agents can decide which tasks they want to solve).
Individual and crowd solutions are scored using a Bayes Modal approach described in
Section 6.2 and based on the respective tasks’ IRF parameters. Resulting ability scores are
presented on a standard IQ scale (µ = 100 and σ = 15). As this study is based on the
IRF parameters developed on the general UK population, the IQ scores reported here can
be interpreted in the context of the general population. For example, a score of IQ = 100
indicates performance equal to the average member of the UK population , while the score
of IQ = 115 is higher than approximately 84% of the general UK population.
items [192], and Bayes Modal ability estimates based on the 36 items and the 3PL IRF pa-
rameters. The Pearson product-moment correlation between 3PL-based and classical scores
was very high (r = .96) suggesting that 3PL scores based on 36 tasks used in this study
provide a very close approximation of the scores computed in the standard manner on the
entire test. Slightly higher score variance of the 3PL scores for the participants of low IQ is
caused by fact that the 3PL scores are based on the relatively difficult subset of SPM items.
Effectively, the test remains accurate for high-ability participants, but is less informative in
the case of the low-ability participants, who found it hard to solve most of the relatively
hard items they were presented with.
6.5.4 Z appropriateness index
The Z appropriateness index [61] measures the degree to which response patterns con-
tain aberrant or unexpected responses, based on the fact that the IRF function defines the
probability of solving each task correctly for test-takers at a given ability level. Usually,
appropriateness indices are used to identify cases of cheating on ability tests, faking on per-
sonality questionnaires, or technical errors in processing test-takers’ responses. Here it is
used to check whether the individual and aggregated solutions acquired in the crowdsourc-
ing environment follow the assumptions of the IRT model developed on the representative
population in a controlled setting. Z measures the maximum height of the likelihood func-
Average performance on the IQ tests may vary slightly between countries and does not necessarily stem
from differences in the average intelligence. Biases can be driven by number of other factors, such as educa-
tion, previous exposure to similar tests, and response styles.
The SPM manual provides look-up tables for translating the number of correct responses (raw score) into
an IQ score.
106 Crowd IQ: Modelling crowd ability using Item Response Theory
60 80 100 120 140
IQ scores based on the IRT model
smron
laiciffo
eht
no
desab
serocs
QI
Fig. 6.7 Relationship between individual IQ scores in the lab sample estimated using Item
Response Theory Three Parameter Logistic Model and standard scoring model from SPM
manual [192]. The darker the dot, the more cases it represents.
tion (L (θ ), see Equation 6.2) produced by a given pattern of responses. Lower maxima
indicate profiles that are less likely, under the given model, for example due to cross-entropy
between the actual and expected responses. Z is defined in terms of the following:
ˆ ˆ
H(L (θ )) = [R log P (θ ) + (1 − R ) log(1 − P (θ ))], (6.10)
u i i i i
i=1
where R ∈ {0, 1} indicates whether participant i correctly solved the item, and P (θ ) is an
i i
expected response probability at θ estimate. Abnormalities in the response pattern (e.g.
incorrectly solving easy items and correctly solving difficult ones) decrease the value of this
index. To ascertain that H(L ) is independent of θ scores, it is standardised using:
H(L ) − M(θ )
Z = (6.11)
3 (cid:113)
S(θ )
6.6 Results 107
where M(θ ) is an expected value of H(L ) given θ :
ˆ ˆ ˆ ˆ ˆ
M(θ ) = [P (θ ) log P (θ ) + (1 − P (θ )) log(1 − P (θ ))], (6.12)
i i i i
i=1
and S(θ ) is its conditional variance:
(cid:32) (cid:33)2
n ˆ
log P (θ )
ˆ ˆ ˆ
S(θ ) = P (θ )(1 − P (θ )) . (6.13)
i i
1 − P (θ )
i=1
Research has shown that Z follows a standardised normal distribution [112]. Conse-
quently, Z values can be interpreted in the context of the general population; for example,
a Z score of 1 indicates that the internal consistency of a given response pattern is higher
than in 84% cases of honestly responding individuals. A Z score of below -3, (expected in
less than .14% cases), indicates that the given response pattern is unlikely to be based on
the modelled trait - this is, that it was generated by random or dishonest responding.
6.6 Results
This section presents the results based on the two empirical samples and simulated response
patterns. First, I verify if the individual and crowd performance in the crowdsourcing setting
follow the same model as the performance of the individuals working in the traditional envi-
ronment. Second, I explore the boost offered by WMV and show that WMV based solely on
agent abilities shows very good performance. Third, I investigate the relationship between
the size of the crowd, the agents’ ability distribution, and the crowds’ ability. Fourth, I mea-
sure the marginal contribution of an agent to the crowd performance. Finally, I explore how
crowd ability depends on financial incentives, agents reputation, and deterring free-riders.
6.6.1 Appropriateness of measuring ability in the crowdsourcing con-
text
This section investigates whether it is appropriate to use IRT to model the individual and
crowd performance in the crowdsourcing environment. Unidimensional IRT models as-
sume that the performance across tasks is driven by a single statistical factor. This implies
that the performance in one or multiple tasks can be used to predict the performance in
other tasks. Multiple features of the crowdsourcing environment may be affecting this rela-
tionship, rendering the IRT approach invalid. For example, the composition of the crowds
108 Crowd IQ: Modelling crowd ability using Item Response Theory
varies between tasks in the crowdsourcing environment; for example, different sub-samples
of agents submit solutions to each of the individual items. Also, motivational factors and
working conditions specific to the crowdsourcing environment may undermine the relation-
ship between the individual intelligence and performance.
The extent to which the response pattern fits the IRT model can be measured using one
of the appropriateness indices such as Z discussed in Section 6.5.4. If the performance is
not driven by the underlying trait, unexpected (in model terms) response patterns can occur.
The Z index is used to show that the IRT model developed on the representative population,
working in a standardised setting, can be applied to the crowds and individuals working in
the crowdsourcing environment.
viduals and the crowds from the Lab and two AMT sub-samples: the high-reputation AMT
sub-sample composed of all batches with reputation above 98%, and the low-reputation
sub-sample of batches with reputation below 90%.
Z : mean (sd)
Sample Type n MV W MV
AMT high-reputation Individuals 83 .4 (.9)
Crowds (n = 1) 1000 -.9 (1.4)
Crowds (n = 2) 1000 -.9 (1.4) .3 (1.0)
Crowds (n = 3) 1000 .2 (1.1) .5 (.7)
Crowds (n = 5) 1000 .9 (.6) .7 (.3)
AMT low-reputation Individuals 20 .7 (1.0)
Crowds (n = 1) 1000 -.6 (1.0)
Crowds (n = 2) 1000 -.7 (1.0) -1.2 (1.2)
Crowds (n = 3) 1000 -.6 (1.0) -.9 (1.3)
Crowds (n = 5) 1000 -.6 (1.1) .2 (1.1)
Lab Individuals 206 1.1 (.8)
Crowds (n = 2) 1000 1.0 (.8) 1.2 (.8)
Crowds (n = 3) 1000 1.4 (.7) 1.3 (.7)
Crowds (n = 5) 1000 1.6 (.5) 1.5 (.6)
consistent response patterns than in the case of MV, confirming its superiority. The aver-
age Z index of the high-reputation AMT agents (µ = .4) is greater than expected in the
3 Z
general population (E[µ ] = 0), suggesting that their responses follow the assumptions of
the standard IRT model. AMT crowd solutions based on a single response per task show
poor model-fit (−.9 and −.6 for high-reputation and low-reputation crowds respectively).
This is to be expected, as mixing (without aggregation) solutions from different individuals
6.6 Results 109
leads to aberrant response patterns (e.g. containing incorrect responses to easy tasks and
correct responses to difficult ones). However, the response patterns based on aggregating
several individual solutions in the high-reputation batches show above average model fit,
confirming the hypothesis that different crowds sourced from the same agents pool, under
stable conditions, produce consistent response patterns. For example the average crowd-
filled tests based on 5 individual solutions contain less aberrant responses than expected in
82% of tests filled by individuals (as represented by z value of .9). This can be explained
by the fact that aggregating responses reduces the occurrences of random errors, and thus
increases the model-fit of the aggregated response pattern.
The Z score for low reputation AMT agents is surprisingly high. Closer investigation
shows that the ability distribution in this sub-sample is highly polarised - that is to say,
predominantly composed of extremely low-ability agents consistently providing incorrect
solutions and few very highly able individuals consistently responding correctly. However,
aggregating consistent but extremely diverse individual response patterns of low-reputation
agents produces inconsistent crowd response patterns, as expressed by low Z values.
Both individuals and crowd solutions in the lab sample show extremely high model fit
suggesting that it is composed of highly motivated individuals.
Two important conclusions can be drawn from those results. First, performance of the
individuals working in the crowdsourcing environments is driven by their general intelli-
gence in accordance with an IRT model developed on a representative population. Second,
aggregate solutions based on the crowds drawn from the relatively consistent pools of agents
show a good fit to the IRT model and hence can be scored using the IRT routines.
6.6.2 Weighting the votes
The WMV aggregation discussed in Section 6.3.3 relies both on agents’ ability estimates
and the parameters of the tasks. While agents’ ability can be relatively easily established;
for example, by using screening tests, the IRF parameters of the crowd-sourced tasks are
not known. While it is possible to approximate task parameters given a sample of agents’
solutions , it would be impractical in the applied setting, where the goal is usually to obtain
a correct solution at the minimum time and monetary expense. This section focuses on two
aspects of weighting. First, the extent to which the performance of the WMV approach is
Note that while MV produces inconsistent protocols when aggregating crowds of n = 2, WMV aggrega-
tion results in consistent response patterns, even when based on as few as two individuals. This is driven by
the WMV ability to choose a more likely of two responses, while MV chooses a random one.
For example, the IRF parameters for psychometric tools are estimated based on relatively large samples
of responses to a given questionnaire or test.
110 Crowd IQ: Modelling crowd ability using Item Response Theory
affected by not knowing real values of the IRF parameters is investigated. Second, two po-
tential approaches to using ability estimates are explored: weighting the individual solutions
and composing the crowds of the most able (or appropriately able) individuals.
The extent to which the performance of the WMV approach is affected by the lack of
the real IRF parameters’ values is studied by comparing the WMV performance across three
conditions. First, agents’ weights are calculated using the actual tasks’ parameters offering
a maximum achievable boost while using weighting procedure described in Section 6.3.3.
Second, the IRF parameters are set to their expected average values: guessing, discrimina-
tion and difficulty are set to c = .125, a = 1, and b = 0 respectively. Third, the difficulty is
set to a relatively high value (b = 4) to maximise the weight of the highly able agents.
Importantly, simulated task parameters and agent’s responses are used in this exercise.
SPM tasks are characterised by uniform discrimination values. However, in the crowdsourc-
ing environment tasks’ discrimination (i.e. the degree to which performance depends on the
underlying ability) may vary, introducing another variable that may potentially affect the
order of weights. To best represent the real-life conditions, the simulated set of(n = 100
tasks is characterised by uniformly distributed discrimination (min = .5, max = 2.5), nor-
a a
mally and relatively widely distributed difficulty (µ = 0, σ = 2), and uniform guessing
b b
parameters c = .125.
outperforms the MV approach. Importantly, weights based on the manually set task pa-
rameters provide performance that is very close to the full-information condition, especially
when using a high difficulty parameter (b = 4).
Relatively high performance of the WMV based on the default IRF parameters values
indicates that the benefits of weighting stem predominantly from the estimates of agents’
abilities. The relationship between the accuracy of the individual ability estimates and the
performance of WMV is explored using the lab sample. Agents’ ability is assessed using a
CAT test of increasing length (from 1 to 36 items). Resulting estimates are used to weigh
agents’ solutions in the crowds of size n = 5, randomly selected from the agents’ pool, and
to select the crowds exclusively from the 10% top-ability agents.
scenarios. Expected general intelligence of the crowds using unweighted MV is IQ = 117
(∆θ = 1.13). Using as few as 10 items to pre-screen individuals, and applying WMV, boosts
the performance by additional 5 IQ points to IQ = 122 (∆θ = 1.46). Even better results are
coalitions on more difficult tasks.
6.6 Results 111
Fig. 6.8 Comparing the performance of the WMV aggregation based on actual and manually
set IRF parameters and using the lab sample. The horizontal axis represents the size of
the crowd, while the vertical axis represents the average intelligence score (across 2000
randomly selected crowds). Standard deviation of the scores across all conditions ranged
from σ = 20 for crowds of n = 2 to σ = 6 for crowds of n = 30. Agents’ ability was
IQ IQ
estimated based on all of the tasks. Unweighted MV aggregation is used as a baseline.
offered by employing the top 10% most able agents, boosting the crowd IQ to IQ = 127
(∆θ = 1.8) when using a 6 item long pre-screening test, and to IQ = 132 (∆θ = 2.13) when
using a 20 item long pre-screening test. To appreciate the boost in the crowd ability, note
that the baseline expected ability (when using MV) of the crowd of size of n = 5 randomly
sampled from a general population, is IQ = 118, or more than 88% of individuals. The
same crowd is expected to achieve an IQ of 122 (or more than 93% of individuals) when
members’ abilities are established using a short 10 item long pre-screening test and WMV
is applied. The crowd composed of the top 10% of the population pre-screened using such
test is expected to have an IQ of 130 or more than 98% of the population.
6.6.3 Ability of the crowd and its members
As discussed in Section 6.3.2 the crowd’s performance depends on the properties of the task
(the IRF parameters and the number of response options) and the properties of the crowd
112 Crowd IQ: Modelling crowd ability using Item Response Theory
Fig. 6.9 The intelligence score of crowds of n = 5 expressed on the standard intelligence
scale. The horizontal axis represents the number of pre-screening ability items used to
estimate agents’ individual abilities. The vertical axis represents the average ability (intel-
ligence) of the crowd members. The left plot shows the average ability of random crowds
drawn from the lab sample when using WMV approach. The right plot shows the average
ability of the crowds sampled from the top 10% of most able individuals. Plots are based on
1,000 random crowds of n = 5, standard deviation of the crowd scores ranged from 4 to 6
IQ points.
(size and the abilities of the agents). This section investigates in more detail the relationship
between the distribution of individual abilities and crowd performance. The relationship
between the maximum and average agents’ ability and crowd’s ability is presented.
and maximum ability of the agents and the ability of the crowd. Note that the WMV ap-
proach performs noticeably better on average, and much better in some cases, especially
when plotted against agents’ average ability. Virtually all crowds of size n = 5 can outper-
form their average member (regardless of the aggregation method used), and a large fraction
can outperform their most able member: 40% when using MV and 71% when using WMV.
As shown in Section 6.6.4, the larger the crowd, the greater its chances of outperforming
its smartest member. Overall, the results presented in this section suggest that both average
and maximum agents’ ability offer good proxies for the performance of the crowd.
6.6 Results 113
Fig. 6.10 Scatter plots illustrating the relationship between the crowd ability and the average
and maximum ability of crowd members for 1980 simulated crowds of size n = 5 with
abilities drawn from the normal distribution. To increase the number of crowds with the
extreme values of µ , 10 random crowds were drawn first and split into percentiles based on
their µ . Twenty random crowds from each percentile were selected to produce the plot. The
horizontal axis represents the average (left plot) and maximum (right plot) intelligence of the
agents. The vertical axes represent the ability of the crowd, with points above the diagonal
line representing the crowds that outperform average or the most able agent respectively.
6.6.4 Size and ability of the crowd
This section explores the relationship between the number of acquired solutions and the abil-
ity of the crowd under the MV and WMV approaches. The boost in performance offered by
aggregating multiple solutions is not surprising and was illustrated a number of times before
(e.g. [218]). However, the application of an ability metric, instead of a performance metric,
offers interesting insights. First, predictions of the ability model can be extended to a crowd
of any size and composition. An ability metric can be also used to compare crowds with in-
dividuals (e.g. the smartest crowd member), or with other crowds (e.g. comparing between
crowds working in a traditional Lab setting, and the AMT crowdsourcing environment).
the average AMT crowd is significantly higher than the IQ of an average individual, even
when using one solution per HIT only (by two-thirds of standard deviation in this case). It
means that the AMT crowd outsmarts 75% of people even before the benefit of aggregating
multiple solutions kicks in. The advantage of the AMT crowds over the lab ones grows with
114 Crowd IQ: Modelling crowd ability using Item Response Theory
Fig. 6.11 Crowd intelligence scores and number of individual solutions to each of the tasks
(i.e. crowd size) based on the lab and AMT samples. AMT batches with all payment levels
(form $0.01 to $0.20) and default reputation threshold (> 98%) are used. The lines and
ribbons show the average crowd IQ score and its standard deviation for 1,000 random sub-
samples of responses.
the number of solution per item and reaches a difference of nearly 20 IQ points at a crowd
size of 19.
This result stands in stark contrast to the relatively low average IQ levels of the AMT
one solution per HIT, can therefore be attributed to differing working conditions. While in
the lab sample agents had to provide solutions to all items, even if they did not know the
solution, AMT agents saw a HIT before deciding to work on it, and could decline to respond
without any consequences for their reputation. Effectively, agents’ self-assessment of their
capability to provide a correct solution resulted in contributions tied to individual ability -
that is, more difficult tasks were “left” for more able agents to solve.
In both samples, regardless of the aggregation approach, the crowd IQ quickly increases
with the number of responses per HIT, but shows diminishing returns. The superiority of
the WMV approach is significant and relatively stable. Notably, while the MV approach
is limited to choosing at random when aggregating two solutions per task, WMV offers a
significant ability boost even for crowds of n = 2.
In AMT’s terminology this is called returning the HIT
6.6 Results 115
Also, the chance of drawing a highly able individual grows with the size of the crowd.
Effectively, rather than focusing on the ability of the crowd, the difference between the
crowd and the smartest agent’s ability should be considered.
Fig. 6.12 Plots illustrating the relationship between the size of the crowd (horizontal axis),
the expected intelligence score (left plot) and marginal intelligence score, or difference be-
tween the maximum intelligence of the crowd members and crowd intelligence (vertical
axis). The ribbons indicate the standard deviation of the given sub-sample. The plot is
based on 1,000 simulated crowds of size between n = 2 and n = 30, 000 and relatively dif-
ficult (normally distributed difficulty, µ = 2, σ = 2) test of length q = 100.
b b
relationship between crowd intelligence and the logarithm of crowd size. The curves rep-
resenting the WMV and WM approaches are relatively parallel. This indicates that WMV
can achieve performance comparable with MB with crowds of smaller size. Note that as the
horizontal scale is logarithmic, WMV offers increasing advantage with the growing size of
the crowd.
The right plot, presenting the relationship between the marginal intelligence of the crowd
and its size, shows a consistently growing advantage. Importantly, using MV for crowds of
n = 5 or below offers a negative marginal intelligence (i.e. the crowd is expected to be less
intelligent than its smartest member). Ribbons representing the standard deviation of the
crowds’ scores can be used to estimate the fraction of cases in which the crowd is likely to
outperform its smartest member; for example crowds of the size of n = 6 are expected to
outperform their smartest member in 84% of cases when using WMV.
116 Crowd IQ: Modelling crowd ability using Item Response Theory
6.6.5 Crowd members’ marginal contributions to crowd ability
Intuitively, individuals of high ability (relative to the crowd) are likely to contribute more
towards the aggregated performance. On the other hand, however, even the low ability
individuals have a some chances of solving difficult items correctly and may effectively
boost crowd ability. This section focuses on the individual’s marginal contribution towards
the crowd ability.
Fig. 6.13 Contribution of the agent towards crowd general intelligence. The horizontal axis
represents the difference between the intelligence of a given agent and the maximum intel-
ligence of other agents. The vertical axis represents the difference between crowd ability
estimated with and without a given agent, using MV and WMV aggregation. The plot is
based on comparing the crowd intelligence between the simulated crowds of size n = 11
with normally distributed abilities (n = 11, µ = 0, σ = 1), and the same crowd with one
additional simulated agent with ability drawn from the uniform distribution (min = −4,
max = 4). The process is repeated 10 times.
There are a number of studies focussing on quantifying an individual’s contribution to a
crowd’s performance. For example, Panagiotis et al. [174] proposed algorithms for detecting
agents’ biases and errors, and showed how to apply them in a real-life setting for inferring
agents’ quality.
In this work I use the IRT framework and simulated samples to estimate the expected
boost in performance offered by adding an individual to the crowd. Importantly, individual
6.6 Results 117
contributions have to be considered in the context of the crowd into which the individual’s
opinions have been aggregated. For example, an individual contributing significantly to the
performance of a low ability crowd, may negatively affect the performance of a high-ability
crowd.
ability, regardless of the new agent’s relative ability and the aggregation method used. While
the marginal contribution grows quickly with the agent’s intelligence (especially when using
WMV), it does not grow as fast as agents’ intelligence. In other words, while adding an
additional agent is always beneficial, it does not boost a crowd’s performance as much as
the difference between a new agent’s intelligence and maximum crowd intelligence.
Notably, while the WMV aggregator virtually ignores the solutions of individuals with
very low relative ability (as expressed by no marginal change when adding such agent to
a crowd), such individuals are expected to offer a small increase in crowd ability under
the MV approach. Note, however, that MV aggregation offers much lower performance in
general, making it easier to boost its performance.
6.6.6 Crowdsourcing conditions and the ability of the crowd
The ability distribution of the crowd members depends on the ability pool of the crowdsourc-
ing platform, the subset of agents given access to the task, and the self-selection process
the average AMT agent is not smarter than the average individual in the general population.
on the lab sample by a large margin, indicating that agents’ self-assignment to tasks can
significantly boost the ability of the crowd.
This section examines the relationship between the ability of the crowd and the crowd-
sourcing conditions, such as the payment offered, agents’ reputation, and deterring free-
most of the AMT batches. However, note that the number of unique agents is lower for
batches characterised by high reward and aimed at agents of low reputation (batches 4, 9,
and 10). This suggests a less considerate approach to self-assignment and submitting tasks
with little or no consideration (spamming).
The results are obtained by estimating the intelligence of crowds randomly selected from
the specific AMT batches. To emphasise the differences introduced by crowdsourcing con-
ditions (and control for the effect of aggregation), the crowd solutions are based on a single
118 Crowd IQ: Modelling crowd ability using Item Response Theory
response per HIT. This procedure is repeated 1,000 times to decrease the selection bias. The
difference in intelligence levels between the crowdsourcing conditions is estimated using
Analysis of Variance and Tukey’s HSD (Honestly Significant Difference) test.
Financial incentives
Research in the offline environment indicates that agents tend to improve their performance
in response to the incentive schemes that rewards their productivity. For example, [147]
found that agents’ productivity increased by 20% after switching to the piece-rate scheme,
where agents are paid a fixed piece-rate for each unit produced or action performed, re-
gardless of time. However, while this and other similar results are increasingly reflected
in management practice, it has been pointed out that under certain conditions increasing fi-
nancial incentives can hamper performance by undermining non-financial motivations (e.g.
work satisfaction) [104], or introducing an excessive psychological pressure [6].
Research in crowdsourcing offers similarly two-fold results. While [107] showed that
most of the agents respond rationally to incentives, [160] showed that quantity but not qual-
ity of work is improved by higher payments. Moreover, the latter paper suggests that the
design of the compensation scheme matters more than the size of the reward; a quota based
scheme resulted in better performance and lower overall cost than a piece-rate scheme.
Interestingly, [200] showed that the quality of work was improved by increasing intrinsic
motivation (e.g. helping others), while pay (extrinsic motivation) did not.
nessed per minute) across the four payment levels—$0.01, $0.05, $0.10, and $0.20 (batches
1 to 4). Crowd intelligence does not increase steadily with the payment offered; increasing
the payment from $0.01 to $0.05 boosts the intelligence, but larger rewards decrease the
performance. On the other hand, the time needed to obtain the full solution to the entire test
(36 tasks) decreases rather sharply with amount paid per HIT - from more than 18 hours in
$0.01 condition to mere 15 minutes in the $0.20 condition.
The results presented above confirm to a large extent those of [160]. Increasing pay-
ments beyond an optimum level ($0.05 in this case) boosts the quantity, but not the quality
of the solutions. A very large difference between the $0.05 condition - up to 20 IQ points
as compared with the $0.01 setting, emphasises the necessity to choose the payment level
with great caution - for instance by introducing variations in rewards and monitoring the
resulting ability. Also, requesters should strive to avoid paying too much, as it encourages
free-riding and may seriously affect the quality of the responses.
Finally, it is clear that when time is an issue it is worth paying well, as paying $0.20 per
6.6 Results 119
Fig. 6.14 The crowd IQ, and crowd IQ per minute as functions of payment levels. Dif-
ferences significant at the p < .001 level include difference between $0.01 and $0.05, and
between $0.05 and $0.20. Difference in the crowd IQ between $0.10 and $0.05 approaches
significance (p = .086).
HIT yields around 14 times more IQ points per minute than paying $0.01. Note, however,
that the total crowd IQ is comparable between those two payment levels while cost increases
by a factor of twenty. If speed is not a priority it seems advantageous to pay less and wait
longer.
Reputation System
Individual agents may attempt to free-ride the system and increase their gains by lowering
the quality of their responses or attempting to solve tasks that are beyond their ability or
knowledge [109, 115, 205, 235]. One of the remedies to this problem, used by AMT and
other crowdsourcing marketplaces, is a reputation mechanism based on the proportion of
the agent’s solutions rejected by requesters in the past [9, 125, 222, 241].
Requesters can use the reputation score and the total number of solutions submitted
by the agent to filter out unreliable agents. However, as requesters compete for reputable
and experienced agents, increasing the reputation and experience thresholds may drive up
the costs of acquiring solutions and slow down the process. Moreover, while the threat of
rejecting low-quality or incorrect responses may increase the crowd’s performance, it also
120 Crowd IQ: Modelling crowd ability using Item Response Theory
has several adverse effects. The threat of rejection increases the risk involved in undertaking
the task and hence drives up the cost of obtaining solutions and decreases agents’ creativity
and innovativeness [134]. Moreover, it may not deter those agents who have nothing or little
to lose, such as spammers.
Fig. 6.15 Crowd intelligence as a function of agents’ reputation scores. Differences between
the three lowest and two highest reputation levels were statistically significant at the p <
.001 level. All other differences were insignificant.
80%, from 81% to 90%, from 91% to 95%, from 96% to 98%, and above 98% (batches
2, and 7 to 10; $0.05 was offered for the correct solution). Unsurprisingly, agents of high
reputation produce solutions of much higher quality. What is surprising, however, is the
sharp fall in performance at the 95% reputation threshold. Crowds below this threshold show
uniformly low ability (of approximately IQ = 80), lower by about 2 standard deviations than
the ability of the crowds composed of agents with reputation equal or above 95%. Those
results, combined with lower difference in the individual agents’ IQ scores and a higher
the decrease in performance is not driven by the lower ability of low-reputation agents, but
rather their propensity to free-ride the system. Such a process can potentially be amplified
by the competition for HITs among low-reputation agents, even if not all of them are free-
riding the system; spending too much time on a task decreases the chances of accepting
another one in a given batch.
6.6 Results 121
Another surprising phenomenon relates to the high levels of maximum intelligence in
agents are characterised by high intelligence. Their low reputation can be an effect of delib-
erate free-riding (in which case their high efficiency while solving IQ tasks could have been
driven by curiosity), but could also result from their propensity to accept challenging tasks
(accompanied by the high risk of rejection).
Fig. 6.16 Crowd intelligence and IQ points per minute in the “rejection” and “no rejection”
conditions. Intelligence difference is significant at the p< .001 level.
In all experimental conditions investigated so far, the reputation of the agents was not
threatened. On the contrary, the task instruction stated that incorrect responses will not be
rejected (though the payments were offered only for the correct solutions and spamming
was discouraged by stressing that spammers will be rejected).
However, as high reputation increases the employability of the agents and allows them to
access better paid tasks, potential rejection of the HIT (and resulting decrease in reputation)
provides an extra motivation to avoid submitting potentially incorrect HITs. Crowd ability
provides a convenient tool to quantify the effect of the rejection risk.
Crowd performance in two different HIT designs is compared - standard HIT described
in Section 6.5.2, and HIT with modified guidelines stating that (“Incorrect answers will be
122 Crowd IQ: Modelling crowd ability using Item Response Theory
REJECTED.”) In both cases a reward of $0.10 was offered for correct responses only, and
the access was limited to agents with reputation scores of at least 98%.
increases (by one standard deviation, or 15 IQ points) the intelligence of the crowd, even if
the threat is expressed by changing one line of the guidelines and is never actually enforced.
The magnitude of this difference is best appreciated by comparing this with the frequencies
of such scores in the general population. Whereas the crowd IQ score achieved in the
no-rejection condition (114 IQ) is higher than roughly 82% of the scores of the general
population, the IQ score achieved under the risk of rejection (129 IQ) can be achieved only
by the top 3 percentiles of individuals.
Importantly, the crowd works more slowly under the rejection condition. Rejection
threat increased the time that AMT required to complete the batch composed of 360 tasks
by almost 50% (9 hours 5 minutes versus 6 hours 12 minutes in the no-rejection condition).
Note that the decreased speed of the crowd cannot be caused by the lower expected pay
- in both rejection and no-rejection condition, the payment was offered only for correct
responses. Effectively, while the rejection threat leads to higher crowd IQ, it also slows
down the crowdsourcing platform.
6.7 Conclusions
This work explored the applications of ability-based models (such as general intelligence)
in the context of crowds of human agents independently submitting solutions to tasks. First,
the relationship between individual agents’ abilities, the parameters of the task (such as
difficulty or number of response options) and the quality of the crowd solution was for-
malised. Second, a weighted voting mechanism (WMV) based on agents’ abilities and the
IRF parameters of the task were proposed. Third, it was suggested that while there is a large
number of abilities that could be potentially useful in the crowdsourcing context, the highest
potential is offered by general intelligence, widely used in psychological and occupational
assessment to predict individuals’ performance in a broad spectrum of tasks, including sim-
ple tasks, highly specialised jobs, academic performance, and creativity [92, 117, 155, 208].
Fourth, it was proposed that the crowd performance across a wide variety of tasks can be
conveniently approximated using its score on a standardised ability test. Finally, the Re-
sults section of this work applied the proposed models and methods to investigate several
In fact, no agents were hurt during this experiment as all of the solutions (including incorrect ones) were
accepted after data collection was finished.
6.7 Conclusions 123
properties of the crowd performance using empirical and simulated samples.
The WMV approach developed in this work was shown to have a number of advantages
over an unweighted MV. When compared with MV, aggregating solutions using WMV lead
to significantly higher and more consistent performance across tasks. Also, the WMV ap-
proach significantly increases the chances of the crowd to outperform their smartest member
and boosts the relationship between an additional member’s intelligence and the increase in
crowd IQ score. Importantly, it was shown that the performance of WMV depends mostly
on crowd members’ ability and is not significantly affected by the lack of actual task IRF
parameters, indicating that WMV can be implemented in the crowdsourcing environment.
In this work, the crowds’ performance is expressed using the crowd’s score on the stan-
dardised general intelligence test, or crowd IQ. It was argued that crowd IQ can provide a
useful proxy for its ability to solve a wide range of tasks. It also offers a convenient way of
comparing between crowds, or between crowds and individuals. Results confirm that both
individual and crowd solutions obtained in the crowdsourcing environment fit well into the
IRT models. Effectively, a wide range of well established IRT models, scoring approaches,
and computerised adaptive testing frameworks, can be applied directly in the crowdsourcing
context solving practical problems and offering important insights. Importantly, it should
be emphasized that while crowd IQ offers a convenient proxy for the crowds’ performance
across tasks, individual agents’ ability estimates are needed to accurately predict and model
the crowds’ performance. Crowd IQ is used to illustrate how the expected ability of the
crowd depends on its size, rewards offered to agents, their reputation, and measures aimed
at threatening agents’ reputation. The marginal contributions of individual crowd members
to crowd performance was also investigated.
Applying crowd IQ to empirical and simulated samples offered a number of interesting
insights. First, the results show a very significant increase (between δ θ = .5σ to 1σ ) in
the quality of the crowds’ solutions provided by the Weighted Majority Voting Approach,
even when the parameters of the tasks are not provided to the weighting algorithm. Sec-
ond, regardless of the aggregation method used, the great majority of crowds outperform
their average member. Third, the chances of the crowd to outperform its smartest member
quickly grow with the crowds’ size, especially when using WMV. Fourth, the marginal con-
tribution to crowd ability grows quickly with a given agent’s ability but not as quickly as
the difference between a given agent’s ability and the maximum ability among other agents.
This indicates diminishing returns offered by adding high-performing outliers to the crowd.
Fifth, results show that it is important to carefully adjust rewards offered to agents; both too
high and too low rewards negatively affected individual performance. Sixth, results show a
124 Crowd IQ: Modelling crowd ability using Item Response Theory
very large increase of crowd ability when threatening agents’ reputation score (δ θ ≈ 1σ ),
and relying on agents of high reputation (δ θ ≈ 2σ )
Currently, agents and crowds are benchmarked using performance based metrics, such
as reputation (or a total fraction of correctly solved tasks), or performance on specific tasks
with known correct solutions, such as labelling or ordering images (e.g. [111, 160]). How-
ever, as illustrated by Item Response Theory discussed in Section 6.2, performance across
different tasks is not simply correlated, but rather mediated by an underlying ability, such as
general intelligence. For example, high performance on an easy task does not automatically
translate into high performance on a difficult one.
I argue that measuring agents’ general intelligence (and other abilities) rather than per-
formance (e.g. reputation) offers a number of advantages in the context of crowdsourcing.
First, it allows a crowds’ performance to be precisely estimated. Consequently, crowds of
desired ability can be sourced from the pool of available agents with known abilities. Sec-
ond, individual agents’ ability estimates can be used to weight their solutions leading to a
significant increase in the quality of the crowds’ solutions. Finally, it can potentially greatly
decrease the crowdsourcing market frictions, such as spamming, overpaying for solutions
that could be achieved at lower cost, or accepting jobs below an agent’s ability. Knowing
agents’ abilities allows requesters to choose the desired level of performance, and tie the cost
of obtaining solutions (and effectively, the rewards offered to agents) to the difficulty of the
task. This would encourage agents to accept challenging tasks, and encourage requesters
to formulate their tasks in the simplest possible way (e.g. to avoid excessive charges for
tasks proven to be difficult).
I believe that the reputation score could be replaced by ability measures, capturing not
only agents’ reliability but also their potential to solve tasks. Ability based benchmarks,
while still discouraging free-riding of the system (incorrect responses decrease the ability
estimate), could also boost the income of the most able agents, and encourage accepting
challenging tasks and associated risk of rejection. Crowdsourcing platform administrators
(or requesters) could estimate agents’ ability relatively easily using screening tests. For
example, Section 6.2.2 shows that agents’ general intelligence could be established using
less than 15 IQ tasks. Ultimately, however, crowdsourcing platforms should assess agents’
ability by comparing their performance with other agents of known ability.
The difficulty of the task can be estimated based on the patterns of solutions of agents of known abilities.
6.7 Conclusions 125
6.7.1 Future research
While general intelligence is an archetypical ability and is the best known predictor for a
wide range of outcomes and contexts, the results presented in this study should be replicated
using other tasks and abilities. General intelligence tasks used here might be more engaging
than many other crowdsourcing tasks, boosting agents motivation and resulting scores. Less
engaging tasks, dependent on the general intelligence or other abilities, might attract less
motivation and result in lower quality solutions. I hope that further studies will expand and
further validate general intelligence as an elegant and convenient predictor of agents’ and
crowds’ ability to solve a wide variety of tasks. Furthermore, the influence of factors such
as personality, gender, or culture on agents’ ability and its link with performance could be
further explored (see [129]).
This work focuses on a relatively simple setting, with agents expressing their opinions
independently. While this is typical for many crowdsourcing environments, interaction be-
tween the crowd members can greatly affect crowd performance (e.g. see [223, 244]). For
example, certain collaborative systems, such as Wikipedia, foster active collaboration be-
tween participants, where a single solution is updated many times by various individuals,
who comment on the work of their peers and argue for their point of view. One direction
for future research should be to gain an understanding of the relationship between a range
of individual abilities, including social and leadership skills, and the performance of crowds
in the more complicated collaborative environments.
Also, I focus on tasks characterised with one correct and a limited number of incor-
rect response options, with uniformly distributed chances of selecting any of the incorrect
solutions. In many real-life scenarios, however, incorrect solutions would differ in their pop-
ularity. Appropriate models for predicting the crowds’ performance and suitable weighting
mechanisms should be developed.
Moreover, factors affecting weighting performance should be studied in more detail. For
example, formulas aimed at estimating the relative influence of the tasks’ parameters on the
aggregation accuracy could be derived.
Finally, the factors driving agents’ decisions to accept, and submit tasks (or return with-
out solving) should be studied. Such factors may include the parameters of the task (e.g.
reward offered) and the characteristics of the platform (e.g. reputation system), but also
agents’ traits, such as personality, gender, or country of origin (see [129]). While the re-
sults presented here may offer some insights into this subject (e.g. increasing the rewards
increases agents’ readiness to submit potentially incorrect solutions), more robust insights
could be obtained by a study design in which agents’ behaviour (e.g. returning the tasks)
126 Crowd IQ: Modelling crowd ability using Item Response Theory
could be observed in more detail.
I believe that this work lays a robust foundation for exploring all the aforementioned
research directions.
Chapter 7
General Discussion
This thesis was aimed at demonstrating the advantages of data-driven social science re-
search. Chapter 2 discussed the advantages and disadvantages of conducting studies in the
social network environment. It was proposed that Facebook, the most widespread online
social network at the moment, combines the general advantages associated with online re-
search with a number of idiosyncratic features producing a highly effective and sophisticated
social science research instrument. This increasingly natural environment for a significant
fraction of the world’s population offers a cheap and quick way to reach large and diverse
samples and record unprecedented amounts of behavioural residues. Additionally, it con-
tains privacy-protecting mechanisms allowing researchers to track participants across stud-
ies and give them control over their data. Chapter 3 introduced the myPersonality sample
and used it to illustrate the validity of data collected in the online environment.
Chapters 4 and 5 demonstrated that human behaviour in the digital environment is
not psychologically divorced from offline behaviour by showing that it is driven by well-
established psychological traits. Based on a sample of over a third of a million users, it
was shown that an individual’s personality is manifested in two popular online activities:
web browsing and using Facebook. Next, the strength of this relationship was confirmed by
showing that records of those online activities can be used to accurately infer a wide range
of psycho-demographic traits, including personality and general intelligence.
Finally, Chapter 6 explored the relationship between individual ability and the perfor-
mance of the crowd. I formalised this relationship building on two established theories:
Condorcet Jury Theorem and Item Response Theory; and introduced the weighting ap-
proach allowing for the crowd’s performance to be significantly increased. Empirical and
simulated samples were used to explore the dependence of the crowd performance on sev-
eral factors, including its size, the distribution of members’ abilities, and the reward offered.
128 General Discussion
Additionally, the influence of the amount of information about task properties on the per-
formance of the weighting mechanism was explored in detail.
There are several important conclusions that stem from this work. Some of them, in-
cluding the benefits of using online environment in research, potential boost in performance
offered by benchmarking crowd member abilities, and the applications of personality pre-
dictions in the personalization of products and services, were discussed in their respective
chapters. The following sections explore four additional issues related to my findings. I start
with discussing the ethics of online research and suggesting a framework aimed at helping
researchers and review boards to design ethical and socially acceptable studies. Next, I
discuss the risk to individuals and societies related to the predictability of intimate traits
presented presented in Chapter 5. Then I present the vision of the future labour markets
enabled by the phenomena touched on in this work, such as automated psychological as-
sessment and reputation mechanisms. Finally, I briefly discuss the potential that digital
behavioural residues hold for the development of new personality and ability models.
7.1 Ethics
Social networking sites have made it easier than ever before to share one’s private infor-
mation, in both greater detail and larger volume. Content that as recently as a few years
ago would have been considered intensely private and worthy of staunch legal protection is
now broadcast through one’s network of friends or followers at the click of a button. A host
of widespread and useful technologies, from Amazon book recommendation to the Google
search engine, depend on the observation of this digital behaviour, but users’ willingness to
share their data in order to use these technologies has been accompanied by growing public
concern for the protection of individual privacy. The question of ethics in the context of
research in the digital environment has become an especially pressing one in light of studies
showing how seemingly innocuous and voluntarily submitted personal information can be
further processed to infer highly intimate details, such as intelligence, sexual orientation and
political views [138, 140].
Digital records of behaviour and online profiles are casually used by institutions and
companies (such as Facebook, Google and mobile network operators) to study their users
and improve their services. While some uses of digitally records of behaviour and prefer-
ences attract a significant negative reaction (e.g. the US National Security Agency [18]),
others become widely accepted and even appreciated. The popularity and willingness of
users to share their data in exchange for access to a convenient or enjoyable service might
7.1 Ethics 129
lead one to conclude that exploratory or “non-specific” use of personal information by re-
searchers and corporations has become an accepted ethical norm. On the other hand, how-
ever, it is possible that people lost control over their information against their will.
Facebook and other digital environments offer easy access to unprecedented amounts
of data about large numbers of participants. With the growing popularity of online Human
Subjects Research (HSR) [96, 164], instances of misconduct in either observing people or
applying experimental interventions could have far greater repercussions than ever before
imagined in the social sciences. Nevertheless, the protocols and guidelines related to design-
ing HSR, storing data and analysing results are scarce and often contradictory [219, 242].
At the time of writing, the American Psychological Association’s website lists only three
documents ([73, 103, 142]) containing guidelines related to research on the internet, the
most recent being from 2002 (2 years before Facebook was founded). On the other side
of the Atlantic, the most recent (2012) edition of the UK Economic and Social Research
Council Framework for Research Ethics [64] states that Internet research presents new eth-
ical dilemmas, without suggesting any reliable guidelines other than the recommendation
that any research conducted on the Internet should be subject to review by the appropriate
Institutional Review Board (IRB).
The requirement that any study involving the digital environment obtains full ethical
review is unsustainable. Rapid technological advances and the lack of any pragmatic guide-
lines for research design are but two issues that severely compromise the review process and
jeopardise the validity of an IRB’s decision, deterring researchers in the meantime from car-
rying out online empirical research or submitting studies for review [96, 215]. This problem
is further exacerbated by the fact that other disciplines increasingly encompass HSR. Ethi-
cal review is an unfamiliar concept in fields such as computer science or engineering, with
the result that an inflated proportion of HSR is conducted without any serious consideration
of ethical and social responsibility [34, 35]. Additionally, HSR is now frequently conducted
outside of a university or research laboratory environment; for example, there are numerous
self-appointed and unaffiliated researchers in the field of computer security [34].
Ethics and social norms may change under the influence of new knowledge or technical
progress [226]. Increasingly blurred boundaries between what is public and what is pri-
vate, and the growing extent to which people’s behaviour is recorded and analysed, may
contribute to the evolution of these norms in the privacy context [162]. Consequently, what
is considered intrusive at the moment may become a norm in the future. As suggested
by [210], journal and conference publications based on data collected in the digital environ-
130 General Discussion
ment should contain a discussion of ethical considerations related to the design of a given
study. Such an approach would not only help the authors to properly consider ethical as-
pects of their work, but would also support the evolution of standards and norms in this
dynamic technological milieu. This work therefore proposes several guidelines related to
HSR in the digital environment. These are based on a review of studies using digital records
of behaviour (see [34, 36, 202]), the authors’ personal research experience, and on feedback
received from research participants. The guidelines are presented in three parts: the first
focuses on public domain data, the second considers studies based on secondary data and
the third discusses HSR.
7.1.1 Public data
I propose that public data encompasses any information that was generated by and know-
ingly published by individuals or organisations in such a way as to be accessible by everyone
in the digital environment. Public data includes individuals’ posts or comments in publicly
accessible forums, people’s personal websites and blog entries, and public elements of Face-
book profiles.
The major problem with public data in the digital environment relates to whether its
collection falls within the regulatory definition of HSR [64]. In other words, it is unclear
whether researchers operating in that environment must obtain participants’ consent and
gain approval from an IRB [210, 219, 242]. Some scholars argue that mining public data
is equivalent to conducting archival research, a method frequently employed in disciplines
such as history, art criticism and literature, which rarely involve protection of the human
subjects [33, 102]. Others, however, point out that the border between public and private
data is not determined by its accessibility, but by social norms and practices [73, 238].
Schultze and Mason [210] illustrate this phenomenon with an example of a small town in
which everyone knows intimate details about everyone else, but people pretend not to know
those facts that would be considered personal.
The speed of technological progress and the tremendous changes it introduces to our
environment seem to be causing a dissonance between privacy norms that are manifest and
those which people employ in practice. Studies of online communities confirm that their
members do not see their interactions or their correspondence as public. For example, in
2004, Chen et al. [47] surveyed members of several online communities and asked whether
researchers should be required to disclose their identity and research purpose to the group
they were studying. They found that respondents unanimously agreed that disclosure was
7.1 Ethics 131
necessary. It is perhaps surprising, therefore, that a large number of large-scale research
projects breach this unanimously expressed expectation. Such projects attract major public
interest but little, if any, negative feedback in terms of privacy protection. Consider Google
Zeitgeist [88], a study of social, technological and political trends that annually examines
the distribution of Google users’ search queries, or the Facebook Gross National Happiness
index [141] that attempts to infer countries’ well-being from the relative frequency of pos-
itive and negative Facebook status updates. Both projects are based on enormous amounts
of behavioural residues generated by users who are most likely unaware that their data will
be used for the afore-mentioned purposes. Given that neither endeavour has been perceived
as breaking any norms related to privacy, it must be possible that those who are active in the
digital environment are ready to accept a greater level of intrusion than they are willing to
admit.
I propose that the boundary between “public” and “non-public” data is set by the neces-
sity to create an account or register in order to access the relevant information, even if the
registration is anonymous and fully open to the public. For example, Facebook profile data
made visible to people who are not logged-in to Facebook ought to be considered public,
whereas information visible only to other Facebook users ought not to be. The same relates
to other platforms, such as forums and virtual reality environments (e.g. computer games).
While non-public data could still potentially be used in research, it ought to be released
with the direct consent of the user (see Section 7.1.3) or that of the administrator of a given
service or platform (see Section 7.1.2).
It is proposed, firstly, that researchers do not need to obtain individual participants’
consent to use public data and, secondly, that such studies could be considered exempt from
IRB approval where the following conditions are met:
1. Data is anonymised after collection and no attempts are made to de-anonymise it
2. There is no interaction or communication with the individuals in the sample
3. No information that can be attributed to a single individual, including demographic
profiles and samples of text or other content, is going to be published or used to
illustrate the results of the study.
If any of the above conditions is not met, it is strongly encouraged that the researcher seek
the approval of an IRB prior to conducting the study.
132 General Discussion
7.1.2 Secondary data
Secondary data in the context of the digital environment may include web browsing logs
collected by Internet Service Providers, web-search queries collected by search engines
or the structure of social networks on Facebook. As with the use of publicly available
data, obtaining individual consent for studies using secondary data might be impossible or
impractical and, arguably, is often unnecessary.
Traditionally, studies based on secondary data of this kind were considered exempt from
full ethical review [64], but the wealth and potential intrusiveness of databases collected
in the digital environment would now suggest that this data should be used with particular
caution and diligence. I therefore propose that research of this kind should be exempt from
prior IRB approval only where it meets all of the conditions listed in Section 7.1.1 in addition
to the two criteria below.
4. The controller of the data, which may be a company, site administrator or other indi-
vidual, collected it with users’ consent in the course of the a normal functioning of
an existing product or service. This would include any situation where the secondary
data was not collected specifically for the purposes of the proposed research project.
5. Before accepting the data, researchers have taken all measures reasonably available
to them to convince themselves that the institution that collected the data has obtained
users’ consent to share it for the purposes of the research project. Such consent could
be explicit or included in the Service Terms & Conditions.
Moreover, both researchers and data owners should avoid publishing secondary-data
datasets in the public domain. Even if personal information, such as user name, address
or Social Security numbers are removed, background knowledge and cross-correlation with
other databases can be used to de-anonymise the individual records. Well-known exam-
ples of databases that have been de-anonymised after publication include the Netflix Prize
dataset [169], the Massachusetts hospital discharge database [224] and AOL search data [97].
These precedents show the paramount importance of only releasing data to parties that can
guarantee an appropriate level of privacy protection.
7.1.3 Human Subjects Research
Projects involving data collected directly from the users by the means of observation or
experiment ought to seek prior ethical clearance and individual consent from participants.
As discussed above, there is a scarcity of up-to-date guidelines related to conducting HSR
7.1 Ethics 133
in the digital environment and some of the guidelines, while appropriate in the context of
traditional research, might no longer be sensible. One of the major differences between
online and offline studies is that in most cases the former environment offers researchers
an unprecedented opportunity to design studies that are so enjoyable and interesting as to
encourage participation without the need for additional motivation (see Section 2.4.2). Not
only does this increase the validity of the data and allow for organic growth of the user
database, it also minimises the ethical concerns related to placing a burden on participants,
securing their well-being, and respecting them as people [45]. Several guidelines specific
to HSR in the digital environment are therefore proposed, which differ from and potentially
contradict those designed for traditional research projects.
I posit that two different approaches to obtaining participant consent could be used,
depending on the type of data collected. The first type would apply to studies that are not
aimed at obtaining personal data. An example of such a study might involve publishing an
online version of an economic game (e.g. prisoner’s dilemma) which records the choices
made and time taken by the players without recording any individual information. Another
example may include an online happiness questionnaire that stores users’ responses but no
personal information. Studies of this type should be accompanied by terms and conditions
clearly stating the intention that the data be used for research purposes. However, such
studies could be exempt from the need for participants to confirm their consent by filling
in a separate form. It can be assumed that by using a given service, its users accept the
accompanying terms, this being standard practice in the digital services.
The second type of participant consent would apply to studies collecting personal infor-
mation. Such projects should include a consent form with a clear mechanism for rejecting
or accepting the conditions of the study (e.g. featuring “Accept” and “Decline” buttons).
The consent form should not be longer than a few sentences in plain language, and would
preferably be presented in a large accessible font and as bullet points where possible. While
it is of great importance that participants understand the conditions and clearly express their
consent, users are constantly exposed to lengthy and incomprehensible agreements; many
participants may habitually agree to or disagree with these formal notices without reading
them. If it is impossible to include all relevant details in a short form, a more elaborate
supporting document describing conditions of use and privacy policies ought to be linked
to.
Studies collecting personal information should also feature the following functionalities:
For example, the Google search engine is accompanied by a link to a "Privacy & Terms" page, which
begins with the following statement: "By using our Services, you are agreeing to these terms. Please read
them carefully."
134 General Discussion
1. Where possible, the request to store participants’ data should be issued at the last stage
of the study (i.e. as a part of the feedback or debriefing). In this way participants are
not forced to decide whether or not to share their data before first having acquainted
themselves with the study. The benefits of this approach are discussed in Section 2.4.3
2. Participants should not be prevented from seeing feedback on their behaviour or
scores, regardless of whether they have consented to their data being stored
3. Participants should be able to review and potentially retract any piece of information
that researchers have collected based on their consent. As discussed in Section 2.2.4,
such a feature is technically possible even when participants remain anonymous
4. Participants should be able to choose whether to be notified about the results of any
study based on their data. This would not only minimise the chances of researchers
attempting to use data in a fashion contrary to accepted norms but may also increase
respondents’ trust in the methods used.
The functionalities described above are not only fair to participants, but could also increase
their willingness to participate in the research and to honestly respond to the questions
posed.
Perhaps the most challenging aspect of recording data in the digital environment relates
to the vague boundaries between data that belongs solely to participants, and data that can be
accessed with their consent but may belong or relate to several individuals. A large fraction
of digital behavioural residues falls within this second category, where it is unclear to what
extent the consent of one individual justifies the use of information relating to others. For
example, requesting respondents to provide the gender, race or age of their spouses seems
to be within ethical norms, even if the spouses do not participate in the research. More
vague, however, is the ownership of emails received from other people, or of Facebook
comments left under content published by participants, or indeed of the detailed structure of
a respondent’s personal friendship network. Analysing pictures or videos authored by the
respondent (e.g. using Google Glass technology) featuring people who do not participate in
the research would also most likely be considered a breach of privacy [158].
I propose that the use of data generated by or containing references to other people, when
acquired from participants, should be limited to those analyses that are primarily focused on
the participant. For example, connections between individuals who do not participate in the
study could be used to establish the participant’s position in the network. Also, it would be
acceptable to study the gender ratios among a participant’s friends or to analyse the language
7.2 Digital exposure 135
used in comments left on content published by the participant. However, the analysis of the
relationship between individual traits and position within the network ought to be strictly
limited to understanding participants’ behaviour, to the exclusion of non-participating users.
7.2 Digital exposure
The widespread availability of extensive records of individual behaviour combined with
the desire to learn more about customers and citizens presents serious challenges related
to privacy and data ownership [41, 169]. This is further exacerbated by the fact that raw
behavioural residues, such as web-browsing logs, or Facebook Likes, can be processed
using predictive algorithms to extract additional, potentially highly personal information,
such as sexual orientation or intelligence. The ability to make such predictions is not limited
to obviously revealing bits of data. For example, while the results presented in Chapter 5
showed that, unsurprisingly, an interest in science was associated with higher intelligence,
so was a preference for curly fries. Moreover, it is highly unlikely that the predictions are
limited to Facebook Likes, which represent a generic class of digital behavioural residues,
very similar to many other, potentially more revealing, records, such as tweets, emails, web
searches, browsing histories, credit card transactions, and purchases made both online and
offline.
This means that it is increasingly difficult, if not impossible, to prevent even a mod-
erately motivated third party from revealing an individual’s highly personal traits. People
might expect that their political preferences remain hidden until they refrain from making
explicitly political comments or visiting political websites, however a prediction of compa-
rable accuracy could be made based on an online radio playlist. Imagine a simple website
application, that could be easily built by a talented secondary school student, allowing its
users to infer their friends’ sexual orientation based on their Facebook Likes or online radio
playlists. Results of such predictions, regardless of their accuracy, could lead to tragedies—
and not just among adolescent boys or in less liberal countries.
This is well illustrated by a real-life case of a major US retail network which used shop-
ping records to predict pregnancies of its female customers. Those highly accurate predic-
tions were used to distribute well-timed and well-targeted offers [62]. In some contexts,
an unexpected flood of vouchers for prenatal vitamins and maternity clothing may be wel-
Note that the accurate prediction models discussed in [140] aimed, among other variables, at sexual
orientation, were based on open-source statistical software, relatively simple methods, and publicly available
records of users’ behaviour.
136 General Discussion
come, but it could also lead to tragic outcomes, for example, by revealing (or incorrectly
suggesting) a pregnancy of an unmarried woman to her family in a culture where this is
unacceptable [114]. As this example shows, predicting personal information to improve
products, services, and targeting can lead to dangerous invasions of privacy.
The realisation that playlists, shopping records or Facebook Likes can be used by ones’
friends, Internet Service Provider, or a government to reveal the information that one wants
to keep private, could destroy the trust between the parties interacting in the online environ-
ment. Should people share their ideas on Twitter, if their tweets could be used by their boss
to reveal their sexual orientation? Should they contribute to Wikipedia, if such contributions
could reveal their political views? Should they keep using credit cards and mobile phones,
given that the behavioural residue they create can be used by an insurance company to infer
their chances of getting cancer? Technological, economical and scientific progress could be
seriously hampered if a significant number of people choose to answer negatively to those
and similar questions.
This issue might be more serious in non-western societies. Digital technologies are
mostly designed and developed in Western, Educated, Industrialized, Rich and Democratic
(WEIRD; [100]) countries, where risks associated with privacy breaches are mild when
compared with places that do not enjoy as much freedom, tolerance, human rights and
rule of law. There is a general consensus that particularly oppressive states should be pre-
vented from obtaining means that would enhance their ability to harm their people, such
as weaponry. However, little attention is paid to the fact that the digital technologies based
on what WEIRD people consider an acceptable level of digital exposure, are being used by
non-WEIRD societies, too. This, undoubtedly, provides those societies with great advan-
tages, such as an ability to circumvent state’s monopoly on information and communication
(e.g. [87]), but is also exposing their members to risks incomparable with those faced by
WEIRD people. The ability to associate purchase logs or lists of Likes with a particular
user might result in intrusive marketing in a WEIRD country, but could be as well a matter
of freedom or life when used by a modern-day STASI equivalent.
There are two processes that, I believe, will shape our approach to privacy in the coming
years. First, as it was discussed in Section 7.1, the social norms related to privacy may
change to better reflect the current levels of exposure. Perhaps, in the close future, people
will no longer expect the information about their religion, political views, intelligence, or
sexual orientation to be private. It is beyond the scope of this thesis to argue whether such
STASI, or Ministerium für Staatssicherheit (The Ministry for State Security) was the infamous official
state security service of the German Democratic Republic or GDR, commonly described as one of the most
effective and repressive intelligence and secret police agencies to ever have existed
7.3 Labour market of the future 137
norms shift would bring us closer to embracing an Orwellian society model, or quite the
opposite—boost our tolerance and strengthen equality.
Second, increasing awareness among consumers and policy makers may, eventually,
reverse the tide stripping us of privacy. This could happen gradually, driven by greater un-
derstanding of technology, education and occasional news about massive privacy breaches,
such as Snowden’s [94]. However, such change may also happen abruptly, driven by a sin-
gle extraordinary case of privacy abuse, leading to a sudden withdrawal from particularly
exposing digital platforms, or a wider exodus from the digital environment.
I believe that, instead of waiting for an exodus of their consumers to happen (gradually
or abruptly), technological leaders, such as Facebook, Google or Microsoft, should be at the
vanguard of the technological revolution limiting their and their users exposure to privacy
abuse. It is now widely accepted that your data is stored and governed by third parties,
such as corporations and governments. But does it have to be this way? Imagine a social
network or online retailer that does not store records of user behaviour, such as Likes or
purchase records. Instead, those might be stored in personal computers or cloud accounts
under a full control of respective users. Such technological solutions already exist, ranging
from the anonymous payment systems such as Zerocoin [163] to privacy e-wallets [76].
Implementing such solutions in mainstream digital platforms, whether driven by industry
or policy makers, could greatly mitigate the risks associated with digital exposure while
allowing users to continue enjoying the benefits of digital services, and service providers to
continue making profits.
7.3 Labour market of the future
Results presented in this work relate to another crucial aspect of our societies: the labour
market. The labour market of today has not, in my opinion, taken as much advantage of the
technological progress as is the case with other markets, such as the retail market. While
platforms like Monster.com connect candidates with employers, recruitment practices re-
main largely unchanged. From the perspective of an employer, recruitment involves broad-
casting the information about a job, sieving trough a large number of applications, and
spending a considerable amount of time and money choosing the best candidate by means
of psychological assessment, interviews, and judgements based on reports of educational
and professional history. On the other side of the market, the job seekers have to invest a lot
of time and effort into applying for numerous positions and, if they are successful, attending
138 General Discussion
interviews and assessment sessions. The resulting cost of recruitment is prohibitive for both
employers and candidates, discouraging the former from hiring and the latter from changing
jobs. Employers spend money on repeating the process that was most likely conducted a few
times before: assessing traits and trustworthiness of the given candidate. This is wasteful,
inefficient, and beneficial only for companies offering occupational assessment. Moreover,
as the new hires can prove to be untrustworthy and expose employers to great risks, the lack
of effective reputation mechanism further hampers the liquidity of the job market. Resulting
barriers to changing jobs implies that people do not progress in their careers as quickly as
they potentially could or do jobs that they are not suited for. This has a negative effect on
their well-being, decreases the efficiency of the economy, and prevents others from entering
the job market.
Two factors discussed in this thesis may transform the mechanism of the labour market.
The first one relates to the potential for developing assessment tools based on the observa-
tion of digitally recorded behaviour. Currently, psychological assessment and the associated
benefits, such as good job-person fit, career planning, and self-awareness of ones’ strengths
and weaknesses is largely limited to better paid jobs and developed economies due to its pro-
hibitive costs. Inexpensive, fast, and automated psychometric tools could help to spread the
benefits of assessment to those who may need it the most but are currently largely deprived
of it: individuals at the beginning of their careers, less able, or the workforce in developing
countries.
The second factor relates to the reputation mechanism; the one employed by the labour
market of today is based on personal endorsements, costly and time consuming psycho-
logical assessment, and infamously inaccurate personal interviews [20]. Such mechanisms,
while well suited for small communities, seem to be largely inefficient in the age of swelling
populations and high mobility of the workforce. As suggested by the results presented in
Chapter 6 a relatively simple reputation mechanism offers a good proxy for the quality of
workers’ solution, and its sheer presence increases the quality of the work. The recent ex-
plosion of successful reputation mechanisms improving the functioning of markets for many
6 7
products and services, such as restaurants and hotels , books or retailers (e.g. eBay) indi-
cates that a similar mechanism is likely to emerge for the job market, greatly increasing the
dependability of employees and their employers. Moreover, as discussed in Chapter 6, the
reputation mechanism could be enhanced to embrace not only workers’ reliability but also
Note that such mechanism will most likely track the reputation of the employers as well. For an example
7.3 Labour market of the future 139
their abilities and skills, significantly improving its utility in matching workers with jobs.
The combined effects of the reputation mechanism and widespread psychological as-
sessment could drastically transform the labour market of the future. First, recruitment is
likely to be replaced by a job-matching mechanism. Instead of collecting candidates’ ap-
plications, a potential employer could enter required qualifications, psychological profile
and job conditions (e.g. offered salary or job location) into a job match-making mechanism
to instantly identify a small number of appropriate candidates willing to offer their labour
given the job conditions. Instead of assessing candidates separately for each of the jobs, they
could be thoroughly assessed once (or be assessed in an on-going fashion), given the own-
ership of their scores and encouraged to publish them on their job match-making platform
profile. A very similar approach is currently often employed in an academic context; for ex-
ample, many of the universities abandoned their internal entrance examination procedures
and rely on external certificates, such as Graduate Record Examination or SAT Reasoning
Test.
Second, decreased cost of finding a well-matching job (or a well-matching candidate),
coupled with a dependable reputation mechanism could allow employees to change jobs
more frequently, and employers to seek separate candidates to deal with any given projects
or task. Moreover, the ease of matchmaking will most likely decrease the duration of
engagements. Perhaps, a model similar to one used by the Amazon Mechanical Turk
will emerge, with employers composing dynamic teams of anonymous employees to work
on a given project, and employees being simultaneously employed by several companies.
Such system could enable further job specialization, as employees would be able to choose
projects that best match their interests, expertise, and career plan.
Third, the quality and accuracy of occupational assessment could be significantly im-
proved. Instead of building employees ability and personality profiles based on the self-
reports or sets of artificial ability tasks, their actual performance in real-life tasks and feed-
back from a large number of employers could be used.
Fourth, the presence of an ongoing ability and reputation assessment would provide a
constant motivation to the employees to deliver work of the highest quality and select tasks
that are challenging, hence allowing to boost their scores, but not far beyond their skills and
abilities, hence unlikely to be solved correctly.
Fifth, increased liquidity of a labour market could improve the pricing mechanism.
Labour market could rapidly react to changes in demand and supply of given skills, pro-
viding a dynamic feedback allowing people to develop most desirable skills. This could
lead to better career planning and more fair distribution of the rewards, with the salaries be-
140 General Discussion
ing driven by peoples’ reputation and demand for their skills and not by luck, connections,
or background.
Finally, such model of a labour market creates incentives for both sides to document the
transaction and be fair, as illustrated, for instance, by eBay [17]. Such transparent market
is relatively easy to police and regulate (e.g. [4]), and is most likely to retain high ethical
standards, such as minimum wage limits (if considered appropriate by policy-makers).
7.4 New personality and ability models
Last, but not least, behavioural residues offer a great potential for development of new
models describing human behaviour and performance. The unprecedented ability to inex-
pensively and conveniently record the behavioural residues of large groups of people and
across long periods of time can be used to identify patterns of behaviour representing exist-
ing or yet undiscovered latent psychological dimensions.
In fact Singular Value Decomposition (SVD) performed on user-Like matrix in Chap-
ter 5 resulted in 100 dimensions that were shown to predict a wide range of psycho-demographic
traits in preferences. Arguably, such 100 dimension represent a model describing humans,
similar in some ways to traditional personality models such as the Five Factor Model. Indi-
vidual dimensions of SVD model, however, are not easily interpretable, nor reliably repre-
sent single latent psychological dimensions. The dimensions of the SVD model go across la-
tent psychological dimensions, cultural differences, demographic dimensions, preferences,
and many other factors influencing human behaviour. In order to use digital behavioural
residues to develop new latent models reliably representing psychological phenomena, re-
searchers will have to find ways of isolating psychological traits from other factors, such
as cultural and demographic dimensions, decide how to rotate the factor structure to best
represent psychological traits, and use statistical methods best suited to analyse large, noisy,
and sparse databases of online behaviour (e.g. non-negative matrix factorization [149]).
7.5 Future research
Large number of questions and future research directions follow from this work. Perhaps
most exciting direction relates to developing new psychological models, such as personality,
based on unprecedented availability of behavioural residues. Another research idea that
stems from this work relates to inferring psychological profiles of entire groups of people,
for example audiences of a given website, in a privacy protecting manner. Such approach
7.5 Future research 141
Fig. 7.1 Dendrogram illustrating the structure of music tastes and its relationship to the
personality trait of Openness among myPersonality users. The structure was produced using
hierarchical clustering of the most popular Facebook Likes from Musician/Band category.
The colour scale represents the average Openness to Experience of its subscribers, ranging
from Conservative (cyan) to Liberal (magenta). The height of the nodes is proportional to
the dissimilarity between individual Likes or clusters at both ends. The shorter the path
between two musicians or bands, the larger overlap in audience.
could be based on the aggregate data related to behaviours of a given audience, such as their
favourite books or websites.
Exciting research ideas stem from the work exploring the relationship between individ-
ual and group performance. Crowd and individual scores on a general intelligence ques-
tionnaire should be compared with the performance in a wide range of actual tasks. Also,
it would be interesting to compare the performance of automated aggregation mechanisms
with aggregation based on discussion or votes of the crowd members. Attempts should be
made at modelling such relationships in the context of crowds interacting while solving the
tasks, similar to what is presented in [244].
Additionally, the preferences expressed in the online environment could help us to un-
142 General Discussion
derstand the relationship between the individual traits and taste for content, such as music,
music taste and its relationship with the personality trait of Openness.
Appendix A
List of myPersonality collaborators and
publications
Acquired from in December, 2013.
http: // mypersonality. org/
A.1 Publications based on data collected by myPersonality
project
1. An, J., Quercia, D., & Crowcroft, J. (2013). Fragmented social media: a look into
selective exposure to political news. In Proceedings of the 22nd international confer-
ence on World Wide Web
2. Mahalingam, V., Stillwell, D., Kosinski, M., Rust, J., & Kogan, A. (2013) Who Can
Wait for the Future? A Personality Perspective. Social Psychological and Personality
Science
3. Schwartz, H. A., Eichstaedt, J.C., Kern, M.L., Dziurzynski, L., Ramones, S.M.,
Agrawal, M. Shah, A., Kosinski, M., Stillwell, D.S., Seligman, M.E.P, Ungar, L.H.
(2013) Personality, Gender, and Age in the Language of Social Media: The Open-
Vocabulary Approach. PLOS ONE
4. Kosinski, M., Bachrach, Y., Stillwell, D.J., Kohli, P., Graepel, T. (2013) Manifesta-
tions Of User Personality In Website Choice And Behaviour On Online Social Net-
works. Machine Learning Journal (MLJ).
144 List of myPersonality collaborators and publications
5. Crosier, B.S., Webster, G.D., Kosinski, M., Stillwell, D. J. & Zolfaghari, Z. (2013)
Personality and Social Networks: The Big Five as Antecedents of Cognitive and Vir-
tual Social Structures
6. Rentfrow, P.J., Gosling, S.D, Jokela, M., Stillwell, D.J., Kosinski, M., & Potter, J.
(2013) Divided We Stand: Three Psychological Regions of the United States and
their Political, Economic, Social, and Health Correlates. Journal of Personality and
Social Psychology.
7. Cantador I., Fernandez-Tobias, I., Bellogin, A., Kosinski, M., Stillwell, D. Relating
Personality Types with User Preferences in Multiple Entertainment Domains In Pro-
ceedings of the 1st Workshop on Emotions and Personality in Personalized Services
(EMPIRE 2013)
8. Kosinski, M., Stillwell D.J., Graepel T (2013) Private traits and attributes are pre-
dictable from digital records of human behavior. Proceedings of the National Academy
of Sciences (PNAS).
9. Quercia, D. (2013) Don’t Worry, Be Happy: The Geography of Happiness on Face-
book (ACM WebSci 2013)
10. Markovikj, D., Gievska, S., Kosinski, M., Stillwell, DS. (2013) Mining Facebook
Data for Predictive Personality Modeling. The 7th International AIII Conference On
Weblogs And Social Media (ICWSM 2013)
11. Bi, B., Shokouhi, M., Kosinski, M., Graepel, T. (2013) A Young Liberal Christian
Woman Is Searching The Web. 23rd International World-Wide Web Conference
(WWW 2013)
12. Celli, F., Pianesi, F., Stillwell, D.J., Kosinski, M. (2013) Workshop on Computational
Personality Recognition (Shared Task). The 7th International AAAI Conference On
Weblogs And Social Media (Boston)
13. Friggeri, A., Lambiotte, R., Kosinski, M., Fleury E. (2012) Psychological Aspects of
Social Communities. SocialCom (Amsterdam)
14. Meade, A. W., Foster Thompson, L., Kuo, E.W., Kosinski, M., Stillwell, D.J. (2013)
The Influence of Culture and Human Development on Personality Measurement. 16th
congress of the European Association of Work and Organizational Psychology (Mun-
ster)
A.1 Publications based on data collected by myPersonality project 145
15. Stillwell, D.J., Kosinski, M. (2012) myPersonality project: Example of successful
utilization of online social networks for large-scale social research. Mobile systems
for Computational Social Science (Low Wood Bay)
16. Quercia, D., Bodaghi, M. & Crowcroft, J. (2012) Loosing “Friends” on Facebook.
Web Science (Evanston, IL). [ Link ]
17. Kosinski, M., Stillwell, D.J., Kohli, P., Bachrach, Y., Graepel, T. (2012) Personality
and Website Choice. Web Science (Evanston, IL)
18. Bachrach, Y., Kohli, P., Graepel, T., Stillwell, D.J., Kosinski, M. (2012) Personality
and Patterns of Facebook Usage. Web Science (Evanston, IL)
19. Rentfrow, J., Goldberg L.R., Stillwell, D.J., Kosinski M., Gosling, S.D., & Levitin
D.J. (in press) The Song Remains the Same: A Replication and Extension of the
MUSIC Model. Music Perception.
20. Quercia, D., Las Casas, D., Pesce, J.P., Stillwell, D., Kosinski, M., Almeida, V.,
Crowcroft, J. (2012) Facebook and Privacy: The Balancing Act of Personality, Gen-
der, and Relationship Currency. 6th International Aaai Conference On Weblogs And
Social Media (Dublin, June)
21. Kern, M.L., Schwartz, H.A., Eichstaedt, J.C., Dziurzynski, L., Ramones, S.M., Kosin-
ski, M., Stillwell, D.J., Ungar, L.H. (2012) Personality and Word Use: Positive and
Negative Expressions of the Self Through Social Media. European Positive Psychol-
ogy Conference (Moscow, June).
22. Wang, N., Kosinski, M., Stillwell, D.J. & Rust, J. (2012) Can well-being be measured
using Facebook status updates? Validation of Facebook’s Gross National Happiness
Index. Social Indicators Research.
23. Crosier, B., Webster, G., Stillwell, D., Kosinski, M., Orozco, S.T., Novell, C. (2012)
Personality Shapes Real-World and Online Social Networks. Thirteenth Annual Meet-
ing of the Society for Personality and Social Psychology (SPSP) in San Diego, CA
24. Rife, S.C., Mickelson, K.D., Kosinski, M., & Stillwell, D. (2012, July). Facebook
Networks and Local Population Parameters. Poster session presented at the biennial
meeting of the International Association for Relationship Research, Chicago, IL.
146 List of myPersonality collaborators and publications
25. Sun, L., Kosinski, M., Stillwell, D., Rust, J. (2011) On the Test-Retest Reliability of
the 100-item IPIP Scales: Differential Temporal Stability of the Big Five Personality
Traits. International Meeting of the Psychometric Society (IMPS), Hong Kong.
26. Quercia, D., Lambiotte, R., Kosinski, M., Stillwell, D. & Crowcroft, J. (2011) The
Personality of Popular Facebook Users. ACM CSCW 2012.
27. Quercia, D., Kosinski, M., Stillwell, D., Crowcroft, J. (2011) Our Twitter Profiles, Our
Selves: Predicting Personality with Twitter. Third IEEE International Conference on
Social Computing (SocialCom2011), Boston, USA.
28. Stillwell, D.J. & Tunney, R.J. (2012) Effects of measurement methods on the relation-
ship between smoking and delay reward discounting. Addiction.
29. Villegas, P., Concejero, P. Advanced Psychometrics With R On Mypersonality Database,
Conference on R, Spain
30. Sun, L., Cek, I., Spindler, S. A. K., Stillwell, D., Kosinski, M., Rust, J. (2011) Con-
firmatory Factor Analysis of the 100-item IPIP measure on a large Facebook dataset.
Poster presented at the International Meeting of the Psychometric Society (IMPS),
Hong Kong.
31. Stillwell, D.J. (2011) Personality data collection on social networks. Paper presented
at the Psychademia Conference, Nottingham Trent University.
32. Stillwell, D.J. & Tunney, R.J. (2011) Delay discounting methodology does not affect
the relationship between discounting and impulsive behaviours. Paper presented at
the Experimental Psychology Society (EPS) Conference, Nottingham University.
33. Hagger-Johnson, G., Egan, V. & Stillwell, D.J. (2011) Are social networking profiles
reliable indicators of sensational interests? Journal of Research in Personality.
34. Goldenberg, J. & Levy, M. (2009) Distance Is Not Dead: Social Interaction and Geo-
graphical Distance in the Internet Era.
A.2 List of researchers registered to use myPersonality database
1. Cambridge University: Professor John Rust, Professor Jon Crowcroft, Dr. Jason
Rentfrow, Dr. Alex Kogan, Dr. Umar Toseeb, Ilmo van der Lowe, Neal Lathia,
A.2 List of researchers registered to use myPersonality database 147
Vaishali Mahalingam, Tom Chase, Xiao Shen, Nishtha Lamba, Luning Sun, Fiona
Chan, Iva Cek, Arielle Bonneville-Roussy, Kate Xu, Blaine Landis, Brendan Doran,
Youyou Wu, Ning Wang, Sabine Spindler, Dr David Stillwell, Michal Kosinski and
Darja Irdam
2. University of Oxford, Internet Institute: Dr Bernie Hogan, Alexander Bassmanow
and Laurin Weissinger
3. Microsoft Research: Dr Thore Graepel, Dr Elad Yom-Tov, Dr Moshe Tennenholtz,
Dr Milad Shokouhi, Dr Yoram Bachrach and Bin Bi
4. University of Pennsylvania: Professor Martin Seligman, Andy Schwartz, Johannes
Eichstaedt, Peggy Kern, Lukasz Dziurzynski, S. Ramones, M. Agrawal, A. Shah, L.
Ungar
5. Penn State University: Professor Dongwon Lee
6. University of Texas at Austin: Professor Sam Gosling
7. University of Texas at Dallas: Professor Jinkyung Na, Eduardo Blanco
8. Universitat de Barcelona: Dr. David Gallardo-Pujol
9. Universiteit Gent: Prof. dr. Martine De Cock, Koen De Couck
10. Katholieke Universiteit te Leuven: Professor Pierre François
11. Westfälische Wilhelms-Universität Münster: Dr Philipp Doebler
12. Univeristy of Namur: Professor Renaud Lambiotte
13. North Carolina State University: Professor Adam Meade, L. Foster Thompson, E.
Kuo
14. University of Zurich: Dr Jan Cieciuch
15. University of California, San Diego: Dr Scott Martin
16. INSEAD: Professor David Dubois, KIM JeeHye Christine and PARK Brian Seongyup
17. The Ohio State University at Newark: Professor Bradley M. Okdie
18. Harvard University: Ethan Fosse, Ethan H. Mereish
148 List of myPersonality collaborators and publications
19. Kent State University: Sean C. Rife
20. Open University: Rien Sach
21. University of Trento: Fabio Pianesi, Dr Fabio Celli
22. Massachusetts Institute of Technology (MIT): Matthew Escobido
23. Columbia University: Matthew Sisco, Professor Jacob Goldenberg, Ali Faraji-Rad
24. University of Minnesota: Professor Nathan R. Kuncel, Professor Colin G. DeYoung
Michael P. Wilmot
25. Iowa State University: Professor Marc H. Anderson
26. Hong Kong University of Science and Technology: Gilad Fili Feldman
27. Knox College: Mike Prentice
28. University of Florida: Professor Renato Figueiredo, Ben Crosier
29. University of Ss. Cyril and Methodius: Professor Sonja Gievska Dejan Markovic
30. University College London: Sebastian Riedel, Garreth Hagger-Johnson
31. University of Leicester: Vincent Egan
32. Moscow Institute of Physics and Technology (MIPT): Professor Alexey Kolessa
33. University of Minnesota (Twin Cities): Professor Deniz Ones and Kevin Stanek
34. University of Surrey: Myles-Jay Linton
35. McGill University in Montreal: Professor Daniel Levitin
36. Polish National Academy of Sciences: Kamil Czarnogorski
37. Universidad Autonoma de Madrid: Ivan Cantador
38. Virginia Tech Network Dynamics and Simulation Science Laboratory, Virginia
Bioinformatics Institute: Samarth Swarup
39. Complutense University of Madrid: Miguel A. Castellanos (Lecturer)
40. Universidade São Francisco: Professor Ricardo Primi
A.2 List of researchers registered to use myPersonality database 149
41. James Madison University: Professor Laura Parks-Leduc, Gilad Chen and Anat
Bardi
42. Columbia Business School: Professor Gita V. Johar
43. SUNY Downstate Medical Center: Professor Janet Rosenbaum
44. École Polytechnique Fédérale De Lausanne (EPFL): Strasser Alina
45. Korea Advanced Institute of Science and Technology (KAIST): Professor Meey-
oung Cha
46. Nanyang Technological University: Professor Lin Qiu
47. NYU Stern School of Business: Professor Foster Provost
48. Istanbul Arel University: Professor Bilge Karamehmet
49. University of Toledo: Professor Jason P. Rose and Erin Vogel
50. Seoul National University: Joonhwan Lee How
51. Universität Leipzig: Professor Stefan Schmukle and Professor Boris Egloff
52. University of Georgia: John Camp
53. Georgia Institute of Technology: Scott Appling
54. Masdar Institute Abu Dhabi: Professor Iyad Rahwan
55. University of California, Los Angeles: Professor Kayuet Liu
56. University of Twente: Qiwei He (Britt)
57. Humboldt-University of Berlin: Professor Jochen Gebauer
58. Technical University of Berlin: Professor Ruediger Zarnekow
59. Yahoo Research: Dr Daniele Quercia
60. The Academic College of Emek Jezrael: Prof. Yonathan Mizrachi
61. University of Melbourne: Aaron Harwood
62. University of Vermont: Emily Cody
150 List of myPersonality collaborators and publications
63. Hebrew University of Jerusalem: Professor Jacob Goldenberg
64. Kuwait University: Dr. Maha H. Faisal
65. Middle East Technical University: Dr. Tugba Taskaya Temizel
66. Vassar College: Dr. Allan Clifton (Department of Psychology)
67. Other Institutions: Professor Lew Goldberg, Annalyn Ng, Zoltan Kiss, David Green-
berg, D Las Casas, J. Pesce, V. Almeida, G.M. Webster, ST Orozco, C Novell, J
Goldenberg, M Levy, Raphael Silberzahn, A. Friggeri, E. Fleury
Appendix B
The reliability of the IPIP FFM
Personality Inventory
152 The reliability of the IPIP FFM Personality Inventory
300-item long IPIP FFM Personality Inventories [84].
Cronbach’s Alpha Reliability
Scale myPersonality sample Reported by author
Personality domains, 100-items inventory, 182,922 cases
Openness 0.85 0.89
Conscientiousness 0.92 0.90
Extraversion 0.93 0.91
Agreeableness 0.88 0.85
Neuroticism 0.93 0.91
Personality facets, 300-items inventory, 8,203 cases
A1: Trust 0.90 0.82
A2: Morality 0.79 0.75
A3: Altruism 0.85 0.77
A4: Cooperation 0.71 0.73
A5: Modesty 0.79 0.77
A6: Sympathy 0.81 0.75
C1: Self-Efficacy 0.84 0.78
C2: Orderliness 0.84 0.82
C3: Dutifulness 0.77 0.71
C4: Achievement-Striving 0.84 0.78
C5: Self-Discipline 0.89 0.85
C6: Cautiousness 0.83 0.76
E1: Friendliness 0.89 0.87
E2: Gregariousness 0.89 0.79
E3: Assertiveness 0.86 0.84
E4: Activity Level 0.73 0.71
E5: Excitement-Seeking 0.84 0.78
E6: Cheerfulness 0.86 0.81
N1: Anxiety 0.87 0.83
N2: Anger 0.92 0.88
N3: Depression 0.91 0.88
N4: Self-Consciousness 0.85 0.80
N5: Immoderation 0.78 0.77
N6: Vulnerability 0.88 0.82
O1: Imagination 0.84 0.83
O2: Artistic Interests 0.82 0.84
O3: Emotionality 0.77 0.81
O4: Adventurousness 0.83 0.77
O5: Intellect 0.83 0.86
O6: Liberalism 0.80 0.86
Note: baseline reliability values were obtained from .
Appendix C
Goldberg’s Psychometric Antonyms
tory.
r Item 1 Item 2
-.67 Dislike myself Feel comfortable with myself
-.53 Find it difficult to approach others Make friends easily
-.54 Avoid contact with others Feel comfortable around people
-.63 Dislike myself Am very pleased with myself
-.56 Tend to vote for conservative polit- Tend to vote for liberal political
ical candidates candidates
-.64 Leave things unfinished Finish what I start
-.52 Find it difficult to approach others Am skilled in handling social situa-
tions
-.54 Find it difficult to approach others Feel comfortable around people
-.54 Find it difficult to approach others Start conversations
-.63 Do not like art Believe in the importance of art
-.52 Don’t like to draw attention to my- Do not mind being the centre of at-
self tention
-.51 Am not easily frustrated Get stressed out easily
-.51 Don’t talk a lot Start conversations
-.51 Find it difficult to approach others Talk to a lot of different people at
parties
-.51 Feel comfortable around people Retreat from others
Note: r represents the Pearson product-moment correlation between the items
Appendix D
Estimating the probability of a crowd
arriving at a correct solution working
under Majority Voting in practice
Equation 6.6 discussed in Section 6.3.2 is computationally inefficient due to the high num-
ber of potential coalitions. Here a more tractable approach to computing the chances of a
crowd to arrive at the correct solution when working under the MV aggregation is proposed.
Resulting formula can be easily implemented in one of the popular programming languages.
Let n denote the size of the crowd of agents characterised by the individual ability levels
of t , ..., t . The probability of an agent i characterised by ability level t to solve a task I
1 n i
correctly is denoted as P (t ) and can be estimated using the IRF (Equation 6.1). Suppose
I i
that the probability of selecting any of the incorrect responses at the given θ is uniformly
1−P (t )
I j
distributed and equals .
k−1
Agents submit exactly one solution to task I, characterised by k response options. Let
n be the size of the coalition of agents that selected response q. For the sake of clarity we
denote correct response as q = 1, effectively the number of agents that selected a correct
response is denoted as n .
The crowd working under the MV aggregation is successful at solving task I when n is
the largest, or one of the largest (tie). In the case of a tie, a random one out of the winning
solutions is chosen, and hence the chance of success has to be divided by the number of the
winning coalitions.
Let Q be a set of tuples containing all unique partitions of n (that is, positive integers
Estimating the probability of a crowd arriving at a correct solution working under Majority
156 Voting in practice
that add up to n) where the votes for the correct response option are not a minority:
Q := {⟨n ≥ n ≥, ..., ≥ n ⟩ : n ≥ 0 & n = n}. (D.1)
1 2 k i i
i=1
Let E be the set of all possible n agent coalitions that selected a correct response:
E := {⟨1, ..., n ⟩ ∈ ({1, ...., n})}. (D.2)
The probability of observing a given winning coalition E, where the distribution of the
incorrect responses among agents matters, equals:
1 − P (t )
I i
∏ ∏
P = P (t ) (D.3)
E I i
k − 1
i∈E i∈/E
The number of ways in which incorrect responses can be distributed among agent parti-
tions n , ..., n equals:
2 k
(n − n )! (n − n − n )! (n − n − ... − n )!
1 1 2 1 k−1
· · ... · =
(n − n − n )!n ! (n − n − n − n )!n ! (n − n − n − ... − n )!n !
1 2 2 1 2 3 3 1 2 k k
(D.4)
(n − n )!
(n !)
i=2
All possible ways to distribute the incorrect n , ..., n coalitions among (k − 1) response
2 k
options equals (k − 1)!. However the order of the coalitions of equal sizes does not matter,
and was already accounted for in Equation D.4. Thus, (k − 1) has to be divided by a factorial
of the frequencies of equally sized coalitions n , .., n
2 k
(k − 1)!
(D.5)
k |n −i|
1 ( 0 )!
∏ ∑ j
i=0 j=2
Finally, the probability of n winning the vote when there is a tie equals:
. (D.6)
k |n −n |
(0 )
∑ j 1
j=1
One or more n , such that n = n
i i 1
Z is defined as the product of those three expressions:
(n − n )! (k − 1)! 1
Z = · · (D.7)
k k |n −i| k |n −n |
∏ (n !) ∏ 1 ( ∑ 0 j )! ∑ (0 j 1 )
i=2 i=0 j=2 j=1
where last two terms can be combined into one. Note that j = 1 in the last term of the
following formula:
(n − n )! (k − 1)!
Z = · (D.8)
k k |n −i|
∏ (n !) ∏ 1 ( ∑ 0 j )!
i=2 i=0 j=1
Combining equations listed above can be used to efficiently compute the probability
of a crowd of the size n, characterised by individual abilities t , .., t , to arrive at the correct
1 n
solution for item I with k response options when aggregating responses using MV approach:
∑ ∑
CP = Z P =
I E
Q E
(D.9)
(n − n )! (k − 1)! 1 − P (t )
1 I i
∑ ∑ ∏ ∏
· P (t )
I i
k k |n −i| k − 1
∏ (n !) ∏ 1 ( ∑ 0 j )!
Q i=2 i i=0 j=1 E i∈E i∈/E
where P (t ) represents the IRF (Equation 6.1).
I i
Appendix E
Order of individual members’ weights
This Section proves that the individual weights stem from agent’s ability and hence are
ordered in the same way independently from a task (i.e. if t ≥ t , than w(t , I ) ≥ w(t , I )
i j i p j p
for any task I ). This is important in practice, as it can be used to establish the order of the
individual weights, even when the task parameters are not known. Section 6.3.3 shows that
the weight of an individual crowd member is given by:
P (t )(k − 1)
I i
w(t , I) = ln (E.1)
1 − P (t )
I i
I claim that:
w(t , I ) ≥ w(t , I ) ⇐⇒ t ≥ t (E.2)
i p j p i j
PROOF: First, let’s show that the order of weights depends on the probability of solving
the task correctly:
w(t , I ) ≥ w(t , I ), (E.3)
i p j p
(cid:18) (cid:19) (cid:18) (cid:19)
P (t )(k − 1) P (t )(k − 1)
I i I j
ln ≥ ln , (E.4)
1 − P (t ) 1 − P (t )
I i I j
P (t ) P (t )
I i I j
≥ , (E.5)
1 − P (t ) 1 − P (t )
I i I j
P (t ) − P (t )P (t ) ≥ P (t ) − P (t )P (t ), (E.6)
I i I i I j I j I j I i
P (t ) ≥ P (t ). (E.7)
I i I j
160 Order of individual members’ weights
By replacing P(t ) with IRF (Equation 6.1) brings:
1 1
1 1 − 1 1 −
k k
+ ≥ + , (E.8)
k 1 + e −a (t −b ) k 1 + e −a (t −b )
I i I I j I
which can be simplified to:
−a (t −b ) −a (t −b )
e I i I ≤ e I j I , (E.9)
−a (t −b ) −a (t −b )
ln(e I i I ) ≤ ln(e I j I ), (E.10)
−a (t − b ) ≤ −a (t − b ), (E.11)
I i I I j I
t ≥ t . (E.12)
i j
In the 3PL IRT model used here, an individual with higher ability has a higher prob-
ability of solving tasks of any difficulty (i.e. if t ≥ t , than P (t ) ≥ P (t ) for any I ).
i j I i I j p
p p
Effectively, individual weights are always ordered in the same way.
Appendix F
Royalty-free intelligence questions
This work is accompanied by a set of 25 royalty-free intelligence questions, inspired by the
SPM matrices. The IRF parameters of those tasks were estimated based on a sample of 9,000
volunteers who took a computerised adaptive intelligence test published on myPersonality
Facebook application [140] between January and December 2011. The item bank used
here contained 6 tasks from the original SPM scale, allowing the IRT model build on the
Facebook sample to be “equated” ([135]) with the original SPM scale. Effectively, the IQ
scores estimated using the royalty-free intelligence questions published here are located on
the same metric as the original SPM scale.
The tasks, scoring key, and scoring mechanism are not revealed here to avoid disclosing
the correct responses to the workers’ community. Interested parties are asked to request the
materials by contacting the authors , attaching a brief description of the intended use and af-
filiation information. The questions are free of charge for commercial and non-commercial
research.
While the SPM-type tasks may not be best suited for many applications in the crowd-
sourcing and AI research, I hope that this initial batch of tasks of known relationship with
general intelligence will foster the development of a larger bank of royalty-free questions
(or question generator) that could be used in assessing intelligence and locating it on the
standardised intelligence scale. Ideally, crowdsourcing platforms or requesters could con-
tinually monitor individual intelligence of their workers.
Please email michal@michalkosinski.com