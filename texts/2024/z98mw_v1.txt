The Crosslinguistic Coordination of Overt
Attention and Speech Production as Evidence for
a Language of Vision
1,2*† 3 4
Moreno I. Coco , Eunice G. Fernandes , Manabu Arai ,
5*†
Frank Keller
1*
Department of Psychology, Sapienza University of Rome, Via dei
Marsi, 78, Roma, 00185, Italy.
I.R.C.S.S Fondazione Santa Lucia, Via Ardeatina, 306/354, Roma,
00179, Italy.
School of Psychology, University of Minho, Braga, Portugal.
Faculty of Economics, Seijo University, Tokyo, Japan.
5*
School of Informatics, University of Edinburgh, 10 Crichton Street,
Edinburgh, EH8 9AB, United Kingdom.
*Corresponding author(s). E-mail(s): moreno.coco@uniroma1.it;
keller@ed.ac.uk;
These authors contributed equally to this work.
Abstract
A central question in cognition is how representations are integrated across differ-
ent modalities such as language and vision. One prominent hypothesis posits the
existence of an abstract, pre-linguistic language of vision that organises mean-
ing compositionally to enable cross-modal integration. This hypothesis predicts
that the language of vision operates universally, independent of linguistic sur-
face features such as word order. We conducted eye-tracking experiments where
participants described visual scenes in English, Portuguese, and Japanese. By
analysing eye-movement sequences alongside spoken descriptions, we demonstrate
that semantic similarity between sentences strongly predicts the similarity of asso-
ciated scan patterns in all three languages, even across scenes and for sentences
in different languages. In contrast, the effect of syntactic similarity was found to
be language-dependent and restricted to scene context. Our findings support a
universal language of vision as an organising principle of meaning that transcends
syntactic structures.
Keywords: Cross-modal attention; scene description; cross-linguistic differences;
semantic and syntactic similarity; eye-tracking; language of vision; scene grammar
1 Main
A longstanding debate in cognitive science concerns the existence and nature of a “lan-
guage of thought” (LoT). First introduced by Fodor [1], the LoT is hypothesised to
have a compositional structure consisting of simple concepts that combine systemat-
ically to form more complex mental representations. These representations underpin
cognitive processes across various modalities, including language, visual perception,
and reasoning. Open questions regarding the LoT include whether its representations
are symbolic or non-symbolic, whether its combination rules are logical or probabilis-
tic, and how patterns of human responses, including neural activity, reflect aspects of
the LoT [2–4]. Researchers have often explored the LoT through the lens of human
language, particularly at the intersection of syntax and semantics [5]. However, lan-
guage is a relatively recent product of human evolution [6]. Perception, a much older
cognitive component, could instead be responsible for organising causal relationships
between events (e.g., agent-patient interactions) into structured chunks of meaning
[7, 8]. In this view, attentional mechanisms are directly involved in constructing repre-
sentations that other cognitive processes can access, driven by an underlying “language
of vision” assumed to be an instance of the language of thought [9].
The present work utilizes an image description paradigm to examine key predic-
tions of the language of vision hypothesis, which entails that the coordination between
modalities is universal across languages and primarily driven by semantic, rather than
syntactic, representations. This perspective can be contrasted with an opposing view
that contends that the coordination between modalities is significantly influenced by
the syntactic properties of individual languages, such as word order, leading to vari-
ability in coordination across different languages. Evidence supporting this opposing
view comes from research on language-vision coordination employing the visual world
paradigm [10–13].
Cavanagh advanced the language of vision hypothesis [9], inspired by Gregory [14],
suggesting that attention structures perceptual information (e.g., visual input) into
coherent messages shared with other cognitive processes, such as memory, language,
and reasoning. This hypothesis aligns with findings demonstrating the crucial role of
attention-motor coordination in motor control (e.g., object fixation before action [15–
18]), facilitating successful learning [19], and joint action between individuals [20–24].
Furthermore, the influence of visual information on language is well-established, with
ration shape language [25, 26] and operate on universal constraints that are shared
cross-culturally [27]. Moreover, visual information exhibits statistical regularity and is
predictable [28] – fundamental characteristics that enable tasks such as categorisation
[29–31], memory recognition [32, 33] or visual search [34–36]. The regularity in visual
information has led researchers to postulate an underlying “scene grammar” [37, 38],
an idea closely related to the language of vision hypothesis. Support for this concept
comes from evidence demonstrating that violations of structural or conceptual regular-
ities in scenes affect human visual processing, measurable both in behavioural [39–41]
and neurophysiological responses [42–47].
Postulating a language of vision implies adopting a variant of the language of
thought hypothesis, which posits that mental representations possess a compositional
structure and combine in systematic and rule-governed ways. Such representations
enable the cognitive system to coordinate across distinct sensory and cognitive modal-
ities. While their nature may be symbolic or sub-symbolic, LoT representations are
fundamentally semantic, encoding the meaning of information rather than its surface
form (e.g., the specific words or the syntactic structure of a sentence). Consequently,
we predict that the coordination between language production and visual attention
during tasks like image description should be primarily driven by the semantic content
of the utterances rather than by their syntax form. This implies that task performance
should exhibit consistency across languages, even when their syntactic properties differ
significantly.
We will contrast this prediction with the “syntactic hypothesis”, which posits that
the coordination across language and vision primarily reflects the surface form of
inputs and outputs, such as word order in a sentence. Key evidence comes from the
visual world paradigm (VWP), in which participants are eye-tracked as they view a
scene and either hear or produce an utterance [10, 48]. VWP studies demonstrate
that overt attention closely tracks spoken language comprehension, with participants
promptly attending to visually available referents mentioned in the utterance. Cru-
cially, syntactic information including grammatical function [11, 12], subcategorization
[49], case marking [50, 51], and tense [52] is used to guide this process.
A similar relationship is observed in language production: Referents tend to be
fixated just before they are mentioned [53–55]. The latencies of these fixations are
modulated by the perceptual characteristics of scenes being viewed, but also by
the syntactic properties of the sentences being produced [56], including grammatical
function [53] and word order [57].
Based on VWP findings, which demonstrate the significant role of syntactic infor-
mation in guiding attention during language processing, we expect that an utterance’s
syntax will be a determinant of cross-modal coordination. Consequently, we should
observe differences in behaviour between speakers of different languages performing
the same multimodal task, such as image description. VWP comparisons of the eye-
movement patterns of speakers of English with speakers of Japanese [50], German [51],
and Spanish [58] provide evidence for this expectation.
This study aims to adjudicate between competing hypotheses: the language of
vision hypothesis, which posits semantic-driven cross-modal coordination, and the
syntactic hypothesis, which emphasizes the role of surface syntactic features. We
investigate whether cross-modal coordination exhibits cross-linguistic consistency, thus
determining whether semantics or syntax primarily drives language–vision integration.
We build on prior results of Coco and Keller [59], who demonstrated a strong rela-
tionship between linguistic production and visual attention for English. In their study,
English-speaking participants were eye-tracked while viewing photorealistic scenes and
produced verbal descriptions after being cued with an object label. The hypothesis
was that speakers who produce similar verbal descriptions also show similar patterns
of overt attention. This was tested by comparing the scan patterns (sequences of fix-
ated objects) across participants and scenes using the longest common subsequence
(LCS) measure. On the linguistic side, similarity between scene descriptions was also
computed using LCS on the word sequences of the descriptions. A regression analy-
sis then showed that sentence similarity predicts scan pattern similarity, both within
the same scene and across different scenes. This suggests strong coordination between
visual attention and linguistic production, which per se is compatible with both the
language of vision hypothesis [9] and the syntactic hypothesis [11, 53]. However, it does
not provide conclusive evidence for either of the two hypotheses because it is limited
to English speakers and, therefore, does not address the question of cross-linguistic
consistency in language-vision coordination for languages with different word orders.
To adjudicate between the language of vision and the syntactic hypothesis, the
present study compares scene descriptions across two dimensions: their semantic sim-
ilarity (with sentences represented as vectors in neural embedding space) and their
syntactic similarity (with sentences represented as sequences of syntactic categories).
Under the assumption of a language of vision (or, more generally, an underlying lan-
guage of thought), we expect the driver of the association between scan patterns and
sentences to be abstract, compositional semantics. We should find that similar scan
patterns predict semantically similar scene descriptions across participants, scenes,
and even languages. This effect should be reduced or absent when looking at syntac-
tic similarity, given that the language of vision is hypothesised to operate on semantic
representations. Under the syntactic hypothesis, we expect the opposite pattern: The
association between scan patterns and sentences should be driven by the syntactic
rather than the semantic similarity between sentences.
A way to distinguish between the two hypotheses is to look at cross-modal coor-
dination across different languages. If cross-modal coordination is primarily driven by
semantics, we would expect to observe consistent patterns across languages, assuming
semantic representations are universal. If it is based on the syntactic properties of sen-
tences, then we should find differences between languages that differ in their syntactic
properties. In this study, we, therefore, compare scene description data for English,
Portuguese, and Japanese. Japanese and Portuguese differ from English in word order:
Japanese is an SOV (subject–object–verb) language with the verb positioned at the
sentence-final position, while English is an SVO language. Portuguese is a modifier-
final language (e.g., a pequena casa “the house small”), while English is modifier-first.
If all three languages show the same association between scan patterns and sentences,
that would be evidence for the language of vision hypothesis; if we see differences, then
of our experiment and provides examples for scenes, scan patterns, and sentences.
Results
In this study, we eye-tracked participants while they described visual scenes in one
of three languages (English, Portuguese, and Japanese). We then computed the pair-
wise similarity between all descriptions using neural sentence embeddings [60], which
Fig. 1 Illustration of our similarity metrics. Examples of pairwise comparisons of English speak-
ers on different scenes with associated sentences and the part-of-speech sequence characterising their
syntactic composition. We plot scenes with overlaid cell grids to map fixation coordinates into cat-
egorical sequences of fixated locations. The scan patterns of the two speakers are identified using
colour (yellow and red); the circle’s size indicates the fixation duration. The lines mark the order of
fixated locations. Below, we report the pairwise similarity measures obtained for scan patterns, sen-
tence semantics, and syntax.
makes it possible to compare the meaning of sentences using high-dimensional vector
representation, abstracting away from differences in syntactic form. We computed the
similarity between the scan patterns associated with the scene descriptions using the
LCS sequence comparison measure. This allowed us to investigate whether scan pat-
tern similarity predicts sentence similarity within and across languages for the same
and different scenes. To analyse the role of natural language syntax, we also computed
sentence similarity based on part-of-speech sequences rather than semantic sentence
embeddings, which we then again used to predict scan pattern similarity. We discuss
our results based on visualisations (see Figures 2 and 3) while also referring to infer-
ential statistics computed using a linear mixed-effects model. The model predicts the
similarity of two scan patterns based on Semantics (semantic similarity between the
two sentences associated with the scan patterns), Syntax (syntactic similarity between
these two sentences), Language (whether the two sentences use the same or a different
language) and Scene (whether the two sentences describe the same or a different visual
scene). The model also includes binary and tertiary interactions between these predic-
tors, as determined by a model selection algorithm (see Methods section for details).
those of theoretical interest (several two-way interactions, for example, are subsumed
by the three-way interactions).
Cross-modal coordination replicates for languages other than English.
ilarity on the y-axis. The top three panels show the results for each of the three
languages we study. We can see that semantic similarity predicts scan pattern similar-
This finding holds across all three languages and when comparing sentences produced
for the same visual scene (blue triangles) and sentences produced across different
visual scenes (red circles). The similarity is lower in the between-scene condition, but
the association is still clearly visible. This replicates prior results for English [59] for
two new languages which differ syntactically from English: Japanese is a verb-final
language while in English, the verb follows the subject; English is a modifier-first
language, while Portuguese is a modifier-final language.
To ensure the association between sentence and scan pattern similarity is not due
to chance or artefacts, such as centre bias in human scene viewing [61] or intrinsic
properties of scenes (for example, we only used indoor scenes; they all contained both
animate and inanimate objects), we shuffled sentences and scenes, i.e., we paired each
scene with a random sentence from another scene rather than with the “correct” sen-
tence uttered to describe it. By disrupting the association between sentences and scan
patterns in this way, the significant effect of semantic similarity disappears, illustrated
Cross-modal coordination generalises between languages
that semantic sentence similarity predicts scan-pattern similarity between languages.
For example, suppose a Japanese speaker utters a sentence that is similar to a sen-
tence spoken by an English speaker. In that case, we observe that the speakers of both
languages follow similar scan patterns. This observation holds across all three lan-
guage pairs (English–Portuguese, English–Japanese, Portuguese–Japanese). However,
this overall effect is still modulated by whether the comparison is within or between
languages: Semantic similarity is a slightly better predictor of scan pattern similar-
ity if the two sentences are in the same language rather than in different languages
the same language (see top panel), the association between semantic similarity and
scan pattern similarity holds both within the same scene and across different scenes.
Nevertheless, semantic similarity predicts scan-pattern similarity more strongly if the
visual scene is the same (right panel), especially when comparing between languages
and to the coefficient of three-way interaction Semantics × Language × Scene). These
results provide further evidence for the universality of the association between sentence
similarity and scan pattern similarity: The predictive power of semantic similarity is
independent of which language the sentences are spoken in. Moreover, our results also
point at language-specific mechanisms fine-tuning scan pattern coordination, likely
attributable to syntactic compositionality, as the next set of results will demonstrate.
Cross-modal coordination is modulated by language syntax within the
scene context
through semantic similarity across languages and scenes. However, they do not rule
out other factors contributing to the association. It is possible that syntactic similarity
between sentences also plays a role. For example, the three languages we study use
similar parts of speech, even though they may order them differently. To elucidate this
of scan patterns similarity (y-axis). Recall that syntactic similarity is measured by
treating sentences as part-of-speech sequences and computing their longest common
the effect of syntax is much weaker. In particular, we observe no clear association for
the between-scene condition (red lines across all languages and language pairs), which
is now almost identical to the baseline, where the association between sentences and
scan patterns is randomised. Furthermore, even in the within-scene condition (blue
lines), the effect mainly occurs if we compare sentences of the same language (top
panel) rather than sentences of two different languages (bottom panel). When the
languages are different, the effect is reversed: more syntactic similarity is associated
effect of syntax is restricted not only by the language but also by the visual context in
which sentences are situated. We observed a three-way interaction Semantics × Syntax
× Scene, which shows that syntactic similarity increases overall scan-pattern similarity
(parallel lines) within the same scene. However, increasing syntactic similarity reduces
how effects of semantic similarity on scan-pattern similarity are modulated across
levels of syntactic similarity between and within scenes). This confirms that syntactic
similarity is a reliable predictor of scan pattern similarity only in the same-scene
Overall, this set of results aligns with the hypothesis that the sentence-scan pat-
tern association is principally driven by semantics, with syntax playing a minor role
confined to pairs in which the visual scene and the language are the same.
Discussion
This study provided empirical evidence that the similarity of participants’ scan pat-
terns is predicted by the semantic similarity of the associated scene descriptions across
languages with distinct word order regularities (English, Portuguese, and Japanese).
Our finding generalises previous work [59] for English to other languages. As dis-
cussed in the Introduction, our findings make it possible to adjudicate between the
language of vision hypothesis [9] and the syntactic hypothesis as explanations for how
the cognitive system coordinates multiple modalities.
The idea of a language of vision builds on the language of thought hypothesis
[1] in assuming the semantic compositionality of pre-linguistic representations. Our
results support this hypothesis and suggest that the relevant semantic representations
are shared across languages and grounded in the perceptual system [2, 8]. This is
also consistent with the idea that visual information shapes the language used to
describe it [25–27, 62, 63]. Furthermore, overt attention, as a motor output of the
visual system, encodes grammatical cues of the compositional processes that occur
when the acquisition of visual information simultaneously engages the human sentence
processor [9, 14, 64]. Lastly, the evidence of cross-linguistic universality of a language
Fig. 2 Scan pattern similarity as a function of Semantic similarity (with dot-product over sentence
embeddings as similarity measure). Within each panel, we compute the mean of scan pattern similarity
(y-axis) over regularly spaced bins of the semantic similarity (x-axis, from 0 to 1 in bins of .1) and
overlay the model fit (as a loess line). Line colour and symbol type indicate whether the similarities
have been computed within the same scene (blue triangle) between different scenes (red circle) or it
is the shuffled condition (grey square). Languages are compared across the panels. The top panels
show similarities computed within the same language (English; Portuguese; Japanese), while the
bottom panels show pairs of different languages (English–Portuguese; English–Japanese; Portuguese–
Japanese).
of vision raised new questions about the interface between semantics and syntax in
language, requiring novel paradigms beyond the logical or experimental approaches
commonly adopted (e.g., [65, 66]; see [67] for a review of the debate).
In contrast to the language of vision hypothesis predictions, the ’syntactic hypoth-
esis’ suggests that the surface form of linguistic input, such as word order, primarily
drives cross-modal coordination. As observed in research using the visual world
paradigm, language processing unfolds word-for-word and reflects how participants
allocate overt attention to a co-present scene [10, 11, 53]. Thus, differences in the sur-
face form of sentences, such as word order (e.g., verb-final vs verb-second languages,
modifier-final vs modifier-first languages), grammatical function, subcategorization,
case marking, and tense (see Introduction) should significantly impact cross-modal
coordination. The ’syntactic hypothesis’ received limited support in our study. Syn-
tactic similarity predicted scan patterns within languages and scenes, but this effect
was absent between scenes and significantly attenuated across languages. While not
contradicting prior VWP studies, which typically focus on within-language and within-
scene coordination, our findings suggest that the strong influence of syntactic factors
Fig. 3 Scan pattern similarity as a function of syntactic similarity (with longest common subsequence
over parts-of-speech sequences as similarity measure). Within each panel, we compute the mean of
scan pattern similarity (y-axis) over regularly spaced bins of the syntactic similarity (x-axis, from 0
to 1 in bins of .1) and overlay the model fit (as a loess line). Line colour font symbol type indicates
whether the similarities have been computed within the same scene (blue triangle) between different
scenes (red circle) or it is the shuffled condition (grey square). Languages are compared across the
panels. The top panels show similarities computed within the same language (English; Portuguese;
Japanese), while the bottom panels show pairs of different languages (English–Portuguese; English–
Japanese; Portuguese–Japanese).
observed in these studies may not generalize across languages and scenes unless
semantic properties are considered.
Considering these theoretical implications, which suggest a crucial role for seman-
tics in cross-modal coordination, it is critical to determine, at a finer level of
granularity, which aspects of sequential visual processing predict semantic composi-
tion and how these processes may diverge across languages during syntactic phrase
building. To experimentally investigate these questions, one promising approach would
be to co-register eye movements and electrophysiological responses during natural
viewing in situated linguistic tasks. This approach would allow us to map electrophys-
iological indices assumed to underlie phrase-building operations (see [68] for a recent
review) against concurrently processed chunks of visual information. A consistent body
of evidence demonstrates how electrophysiological activity directly maps onto overt
attention in tasks such as reading, visual search or short-term memory during nat-
ural viewing behaviour (e.g., [43, 69, 70]). Therefore, components in neural activity
indicating the semantic (e.g., the N400) or structural (e.g., the P600) predictability
Fig. 4 Visualisation of marginal means (adjusted model predictions) of scan-pattern similarity in
selected significant interactions with Semantics, Syntax, Language and Scene. The 95% confidence
intervals of the predicted values are represented either as error bars or shaded bands. (A) Semantics
× Language × Scene: with the semantic similarity in the x-axis, the same language represented as
triangles (dashed line) or different languages as circles (solid line) organised in panels for different
visual scenes (left) and the same visual scene (right); (B) Semantics × Syntax × Scene: with semantic
similarity in the x-axis, syntactic similarity evaluated at three representative values of its distribution
(.34, .48, .63) organised in panels for different visual scenes (left) and the same visual scene (right);
(C) Semantics × Language (left panel) and Syntax × Language (right panel): within each panel, we
distinguish comparisons between languages (circle, solid line) and within the same language (triangle,
dashed line).
of visual information [41, 42, 44–47] may be concurrently mapped onto language pro-
cesses to define more precisely the characteristics of a language of vision or scene
grammar [37, 38].
Semantics (semantic similarity between the sentences associated with the scan patterns; dot-product over
sentence embeddings), Syntax (syntactic similarity between the sentences; LCS over part-of-speech
sequences), Language (sentences use the same or a different language; different as reference Level) and Scene
(sentences describe the same or a different visual scene; different as reference level). The participant pair
(n = 1758) and scene pair (n = 9216) were the random variables introduced as intercepts and slopes. A
model where sentences and scan patterns are shuffled (see text) is included to test for the effect of chance
and artifacts.
Dependent Variable Predictor β(Std. β) CI (2.5 %; 97.5 %) t-value
∗∗∗
(Intercept) .195(0) .191; .198 113.86
∗∗∗
Semantics .052(.07) .047; .057 19.27
∗∗∗
Language -.004(-.02) -.006; -.002 −3.68
1 ∗∗∗
Scan Pattern Scene .07(.1) .06; .08 13.55
∗∗∗
Syntax .041(.058) .034; .047 12.65
∗∗∗
Semantics x Language .012(.015) .009; .015 8.31
∗∗
Semantics x Scene .031(.019) .01; .052 2.88
∗∗∗
Language x Scene .02(.016) .014; .025 7.08
∗∗∗
Semantics x Syntax -.028(-.02) -.036; -.019 −6.25
∗∗∗
Language x Syntax .021(.059) .019; .024 15.23
∗∗∗
Scene x Syntax .034(.024) .017; .052 3.84
∗∗∗
Semantics x Language x Scene -.035(-.013) -.047; -.023 −5.82
Semantics x Syntax x Scene .045(.015) .01; .08 2.53
2 ∗∗∗
Scan Pattern (shuffled) (Intercept) .22(0) .22; .22 2742.48
∗∗∗ ∗∗ ∗
p < .001 ; p < .01 ; p < .05
Formula: (Intercept) + Semantics + Language + Scene + Syntax + Semantics x Language + Semantics x Scene + Language x Scene
+ Semantics x Syntax + Language x Syntax + Scene x Syntax + Semantics x Language x Scene + Semantics x Syntax x Scene + (0 +
Semantics — Scene Pair) + (0 + Semantics — Participant Pair) + (0 + Syntax — Scene Pair) + (0 + Syntax— Participant Pair) +
(0 + Language — Scene Pair) + (0 + Language — Participant Pair) + (0 + Scene — Scene Pair) + (0 + Scene — Participant Pair)
Formula: (Intercept)
Moreover, it is critical to determine how particular eye-movement routines reflect
the composition of specific semantic structures. Scene descriptions in our study did
not tap into precise compositional processes such as symmetric relationships [62], the-
matic roles assignment in agentive verbs [71] or ergative verbs, which emphasise the
object role in an action (see [72] for an example on gestures). Developing experimen-
tal designs that elicit descriptions involving actions, agents, and patients could yield a
more comprehensive understanding of the language of vision underpinning event rep-
resentation and how patterns of overt attention may reflect it. This could build on
relevant work using the visual world paradigm (e.g., [12]), extending it to typologically
different languages.
Computationally, the current study did not directly investigate the real-time align-
ment between visual and linguistic processing, as we focused on the role of similarity
in the two information streams, using sentence similarity as a predictor of scan pattern
similarity. However, in the past decade, there have been rapid advances in represent-
ing visual information using formalisms previously applied to linguistic information
(e.g., dependency grammars, [73]), leading to new developments in multimodal mod-
elling [74], with applications to tasks such as image captioning [75], text-to-image
generation [76], or sign language recognition [77] (see [78] for a review of state of the
art).
An essential aspect of our study was the use of linguistic embeddings. Word embed-
dings – dense vectors trained to represent the contextual properties of words – were
first introduced by Mikolov et al. [79]. The embeddings of similar words (in terms of
which other words they co-occur with) are close together in vector space. Therefore,
measures such as dot product or cosine can compute word similarity. Embeddings have
been generalised from words to sentences, and multilingual embedding models have
been developed. In this study, we deployed the Universal Sentence Encoder (USE)
[60], which enabled us to compare the meaning of sentences across different languages,
abstracting away from the syntactic realisation of these sentences. While we make no
claims about the cognitive plausibility of the specific sentence embedding model we
have employed, we would like to note that linguistic embeddings have a long history
as models of human language processing (for an overview, see [80]). More generally,
neural networks, typically the modelling framework used to compute word or sen-
tence embeddings, are a classic tool in cognitive science, going back at least to the
Parallel Distributed Processing framework [81]. On the other hand, neural network
models have been criticised because they can be uninterpretable “black boxes”, not
constrained by biological realism, engineered to optimise performance in specific tasks,
and requiring learning regimes that are unrealistic compared to humans (see [82] for
a discussion focused on the visual system). Yet, other researchers disagree with these
statements, instead advocating neural nets as credible models of human cognitive
processes, including vision [83, 84], memory [85], and linguistic processing [86, 87].
Our results indicated that semantic similarity predicted scan pattern similarity
across languages, while syntax played a more limited role. The effect of syntax was
restricted to comparisons within the same scene context and predominantly when sen-
tences originated from the same language. This suggests a representational hierarchy
where semantics guides the sequence of eye movements related to verbalising meaning,
while syntax fine-tunes these responses based on language and context. These adjust-
ments likely occur at the syntax-semantics interface, as proposed by cognitive linguists
[5]. However, they seem to play a secondary role compared to the initial meaning struc-
ture provided by a pre-linguistic language of vision (see [88] for evidence on artificial
language learning). Our syntactic similarity measure was sequential and based on a
simple part-of-speech model. Future studies could include dependency structures to
capture deeper syntactic connections. Comparing dependency graphs may offer new
insights into how syntax influences eye movements for speakers of the same language
or within the same visual context.
Moreover, though diverse, the range of languages investigated in our study needs to
be expanded. In follow-up work, examining a more comprehensive array of languages,
especially those with diverse linguistic features, would be essential to strengthen the
cross-linguistic validity of our findings. Linguistic diversity is also a heavily debated
topic, which has at its core the relativism hypothesis by Sapir-Whorf, positing that
language has a role in structuring our thoughts [89–91]. Our findings indicate that
the connection between thought processes and perception may be more robust than
the connection between thought and language [7, 92], even if they don’t rule out the
possibility that language plays a critical role in shaping thought [93–96]. More research
is needed to investigate languages with distinct grammatical structures (see [97] for
a world atlas), shedding light on the impact of culturally specific experiences on the
development and expression of the language of vision.
Altogether, our study suggests that the visual system employs structured meaning
representations, as predicted by the language of thought hypothesis, and that these
representations can be interpreted through patterns of overt attention. These patterns
are shared across languages, even when their surface realisations differ (e.g., subject-
verb-object vs. subject-object-verb). This finding critically shapes our understanding
of the compositional mechanisms responsible for underlying cognitive processing across
multiple modalities.
Methods
In this eye-tracking study, participants were asked to describe photo-realistic pic-
tures of indoor scenes verbally. Before they saw each visual scene, participants were
prompted with a cue which referred to an object present in the scene. We collected
participants’ eye movements from when the scene appeared on the screen, capturing
the planning process and verbalisation of an utterance.
Design
The experimental design crossed scene Clutter (Minimal vs. Cluttered) and the
Animacy of the visual target (Animate vs. Inanimate). Moreover, the experimental
materials were designed such that each cue word could ambiguously refer to two visual
objects in the scene. In this study, however, we are not interested in these experimen-
tal manipulations, as the influence of factors such as clutter, animacy, and ambiguity
is tangential to the theoretical objectives of this work and has already been examined
elsewhere [56].
Participants
Seventy-four participants (24 speakers of British English, 13 Female, age = 23.08±3.99;
28 speakers of European Portuguese, 20 Female, age = 20.83 ± 3.39; 20 speakers
of Japanese, 17 Female, age = 23.65 ± 4.30) took part in the study. The English
speakers were all students of the University of Edinburgh and paid five pounds for
their participation. The data for this sample has already been published [59]. The
Portuguese and Japanese speakers were students at the University of Lisbon and took
part as unpaid volunteers. The Japanese speakers resided for an average of 5.87 ± 4.70
months in Portugal before data collection, which is a relatively short time. It makes
it unlikely that they had developed attrition of their native language.
Power analysis to estimate the sample size was computed using the approach by
[98], designed to handle linear-mixed effects models. Specifically, we estimated the
observed power from the data published in [59] by fitting a linear mixed model pre-
dicting scan pattern similarity as a function of sentence similarity, as computed in
the original paper. We aggregated over participants (introduced as random effects)
because the simulations were not computationally tractable on a standard PC other-
wise (i.e., from 123,256 data points to 576 data points). Even after such a substantial
reduction of data points, we obtained an estimated power of 100% (CI = 99.63%–100%
with an effect size of .2 that scan pattern similarity is predicted by semantic simi-
larity. Moreover, simulations to explore the trade-off between sample size and power
indicate that we need 21 participants to have a power of 88% (CI = 86.68%–90.69%)
to detect a fixed effect of semantic similarity with an effect size of .2.
The study followed the 1964 Declaration of Helsinki, and the ethics committees of
the Universities of Edinburgh and Lisbon approved the experimental protocol and its
material before data collection. All participants gave written informed consent at the
start of the experimental session.
Material and Apparatus
Seventy-two photo-realistic scenes were created, starting from a bare scene and past-
ing the target and distractor objects into it. Of these scenes, 24 were experimental,
i.e., crossed the four conditions mentioned in the Design section, while 48 were used
as fillers. Since the primary goal of this study is to compare with the results pub-
lished in [59], our analyses will focus on the eye movements and descriptions associated
c521d5bb6069496a9149060aa34edc38 to access the scenes, the data and script to
replicate the inferential statistics).
The eye-movement data of the English speakers were acquired using an EyeLink II
head-mounted eye-tracker (SR Research Ltd., Toronto, Canada) at a sampling rate of
500 Hz. Images were displayed on a 21” Multiscan monitor at a resolution of 1024×768
pixels. Speech data was recorded with a lapel microphone.
Eye-movement data of the Portuguese and Japanese speakers were acquired
through an SMI IVIEW X HI-SPEED tower-mounted eye-tracker (SensoMotoric
Instruments, Teltow, Germany) at a sampling rate of 1,250 Hz. Images were presented
on a 19-inch LG Flatron L194ws LCD screen at a resolution of 1920×1080, and verbal
responses were recorded using a standard computer microphone placed on a desk sup-
port. Only the dominant eye was tracked in both experimental setups, as determined
by a simple parallax test.
Procedure
Each trial began with the presentation of a cue word at the centre of the screen for
a duration of 750 ms. Immediately after, the scene was shown, and sound recording
was activated. Participants could take as long as they wanted to describe the target
object within the scene and were free to start their description as soon as they felt
ready. When they decided their description was completed, they pressed a button on
a response pad, and the subsequent trial was shown. The experiment was explained
using written instruction, and participants were familiarised with the task at the start
of the session on four practice trials. The experimental session took approximately 45
minutes to complete.
Similarity Measures
This study examines whether scan pattern similarity predicts sentence similarity
within and across languages. To implement this, we need two similarity measures: one
for the scan patterns and one for the semantics of the associated sentences. To assess
the role played by syntax, we need a further similarity measure for this variable (refer
Scan pattern similarity
Raw eye-movement data was first parsed into fixations and saccades using the Data
Viewer SR software for the data collected using the EyeLink II (English sample) and
the BeGaze SMI software for the data collected using the SMI system (Portuguese
and Japanese samples). Both algorithms discriminate fixations and saccades based on
the velocity profiles of raw x- and y-coordinates. We applied the default settings in
both cases.
Then, we mapped fixations of each trial into a categorical sequence of attended
locations, i.e., a scan pattern. We defined a grid of 6 × 4 equally sized squares. Each
cell of the grid was marked with a categorical identifier (e.g., “aa”), and fixations were
assigned to consecutive windows of 25 ms labelled with the categorical identifiers of
detail). Then, to measure the similarity between the scan patterns, we needed a metric
robust to variability in the length of the scan patterns (participants’ descriptions
differ in length) while preserving their sequential nature (refer to Appendix A for
corroborating results obtained by mapping fixations onto visual objects rather than on
the grid). We adopted the same approach as our previous work [59], using the Longest
Common Subsequence algorithm (LCS, [100]), which is a classic dynamic programming
technique used to find the longest subsequence that two categorical sequences have in
common, i.e., the ordered scan pattern’s common subsequence in our case.
The LCS algorithm works in three steps. First, it creates a 2D matrix C with
dimensions (m + 1) × (n + 1), where m and n are the lengths of the two sequences
and the extra row and column at index 0 are initialised with zero. Then, it fills the
of C[i][j] is updated as C[i][j] = C[i − 1][j − 1] + 1. Otherwise, the value of C[i][j]
is given by the maximum value of the cell above C[i − 1][j] and the cell to the left
C[i][j − 1]: C[i][j] = max(C[i − 1][j], C[i][j − 1]). This way, LCS evaluates all possible
sequence 2 is “BDCABC”, then the longest subsequence they have in common will be
“BCAB”. Similarity is then obtained as the ratio between the length of the LCS and
the geometric mean of the lengths of the two sequences.
Semantic similarity
To measure the semantic similarity between two sentences, we use the Universal Sen-
tence Encoder (USE, [60, 101]), a transformer-based language model that embeds
sentences into a 512-dimensional vector space. USE embeddings represent the distri-
butional meaning of sentences, in analogy to how the meaning of words is defined by
Fig. 5 Similarity matrices for the semantic similarity (dot product) of a selected sentence sample
obtained through the Universal Sentence Encoding. To the left, we show similarity scores for sentences
in the same language (English), whereas to the right, we show the values observed when comparing
different languages (English, Portuguese). All reported sentences were generated within the first (top-
word embeddings such as word2vec [102]. We can then compare the semantic similarity
of two sentences by computing the similarity of the two embedding vectors associated
with the sentences. Several distance measures have been used in the literature; here,
we will use the dot product (cosine similarity yields identical results as they positively
correlate; r(1, 513, 024) = .97, p < .0001).
Crucially, USE embeddings are sensitive to word order, so the sentence embedding
of Peter likes his dog and his dog likes Peter are different, reflecting their difference in
semantics. Furthermore, USE embeddings are multi-lingual, so the USE model makes
it possible to obtain embeddings for sentences in multiple languages from the same
semantic space. Therefore, we can compare sentences such as Peter likes his dog and
O Peter gosta do seu cachorro by computing the similarity between their USE vectors,
even though the first sentence is in English, and the second one is in Portuguese (refer
languages, including English, Portuguese, and Japanese. We used the pre-computed
embedding models available with TensorFlow.
Syntactic similarity
We used a simple approach to computing syntactic similarity. We turned each sen-
tence into a sequence of part-of-speech (PoS) labels. For example, the sentence Peter
likes his dog becomes the PoS sequence PROPN VERB PRON NOUN (proper noun, verb,
pronoun, noun). This represents a sentence’s surface syntax (including word order)
while abstracting from the individual lexical items. If we assume a set of PoS tags
that is the same for all languages, then we can compare syntactic structures across
languages. For example, the Portuguese version of this sentence, O Peter gosta do
seu cachorro becomes DET PROPN VERB ADP PRON NOUN (determiner, proper noun,
verb, adposition, pronoun, noun). The PoS sequences associated with two sentences
can then be compared using the Longest Common Subquence algorithm, which will
return a similarity score, as explained above. This works if the two sentences are in
the same language and if the two are in different languages, provided that the set of
PoS labels is universal.
In the current work, we used the SpaCy models for English, Portuguese, and
Japanese to automatically PoS label the sentences participants generated in these
three languages. SpaCy is an open-source, highly accurate PoS tagger. SpaCy uses
the inventory of PoS labels stipulated by the Universal Dependencies framework [103],
which was developed as an annotation scheme for syntactic structures that is universal
across languages and stipulates a set of 18 common part of speech labels. The Univer-
sal Dependencies annotation scheme has successfully annotated 200 datasets across
150 languages with dependency trees and part-of-speech labels.
Data processing and statistical inference
For each participant (N = 74), we considered the 24 sentences and associated scan
patterns generated for the experimental trials, which gives us a total of 1,776 (74 × 24)
trials. Of those unique trials, we excluded 15 scan patterns because of poor quality
data and 21 sentences that were too short or too long (i.e., bottom 3 and top 97
percentile of their distribution, corresponding to sentences smaller than 4 words or
longer than 109 words). As similarities are computed pairwise, i.e., every sentence (or
scan pattern) is compared with every other sentence (or scan pattern), we obtained a
total of N = 1, 513, 026 data points contributing to the analysis. Statistical inference
used the linear-mixed effect modelling framework implemented by the lme4 package
in R [104]. Our dependent variable is scan pattern similarity, which we predict in a
single model as a function of (a) the semantic and syntactic similarity of sentences
(b) whether the similarity score is within (or between) scenes (with between as the
reference level) and (c) whether the similarity score comes from comparing sentences
(or scan pattern) of the same (or different) language (with different as the reference
level).
We also test what happens to scan-pattern similarity if we shuffle semantic and
syntactic similarity using the same model structure. This control analysis assesses
whether the association between a sentence’s semantic and syntactic information mod-
ulates the associated scan-pattern similarity rather than being a result of an artefact
independent of which scan pattern a sentence is paired with.
The random effects considered in our models are Pairs of Participants (N = 1, 758)
and Pairs of Scenes (N = 9, 216). Initially, we build models with a complete fixed
effect structure, i.e., all possible main effects and interactions, and a maximal random
structure, i.e., evaluating slopes of the fixed effects for each random variable (only on
the main effects). Then, we reduce the model backwards using the step() function
from the lmerTest package [105] to obtain parsimonious models that best fit the data
which can be used to reconstruct the size and reliability of the estimates [107], its
io/models/ja.
t-values and p-values based on asymptotic Wald tests, with asterisks indicating their
level of significance (e.g., ∗ = p < 0.05).
Supplementary information. The article has an appendix, made available for
review directly in the manuscript and will be moved to supplementary information if
requested.
Acknowledgments. This work was partly supported by the ERC grant “Syn-
chronous Linguistic and Visual Processing” awarded to FK under grant agree-
ment 203427 and by the European Union (NextGenerationEU) grant (PRIN 2022;
Codice progetto 2022APAFFN - CUP: B53D2301448 0001) awarded to MIC.
Appendix A Replication of core result with
scan-patterns mapped onto objects
rather than grids
In this section, we corroborate our core finding, i.e., scan-patterns similarity is pre-
dicted by semantic similarity within and across languages, but this time mapping
sequences of fixations onto objects rather than using regularly spaced grids. This
approach aligns with the original study by [59] upon which the current study is built.
Each scene was fully labelled using the LabelMe toolbox [108] by drawing polygons
around objects and assigning them an identifier word. Then, polygon coordinates were
used to map fixations falling inside of it and so assign a label to each fixation. The
smallest object containing the fixation was prioritised when objects were inside other
objects (e.g., a person’s hand is part of their body). On average, each image had 28.65
(SD = 11.30) objects. A scan pattern represents a categorical series of object labels,
showing the temporal order in which objects were looked at. Similarities between scan
patterns and sentences were computed following the methods detailed in the Method
section. Statistical inference was also performed using linear-mixed effect models, and
scan patterns mapped onto grids.
obtained by mapping scan patterns into grids. We confirm that the semantic similarity
of sentences predicts scan patterns and that this effect is more robust within the same
language or scene. We also replicate that scan patterns are more similar within the
same scene, especially for speakers of the same language. This result indicates that
even when fixations are mapped to objects rather than spatial locations, their inherent
sequential order explicitly codes for the meaning of associated sentences.
objects) as a function of Semantics (semantic similarity between the sentences associated with the scan
patterns; dot-product over sentence embeddings), Language (sentences use the same or a different language;
different as reference Level) and Scene (sentences describe the same or a different visual scene; different as
reference level). The random variables introduced as intercept and slopes were participant pair (1758) and
scene pair (9216).
Dependent Variable Predictor β(Std. β) CI (2.5 %; 97.5 %) t-value
∗∗∗
(Intercept) .11(0) .11; .12 96.83
∗∗∗
Semantics .02(.024) .017; .028 8.69
∗∗∗
Language -.003(-.013) -.005; -.002 −6.26
∗∗∗
Scan Pattern (on objects) Scene .2(.23) .19; .21 32.21
∗∗∗
Semantics x Language .03(.031) .028; .033 24.11
∗∗∗
Semantics x Scene .06(.032) .049; .079 8.61
∗∗∗
Language x Scene .008(.005) .003; .013 3.08
Semantics x Language x Scene 0.006(.001) -0.004; 0.016 1.17
∗∗∗ ∗∗ ∗
p < .001 ; p < .01 ; p < .05
Fig. A1 Scan pattern similarity (computed over objects) as a function of Semantic similarity (with
dot-product over sentence embeddings as similarity measure). Within each panel, we compute the
mean of scan pattern similarity (y-axis) over regularly spaced bins of the semantic similarity (x-
axis, from 0 to 1 in bins of .1) and overlay the model fit (as a loess line). Line colour and symbol
type indicate whether the similarities have been computed within the same scene (blue triangle) or
between different scenes (red circle). Languages are compared across the panels. The top panels show
similarities computed within the same language (English; Portuguese; Japanese), while the bottom
panels across different languages (English–Portuguese; English–Japanese; Portuguese–Japanese).