Running head: Effect Sizes in Individual-Differences Research
Beyond Increasing Sample Sizes: Optimizing Effect Sizes in Neuroimaging Research
on Individual Differences
Colin G. DeYoung1+, Kirsten Hilger2+, Jamie L. Hanson3, Rany Abend4, Timothy A. Allen5,
Roger E. Beaty6, Scott D. Blain7, Robert S. Chavez8, Stephen A. Engel9, Ma Feilong10, Alex
Fornito11, Erhan Genç12, Vina Goghari13, Rachael G. Grazioplene14, Philipp Homan15, Keanan
Joyner16, Antonia N. Kaczkurkin17, Robert D. Latzman18, Elizabeth A. Martin19, Aki
Nikolaidis20, Alan D. Pickering21, Adam Safron22, Tyler A. Sassenberg1, Michelle N. Servaas23,
Luke D. Smillie24, R. Nathan Spreng25, Essi Viding26, Jan Wacker27
+ shared first authorship
1 Department of Psychology, University of Minnesota, Minneapolis, MN USA
2 Department of Psychology I, Würzburg University, Würzburg, Germany
3 Department of Psychology, University of Pittsburgh, Pittsburgh PA USA
4 School of Psychology, Reichman University, Herzlia, Israel
5 Department of Psychiatry, University of Pittsburgh, Pittsburgh PA USA
6 Department of Psychology, Pennsylvania State University, PA USA
7 Department of Psychology, Ohio State University, OH USA
8 Department of Psychology, University of Oregon, Eugene, OR USA
9 Department of Psychology, University of Minnesota, Minneapolis, MN USA
10 Department of Psychology, Dartmouth College, Hanover, NH USA
11 School of Psychological Sciences, Monash University, Australia
12 Leibniz Research Centre for Working Environment and Human Factors (IfADo), Technical
University, Dortmund, Germany
13 School of Graduate Studies, University of Toronto, Toronto, Canada
14 Department of Psychiatry, Yale University, New Haven, CT USA
Effect Sizes in Individual-Difference Research 2
15 Department of Adult Psychiatry and Psychotherapy, University of Zurich, and Neuroscience
Center Zurich, University of Zurich and ETH Zurich, Zurich, Switzerland
16 Department of Psychology, University of California, Berkeley, Berkeley, CA USA
17 Department of Psychology, Vanderbilt University, Nashville, TN USA
18 Takeda Pharmaceuticals, Cambridge, MA USA
19 Department of Psychological Science, University of California, Irvine, CA USA
20 Child Mind Institute, New York, NY, USA
21 Department of Psychology, Goldsmiths, University of London, London, UK
22 Department of Psychiatry and Behavioral Sciences, Johns Hopkins University School of
Medicine, Baltimore, MD USA
23 University of Groningen, University Medical Center Groningen, Department of Psychiatry,
Interdisciplinary Center for Psychopathology and Emotion regulation, Groningen, The
Netherlands
24 School of Psychological Sciences, University of Melbourne, Melbourne, Australia
25 Montreal Neurological Institute, Department of Neurology and Neurosurgery, McGill
University, Montreal, QC Canada
26 Clinical, Educational, and Health Psychology Research Department, University College
London, London, UK
27 Faculty for Psychology and Human Movement Science, University of Hamburg, Hamburg,
Germany
WORD COUNT: 4994
ORCIDs:
Colin G. DeYoung: 0000-0001-5621-1091
Kirsten Hilger: 0000-0003-3940-5884
Rany Abend: 0000-0003-0022-3418
Timothy A. Allen: 0000-0002-3512-9475
Roger E. Beaty: 0000-0001-6114-5973
Scott D. Blain: 0000-0002-2751-3629
Robert S. Chavez 0000-0001-7877-299X
Stephen A. Engel: 0000-0002-5241-6433
Effect Sizes in Individual-Difference Research 3
Ma Failong: 0000-0002-6838-3971
Alex Fornito: 0000-0001-9134-480X
Erhan Genc: 0000-0001-5183-7415
Vina Goghari: 0000-0003-1120-9152
Rachael Grazioplene: 0000-0001-8708-4531
Philipp Homan: 0000-0001-9034-148X
Keanan Joyner: 0000-0002-5759-0879
Antonia N. Kaczkurkin: 0000-0002-0943-3094
Robert D. Latzman: 0000-0002-1175-8090
Elizabeth A. Martin: 0000-0001-8893-1667
Aki Nikolaidis: 0000-0001-8960-1934
Alan D. Pickering: 0000-0002-7301-5321
Adam Safron: 0000-0002-3102-7623
Tyler A. Sassenberg: 0000-0002-2890-5560
Michelle N. Servaas: 0000-0002-9222-7397
Luke D. Smillie: 0000-0001-5148-8358
R. Nathan Spreng: 0000-0003-1530-8916
Essi Viding: 0000-0001-8468-8874
Jan Wacker: 0000-0001-8627-7294
Corresponding authors:
Kirsten Hilger, University of Würzburg, Marcusstraße 9-11, Germany.
Email: kirsten.hilger@uni-wuerzburg.de Phone: +49 931 31-81141
Colin DeYoung, Dept. of Psychology, 75 East River Rd, Minneapolis, MN 55455 USA.
Email: cdeyoung@umn.edu Phone: +1 612 624 1619
Effect Sizes in Individual-Difference Research 4
Abstract
Linking neurobiology to relatively stable individual differences in cognition, emotion,
motivation, and behavior can require large sample sizes to yield replicable results. Given the
nature of between-person research, sample sizes at least in the hundreds are likely to be
necessary in most neuroimaging studies of individual differences, regardless of whether they are
investigating the whole brain or more focal hypotheses. However, the appropriate sample size
depends on the expected effect size. Therefore, we propose four strategies to increase effect sizes
in neuroimaging research, which may help to enable the detection of replicable between-person
effects in samples in the hundreds rather than the thousands: (1) theoretical matching between
neuroimaging tasks and behavioral constructs of interest; (2) increasing the reliability of both
neural and psychological measurement; (3) individualization of measures for each participant;
and (4) using multivariate approaches with cross-validation instead of univariate approaches. We
discuss challenges associated with these methods and highlight strategies for improvements that
will help the field to move toward a more robust and accessible neuroscience of individual
differences.
Effect Sizes in Individual-Difference Research 5
1. Introduction
We are researchers who use neuroscientific methods to investigate psychological
individual differences. Humans differ in their thoughts, feelings, and behaviors, and such
variations among individuals are neither random nor entirely determined by the current situation.
Many individual differences, described with terms such as traits, dispositions, attitudes, abilities,
and symptoms, are relatively stable over time and are caused by a mixture of genetic and
environmental influences (Polderman et al., 2015). We will use the term “traits” as a generic
descriptor for all such constructs. Trait levels represent the probability of particular thoughts,
feelings, and behaviors; they are relatively stable within individuals over time, reasonably
consistent in rank order between individuals, and typically observable in many situations. Many
trait measures are useful for predicting future behavior and important life outcomes (Deary,
2012; Soto, 2019). A long tradition of research on traits has focused on identifying their causes.
Regardless of the proportion of the distal causes of a trait that are genetic versus environmental,
trait differences must be caused proximally by differences in brain function, because the brain
governs behavior and experience.
Neuroimaging research increasingly investigates associations of psychological traits with
individual differences in brain structure and function (DeYoung et al., 2022; Hilger & Markett,
2021). Our aim in this article is to discuss how best to conduct neuroimaging research on
psychological individual differences in order to achieve robust, replicable results, in light of
recent debates about sample size (e.g., Grady et al., 2021; Marek et al., 2022; Spisak et al.,
2023). If samples are too small, estimation of parameters will be imprecise, and the chance of
detecting true effects as significant (i.e., statistical power) will be low. Precision and power both
depend crucially on sample size, and the fact that underpowered samples yield imprecise
estimates has an important consequence that is often overlooked in neuroimaging research (Nebe
Effect Sizes in Individual-Difference Research 6
et al., 2023): Not only are underpowered studies more likely (by definition) to yield false
negatives (Type II error, failing to detect a true effect) than adequately powered studies, but they
also yield a higher proportion of significant results that are false positives (Type I error,
detecting an effect that is not true) because the estimated effects fluctuate widely around the true
value. Thus, significant effects in small samples are often inflated or even completely spurious,
which contributes to the prevalence of unreplicable results in scientific publications (Yarkoni,
2009).
It is often said that power depends on three things: the significance criterion (a), the
effect size, and the size of the sample; however, power also crucially depends on the statistical
model being used. Neuroimaging research in general is often underpowered (Poldrack et al.,
2017; Szucs & Ioannidis, 2020), and this problem is amplified in research on individual
differences because the statistical models used to estimate between-person effects require larger
sample sizes than those for estimating within-person effects. When studying the function of the
average brain, as in typical research on task-evoked brain activity, one compares neural activity
in different conditions within the same individuals—in other words, participants serve as their
own controls—and this reduces noise. For example, to achieve 80% power to detect a simple
bivariate correlation of r = .20 as significant (a = .05; two-tailed) requires a sample of 194
participants, which is considerably larger than would be required to detect the same effect size as
a difference between conditions in typical within-person designs (e.g., 49 participants required
for a paired samples t-test with d = .41, a = .05, two-tailed).
Achieving sufficient power is additionally challenging in neuroimaging because many
statistical tests are often conducted within a single analysis. In the common case of univariate
brain-wide association studies (BWAS), this entails using values from voxels, vertices, or parcels
across the entire brain and testing for associations of psychological variables with each neural
Effect Sizes in Individual-Difference Research 7
value independently. Such multiple testing reduces power by effectively requiring a more
stringent significance (a) threshold for each individual test to maintain the same overall a. This
approach also increases the temptation to engage in selective reporting, in which some tests or
analyses are not reported, obscuring the true burden of multiple testing by reporting results that
are nominally but not actually significant. This kind of selective reporting within neuroimaging
studies increases publication bias—the tendency to report only significant effects—and has
contributed greatly to the proliferation of false positives and the resulting replication crisis
(Stanley, 2005).
All of this entails that sample sizes for individual-differences research in neuroimaging
need to be considerably larger than samples sizes traditionally used in this field. The big question
is, “How much larger?” Recently, an influential article argued that “thousands” of participants
are necessary for “studies of the associations between common inter-individual variability in
human brain structure/function and cognition or psychiatric symptomatology” (Marek et al.,
2022, p. 654). This important study demonstrated the limited power of common neuroimaging
approaches to individual-differences research and served as a clarion call to develop more robust
approaches for identifying associations between traits and neural variables. Here, we attempt to
answer that call by discussing potential solutions to this problem that might not require
thousands of participants. We argue that appropriate methods may allow replicable
neuroimaging research on individual differences with hundreds of participants.
The key question we consider is how to increase expected effect sizes, because larger
effect sizes require fewer participants to achieve the same statistical power. Fundamentally, the
motivation for claims that thousands of participants are necessary comes down to effect size.
Using three very large MRI samples, Marek et al. (2022) examined brain-wide associations of
structural parameters and resting-state functional connectivity with performance tests of
Effect Sizes in Individual-Difference Research 8
cognitive ability and questionnaire measures of features of psychopathology and observed that
the largest 1% of replicable univariate effects were between |r| = .06 and .16. It is worth
emphasizing that this means 99% of the replicable effects were even smaller than .06. If all
expectable between-person effect sizes were indeed this low, then it might be true that samples
in the thousands were always necessary, not only when conducting many statistical tests (though
of course this makes the problem more acute), but even when conducting more focused studies
that are not “brain wide.” However, the observations of Marek et al. do not necessarily
generalize to all individual-differences research in neuroimaging.
Expected effect sizes cannot be generalized from one set of methods to all others. Marek
et al. (2022) drew conclusions based on analyses using some of the most common methods in the
field, but these nonetheless represent only a small subset of available methods and some of them
are suboptimal. Here we discuss alternative methods, focusing on four categories of
methodological improvement designed to increase expected effect sizes and, therefore, to
increase power independently of sample size: (1) theoretical matching between tasks and trait
constructs, (2) improving measurement reliability; (3) individualization of measurement for each
participant, and (4) pivoting from univariate to multivariate analytic approaches. Our aim is not
merely to discuss arguments made by Marek et al. (2022), though we do address some of them
directly. Rather, our aim is to consider the broader issue of improving neuroimaging methods for
individual differences research.
2. Trait-Relevance: Matching fMRI Tasks to Trait Constructs
The first strategy we recommend to increase effect sizes concerns the modality of
neuroimaging assessments. Many neuroimaging studies of individual differences have relied on
structural MRI or resting-state fMRI data, but effect sizes may be larger in studies using
appropriate fMRI tasks (Finn, 2021). Structural and resting-state data have been widely used in
Effect Sizes in Individual-Difference Research 9
individual-differences research for multiple reasons. For one, it is assumed that any trait could
potentially be related to parameters derived from these imaging modalities because they do not
target any specific psychological content or processes. This makes it possible to study many
psychological traits in relation to the same structural and resting-state data, whereas data from
any particular task seem likely to be relevant to a more limited set of traits. Additionally, brain
structure and resting-state functional connectivity are sometimes assumed to be more trait-like
than task-evoked activity because they have been demonstrated to have adequate retest reliability
(Zuo & Xing, 2014) and are independent of the situational demands of any specific task (Hilger
& Markett, 2021). However, task-induced neural activity may be equally trait-like in its retest
reliability, given the right tasks (Fluornoy et al., 2024). The possibility of reliable signals from
task fMRI is consistent with the excellent reliability of various performance-based cognitive tests
(e.g., IQ tests) and with the common conceptualization of traits as tendencies to respond in
consistent ways to specific classes of stimuli (DeYoung et al., 2022). Choosing the right task can
provide the kind of stimuli that are particularly relevant to the processes underlying the trait in
question.
The potential relevance of structural and resting-state measures to any trait is both a
strength and a weakness. Neither modality directly assesses an aspect of brain function that is
transparently relevant to most psychological traits. (Also, brain structure is ultimately relevant to
psychological traits only to the degree that it influences function.) In contrast, task-based fMRI
may induce brain states directly relevant to the trait of interest whenever that trait is theorized to
reflect variation in psychological processes like those involved in the task. Identifying
appropriate tasks requires at least some minimal amount of theory regarding the processes
underlying the trait in question, and we encourage researchers to consider the theoretical
background of any trait they are studying (DeYoung et al., 2022). Theoretically-informed
Effect Sizes in Individual-Difference Research 10
research can potentially increase effect sizes by identifying likely associations among traits,
underlying psychological processes, and the brain systems that support them.
That task-based fMRI may lead to meaningful between-person effects has been shown in
studies considering task-induced neural activation (e.g., Tetereva et al., 2022) as well as in
studies focused on functional connectivity during tasks (e.g., Greene et al., 2018). In fact, task-
based brain–behavior associations are consistently stronger than resting-state based associations
in cross-validation studies of very large samples (Chen et al., 2022; Feilong et al., 2021; Greene
et al., 2018; Sripada et al., 2020). For example, working memory is well-established as a
cognitive function involved in general cognitive ability, and several of the studies cited in the
previous sentence show that associations of intelligence with neural functioning during working-
memory tasks are stronger than associations with resting-state or structural data.
Marek et al. acknowledged that fMRI task data may yield larger effect sizes than resting-
correlation between cognitive ability and activation of the dorsal attention network during a
working-memory task was .34. They dismissed this finding by characterizing working-memory
performance as a “confound” that needs to be controlled for in analysis (yielding a much smaller
correlation of .14), but the plausible causal arrangement of the three variables—activation of the
dorsal attention network, working-memory performance, and general cognitive ability—is not
one of confounding. Working-memory performance is a relatively stable trait strongly correlated
with general cognitive ability and thought to be a crucial process facilitating that ability (Kovacs
& Conway, 2016). Therefore, it should act as a mediator between neural activity and general
cognitive ability, rather than as a confound. A correlation of .34 can readily be detected in
samples considerably smaller than a thousand. We would not recommend assuming that task-
based effects in general will be this large for the purposes of power calculations, but any effect
Effect Sizes in Individual-Difference Research 11
larger than .16 is larger than all of the replicable univariate structural and resting-state effects
reported by Marek et al. (2022) and can be detected in hundreds of participants if the multiple
testing burden is not too high. We suspect that such effects may be relatively common, given the
right pairings of traits with fMRI tasks.
Despite our enthusiasm for task-based fMRI, we want to be clear that we are not
suggesting individual-difference researchers should abandon structural and resting-state
neuroimaging studies. The remaining three categories of strategies we endorse are applicable to
all neuroimaging modalities.
3. Improving Measurement Reliability
After deciding what question to address, an important strategy for increasing effect sizes
is to improve the reliability of both trait and neural measures, because the joint reliability of two
measures sets an upper bound on the possible strength of association between them (Nikolaidis
et al., 2022). Many recent articles have discussed reliability in the context of human
neuroscience (e.g., Haines et al., 2023; Nebe et al., 2023; Nikolaidis et al., 2022), so our
discussion here is not intended to be exhaustive. However, we will highlight some opportunities
for improving reliability for three different types of assessment: neural measures, behavioral
tasks, and questionnaires. (Additionally, our suggested methods in the next section also tend to
increase reliabilities of neural variables.)
The reliability of neural parameters can be improved by a variety of means, both in fMRI
data acquisition and in subsequent data processing and analysis. For acquisition, we recommend
the use of multi-echo sequences. Multi-echo fMRI significantly improves whole-brain temporal
signal-to-noise ratio and reduces signal drop-out in typically problematic regions along the
ventral-anterior surface of the brain (Kundu et al., 2017; Lynch et al., 2020). Further, it enables a
biophysically-based removal of noise from fMRI datasets during preprocessing because of the
Effect Sizes in Individual-Difference Research 12
known echo-time dependence of the blood-oxygen level dependent signal. This has been shown
to improve reliability substantially (Kundu et al., 2017; Lynch et al., 2020). More costly but also
effective is simply increasing the amount of fMRI data that is collected for each participant (Cho
et al., 2021; Noble et al., 2017). In data processing, reliability can be improved by modeling the
hierarchical structure of neural parameters, using machine learning methods, or generating
aggregates from multiple measures (Blair et al, 2022; Schubert et al., 2022).
In task-based fMRI, one pitfall to avoid is selecting regions of interest (ROI) for
individual-differences research by using group-level fMRI contrasts to identify regions where a
task significantly activates the brain relative to a control condition (DeYoung et al., 2022). The
problem with this approach is that group-level contrasts ensure identification of ROI where
interindividual variability is low enough to conclude that the task activates the brain similarly
across individuals. In other words, it minimizes individual differences in brain activity. The most
important brain regions for a given trait may be ones that are not significant at the group level,
precisely because different brains respond differently to the task in those regions. Because the
reliability of measures of individual differences depends on variability, focusing on robust
within-persons effects works against reliability at the between-person level, a phenomenon
described as the “reliability paradox” (Hedge et al., 2017).
The reliability paradox applies not only to neural variables, but also to behavioral
variables extracted from tasks. Some tasks, such as those that make up standard intelligence
tests, have been designed specifically to optimize the assessment of individual differences, but
many experimental tasks used in neuroimaging have not. Instead, those tasks have usually been
designed to minimize between-person variability to aid in studying typical function as the group
average in within-person designs. Researchers should investigate the degree to which tasks used
in neuroimaging are reliable as measures of individual differences and take steps to improve
Effect Sizes in Individual-Difference Research 13
them (Blair et al., 2022). Sometimes better options are already available; for example, new
versions of the Stroop and Flanker tasks have recently been designed to improve measurement of
individual differences, and they show excellent internal consistency and retest reliability
(Burgoyne et al., 2024).
For questionnaires, reliability is most often measured as internal consistency, but, for
constructs that are conceived as relatively stable features of individuals, retest reliability (i.e., the
degree to which a sample’s rank order is consistent over time) is an even more relevant metric
(Nikolaidis et al., 2022). Another important consideration is that measures may differ in their
reliability across the range of the variables they are assessing, which requires more sophisticated
methods to detect. Although Marek et al. reported adequate reliability for their measure of
psychopathology, analysis of the same data using item response theory showed that it was
inadequate for assessing individual differences in the lower range of the scales, where most
healthy individuals score (Tiego et al., 2023). Measures should be investigated to make sure they
are appropriate for the population being studied.
Additionally, although it may be tempting to use short forms of questionnaires, longer
measures generally have better validity and reliability (Credé et al., 2012). Using multiple
informants is also valuable, as they provide incremental validity and reduce the biases introduced
by individual raters. This principle can also be extended to the use of multiple measurement
modalities for the same trait (e.g., questionnaire and behavioral task), though identifying
adequately parallel measurements across modalities can be challenging (Joyner & Perkins,
2023). Whenever multiple measures of the same variable are collected, measurement can often
be improved by modeling constructs as latent variables representing the shared variance of
multiple indicators.
Effect Sizes in Individual-Difference Research 14
4. Individualization of Measures
The third strategy we recommend to increase effect sizes is to improve measurement
through a family of procedures known as individualization. A serious measurement challenge
arises from the uniqueness of every human brain. Standard procedures to align individual brains
to a common anatomical template cannot handle variations in which an anatomical feature is
present or absent. For example, in anterior cingulate cortex (ACC), people vary in whether they
have only one sulcus or two, and the second sulcus (known as the paracingulate sulcus; PCS), if
present, can be very short, or it can extend the entire length of the ACC. Warping a brain with a
PCS to a template without one (or vice versa) causes inaccuracy in subsequent comparisons
across individuals because presence or absence of PCS has important consequences for the
brain’s functional and structural organization (Amiez et al., 2018; Fornito et al., 2008). Such
structural idiosyncrasies can be taken into account to improve measurement (Miller et al., 2021;
Voorhies et al., 2021).
Not only do different brains differ in structure, but also the localization of functions
relative to the brain's anatomical landmarks differs from person to person. This means that, even
if structural alignment were perfect, comparing brains based merely on location would remain
suboptimal. Neuroimaging studies often use canonical brain atlases or parcellations to identify
ROI and define brain networks (Moghimi et al., 2022), and these schemes often rely in part or
entirely on functional information to parcellate the cortex (which is appropriate given the
primacy of function for psychology). However, using the same standard parcellation for all
participants means that the parcel boundaries will not precisely reflect the relevant functional
boundaries for any participant (Mueller et al., 2013; Chong et al., 2017).
To overcome this problem, we recommend methods that individualize standard
parcellations by optimizing the boundaries of each parcel for each participant. These include
Effect Sizes in Individual-Difference Research 15
group prior individualized parcellation (GPIP; Chong et al., 2017) and multi-session hierarchical
Bayesian modelling (MS-HBM; Kong et al., 2019). Both of these methods have been found to
increase effect sizes to a small but meaningful degree in individual-differences research, as
compared to using the same atlas or parcellation scheme without individualization (Kong et al.,
2021; Sassenberg et al., 2023). Notably, individualized parcellation is preferable to using dual
regression following independent components analysis (an earlier strategy to deal with the same
problem) because, unlike dual regression, individualized parcellation retains the benefit of
canonical atlases in allowing comparison of the same parcels across individuals and samples
(DeYoung et al., 2022).
Individualization can be taken even a step further than shifting boundaries of parcels, to
identifying different collections of voxels that encode the same information in different brains.
Even within a given brain region that is well aligned through GPIP or MS-HBM, information
may be encoded differently in different people. A technique known as hyperalignment identifies
different sets of voxels with similar patterns of neural activity for each participant and treats
them as the relevant neural unit of analysis. Hyperalignment increases effect sizes relative to
other methods (Feilong et al., 2021; Haxby et al., 2020), considerably more than the increase
generated by individualized parcellation, though both methods can be used together.
An older method of individualization is the use of functional localizers, which are fMRI
tasks that reliably activate a particular brain system and thus can be used to identify a specific
region or regions activated by that task in each participant before correlating parameters derived
from those regions with measures of psychological traits. This is a powerful method for theory-
driven research and may be especially valuable for focal hypothesis testing that maximizes
power by minimizing the multiple-testing burden. However, it is also limited in that it can only
identify regions that are relevant to the particular task used, rather than being able to
Effect Sizes in Individual-Difference Research 16
individualize the whole brain. Individualized parcellation using the methods described above can
match the functional localization of neural activity by tasks, even when the individualized
parcellation is derived from resting-state data (Chong et al., 2017; Kong et al., 2021; Uddin et
al., 2023).
Finally, people vary not only in the spatial layout of brain function but also in the timing
of brain function as measured by fMRI. An early fMRI study revealed marked variability in the
hemodynamic response function (HRF) across individuals and brain regions (Aguirre et al.,
1998). Although fMRI research typically assumes a canonical HRF, methods are available for
estimating the idiosyncratic shape of the HRF for each participant separately (Singh et al., 2020;
2022).
Up to this point, we have focused on individualizing neural data, but it is worth noting
that individualization of psychological measurements is also sometimes possible, for example
using computerized adaptive testing (Wainer et al., 2000) or fitting computational models to each
participant’s trial-by-trial task data. The latter is useful in part because different individuals can
employ different strategies when performing the same task. For example, studies using learning
tasks can estimate the degree to which participants engage in model-free versus model-based
learning (e.g., Kool et al., 2017).
5. Moving from Univariate to Multivariate Approaches
Our fourth suggestion is to increase effect sizes by transitioning from univariate to
multivariate analytic approaches. Multivariate analyses involve using multiple variables to
predict the criterion variable, and follow naturally from the premise that psychological traits are
determined by many neural parameters. These variables could all be of the same type (e.g.,
activation values of individual voxels, as in multi-voxel pattern analysis, or parameters derived
from resting-state EEG; Thiele et al., 2023), or they could involve parameters from multiple
Effect Sizes in Individual-Difference Research 17
measurement modalities, such as structural and functional MRI, from which parameters can be
combined in the statistical model (Jiang et al., 2020; Rasero et al., 2021).
It is unsurprising that multivariate models using many predictors can yield better overall
predictions than univariate models using a single predictor. In a commentary on the work of
Marek et al. (2022), Spisak et al. (2023) showed that multivariate BWAS effects are larger than
univariate effects and can be replicable even when identified in smaller samples. Although they
suggest these samples can be as small as 75, we would not recommend samples that small, given
the resulting lack of precision of parameter estimates. Indeed, Spisak et al.’s analyses show that
multivariate effects in such small samples may be replicable in the sense of producing a
significant effect in the same direction, but the effect size in replication is often substantially
different. Nonetheless, they showed that samples of 300-500 generally yield multivariate effects
reasonably replicable in magnitude as well as significance, when they are cross-validated to
prevent overfitting.
Tervo-Clemmens et al. (2023) rejected Spisak et al.’s (2023) conclusion, but their
exchange makes it clear that the argument between the two research teams is based largely on a
terminological disagreement about the proper way to use the phrase “out of sample.” Marek et al.
(2022) and Tervo-Clemmens et al. (2023) use it to refer to any test of a model in data not used to
train the model, whereas Spisak et al. use it to refer to testing a model in an entirely new dataset.
These uses diverge in k-fold cross-validation because this method involves iteratively training
the model on a subset of the data and then examining its performance in the rest of the data. K-
fold validation is done within a single sample, with the resulting effect size being the average of
the iterated correlations between predicted values and true values in the holdout data. This effect
size is “out of sample” in Marek et al.’s sense, but “in sample” in Spisak et al.’s. From our
perspective, Spisak’s perspective more directly addresses the question of how many participants
Effect Sizes in Individual-Difference Research 18
are needed in total to identify effects with sufficient precision that they will be replicable in
subsequent studies. In other words, how large must a single sample be to yield accurate results
after cross-validation in that sample? The exchange between Spisak et al. and Tervo-Clemmens
et al. makes clear that replicable results considerably larger than |r| = .16 can be achieved using
multivariate prediction in samples in the hundreds rather than the thousands, as long as the effect
sizes are estimated using appropriate cross-validation procedures, rather than in training data.
This conclusion is supported by the existing literature, especially in relation to cognitive
ability. One study of the same sample used by Spisak et al. found that neural activation during
various tasks predicted general cognitive ability in multivariate models with r ≈ .30 (Sripada et
al., 2020). Similarly high multivariate correlations between task connectivity and cognitive
ability were found in another large sample (Chen et al., 2022). Multivariate effects can be even
larger when combined with individualization approaches. In the same sample analyzed by Spisak
et al. (2023), Feilong et al. (2021) were able to predict cognitive ability using estimates of
functional connectivity based on hyperalignment, with average multivariate effect sizes of r =
.53 for task data and r = .44 for resting-state data.
When transitioning to multivariate analysis, it is important to keep in mind that not all
multivariate methods are equally good, and some yield larger effects than others (Spisak et al.,
2023). Even with cross-validation, effects are likely to be larger if multiple modalities of
imaging data are employed (Schulz et al., 2024). Multivariate approaches are often understood as
wholly exploratory rather than hypothesis-driven, and this impression is reinforced by their
association with brain-wide analyses. However, multivariate methods need not be applied brain-
wide and can easily be used in theoretically-driven research, in which multiple parameters are
derived, for example, from particular brain regions or systems of interest. When multivariate
approaches are designed to facilitate interpretable insights rather than only maximizing
Effect Sizes in Individual-Difference Research 19
prediction, they can also be used to test hypotheses and provide evidence for or against specific
psychological models (Thiele et al., 2023). For example, a study of 257 participants used many
of the strategies we recommend and identified significant multivariate correlations (ranging from
.18 to .47) of a trait measure of autobiographical memory with functional connectivity between
two theoretically identified brain regions (hippocampus and temporal pole) and the default
network (Setton et al., 2022).
6. Conclusion
according to the typical sequence of neuroimaging research. Our list is not exhaustive; anything
that improves the reliability and validity of measurement should increase expected effect sizes in
research linking neural variables to individual differences in psychological traits. Careful
application of these methods may provide a path to identifying more robust, generalizable, and
scientifically or clinically useful brain–behavior relationships in samples with hundreds of
participants. The debate about how large samples sizes should be is currently prominent in the
field, but focusing exclusively on increasing sample sizes ignores other ways to achieve higher
statistical power. Increasing effect size contributes independently to power and is often easier
and cheaper than increasing sample size.
Evident in the research we reviewed above is that multivariate effect sizes are generally
larger than univariate effect sizes, and shifting to multivariate methods with cross-validation and
samples in the hundreds seems likely to be very effective for increasing effect sizes, statistical
power, and replicability. However, smaller replicable effects can also yield important conceptual
insights, and univariate research will continue to be valuable, especially for theory-driven
hypothesis testing. For univariate research where the focal analysis involves only a single
statistical test, we suggest using a sample size of at least 200, provided there is reason to expect
Effect Sizes in Individual-Difference Research 20
an effect size of at least r = .20 (and such expectations should never be based on a single study).
Note, however, that this describes only a small fraction of published neuroimaging research on
individual differences, in which extensive multiple testing is common. If univariate research
involves conducting multiple tests, then the sample size needs to be adjusted upward accordingly
(or researchers must be confident that their expected effect sizes are even larger, which will
probably be rare). We have mainly contrasted increases in sample size with increases in effect
size as two ways to improve statistical power, but it is important to keep in mind a third way:
reducing the multiple-testing burden by using theory to devise more focused hypotheses.
In conclusion, our suggestions for increasing effect size can help neuroimaging
researchers conduct robust research on psychological individual differences in situations where it
is difficult to amass thousands of participants for a single study. Marek et al.’s (2022) impressive
work showed that common approaches to investigating univariate associations of traits with
resting-state or structural MRI data are likely to require thousands of participants. If this were
true for all approaches to neuroimaging research on psychological traits, it would be truly
daunting, especially given the cost of neuroimaging. This issue is especially critical for those
with less access to resources, such as early-career researchers or researchers in developing
countries. Fortunately, this is not the situation we face as a field. There are many available ways
to improve our methods and increase effect sizes, leading to sample requirements that are larger
than traditional norms in neuroimaging research but still less than a thousand.
Effect Sizes in Individual-Difference Research 21