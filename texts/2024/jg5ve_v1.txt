Evaluation of a Predictive Model – Montana Early Warning System
Technical Report Draft: RQ 2 (Mixed Methods)
Using Longitudinal Data to Support State Policymaking Competition (NCER)
Dr. Robin Clausen, Montana Office of Public Instruction
robin.clausen@mt.gov; robinlclausen@gmail.com
Executive Summary
Research over the past two decades has focused on one aspect of dropout prevention: early
identification and monitoring of at-risk youth. Behavioral interventions have been the primary
focus of this literature; however, there is renewed emphasis placed on attendance and academic
risk factors under ESSA (O’Cummings, M. & Therriault, S.B., 2015). Recognizing a need to
promote early identification, stakeholders within the Montana Office of Public Instruction (OPI)
framed a comprehensive system of supports in FY 2013, principally a diagnostic tool, which
allows for the linkage of data to intervention and the ability to target resources where the
intervention is most likely to be effective. Such supports became known as the Montana Early
Warning System (EWS).
These supports target risk factors and the dropout probabilities associated most importantly with
attendance, behavior, coursework, and mobility based on eleven Logistic Regression models
(disaggregated by grade level). These models predict binary outcomes (for example, graduate or
not) and have been refined over the past decade by training the data against the actual data from
graduates and non-completers during the past year. In doing so, OPI has achieved a high degree
of accuracy. By 2022, only 3.04% of students were identified as graduates, but instead go on to
dropout (False Negative). In addition, 5.70% of students are identified as at-risk, but instead go
on to graduate (False Positive). 91.26% of students were identified and proceed to graduate or
dropout in line with this identification. This study, an evaluation of the Early Warning System,
establishes an evidence base in Montana surrounding early identification and monitoring. It also
fills a gap in the literature nationally where research literature on the effectiveness of Early
Warning Indicator Systems (EWIS) was identified to be sparse (Faria, A.-M., Sorensen, N.,
Heppen, J., Bowdon, J., Taylor, S., Eisner, R., & Foster, S., 2017; Marken, A., Scala, J, Husby-
Slater, M. & Davis, G., 2020).
This evaluation is based on three separate tasks. The first of which is an analysis of the
functional process of predicting dropout and graduation. Does the system predict as reliably
dropout and graduation? We know the system predicts dropout well based on the process of
refinement of the model. However, we do not know if the model reliably predicts graduation.
This addresses ‘false alarms’ and false negatives involved in model building. It also addresses an
important kind of False Positive, those students identified by the system, receive timely
intervention, and proceed to graduate. The second task focuses on analyzing the degree of
implementation of the program by creating a classification of low, medium, and high adopters. It
further analyzes mediating and moderating factors to the implementation, the quality of the tool,
the role of interventions, and the effectiveness of the tool. The third task is primarily quantitative
and focuses on subgroups identified in the analysis. It is our hopes to triangulate findings
between tasks in line with a mixed methods framework to allow analysis and conclusions based
on both qualitative and quantitative findings. This technical report focuses on Task 2 (Mixed
Methods). It is based on five research questions that address the degree of implementation:
a. What is the level of adoption in participating schools?
b. What are the mediating and moderating factors that impact implementation?
c. Does the OPI EWS work as Intended?
d. What are the perceptions of the quality of the EWS?
e. How does the OPI data tie into evidence-based interventions?
f. What are the perceptions of the success of the program at the school level?
This process involving adoption, factors which heighten adoption, the EWS business
requirements and professional development, the tie of data to intervention, and the perceptions of
the quality of the EWS implementation provide a lens on how one SEA may establish a data
culture surrounding graduation in schools. It identified four transversal themes which are present
in the data: the development of a data culture, longitudinal analysis, progress monitoring, and the
process of building and sustaining relationships.
This evaluation study focuses on one period of the EWS (between 2015 and 2019) which
represented the timeframe that supports were scaled up to a statewide rollout. In 2015, the EWS
was brought to scale based on the success seen during a pilot phase in both model refinement and
in data use among participating schools. We choose to analyze the next five years as this system
was brought to scale.
During this period, roughly a third of the schools were Pilot schools (representing 18 districts)
and the remainder were schools inspired by bringing the intervention to scale. It was also
identified that much of the work of the Early Warning System was paused during the COVID
pandemic. By focusing on the years prior to COVID, we are more likely to identify trends
anticipated as schools emerge from the pandemic. This allows for conclusions based on whether
efforts should be made about the future viability of the program in the post COVID era.
Three levels of adoption were found with the use of archival, interview, and survey data
collected for this study. The hallmark of high adopters is that they have both formal and informal
dissemination practices and have developed a data culture around dropout. This last point is
important. Many schools interviewed did express engagement (tight coupling) and survey data
indicates that EWS data was helpful in making decisions surrounding dropout. We do see
expressed engagement occurring with many schools who also have well established indicators
such as a direct tie of the data to an intervention as well as a high degree of follow up and
progress monitoring of students over time. The latter distinguishes between types of high
adopters (the degree to which progress monitoring using the tool occurs). While there is variation
in which the model is adopted within levels, we do see that many high adopters want
longitudinal data, want additional data tools, and have made steps to make a data culture
surrounding dropout prevention function and thrive.
There are many schools that have medium adoption processes. The hallmark of this level is the
form of dissemination of data related to the diagnostic tool. Medium adopters have informal
(many times only 1 person looks and acts upon the data) and no formal dissemination practices
(data is used only as a reference). Different trends with these schools are apparent. The first is
they do not have a well-formed MTSS process or a school team specific to dropout prevention.
Less than 4 people have access to the data. Frequently only the district administrator or principal
uses the data as a reference. Teachers in these schools do not have access to the data. Also,
progress monitoring, a key indicator defined by this study, is ill formed, and lacking.
The low adopters are those schools that have attempted to meet the basic business requirements
of the model (data sharing), however over time they opted out of the model and uploaded less
than 3 times. 132 schools are in this category representing 59.46% of all schools that expressed
interest. Based on the interview data, many of these schools did indeed view the value of a
EWIS. This value was often well formed and targeted to how they would be able to use the data.
Moreover, archival data indicates that there are many aspects of need that are shared with
medium and high adopters. However, they did not find their solution with the EWS.
Not all schools place an emphasis on dropout prevention let alone develop a data culture
surrounding the use of a diagnostic tool. Evidence suggests that this occurred with non-adopters.
This finding is triangulated from those schools with no experience with the EWS who said prior
to learning about a EWIS, their school did not emphasize dropout prevention. Moreover, non-
adopters have different contextual, institutional, and outcome indicators than adopters. This
difference suggests that they are less likely to have a defined need for the program.
As revealed in the interviews there are a variety of mediating and moderating factors that
determined the types of implementations and the variation in implementation within each type of
schools. The primary factor seen in the data was the presence of a MTSS team or a team of
educators working toward dropout prevention. Vision and dissemination of the data are
important mediating and moderating factors as it indicates the level of the development of the
data culture. Moreover, time in the program and time spent developing this data culture is
important. For example, Pilot schools are more likely to use the EWS data when constructing
interventions (p = 0.021).
The core of these factors is how the district finds value in the data and what they decide to do
with the data. OPI can help this process. This process of finding value in the EWS tool was
related to OPI professional development and outreach. Finally, in schools that have a high
degree of spread of this data culture there is a focus on building relationships with OPI, within
the school community (leadership and staff), and a focus on the relationships built with students.
At the heart of dropout prevention is the belief that relationships matter. Survey results indicate
that schools in small districts (less than 1000 students) are more likely to engage in follow up (p
= 0.037). This stresses the importance of building relationships and the degree to which this
occurs in small schools (Jerald, 2006; Bruce, M., Horning Fox, J., & Balfanz, R., 2011; Faria, et
al., 2017). Follow up is crucial and is a sign of a well-developed data culture. There is need to
identify supports as a student’s risk profile changes over time. By encouraging this, OPI could
encourage progress monitoring using the tool. This involves setting markers, or triggering
events, which frame the course an intervention. They identify when to start an intervention, when
to confirm or adapt an intervention, and when to discontinue an intervention.
Based on the data, we conclude that the EWS model did work as intended. In fact, the
professional development of the model was responsive and did change over time to meet
emerging needs. The design of the tool was found to be adequate, similar to online tools
associated with the MAPS test administration. The tool was found to be accurate among users.
Even when the tool missed a particular student it tended to be the case that subsequent uploads
identified the student.
Interview respondents reported many beneficial aspects to the EWS program. Much of this is
seen in context to the benefits of a EWIS in early identification and monitoring. One principal
remarked that he doesn’t know what he’d do without the tool. The alternatives take time away
from engaging the students and building relationships. One counselor remarked that she knew
her students better through the tool and there were more opportunities to build relationships. In
fact, these relationships were seen as crucial to student success, even among the most challenged
students receiving the most intense supports.
Users also cited the least beneficial aspects of the program, two of which are noteworthy.
Multiple users identify issues with access of the EWS on GEMS. This was reportedly an
inhibiting factor to using the tool. Also, there were many issues regarding the inclusion of
student mobility in the model. Respondents report this caused issues with false alarms; students
that were identified by the system but who would succeed using universal supports. Indeed, false
alarm were seen as the principal sources of inefficiency with the system and confirms findings in
the national literature (Bruce et al., 2011).
The tie of data to intervention was strong in high adopting schools. 50% of the survey
respondents reported that they use the EWS data ‘often’ or ‘sometimes.’ An additional 25%
report intense engagement with the tool. Stakeholders identified the supports they provide. These
supports are generally used among all schools. These involved relationship building with the
student, setting up clear goals, and follow up to this process. In high adoption schools, a MTSS
model or a student intervention team were developed. These processes followed targeted student
supports such as universal (Tier 1), intensive small group support (Tier 2), and comprehensive
supports (Tier 3) with a focus on mentoring programs and relationship building. Regardless of
the type of EWIS the system used, these strategies were apparent and used. Respondents claimed
that they continued to use these strategies when transferring to a vendor model and noted the
process of addressing issues with the EWS or the vendor data were the same.
Finally, there is a focus on the effectiveness of the model. This addresses four factors: progress
monitoring, accuracy, students that the EWS may have missed, and perceived successes of the
EWS model in their schools. Follow up to this study can investigate progress monitoring as it
pertains to each school. It not only addresses the differentiation among high adopters and
between high and medium adopters; it also provides important insights into the formation of a
data culture. Students that may have been missed by the EWS was identified as an important
topic. What users focused on was the value of early identification tied to the perception that they
could not receive services from a EWIS elsewhere. For example, there are many student
information systems in Montana, some of which do not have a EWIS and stakeholders rely on
the EWS model. Stakeholders focused on perceived successes of the system. Survey and
interview respondents alike note that at least 75% of their identified students go on to graduate.
Nonetheless, respondents focused on the fact that their system was a work in progress and can
only be judged from the standpoint of how to improve it next year. This is seen in the fact that
the dropout profile of their students changes over time with new cohorts of students. Meeting
ever changing demands, schools focused adaptation of the model each year.
There are many successes to note, such as the ability to democratize access to a system of early
identification. Research literature has identified that through much of their infancy, EWIS were
the purview of large school districts. By offering the opportunity to access an evidence-based
system that is designed for a range of district sizes, the EWS programs provided accessibility and
coverage. Moreover, according to respondents, the system is highly accurate, OPI outreach and
support is comprehensive, and costs of the model are minimal even when factoring in staff time.
While making the interventions more efficient, there was a decline of supports necessary per
student due to early identification. An additional benefit of the EWS data is the ability to predict
behavior in a way that is not dependent on demographic variables, economic disadvantage, and
student status (e.g., disability or ELL status). Indeed, it removes potential barriers to an
intervention and biases by instead focusing on areas that are under the control of educators.
These costs served to make interventions more effective and staff time more efficient. Many
respondents to the interview commented that the overall costs of their drop out program did not
go down. They were able to offer additional services to more students given the decline in cost
per student. In many cases, when identified early, dropout prevention avoids the cost of more
expensive long-term options such as alternative schooling.
Scale is crucial to understanding the degree of implementation. The process of scaling up the
program met many successes and challenges. There are important differences in institutional
variables and student outcomes between kinds of adopters and non-adopters. The model is fully
adopted among many high adopters; however, the model is still in the process of implementation
in most participating schools. One of the reasons why this is occurring is that local factors
dominate the decision to access and make use of the data. Local conditions are important. The
similarity between medium to high adopters and low adopters stresses the point that it is the
larger school environment that set the course for interest in the EWS model. Combined with this
are the differences between this group and non-adopters. Non-adopters may have not seen this
need for this tool given that their environmental factors are different and school size is
predominantly among schools with less than 150 students.
Scale varies, although it is important to note that scale should meet the identified need and
capacity in the school for a EWIS to be successful (Bruce et al., 2011). Identified need varies by
schools. Some schools do not have priorities that would warrant the use of the EWS. Others lack
the capacity, as seen in some small schools. While the EWS model is seen as successful, there
are many areas for improvement. Universal coverage remains the goal in schools, however in
two cases use of the data stopped when the grant that targeted that grade level ended.
Reengagement in the core reasons for using the tool and promoting dropout prevention policies
is needed. Scale, capacity, and priorities will remain the defining elements of the future spread
of the program and efficiencies of its programs.
Selected Recommendations
Longitudinal data was seen as crucial to identifying interventions, modifying interventions once
in place, and creating thresholds for students to discontinue supports. The primary
recommendation among users was to create ways to manage longitudinal data with reporting
based on each risk factor and the dropout probability. Respondents request professional
development on the need is to establish guidelines (FAQ) when an EWS inspired intervention is
not successful and ways to reassess and reassign interventions.
Interview respondents also discuss how the outreach by OPI was of good quality. They
commented on the need for more presentations and workshops about the EWS. Respondents
discussed how they wanted to focus less on the use and requirements of the tool, and more on the
process of identifying students at risk, establishing thresholds for support, and processes for
progress monitoring of students.
Contents
Executive Summary ........................................................................................................................ 1
Acronyms ...................................................................................................................................... 12
Introduction ................................................................................................................................... 13
Background ................................................................................................................................... 20
Using a EWIS ............................................................................................................................ 23
Risk Factors ............................................................................................................................... 26
How are EWIS and Tiered Interventions Tied Together? ......................................................... 28
What makes a EWIS Effective? ................................................................................................ 29
Warrant for Additional Research .............................................................................................. 32
Data and Methods ......................................................................................................................... 33
Archival Data ............................................................................................................................ 33
Sample ....................................................................................................................................... 34
Interview Data and Methods ..................................................................................................... 36
Survey Data and Methods ......................................................................................................... 38
Findings......................................................................................................................................... 39
Level of Adoption ..................................................................................................................... 39
What are the Mediating and Moderating Factors that Impact Implementation? ....................... 47
Does the OPI EWS Work as Intended? ..................................................................................... 56
Quality of the EWS ................................................................................................................... 60
School Led Interventions .......................................................................................................... 62
Effectiveness of the Local EWS ................................................................................................ 66
Conclusions ................................................................................................................................... 70
Recommendations ......................................................................................................................... 76
References ..................................................................................................................................... 79
Appendix A: Perspectives on a Vendor EWS Model ................................................................... 81
Appendix B: EWS Diagnostic Tool .............................................................................................. 83
Acronyms
AIM Achievement in Montana
API Automated Programming Interface
CEDS Common Education Data Standards
EWS OPI Early Warning System
EWIS Early Warning Indicator System. Referenced to national literature.
ESSA Every Student Succeeds Act
GEMS Growth and Enhancement for Montana Students
MTSS Multi-Tiered System of Support
NCLB No Child Left Behind Act
NCER Nation Center for Education Research
OPI Montana Office of Public Instruction
ROC Receiver Operating Characteristic
RTI Response to Intervention
SEA State Education Agency
SIS Student Information System
SLDS Statewide Longitudinal Data System
Introduction
The spread of Early Warning Indicator Systems (EWIS)--diagnostic tools used in dropout
prevention-- reached their tipping point in 2012 following the spread of policies tied to dropout
prevention, an emphasis placed on graduation in federal legislation and SEA accountability
systems, and a plethora of research articles that normed various risk factors using common
metrics and supports (Bruce, M., Horning Fox, J., & Balfanz, R., 2011; Heppen, J. & Therriault,
S.B., 2008; Jerald, 2006). As the Data Quality Campaign (2013) noted, 15 states began to collect,
manage, and distribute data on early warning indicators. The focus of these efforts was on the
development of a EWIS that fosters student level data including early identification,
collaboration within school communities, and the development of tiered interventions. Three of
these states fostered advocacy and communications by offering a web portal containing a EWIS
diagnostic tool.
Montana soon became one of these states. In 2012, Montana focused its attention on developing
an early warning indicator system to address issues in drop out and graduation. Over the next
decade, Montana progressively rolled out its early warning system, from a pilot stage in 2015. It
gained an important online presence. It also began to focus on the scale of the program and
adoption based on an opt in model. Spread of this early warning system became focused on
schools of various sizes, which have different student information systems, that have a different
scope to their dropout prevention team, that exhibited variation in their implementation
strategies, and who focused on what longitudinal data could reveal about student progress. Prior
to this research Montana had sparse and unclear evidence to the degree of implementation and
the patterns of data use among schools that used EWIS diagnostic tools. There are also
indications that research on the effectiveness of EWIS nationally is sparse (Faria, A.-M.,
Sorensen, N., Heppen, J., Bowdon, J., Taylor, S., Eisner, R., & Foster, S., 2017; Marken, A.,
Scala, J, Husby-Slater, M. & Davis, G., 2020).
This evaluation study of the Montana Early Warning System (EWS) aims to fill these gaps by
investigating variation in school and student data coinciding with the EWS program, understand
the reason why it is occurring, and identify ways that best practices may be sponsored. It
focuses on three tasks tied to essential research questions:
Task 1: We want to know the propensity of the model to predict graduation. In doing so
we will look to six factors involved in the model: Dropout Probability, Grades Risk
Factor, Attendance Risk Factor, Previous Dropout Risk Factor, Behavior Risk Factor, and
Mobility Risk Factor. We use the staggered rollout of the EWS to estimate the effect of
the EWS on student outcomes.
Task 2: We investigate the degree of implementation of the model in these schools. Has
access to EWS data inspired increases in targeted interventions with identified students or
interventions and policy modification at the school-level?
Task 3: We focus on how robust the student outcomes are in these schools and the
impact of dropout interventions on graduation and postsecondary enrollment. We look to
the same risk factors and gauge the viability of each to predict these two opportunities.
Emphasis is placed on trends within subgroups and college enrollment.
This technical report focuses on Task 2 and addresses the degree of implementation in EWS
schools. The measure is the degree to which evidence-based change is occurring when targeted
The Montana Office of Public Instruction (OPI) received a grant from the National Center for Education Research for this
evaluation. This work is a collaboration between OPI and the Montana State University (Department of Agricultural Economics
and Economics). OPI is charged with Task 2 and coordinating all aspects of the research.
interventions lead to change in policy and student success. We specifically look for the level of
adoption, mediating and moderating factors to the implementation, degree to which all school
stakeholders are involved (vision, value, and dissemination), and the degree to which the data is
tied to an intervention based on locally defined thresholds or triggering events. An example of a
triggering event is the transition from middle school to high school.
To complete Task 2, we focused on broadening our literature review, collecting survey data from
EWS stakeholders, analyzing archival data, and collecting interview data from schools which
have considered using a EWIS or use the Montana Early Warning System. The sub questions to
Task 2 are identified based on the theoretical framework to the study, the Theory of Change, and
the insights of the data. These questions are as follows:
a. What is the Level of Adoption in participating Schools?
b. What are the Mediating and Moderating factors that Impact Implementation?
c. Does the OPI EWS Work as Intended?
d. What are the Perceptions of the Quality of the EWS?
e. How Does the OPI Data Tie into Evidence Based Interventions?
f. What are the Perceptions of the Success of the Program at the School Level?
Certain themes are apparent in these questions. Most importantly the impact of the development
of a data culture, relationship building, progress monitoring, and longitudinal analysis. A core
process in understanding the degree of implementation is identifying the level of adoption. The
investigation attempted to draw out what are the mediating and moderating factors; including
scope of the implementation, costs of the implementation, and alternatives that exist in Montana
for dropout prevention planning. Third, focusing on the diagnostic tool, does the EWS function
as it should (as a function of business requirements, OPI outreach, and the web portal). Next, we
look to the quality of the EWS by outlining some of the most beneficial and least beneficial
qualities. Fifth, we also place a focus on the tie of data to interventions. We provide detail on
how the tool works in conjunction with Multi-Tiered System of Support (MTSS) frameworks
and provide data about the effectiveness of the EWS. We also comment on the degree of follow
up to an intervention and to what degree EWS data is used for progress monitoring. These
factors impact the perceptions of success of the local model.
The OPI strategy with the EWS was to build the diagnostic tool, norm it over time given prior
years of data, and engage in outreach regarding the tool for well over a decade. These framed the
system of supports provided to participating schools. The EWS program aims to achieve four
overarching goals:
Goal 1: Create and maintain a statistical model that accurately predicts the odds of a
student dropping out.
Goal 2: Identify at-risk students before they drop out.
Goal 3: Help schools identify factors that are impacting each student’s dropout risk to
prioritize and target interventions.
Goal 4: Help schools understand dropout risk trends at the school level to make decisions
regarding policy and programs that may influence dropout risk.
The Early Warning System has a strong support network within the OPI. A decade into our
Statewide Longitudinal Data System (SLDS) undertaking in Montana, we have a robust K-20
data warehouse, an excellent reputation amongst our users, and as well as support from
leadership, partner agencies, and policymakers. This translated into action with the 2019 SLDS
grant in which the focus was modernization of the GEMS platform and transformation of the
data warehouse in lines with CEDS architecture. Our school districts heavily utilize GEMS
data. More specifically, by 2019, 750 school district users have recognized the value of GEMS
and requested secure access to utilize district-specific confidential data, of which 366 have an
assigned Early Warning System role since 2015 and before the start of COVID (2019).
When a school expresses interest in the Early Warning System, the OPI provides onboarding by
training school staff on how to extract their data, load it into Montana’s SLDS, how to access
and interpret the reports, and how to incorporate the reports into their own school processes for
responding to dropout risk. The OPI has staff at the agency who maintain the model and can
provide technical and user support, and trainers in the field who assist with onsite
implementation. We have implemented special competitive grant programs to encourage schools
to use the EWS when making decisions about interventions with students. One of these grants
encourage schools to participate in evaluation activities sponsored by the National Center for
Education Research.
Evaluating the EWS involves a concerted effort between tasks that is based on the study’s
Theory of Change. This study, in specific the response to Task 2, is a mixed methods study. The
EWS involves a combination of OPI outreach and decisions by stakeholders, of which the
primary is to opt-in to the system. Users identify the need for a EWIS, understand the
alternatives to an EWIS in Montana, become aware of the MT Early Warning System, and
Currently much of the data in the warehouse is mapped to CEDS elements. Under Montana’s FY 19 SLDS grant,
we will be mapping all databases at OPI to CEDS standards. Much of this work will be completed before 6/30/2024
(the end of the NCER grant) and we do plan on mapping all the data elements involved in this study to the CEDS
framework so that the study’s data elements have their own mapping.
Use of the funds included supports for teachers related to time spent mentoring student outside of normal school
hours and professional development, attendance incentives, targeted interventions such as providing pupil
transportation from home to the bus stop, and as one counselor referenced, ‘books and shoes’ which were the most
important requests in their school community.
decide to opt in to the model. There is evidence that schools who showed interest in the EWS
(adopters) have different school level contexts, institutional traits, and trends in student outcome
measures than schools that showed no interest. There is a shared context among adopters which
inspired interest.
For most schools, real time data is provided by the schools. However, most data are found within
the GEMS data warehouse. Pilot schools received the support to have the data uploaded for
them. There were two options for other schools. First, uploads were enabled by an extract from
the Infinite Campus system specifically designed to upload to the EWS. The second option was
to collect local data needed to run the EWS according to a standard template. This is summarized
in the first two steps of the Theory of Change.
The diagnostic tool creates profiles by school and student. OPI provides professional
development about the use of the data and provides insights into what triggering events will start,
revise, and end an intervention. The principal element of the diagnostic tool is the interface that
users see with the results of the Logistic Regression analyses depending on the level of analysis.
The EWS reports include:
School level report - Summarizes data and creates visualizations for school level
dropout risk, and specific trends including grades, attendance, behavior, and
mobility.
Student summary report - Generates a spreadsheet containing all student data
for the school, including risk rankings, percentage risk, change in risk, and odds
ratios for specific risk factors.
Student detail report - Provides data and visualizations for a single student
within that school, including their current dropout risk, change in risk over time,
information on missing data, and predominant risk factors where interventions
may be warranted.
Using this actionable data, schools may design and implement interventions that abate the chance
of students dropping out. Part of our research plan is for us to understand both the process
schools undergo when using the data and the outcomes of this data use. By investigating key
outcomes and better understanding the processes involved (the process of the Theory of Change),
the OPI is better equipped to report to key educational leaders at the Montana Office of Public
Instruction about the efficiency evident in the processes and the relative effectiveness seen in the
outcome experienced by schools.
Background
Discussions about non completers of K-12 education revolve around individual or systemic
failure. The reality of the matter is that drop out can result from both conditions, of which some
factors are more under the control of educators than others. Regardless of where the root cause is
found, the incidence of dropout is seen as having a series of individual and societal outcomes
that raise the importance of the issue. Median earning of families headed by a high school non-
completers declined by a third between 1974 and 2004 (Jerald, 2006). This aggregates to have an
impact on the larger economy. In 2009 it was estimated that there is a 2 - 4% decrease in Gross
Domestic Product due to the individual and societal costs of dropout (Bruce, Horning, &
Balfanz, 2011). Compounded with this trend, there are increased rates associated with drop out
of the use of social services, health care costs, and mounting costs within the criminal justice
system.
Examining root causes is difficult given the penchant for blaming the individual. The reality of
the matter is that dropout occurs within the context of the education system and school level
factors share the blame. Drop out has as much to do about schooling as it has to do with the
individual success in coursework and choices involved in the decision to drop out or be pushed
out (Jerald, 2006). Schools are at the nexus of these factors, whether it be where attendance is
taken, grades are monitored, and behavior is managed (Bruce et al., 2011). Indeed, schools
themselves may be risk factors. On this end of things, educators have more influence and control.
It is possible to identify students that are in danger of dropping out using established individual
and school level indicators that can give warning to the signs of potential dropout (Frazelle &
Nagel, 2015; Koon & Petcher, 2015; O’ Cummings & Therriault, 2015; Marken, Scala, Husby-
Slater, & Davis, 2020). Moreover, by focusing on attendance, behavioral, and course work (the
ABCs) these systems identify those markers in a manner that is independent of demographics,
indicators of economic disadvantage, or factors that may be out of a student’s controls (e.g.,
foster care or disability). The decision and the context behind drop out are gradual processes with
many identifiable markers along the way. The warning signs are based on established, evidence
based, indicators. Such systems and data tools are known as Early Warning and Indicator
Systems (EWIS). They are a diagnostic tool intended to enable effective interventions and
dropout prevention programs.
Research shows that students that are identified early on as at risk can turn resiliency into
graduation given the proper support (Bruce et al., 2011). Nonetheless, most dropout prevention
programs over the last 40 years have only had disappointing results, perhaps due to the difficulty
of identifying students that are at risk and ensuing difficulty of focusing on so called ‘false
alarms,’ student that received dropout prevention services but most likely would have graduated
anyways (Ibid). This is seen in some dropout prevention programs that focus on the whole
school, rather than those students that are identified as most in need. On the other side of the
coin, signs of inefficiencies are ‘false negatives,’ those students marked identified as most likely
to graduate and who may not receive services, yet in turn drop out. A focus of this Montana
study is on these misidentified students as we attempt to evaluate how well the tool diagnoses
graduation and drop out.
Early Warning Indicator Systems rely on one premise: that the process of a student dropping out
is a gradual process with many identifiable indicators. And it is best that these indicators are
monitored early on. Often these systems are coupled with other data systems such as college and
career readiness indicators. There is an established research base that provides evidence to the
effectiveness of these indicators (e.g., Koon et al., 2015). The systems allow for systematic
planning by school teams that build off this ability to identify students and the more difficult task
of assigning students to appropriate and effective interventions.
EWIS are used by states and districts across the United States to identify students who are at risk
for dropping out of school. They do so in a variety of ways but at their core, a EWIS identifies
students who are at risk, typically when students are in middle or high school, and implement
interventions to keep them on track to graduate (Allensworth & Easton, 2007). As of 2012, 28
states produced early warning system reports with 15 of those states using the system to collect,
store, and analyze EWIS data (Data Quality Campaign, 2013). In three states the SEA provides
an analytical tool that allows districts to upload their own early warning data. By 2015, Montana
joined their ranks offering a research based online tool that encourages districts to upload local
data to monitor EWIS data and results.
Initially seen as the purview of large school districts, access to a EWIS within a statewide
longitudinal data system (SLDS) has served to democratize access to a EWIS for districts that do
not have the research capacity, or time to construct a system themselves. The costs of
establishing a EWIS are relatively small in that it allows schools to identify potential dropouts
and target resources to those groups in which it is easier to establish a causal relationship (for
example, chronic absenteeism) to an outcome (drop out). States seeking to create a statewide
early warning system typically follow three distinct steps: identify or validate EWIS indicators,
customize data tools to districts and schools, and encourage and support EWIS based
interventions (O’Cummings & Theriault, 2015).
State approaches to Early Warning Systems have been multifaceted. Some states have
incorporated use of the EWIS in the state’s accountability system. Other states, such as Montana,
use an opt in, school led, approach to using the data. Some data systems reflect data that is real
time, meanwhile other systems may look at data that is a snapshot of data over the school year.
The EWIS in Montana is based on a mixture of the two elements in that some data is real time
provided by the district; meanwhile other indicators are based on data held by the state which are
both real time and periodic. In addition, professional development on the use of the system is
important. Some states frame use of the EWIS in the context of a MTSS framework, meanwhile
others focus more on the use of the tool and concentrate less on the tie of the data to an
intervention.
Using a EWIS
Quite simply, one of the main benefits to a EWIS is that it saves time. Principals and counselors
are freed to spend more time with the students, more time monitoring interventions (using EWIS
tools to monitor interventions) and more time focusing on root causes and potential outcomes of
each student’s circumstances. With a EWIS there are less administrative costs. Working without
a EWIS takes time. Often this involves juggling multiple data systems when accessing
attendance, coursework information, and behavioral data. Some districts choose to do a targeted
data collection as a form of universal screening. Many small to medium size schools do not have
the research capacity or a sufficient sample size to execute a valid, normed, evidence-based data
collection strategy. SLDS EWIS systems provide data to participating districts using data
originated from a centralized data warehouse that is customized, out of the box data, that districts
do not have to centrally manage. Using a EWIS represents a culture shift towards data driven
success and improvement. According to one Superintendents advice, data system managers
should make it manageable, make it meaningful, make it matter (Bruce et al., 2011). Part of
making it matter is to make clear the tie of the data to an intervention strategy.
Identification
Potential dropouts may exhibit the similar signs, but each student has a unique situation.
Identification of a fixed set of outcomes for certain student profiles may lead school leaders to
ignoring the traits of other kinds of student profiles. Students have different reasons associated
with dropout. There are different types of non-completers and students may exhibit the traits of
more than one group. Any indicator system that assumes the same relationship between context
and outcome will fail to identify whole subsets of non-completers (Knowles, 2015).
This is seen in the relation to the number of transitions that as student goes through as part of
their schooling. Different indicators may play a more important role at certain levels of
education. It is important to look at milestones and the various risk factors that may surround
each milestone. For example, EWIS research has placed emphasis on student transition into high
th
school (9 grade). At that grade level, there is a certain pattern and pathway that students follow
when making the decision to drop out or being pushed out. The grade that has the most incidence
th
of non-completers is 9 grade and the incidence of subsequent dropout by students held back in
th
9 grade is high (Heppen & Therriault, 2008). Knowing these stresses allows for systems to be
created to factor in this transition and predict drop out (Bruce et al., 2011).
This highlights that when statewide longitudinal data systems create and manage a EWIS, the
system should be flexible to context; whether that context is within a school or between risk
factors identified at the local or state level. School level poverty can and does have an impact in
these systems but may not be an accurate indicator in the EWIS. The same for race/ethnicity and
student status.
System
Most Early Warning System incorporate multi-level logistic regression (Koon & Petscher, 2015).
Many have coherence and transparency as a goal. Coherence is gained by focusing on a small
number of objective, evidence-based risk factors. Transparency is furthered by using this small
set of risk factors in a manner that is understandable for end users – the school teams which
identify thresholds and use the data to implement interventions.
Logistic regression uses multiple predictors is a hierarchical manner that each shed light on a
binary outcome variable, in the case of a EWIS, whether that student has a certain risk factor or
the probability that a student will not graduate from high school. In doing so, the model produces
a log odds for the likelihood of achieving one of the two categories selected for an outcome. In
Montana, the model produces two sets of indicators. First, calculations are made for each risk
factors providing the odds ratios (attendance, behavior, academics, and mobility). Second,
regressions identify an overall dropout probability for each student. Predictions are then verified
by a ‘training’ process in which known completer data is mapped to known outcomes for
students and then used to verify the risk profile of the model for future predictions. In the
Montana model, there are annual revisions to the model considering past trends in the data.
In face of this complexity, it is important to maintain transparency in how a system is
constructed and intended to be used. In Montana, many stakeholders have questions about the
specific ingredients that go into a EWIS calculation, for example, what is the impact of student
mobility on the model. Local validation of data is important and is enabled by this transparency
since this is the context in which decisions about interventions are made and students are in
danger of not meeting education milestones. The focus of local validation is establishing
thresholds for student supports.
Risk Factors
There are different thresholds for the EWIS indicators in different contexts (Knowles, 2015). In
the case of Montana, these thresholds are established and monitored by schools who often details
the tie of data to intervention. Often discussions about thresholds and the point to trigger a
particular intervention, is the focus of local Multi-Tiered System of Support (MTSS) teams
which focus on attendance, behavior, and academic interventions. Not all MTSS teams focus on
academics and coursework. Indeed, it is common for MTSS teams to focus exclusively on
behavioral factors. Hence, there is variation by school in the scope of the implementation
framework. Even the characteristics of the school building itself can reveal variation in risk
factors. This variation can lead to complexities in model development and use. Within a system
there is a tradeoff between accuracy, complexity, and identification of clear outcome measures
(Knowles, 2015).
Research validating risk factors has identified that three indicators are at the core of all EWIS
(Frazelle & Nagel, 2015). These include the ABC’s (attendance, behavior, and coursework).
Common measures of these components include missing 20 days or absent 10% of the school
days, two or more behavior infractions, and inability to read at grade level, low GPA, or 2 or
more failing grades in courses in the previous year (Ibid).
Achievement data is not commonly used in these systems. This is because grades are seen as a
stronger indicator as they reflect performance over time. Course passing is directly tied to credit
attainment which creates a casual linkage to drop out or graduation. By focusing attention on a
small set of evidence-based indicators, users can conserve time and resources when managing
the EWIS data. By keeping it simple it is easier to define objective, evidence-based relationships.
Indeed, this small subset of factors enable effective interventions since the tie to monitor
interventions once in place is clearer, allowing for clear management of the students changing
risk profile.
The higher the number of dropouts associated with a risk factor, the more effective the indicator.
In a similar manner, over identification of students as at risk is seen in the same light as
overidentification of special education students (an inefficiency of the school system). However,
false alarm (a portion of the incidence of false positive) should not be confused with the
successes of the system where effective intervention adverted drop out. What false alarm
addresses is over identification of at-risk status not a successful intervention led to the desired
outcome (graduation). Montana’s EWIS helps prevent false alarm by establishing a gradient of
dropout probability that allows educators to focus resources on those students whose success is
the most challenged and/or where interventions can matter the most. This metric is backed up by
data on risk factors identifying which areas students may face challenges. This allows the
targeting of intervention that may have the greatest impact in situations that are manageable and
solvable.
Focusing on fewer indicators that are robust and evidence based can reduce the incidence of false
alarm. Wisconsin represents one SLDS whose application has been validated across many kinds
th
of schools (Knowles, 2015). One highlight of the system is to use a 9 grade on track composite
th
indicator. In similar students, authors found that the 9 grade indicator was 85% effective in
predicting drop out (Jerald, 2006). Wisconsin uses the receiver operating characteristic (ROC)
metric to identify the best set of indicators and models to fit the data to make accurate early
predictions about students’ risk of dropping out. Common among EWIS is to use training data
consisting of three to five years of historical data to improve model fit (Frazelle & Nagel, 2015).
In Montana this is done annually based on the previous years’ data. Frazelle, et al, 2015 notes
that care should be taken to avoid over fitting the data to estimate the error rate of the model on
new data.
How are EWIS and Tiered Interventions Tied Together?
The focus of an Early Warning System is not just the risk factors. The onus of the system is what
users decide to do with the data. Data use can occur in multiple stages from simply identifying
potential non completers, to assigning interventions using locally defined thresholds, to
monitoring interventions once in course, to re assigning interventions based on the available
data. In addition, sometimes dropout prevention programs are more intensive in that there is
grade level or school wide implementation of the program. Scale varies, although it is important
to note that scale should meet the identified need and capacity in the school for a EWIS to be
successful (Bruce et al., 2011).
Much of the research behind EWIS and the relationship to dropout prevention programs occurred
in the context of the No Child Left Behind (NCLB) legislation and expectations placed on
schools and accountability systems surrounding graduation. As a result, early warning systems
were paired with dropout prevention programs and behavioral strategies as seen in the Response
to Intervention framework (RTI). RTI frameworks arose out of the need of implementing
evidence-based strategies at scale in the context of over identification of students for services,
especially in the context of special education. Benefits to the program is that students receive
swift and appropriate supports based on identified need as seen in the early warning systems or
other forms of universal screenings. Moreover, the system allows interventions to be monitored
over time for appropriateness and progress. Montana developed a clear tie to this framework
during the model’s pilot stage.
A revision of RTI strategies making it relevant to ESSA exigencies is the Multi-Tiered System of
Support schema. The MTSS model brought together attendance, academic, and behavioral
supports under the same framework, something which had been separated under the RTI
framework. The classification of interventions focuses not on the student’s behavior or
circumstances, but rather on the level of support a student may receive. Tier 1 interventions are
applied to all students – Universal. Tier 2 inventions are moderately intensive and focus on small
groups of students. Tier 3 is more intensive and focus on the individual supports a student may
need to be successful (Marken, Scala, Husby-Slater, & Davis, 2020). One example of a Tier 3
intervention is Check and Connect; a faculty led mentoring program. The focus of the
intervention is to see what motivations a students might have and the ways an objective advocate
may foster academic and social pursuits. The core of the intervention is relationship building.
Goal setting is a hallmark of these programs in helping students develop academic goals based
on their strengths and to create measurable markers in pursuit of that goal. Evidence based
research shows the efficacy of these programs that focus on building relationships and increasing
student engagement (Frazelle & Nagel, 2015).
What makes a EWIS Effective?
A EWIS is just a colorful tool if intervention is not written into the system. There are many ways
to make intervention that core of local implementation models. Leadership and vision are local
factors that can be enabled by SEA policies. The hallmark of a successful EWIS is that there is
identified need and clear leadership. According to Marken et al., 2020, effective leaders are not
just school leadership. Often a lead teacher or school counselor coordinates the EWIS with the
support of the building leadership. Nonetheless, this vision must be shared by leadership.
Successful systems focus on the process of implementing the EWIS and how change may be
gradual.
O’Cummings et al. (2015) notes that effective schools have learned to triage students and define
student needs based on the findings. The core of triage is to establish thresholds of intervention
based on individual student needs. As a result of identifying subsets of students with identified
need, a school may launch a dropout prevention program targeting this subset looking for areas
in which intervention may have the greatest short-term impact. This process allows educators to
dig into students’ needs while the becoming familiar with the EWIS process (Ibid). The long-
term vision is to establish rigorous progress monitoring using EWIS tools for all students with
interventions tied to the appropriate tier (Marken et al., 2020; O’Cummings et al., 2015).
In many EWIS, local analysis and context determined the thresholds to which students would be
given interventions. This means that academic, attendance, and behavioral support were given to
students at different rates over time given the demands of the intervention and local capacity of
the system (Bruce et al., 2011). Often this is based on locally constructed thresholds based on
each of the risk factors. A student may not exhibit risk in all areas. The locally defined threshold
Studies suggest that paralysis is often the trait of EWIS educators that feel overwhelmed by the data and daunted at
the task of meeting every student’s needs (O’ Cummings & Theriault, 2015). Understanding that processes reflect
gradual change help educators take on larger issues without feeling overwhelmed.
can identify the specific factors to target, for example, with student challenged by attendance, by
creating a system of meaningful attendance incentives to recognize student growth.
Fidelity of the model to established standards is important. Research in some cases shows the
success of the model anecdotally and with randomized control trails. In particular, research on
EWIS indicators is well development and do frame the functional process of each EWIS
(algorithm). Marken et al. (2020) mentions that EWIS schools should focus on understanding the
fidelity of their model to established research based on duration, quality of delivery, program
specificity, and student engagement. Moreover, research is the key to developing an effective
early warning system whether it is capacity of the educators to examine if the school met
students’ needs (evaluation), if the use of evidence-based practices enable success in the
intervention, or even to understand the data culture at the school to see if it has focused on
evidence-based practices.
Research and evaluation are important at another level, including evaluation studies.
Longitudinal studies of the fidelity and effectiveness of early warning systems are warranted and
are key to the long-term success of an early warning system and its affiliated dropout prevention
programs. Research on the implementation of early warning systems has been sparse and has
generally focused on the establishment of early indicators (Faria, Sorenson, Heppen, Bowdon,
Taylor, Eisner & Foster, 2017; Frazelle & Nagel, 2015). One exception is a randomized control
trial in schools in the Mid-West implementing a tool available from the American Institute of
Research (Faria et al., 2017). This study found that absenteeism indicators and course markers
were more positive in treatment schools (those that used a EWIS). However, overall GPA and
behavioral data points tended to favor the control condition (never implemented a EWIS). In
addition, metrics were analyzed among schools as to whether the development of a data culture
occurred. Treatment schools did not differ from control schools as to the extent leadership
developed a data culture. Development of a data culture was hypothesized to the be the first step
to identifying a model that had fidelity to research and vision to implement change in line with
the EWIS model (O’Cummings et al., 2015).
Warrant for Additional Research
In 2008, a US Department of Education practice guide on non-completers focused on the need to
develop diagnostic instruments to support dropout prevention programs. At the time there was no
established research basis surrounding these indicators and we knew less about outcomes (Faria
et al, 2017). There has been much progress to date with understanding the indicators and their
algorithms (Koon, S., & Petscher, Y., 2015). However, to date, the research base about the
outcome of early warning system inspired interventions continues to be sparse (Faria et al, 2017;
Frazelle & Nagel, 2015; Marken et al., 2020). This is surprising since early warning systems
have flourished, especially in the context of vendor driven models. Research has focused on the
development and norming of indicators, but beyond that little is known about outcomes and
process of the EWIS models, including the effect of EWIS inspired intervention programs (Faria
et al., 2017). Impact studies of this diagnostic tool to intervention cycle are warranted and
currently lack a clear the evidence base.
Regardless of the model used to implement the EWIS, whether it is originated from a statewide
longitudinal data system or vendor provided models, SEA leadership in advocating frameworks
for implementation is critical. Key to the success of educators who participate in a EWIS
program is the degree to which their efforts are supported by a framework of implementation
provided by the SEA (O’Cummings et al., 2015). A data driven intervention culture is needed at
the SEA level to support the development of a EWIS. Indeed, a statewide evaluation of
effectiveness is warranted to gauge progress and fidelity to established models.
Analysis of this data driven culture should occur over time. This evaluation of the Montana Early
Warning System establishes trends seen during the 2015 – 2019 period. Success has been noted
in different contexts however success is unclear. The link between the diagnostic tool and the
outcome of the interventions (e.g., improvement in graduation rates) warrants further
investigation since evidence is uneven across systems and school types due to the low incidence
of rigorous, evidence-based studies.
Data and Methods
To complete Task 2, we collected data regarding the degree of implementation of the EWS
model using archival data, interviews, and surveys. Archival analysis was completed January
2023 from the Montana SLDS. This analysis focused on contextual, institutional, and student
outcome variables found in the GEMS Data Warehouse. Specifically, this addresses the scale of
the program and differences between three groups: medium to high adopters, low adoption
schools, and non-adopters. The interviews explored the ‘in what form’, ‘why’ and ‘how’ the
EWS program scaled up. Interview data was collected in November 2022. Our survey, sent to all
schools who have participated in the EWS program at various levels of adoption between 2015
and 2019, seeks data about how schools use the EWS, interest in the EWS, and how schools
transition from data to intervention. We implemented this survey in May 2022.
Archival Data
Archival data was collected from the Statewide Longitudinal Data System. This data includes
counts of times a school shared data with the EWS (upload), contextual (data on economic
disadvantage, locale, and demographic), institutional (per pupil expenditures, teacher salary,
teacher tenure) and outcome variables (cohort graduation rate, post-secondary enrollment rate,
dropout rate, attendance rate, student proficiency – ELA and math, and ACT Composite mean).
Data was analyzed three ways:
Continuous variables were analyzed with a General Linear Model approach where the
outcome was the dependent variable and level of adoption was the fixed factor. Analyses
are presented for those findings that are significant at the p < .05 level and expressed
commonly in a ratio.
For variables such as Locale, a crosstabs feature was used along with a Chi Square
analysis to gauge significance. For instance, are there significantly more rural schools
among non-adopters? Locale is a categorical variable, and the Chi Square analysis targets
the different patterns of counts of schools by locale (category) between these three
groups.
Upload data was tracked, and we explored the hypothesis that the presence of
instructional and non-instructional staff would impact the degree of upload. We
separately regress upload data by pupil teacher ratio, counselor to student ratio, education
psychologist to student ratio, social worker to student ratio, and librarian to student ratios.
Sample
In the archival analysis, all schools with a profile were selected. The point of first upload is
where schools established a profile. From this list of schools, we crafted our invitation list for the
surveys and interviews. Invitations were sent to superintendents, building leaders, counselors,
and all staff that have access to their district’s SLDS profile with the specific Early Warning
System role. The invitation for the interviews was sent in October 2022 to this list. The invitation
for the surveys was sent in May 2022. The basis of the list was staff with access to the SLDS.
To that list superintendent, principal, and counselor contact information was added in districts
where those individuals did not have access. In all, the interview and survey recruitment letters
were sent to 274 individuals. It is our estimate that we achieved a 15% - 20% response rate on
the survey and recruited 32 participants for 15 group interviews.
Response rates were impacted by the following conditions. These emails were sent with the Gov
Delivery application. Many district level users opted out of emails from the OPI (approximately
a third among all the lists statewide). This count factored into the eventual list which has 274
email addresses. Also, the list contained invalid email addresses (bounce backs). This ranged
between 10% - 20% depending on when the survey was launched and when the interviews were
conducted. Reasons for the number of invalid addresses were primarily employment termination,
often the transfer of an employee to another school system. There were also a fair number of
districts that changed the domain to their email addresses.
Response rates to the survey and requests for the interviews were also dependent on the fact that
there was no program requirement to participate in evaluation activities, OPI policies and
practices surrounding emails to districts, how familiar the user was with the EWS, if the user was
the appropriate person to respond to the survey, the degree to which the tool was used, and
perceptions regarding the status of EWS related implementation in the schools. The lack of a
program requirement for evaluation activities was the primary constraint to response rates.
Hence, districts volunteered their data and their time, in the case of the survey there was no
incentive to participate. Next, are the individual motivations. This involves incentives and
awareness. The interview was conducted as part of a Mini Grant (incentive) offer to schools
($2000); however, participation by schools was grounded in a willingness to participate in the
evaluation. Response rates were also dependent on the degree of awareness the person had about
the EWS. Approximately half of the sample did not have access to the online system. In addition,
people may not be the appropriate person to respond or the degree the respondent was unaware
how the EWS was used during the 2015-2019 period.
Interview Data and Methods
Our goal was to triangulate findings regarding EWS implementation, which provides the basis to
complete Task 2. We define the degree of usage by categorizing schools as low, medium, and
high adopters by starting first with interview data (which represent a range of implementations),
then the survey data, and next archival data. Triangulation was conducted between users and
between themes of the analytical framework. Descriptions for each level originate in the
interview data. Two linked protocols were developed depending on whether a school was a
current EWS user.
The OPI purposively solicited districts to participate in the interviews by using an email list and
a posting in the agency newsletter. We received 18 inquiries about the evaluation There were 15
group interviews selected representing fifteen districts with an average of two participants per
interview. Most often, this was the high school principal and the school counselor. The design
was to allow us to triangulate the degree of implementation at the school level between
participants.
Criteria for the interviews were targeted. They include a range of school sizes, different levels of
adoption/non adoption, and different number of years participating in the program. Fifteen
districts were chosen among the 18 districts that expressed interest based on these criteria. This
sampling strategy differs from network (snowball) sampling in which different participants
catalyze the participation of others. Purposeful sampling was used to gather data from a range of
different districts with different upload counts, different situations of economic disadvantage,
and different situations of rurality. Network sampling is seen within the districts. Based on who
was originally contacted (often the school principal), districts choose two to three individuals to
participate in the group interviews. This often included the principal, a school counselor or lead
teacher (network). We focused on 11 medium size districts (less than 1000 students), primarily
rural, and districts with a high proportion of native students (5) which can be correlated to
situations of economic disadvantage along with many rural schools.
We conducted interviews using the Zoom platform. Two schools choose to participate by phone,
with the remainder involving a video connection. All interviews according to our IRB protocol
are kept anonymous, and transcripts are stored on a password protected shared folder used for
research purposes only. This allowed us to focus on the audio recording and the rough written
transcript provided by Zoom.
Two cross referenced protocols had been developed based on if the school was currently
count by category.
Currently participating in EWS 9
Participating in Vendor EWS 4
Other participation (Considering Alternatives) 2
EWS Pilot Schools 5
EWS Pilot School that use Vendor Model 2
Schools who started 2015-2019 6
Five of the schools contacted in the interviews were Pilot schools, schools that participated in the
pilot phase of the OPI EWS launch in 2012. They continued to use the EWS through the 2015-
2019 period, however two discontinued their participation by 2022.
We coded interview transcripts via an inductive and deductive process. The rough Zoom
transcript was consolidated by comparing the transcript with the audio recording. At the same
time, the responses were open coded (inductive) that allowed certain patterns to emerge. Open
coding focuses on the process of interrogating the data by asking questions relevant to emerging
themes, verifying these themes across interviews, and organizing the emerging analytical
framework based on the findings. The process is highly iterative and required revisions to the
study’s procedures in line with emergent data and themes. This resulted in the modification of
the sub questions to this study (Task 2).
This coding schema was confirmed with data in existing research literature, archival data, and
the results of the survey (deductive). Patterns from surveys were included in the interview coding
process. Themes were constructed based on the coding framework and this deductive process.
Data for each theme was summarized and placed in a matrix that cross referenced interview data
by source. Frequency counts of the codes were not collected due to the low sample size of each
protocol (EWS participant versus vendor model in the case of the interview). Nonetheless, it
soon became apparent that there was a saturation of themes between respondents and that a
coherent and consistent dataset had been collected.
Survey Data and Methods
The survey was launched in May 2022. A total of 29 survey responses were collected. Invitations
were sent to 274 individual email addresses. The sampling process was purposeful between
groups (types of districts/schools) and random within groups (who in the district responded to the
survey). However, the sampling process was less focused than the interview. Criteria included all
people likely knowledgeable of the EWS process. This criteria for the survey did not include
those people that only were aware of a EWIS through a vendor.
Since the survey is a stand-alone survey (not a pre/post), we focused on frequency counts and
Chi Square analyses comparing the patterns of responses to categorical questions between two
groups whose data is related and aligned on the same categorical scale. Analyses were conducted
based on years of implementation (six or more / five or less years), size of the district (less than
1000 / greater than 1000 students), and number of individuals aware of the EWS data by district
(5 or more / 4 or less). The comparisons (Chi Square) allow us to focus on trends apparent
between two categorical variables. For example, we focused on if the school was a pilot school
(six or more years of implementation). The sample size for these schools is small (12). The same
occurred with the size of the districts, with a majority in districts with less than 500 students
(13). The sample size also impacted the analysis of variation in responses from district where 5
or more individuals were aware of the EWS data.
Findings
Level of Adoption
Three courses of implementation among EWS schools can be identified. These do not include
those schools that did not show interest in the tool (non-adopters). Differences between the three
groups are based on the dissemination of the diagnostic tool and local initiative to decide what to
do with the data. Specifically, it focuses on a framework of adoption (formal or informal), the
development of a data culture, and a clear tie between data and intervention. Four transversal
themes were identified that add to the analytical framework to this study: the development of a
data culture, longitudinal analysis, progress monitoring, and relationship building.
Adoption of the EWS follows an opt in model. The dynamic of the model is that schools became
aware of the tool, received professional development about usage, and used locally defined
strategies to implement interventions often by establishing a threshold or triggering event to
warrant a particular student intervention. These efforts combined with a formal mechanism to tie
the data to an intervention often stems from a school intervention team focused on graduation.
These are characteristics of high adoption schools. Schools have engaged the data and
implemented findings into a formal MTSS framework or a school-based intervention team. 60%
of the respondents to the interviews had a formal framework for providing attendance, behavior,
and academic supports combined with informal mechanisms such as conferences with teachers
or with use of a database tracking student trends based on EWS data. More often these
intervention teams focused on behavioral issues and had only recently begun to incorporate
interventions based on attendance and academics. Hence, interview respondents reported that the
shape and scope of these school intervention teams was highly variable; with many schools
identifying that the formal mechanisms were in the infancy. This confirms trends identified in
the research literature that claimed these programs are highly variable and diverse in their make
up (Bruce et al., 2011). Variation was also seen about progress monitoring, with schools having
different levels of engagement. Variation is also seen according to the time-period. Schools
reported that formal mechanisms were suspended during COVID and only have recently started
up again.
Second, districts may use the data, but they do not have formal dissemination or clear ties
between the data and drop out strategies (Medium adoption). Delivery of the student supports in
these cases is often on an ad hoc basis and consists of the same methods as what would have
occurred without the diagnostic tool. Many times, there is no use of the data tool for progress
monitoring and any feedback given about an intervention is likely anecdotal. In these cases, it is
also frequent that relatively few stakeholders have access to the data. In many cases only the
superintendent or principal may have access to the data. Relatively few in the school community
are aware of the data. In short, the data culture surrounding the EWS tool is in its infancy and in
many cases the data culture surrounding dropout prevention is also lacking.
Third, some schools found value in a EWIS, tested the OPI diagnostic tool, and opted out of the
model after initial trials (Low adoption). In short, districts started to use the tool, but opted out
since local use of the tool was unclear, the implementation lacked vision, or the process of
accessing the EWS data was an inhibiting factor. These included issues with login and upload. In
fact, 132 schools uploaded less than 3 times during the 2015 – 2019 period. Interventions were
prescribed and resources were targeted no matter what the degree of EWS implementation.
Evidence from stakeholders (schools that did not use the EWS but began to use the vendor
EWIS) points in the direction that there was only a loose coupling of data to intervention prior to
use of the EWIS.
Though highly recommended in RTI workshops and conferences, the EWS was not required by
the OPI. This led to an uneven implementation, however awareness and interest in the model
spread. This was derived by perceptions of the need for the model within the school community.
Among all schools it is apparent that the degree of implementation was based on local initiative
and choices. This pattern was uneven in scope and intensity. What they ‘got’ from the data is
dependent on their degree of willingness to implement this evidence based diagnostic tool and
associated tiered intervention strategies. The resulting implementation was ultimately dependent
on local initiative.
Inefficiency in the recruitment process is seen in the number of people that tested the application,
however, did not return. Reasons why focus on factors internal to the OPI and external. An
example of internal factors include access to the system and upload. During the interviews, many
respondents revealed that they overcame issues with organizing district data and the upload. Two
districts remarked how the extract from a SIS vendor made the process simple and
straightforward. Nonetheless, the problems with accessing the system at login remained the key
challenge.
Certain patterns emerge based on the processing of scaling up the program. The focus of this
evaluation is on the years from 2015 and the years prior to COVID. By 2019, there were 121
schools involved in the program that year. This involved a total of 34,117 students that had
records uploaded to the EWS system. This compares to 600 non-participating schools that served
children in grades 3-12. Schools that who began to participate in the early years of this initiative
but curtailed their participation by 2019 are represented as non-participants in 2019.
Year Participants Non-adopters
Districts Schools Students Districts Schools Students
2015-16 47 56 15,792 334 597 66,515
2016-17 47 77 22,465 332 595 68,461
2017-18 48 72 18,849 331 595 69,620
2018-19 62 121 34,117 333 600 71,167
Noteworthy, these numbers are the number of schools active during that year no matter the
adoption type. We can further look across the period 2015 – 2019. Based on instances of EWS
data sharing, there were 90 medium to high adopters of the EWS model who uploaded more than
three times. Of note, there were 132 low adopters and 600 schools who did not consider the
model. There are some important differences between the groups based on school size. Among
medium to high adopters, there were fewer schools with less than 150 students in comparison (p
= 0.000). Schools were more frequently of a medium size (between 151 – 850 students).
Med-High Adoption Low Adoption Non-adopters Count
Less than 150 students 22.22% 41.68% 72.83% 512
151 to 400 41.11% 31.06% 21.00% 204
401 to 850 26.67% 21.97% 5.83% 88
Above 850 students 10.00% 5.30% 0.33% 18
Total 90 132 600 822
Moreover, medium to high adoption schools more frequently had 400 or more students in
comparison to the low adoption schools and all other schools. As a percentage of total, low
adopters, those schools that started the data sharing process but did not continue, were less
frequently schools with more than 400 students. Both school size, with 41.68% of the schools
had less than 150 students, and school capacity to gather resources, as seen in per pupil
expenditures and economic disadvantage, were key determining factors among low adoption
schools.
A school’s need for the EWS can be seen in various indicators, including outcome and
institutional Context can be provided based on locale, economic disadvantage, and student
demographics. The composition of the school community based on locale is significant (p =
0.000). In medium to high adoption schools, equal percentages of schools are in town (45.56%)
and rural areas (45.56%). This compares to the town (21.71%) and rural (63.57%) from schools
that are low adoption. What we see in the rates of low adoption versus medium to high adoption
is that low adoption occurs more frequently in rural areas. The interview data suggested the low
adoption phenomenon in small schools occurs because the time spent uploading to the EWS is not
substantially less than a do-it-yourself model of data collection and analysis."
Two measures of economic disadvantage are used in this study. The FRPL count from March
2019 was insignificant at the p < .05 level. The Spatially Interpolated Demographic Estimate for
these schools was significant (p = .002). In medium to high adoption schools (247.96), there is
significantly more economic disadvantage than in low adoption schools (257.50) and with non-
adopters (267.60). This degree of economic disadvantage is mirrored by the overall risk profile
that each school shares. As revealed in the interviews, the likelihood a student will drop out is
correlated with the degree of economic disadvantage. The dividing line between medium to high
adoption schools and non-adoption schools is clear. However, the line between medium to high
adoption schools and low adoption schools is less clear. Similar trends can also be seen based on
demographics. This study identified the percent of students that are non-White. That count is
significantly higher in medium to high adoption schools and low adoption schools in comparison
to non-adopters.
To further distinguish between these schools, we focus on three institutional characteristics:
mean teacher salary, teacher tenure at school, and per pupil expenditure. We found that salaries
were significantly higher in high and medium adoption schools ($49,291.37) than in low
adoption ($45,625.14) and among non-adopters ($40,211.72). Part of the difference in salaries is
that the percent of small rural schools is higher among non-adopters (72.83%). The difference in
this General Linear Model analysis is significant (p = 0.000). Part of the reason behind this
finding is longer teacher tenure.
Significant trends are seen with teacher tenure in schools (p = 0.012). Experienced teachers are a
measure of the quality of instruction. Teachers in medium to high adoption schools have longer
tenure than the other groups. Significant trends can be identified based on per pupil expenditures
(p = .007). Per pupil expenditures provides a glimpse into both the costs of education and the
revenues. The amount able to be spent are gauged using local, state, and federal sources
(revenue). Expenditures focus on what is needed for each student in the building (from teacher
salaries to the physical plant). Among medium to high adopters the per pupil expenditure is
$14,459.83. This compares to $15,574.57 for low adopters and for all other schools $16,964.73.
Comparatively speaking, the medium to high adoption schools have less to spend on students.
A variety of outcome data can be found with these schools. First, cohort graduation rates were
higher (93.21%) among non-adopters in comparison to 86.50% among low adoption schools and
86.24% for medium to high adoption schools (p = .001). The risk profile for medium to high
adopters and low adopters is similar and the difference with non-adopters is clear. For schools
that reached out, there was an identified need to focus on the topic of graduation. This is
important in that it lays the context for challenges surrounding graduation. It is quite possible
that schools in the last category (non-adopters) may not have a defined need for dropout
prevention programs.
Satisfactory Attendance rates are also higher among non-adopters (49.24%) in comparison to
low adoption schools (40.39%) and medium to high adopters (40.16%). This highlights the
degree of the challenges in these schools. Schools with lower satisfactory attendance rates would
have a higher proportion of students that have issues surrounding attendance. This demand may
have driven interest in the tool. A comparison of high school dropout rates finds differences,
although they are insignificant. Trends can also be identified based on college enrollment
however differences are not significant when using a General Linear Model analysis.
Students in non-adopting schools are more proficient (50.71%) on the ELA assessment than their
peers in medium to high adoption schools (44.43%) and low adoption schools (42.88%). This
General Linear Model analysis, with percentage of students scoring proficient as the dependent
variable with the fixed factor defined by level of adoption, is significant at the p = 0.002 level.
We see similar trends with the Smarter Balanced math assessment, although the findings are
insignificant. The trends regarding the ACT Composite average are significant (p = 0.020) and
shows that the non-adoption group scores higher (19.54) than the low adoption schools (18.54)
and medium to high adopters (18.72). This last trend is important. In high schools, the lowest
ACT Composite mean is among low adopters. This shows another level of need that the
diagnostic tool is designed to meet at the high school level. This reinforces the fact that trends
between non-adopters and EWS adopters (no matter what the level) were driven by interest in
the tool and a defined need.
The trends between medium to high adopters and low adopters are similar on each of the
outcome measures. The same is true with the institutional variables. Based on these trends, it is
likely that medium to high adoption schools and low adoption schools have more demonstrated
need and hence reached out to investigate the use of the tools. Between the two sets, low
adoption schools had a greater percentage of small schools, although it is important to note the
proportion of medium to high adopters that had between 151 and 400 students is above 40%.
There was a high incidence of data use among medium sized schools. Diffusion can be said to
come down to finding value. It is exactly the process of meeting identified need, finding value,
and frequently returning to the data that demonstrates the intensity of data use that distinguished
between different kinds of adopters for all sizes of schools.
What are the Mediating and Moderating Factors that Impact Implementation?
As revealed in the interviews there are a variety of mediating and moderating factors that
determined the types of implementations and the variation in implementation within each type of
schools. In this section we discuss the perceived value of the tool (why it matters), the identified
factors (the impact of MTSS teams, vision, dissemination, progress monitoring, OPI outreach,
relationship building) discuss the costs associated with the tool (efficiencies), and the alternatives
in Montana that school leaders have when planning dropout prevention.
Perceived Value of an Early Warning System
“The EWS is just a small picture of the bigger picture that we have with each kid” (Principal).
EWS data is often combined with other data points so that schools may identify thresholds of
support and be better able to target resources to those most vulnerable or to identify situations in
which interventions are most likely to have an impact. “It gives us great access to information
that really helps us identify those most at risk kids, so that our teachers can swoop in and do what
they do best in terms of building relationships and helping that sense of belonging in school and
by adding those supports and intervention pieces so that student have the potential to be
successful” (Principal).
One principal noted that exactly the point of an EWS is to “go ahead and find solutions for
problems that are actually solvable.” The first step of this intervention cycle is reportedly the
diagnostic tool and identifying where interventions can have the greatest impact. According to
another high school principal, “If we can get as much data on them early enough to identify
them, we can do this sharing of data with the elementary schools and identify those students with
the highest needs before they step into the building.”
In responses to the question regarding motivation to use the diagnostic tool, five schools noted
the ability to identify students, set thresholds, and prioritize students. This triage was seen as
focal to the success of the MTSS team and highlights the degree to which educators diagnosed
risk, mitigated risk factors, and targeted resources to the areas most needed, or at least prioritized
to the school. Prioritization based on dropout probability was common.
School Team Dedicated to Dropout
The primary factor seen in the data was the presence of a MTSS team or a team of educators
working toward dropout prevention. The latter occurred in the context of a program, often a
grant, that was targeted to dropout in a certain grade. Focus has been placed on broadening the
scope of the MTSS process to attendance and grades. This would allow for specific attendance
related and academic interventions that had not been considered by the MTSS team in the past,
in this case the use of the EWS attendance and behavior risk factors in discussions. As one
principal notes, “We are looking to expand our MTSS group into more things like attendance and
grades and those kinds of interventions to catch students before they get a special referral. We’re
hoping that with EWS data we can proceed down that path and develop prevention strategies.”
Vision
Vision is an important mediating and moderating factor. The importance of having a shared
vision rooted in the leadership of the school leader was important. Further the degree to which
this vision became communicated to counselors and faculty distinguished programs was also
important. Most of the participants in the survey were principals and superintendents. This is
important since in the survey there is emphasis placed on the vision of leadership. And this
leadership stems from a central source. As the interview data reveal, in most cases this vision
was communicated to others involved in dropout prevention. Indeed, dropout prevention was a
priority of many districts, but not all. Vision is seen as crucial along with the willingness to place
importance on the data and tie that to meaningful interventions.
The impact of vision can be seen in the cases where EWS implementation occurred without
vision. One school liaison reported that she used the system intensively for placement of students
into an alternative setting. Once she left that position and began work as a school counselor in
the lower elementary grades, no one else in the district used the tool or tied any EWS data to
evidence based interventions. There was reportedly no buy-in or recognition of the usefulness of
the EWS from leadership. Although the district did reportedly have a data culture highlighted by
data driven decision making, the EWS was not recognized or sponsored by school leadership.
Due to the lack of shared vision and the data culture in the district which did not focus on
dropout prevention, the district slouched out of the program.
Educators report success of different kinds of leaders playing different roles if the vision is
shared among stakeholders. Most often the use of the EWS as a diagnostic tool and integration of
the tool in the MTSS framework is led by the principal, although at times assistant principals,
school counselors, and department heads reportedly have success leading the intervention
process if vision about student level and school outcomes are consistent (O’Cummings et al.,
2015). The role of advocates to function as a case manager was important. This advocate was
typically a school counselor or lead teacher. These advocates streamlined the work of the MTSS
committee.
Dissemination
This is also seen in the number of people that had access and reviewed the data. This indicated
the scope of the dropout remediation efforts and school wide supports (data culture). Review by
faculty of the data was seen as crucial in verifying the accuracy of the data and in progress
monitoring. Faculty were the focus of academic supports (Tier 2) and mentoring programs (Tier
3). Most survey respondents report that their school uses semi formal ways to disseminate data
prior to an intervention. Semi formal is defined in the survey as at least one stakeholder reviews
the data prior to an intervention.
Given that most survey respondents were from district or school leadership, the fact that most
respondents report that only one person reviews the data prior to an intervention (semi formal)
indicates that the review likely took place by leadership and not in the context of a team. And
that data was not communicated to other stakeholders. Formal, in this context was frequently
described in the context of an MTSS committee. No formal indicates that there may have been
engagement with the data, but often this occurs when the data is not acted upon to use in
intervention planning. The most frequent use of informal dissemination was in confirming
existing data.
Interview participants noted that dissemination of data occurred in formal and informal ways.
Often, there was a structured time dedicated to report review by school counselors and an
administrator prior to introducing the data to an MTSS team. Data was also distributed through a
modified version of an EWS download built to suit the team’s purposes. This process was seen
as informal but did include structured conversations with teachers. Half of the survey
respondents report that less than 4 people have access to the data. This metric includes those that
have online access and those that may have received the data from a principal or counselor.
One example of how this data was aggregated with other school level indicators was the
introduction of testing data, most often MAPS testing data, to the spreadsheet/database
containing the OPI download. Some administrators note that they distribute the detail report in
paper form to counselors and teachers. This entailed informal conversations with small groups of
teachers. A principal noted that this process was difficult and time consuming (downloading each
detailed report) and wished that the OPI would have a batch download of all detailed reports by
grade level. She said a pdf, or a spreadsheet would serve that purpose.
Overall, based on reports, dissemination was highly localized and in high adoption schools was
designed to meet counselors and teachers’ needs. The key was finding a tool easy to
communicate and let data turn into formal and informal conversations. According to one district
administrator, “My role was getting it into a presentable fashion, sharing, making sure everyone
on the team had available to them. Making it user-friendly.” One point of feedback is that this
process was labor intensive. One principal whose school transitioned to the vendor model noted
that this prep time of data distributed to counselors and teachers is cut in half since the vendor
already has all the data elements in the report.
The more involvement that was inspired by the data there is a greater degree of spread and
adoption throughout the school community, as seen in the formation of a data culture
surrounding dropout. And evidence points in the direction that this coincides with more
successes of the implementation process. This culture highlights the importance of informal and
formal dissemination of the data working in tandem with intervention teams and faculty engaged
in the actual work of the intervention, primarily mentoring. Culture raises issues of how school
level issues surrounding dropout are identified, how the community structures support targeted to
potential non completers and intervenes with data in hand that establishes progress indicators
about the intervention. The basis of this culture can be seen in conversations and engagement
surrounding dropout that is data driven and informed by the tool.
Progress Monitoring
Fourth, progress monitoring became a clear indication of the success of this use of the tool. Most
schools reported focus on early identification and the resulting benefits of the tool. However,
many schools did not engage in monitoring over time using the tool. One issue cited was the
difficulty in tracking longitudinal data. Nonetheless, in many cases schools identified the
opportunity to track as what highlights the spread of this implementation of the tool and the
reason why they use the tool in the first place.
Degree of OPI Outreach
Fifth, the degree of OPI outreach and individualized support was important. As with many
aspects of Montana education, there is local control. The degree of EWS implementation is
localized and based on multiple interrelated factors. The core of these factors is how the district
finds value in the data and what they decide to do with the data. Given the scope of these factors,
OPI support was seen as a catalyst to school level change.
Relationship Building
Sixth, in schools that have a high degree of spread of this data culture, there is a focus on
building relationships within the school community focused on the relationships built with
students. Respondents to the interview were asked questions to the degree to which they feel they
know their students without having to use data supports. Three users with school populations
under 150 students remarked that both qualities are important. The diagnostic tool allows for you
to confirm what you know already, what data you can gather from data systems, and what ways
it is best to intervene with a student and monitor progress over time. The alternative is to rely on
the time of individual counselors and teachers as they identify and provide data about each
student. This presents increased administrative burdens. For example, studies suggest that small
schools matter when it comes to implementation since a large part of the process involves
building relationships with students, creating a supportive environment, and a curriculum that is
more focused on success (Jerald, 2006). Relationship matter on a personal level. The benefit of
using an EWS is described by one counselor as providing the opportunity to get to know your
students better. She felt that by using the system she is closer to her students than she would have
been without it.
More Effective Use of Intervention Dollars
The consensus among respondents to the interview was that interventions cost less or about the
same when using an EWS due to ability to identify resources, identify support, and avoid issues
with ‘false alarm.’ One principal commented that costs are minimal per student, but costs would
be higher if they didn’t have the EWS data or the ability to target resources. Moreover, it allows
for the retention of goals to return students to core instruction rather than alternative school
settings.
The main cost is staff resources, where there are two potential effects. First, costs could increase
as the EWS identifies more students with need and more intensive interventions are prescribed.
Second, EWS can make existing resources more effective perhaps by lowering costs per student.
Costs need to be viewed in context. According to the principal use of staff resources is often less
than what would occur with alternative schooling. Summarized by another principal, “the EWS
lets us target interventions to those kids that need it most. I’m not sure if it costs any less than
without the EWS. It allows us to be more effective because it can be targeted since we have data
to decide who needs it most. It is a more efficient use of our intervention dollars because we can
target it better (Principal).”
Another user points to the ability to share data both internally and externally to the school brings
costs down. This sharing is often seen in the context of engagement with parents or external
partners such as social services or law enforcement. One respondent notes that the EWS data
may in fact bring in funding that would otherwise not have been found except for the grant
writing process involving EWS data and the use of EWS indicators as progress monitors for the
grant.
Alternatives in Montana
In Montana many districts have found alternatives to identifying the incidence of drop out. The
first is quite simple and rests on doing what educators did before the advent of early warning
systems. Many educators place this in context in that it takes time to access existing data on
attendance, behavior, and course work which many times are in different datasets spread across
data systems.
Second, larger schools may be able to invest the time and have the research capacity to conduct a
universal screening (data collection). A Vice-Principal from a large high school in Montana takes
pride in the way that their school does their own data collection, management, and analysis of
their diagnostic survey given to students. Many districts, especially large districts with research
capacity and sufficient sample size, decide to customize data to fit their districts needs and
vision. However, there are questions to which local tools are evidence based and normed to local
and national standards.
Third, as Data Quality Campaign (2013) noted, there was a broad movement by Statewide
Longitudinal Data Systems to offer reporting based on early warning system indicators.
Recognizing the evidence-based benefits of a EWIS, the OPI began its pilot phase of developing
the OPI Early Warning System. By 2015, this process became more readily accessible while
being scaled up and was moved online into the Statewide Longitudinal Data System web portal
(GEMS). There users could upload their own data according to the needs of the school and
access three dynamic reports (school summary, student summary, and student detailed report).
Schools report that they must look at over five different data systems to get a view of the same data the EWS
provides. One principal remarked how the alternative, do it yourself, is no longer attractive because it takes needed
time away from the interventions. And the results are more consistent and insightful with a diagnostic tool that is
focused, and evidence based. This tool is seen as easy to communicate, something which schools before had issues
completing in the past. Indeed, educators claim that a EWIS is the bigger, better offer and allows for more
opportunities to individualize data based on students evolving needs.
The poll the Vice Principal mentions targets social emotional interventions, specifically focused on resiliency and
school satisfaction/student engagement. By having ownership of their data collection and crafting it to their data
needs, the linkage between diagnostic instrument to outcome is reportedly easier to explain and make relevant to
local contexts. This poll involves a universal screening to identify social emotional habits that can make the student
a success. Interventions focused on universal supports; there wasn’t a clear linkage to dropout prevention.
As of 2019, a total of 121 schools were either pilot schools or provided a minimum amount of
data to generate a school profile in that year.
Fourth, as vendors began to see the benefits to this education market, a fourth alternative based
in their Student Information System (SIS) was found. Respondents remarked that vendor led
models are more relevant to their situations in that they are engrained in the schools’ primary
data system. Respondents claim they see a clear benefit to using a EWIS and value that their SIS
has an early warning system.
Does the OPI EWS Work as Intended?
Users that reported a robust data culture in the school indicated that in those cases the OPI
diagnostic tool worked well, and the process developed as intended. There became a clear tie of
the data to intervention to outcome. The system as intended relies on data sharing, OPI support,
and functionality of the web portal.
Data Sharing
When it came to administrative tasks involving the use of the EWS, the survey data reveals that
most persons that uploaded were not leadership or counselors. Often this involved an attendance
secretary, technology professional, or school librarian. This signals that the person uploading
may not have experience with the benefits of the EWS or see the position of the data in their
school community. This may have impacted buy-in for the evaluation.
Schools interviewed suggested they upload typically at each quarter, although respondents
mentioned that they upload monthly. This variation is noted in the data. Evidence indicates that
schools that first started participating after 2015 upload less frequently that the incidence data
was uploaded for Pilot schools. Survey respondents reported that data was uploaded on average
for fewer students in the school than the school population. As the interviews revealed, at times
the focus of the school’s dropout prevention program was on a certain grade level, hence users
only updated for a portion of their students. Also cited was the decision to test the EWS with a
smaller set of students to make sure the process was correct before adding everyone.
What mediating factors impacted this data sharing? To explore the hypothesis that the presence
of faculty and certain non-instructional staff is related to the incidence in which a school has
shared data with the EWS, we explored five different regression models to see how much, for
example, is the incidence of upload explained by the pupil teacher ratio. We found that pupil
teachers ratios have a moderate association with upload frequency (r = 0.230). The higher the
per pupil ratio the more likely a school would upload more frequently. We also looked at
counselor ratios, psychologist ratios, social worker ratios, and librarian ratios and found that they
explained little of the variation in average uploads / year.
The decision to make the tool a universal screening was a local issue, as was the decision to
upload a certain number of times. The counts of students uploaded on average show the intensity
of engagement in the data sharing process and do indicate there was an ambition towards
focusing on how many students uploaded at a time.
Greater than 1000
Less than 100 students
students
14%
25%
Between 501 and 1000
Between 100 and 500
students
students
14%
47%
Less than 100 students Between 100 and 500 students
Between 501 and 1000 students Greater than 1000 students
Most frequently, users uploaded data for 100 to 500 students. This is in line with the analysis that
EWS schools are most frequently under 500 students. Comments from schools that shared data
were minimal (4) and focused on the ease of the system to upload data. One counselor remarked
that it was not difficult at all, and she just needed to review a tutorial. A principal placed this in
context: “I think it’s really user friendly. I understand the issue with OPI and all the different
student information systems in the state. Whether it’s Power School or Infinite Campus. It’s hard
to get one size that fits all.”
OPI Outreach
One principal remarked how he was involved in the design process of the OPI Early Warning
System since 2012. As a result, he presented with OPI analysts on the benefits of using the OPI
system and the highlight of EWS inspired mentoring programs. He also frequently talked about
the EWS with groups in local associations (School Administrators of Montana). His takeaway
from this experience is that success depends on the quality of relationships established. He says
this about his own district and with others. Larger districts were seen in a different league and
struggled to find shared vision and to communicate the value and use of the EWS. OPI took the
lead in fostering programs that are framed by building relationships between at risk students and
school faculty.
Some respondents praised the work of OPI staff in designing and supporting the EWS program.
Two respondents remarked that no matter what the issue, they feel that their voice was heard at
OPI no matter the complexity of the issue. This support included the revisions of the tool during
the pilot phase, extensive outreach at conferences about the tool, and one on one support from
OPI staff about integration of the diagnostic tool in their schools. Three users wish OPI would
provide additional supports including more presentations at conferences or to groups such as the
School Administrators of Montana. This especially occurred in the context of COVID and with
supports to schools as they emerged from COVID to reestablish MTSS processes and dropout
prevention programs.
Accessing reports remained the main area of concern and reason why users reached out to OPI.
A user expressed frustration at difficulties in accessing the data and issues when trying to
collaborate with others about the data. Many educators expressed the desire to engage their
whole faculty with the results of the diagnostic tools. One noted a difficulty in doing so was that
access for each teacher would require the signature of the Authorized Representative of the
district, something that was not an easy task to complete. She also noted the difficulty in
accessing FAQs and online support within GEMS.
Perspectives on How the Reports Function
Opinions on the design of the EWS range from too ‘cluttered and cumbersome’ to ‘similar to
other reporting systems.’ One user specifically remarked on the similarity to MAPS reporting,
which was seen as a benefit since teachers were familiar with the format. Users commented
about the summary report and the detailed report. One principal distinguishes between the
summary report (where ‘he lives and dies’ with data distributed to the faculty) and the
individualized detailed report (which is good for a focused meeting about an individual student,
for example, with parent meetings).
Users commented on the overall transformation with GEMS modernization. Users were pleased
with the ease of access of data elements on GEMS and commented about the professionalism of
the Power Bi dashboards. Yet, they pointed out that the EWS reports have not changed
significantly and appeared out of date. This was felt especially with the reported need for
longitudinal data and analysis. Respondents note issues. A user remarked that the decision to
switch to a vendor model was that the EWS was subpar and inefficient. Another school
counselor expands on this point by saying “You must prepare how you are going to organize the
data. It doesn’t help that we don’t have longitudinal data. You must reorganize it the same way
each time.” And this process of preparation reportedly takes time especially when schools try to
create their own spreadsheets/databases based on EWS data that integrates additional data points,
including achievement data (MAPS) and qualitative data from teachers.
Quality of the EWS
Overall, user reports about the quality of the EWS were positive and showed encouraging signs
of the development of a data culture surrounding dropout prevention. Even the schools that did
not participate in the EWS remarked at the benefits of an early warning system. When discussing
quality, respondents describe the most and least beneficial aspects of the EWS.
A principal noted the time focused on EWS data has been a routine of her school’s data culture:
“I think EWS just keeps it in the forefront. If we didn’t have a system in place, we would forget.
But now, it’s always in the forefront. Teachers have gotten used to receiving the data. I think, it
just allows us to keep the students in the forefront.” Establishing the EWS as part of a set routine,
often part of a MTSS process, allowed users the opportunity to focus less on identifying students
and more on placing time and resources on the actual supports for student success. This was seen
as a catalyst for a data culture surrounding graduation and student engagement. Another
administrator commented on how EWS data allows the district the opportunity to focus more
closely on early identification of attendance, behavior, grades issues and provides markers where
to construct supports.
There were many remarks about the reliability of the data. A principal remarked about the
consistency of the data: “Everything is consistent and easy to see.” Another principal remarks
that this consistency and objectivity makes EWS data more relevant than anecdotal data from
teachers or counselors. And establishing trends in this data across multiple times that data is
pulled is seen a necessary task of the MTSS team. Themes highlight the ability to target
resources to those area most in need and that would likely have the largest impact.
One principal remarked at how easy the data is to share with fellow administrators, faculty, and
parents. Administrators further comment on the ability to share data in an objective fashion that
focuses on attendance, behavior, and grades. Often this process was done by paper handouts and
spreadsheets that administrators and counselors disseminated. One principal discusses the most
beneficial aspect of the EWS in context of the reliance on the tool: “I think without the EWS data
We don’t have the time or resources to do what the EWS does. You know it is quick and easy in
terms of time and resources. I imagine all the time I’d spend trying to compile all that data. The
per student cost is minimal compared to what it would cost without an EWS.”
All persons interviewed saw the value of a EWIS and expressed that they were in one phase or
another of integrating the tool into their school’s data culture. Often these stages remained
preliminary. Users focused on the process involved with the OPI EWS. Accessing the portal is
the most cited difficulty that users experience with the EWS. An Assistant Principal remarks the
process of pulling EWS data is wasteful: “I don’t even try that day to have a meeting because
I’m going to get an error that doesn’t allow me in. So really, you must plan to pull the data.
Really a nightmare and discourages a lot of people from using it.” That issue with accessing the
data is compounded by issues with sharing the data. There seemed to be a reliance on paper
copies and excel files, something that may indeed infringe on student confidentiality and privacy.
Of the risk factors, users generally trusted the data and the found the data displays adequate.
However, mobility was mentioned by multiple users as misleading. Users remarked that EWS is
piecemeal especially in the context of the mobility piece that may identify students who would
otherwise receive universal support into a tiered intervention. They were confused of the weight
mobility factored into the overall calculation of dropout probability. Mobility is seen by multiple
users as raising ‘false alarms’ that can only be resolved by monitoring students over time.
Requests were made for FAQs which focused on mobility, how it is calculated and what impact
it has on dropout probability.
School Led Interventions
Interventions remained central to the development of the data culture. OPI fostered individual
supports to engage schools about the range of interventions available. Often, this occurred at
conferences about Response to Intervention and Multi-Tiered System of Support where the OPI
EWS was included as a diagnostic tool meant to support these interventions. As with any data
tool, data use is the most important factor. In the OPI EWS model, this data use goes beyond
dissemination and communication, but also embraces the steps to turn data on risk factors into
student successes on the road to graduation. Important to intervention processes is the explicit
tie of the data to an intervention.
Use of a EWIS encourages engagement with established school level procedures for support.
Regardless of which EWIS, users engaged the MTSS in the same manner although at different
levels and degrees of intensity. Consistency of intervention is important. One district when
switching over to a vendor EWIS had the same process (tie of data to intervention) as they used
with the EWS. Of the fifteen schools interviewed, nine identified that they had a MTSS system
in place, were in the process of constructing this formal procedure, or they were in the processing
of reviving MTSS procedures that had been suspended because of COVID.
Most survey respondents reported that identified students receive support, regardless of if there
was a reliance on EWS data, at least 50% of the time (14 stakeholders). An insight of the data is
that 38% of educators report that their school provides intervention less than 50% of the time.
Non-identified students that the system missed received support less frequently and intensely at
the 50% level (7 stakeholders). This indicates that internal procedures surrounding identifying
dropout were still in development. With the students identified by the school, 13 schools reported
these students received EWS based support at least 50% of the time. In which case 16 schools
did not use EWS data at least 50% of the time. Based on the interview data, those schools are
likely to have fewer people review that data (informal) and a data culture surrounding the tool
likely was not developed.
No Opinion Never Rarely
11% 0% 14%
Intense
25%
Sometimes
32%
Often
18%
Never Rarely Sometimes Often Intense No Opinion
Just as important as the frequency of intervention in EWS school is the tie made between the
intervention and the data. In Pilot schools, there is a clearer linkage between the diagnostic tool
and the intervention, where users more frequently claim this link occurs most of the time
(p=.031). This may indicate that pilot schools had more experience with the EWS and that
development of this data culture surrounding dropout prevention takes time and vision.
The engagement with the formal MTSS procedures is summarized by an Assistant Principal as
entailing a process framed by individualized supports depending on the degree of support
provided to the student: “We try to honor students for making good choices and working hard.
So, that was a big thing we did when using the data. When we looked at risk factors, we would
identify some things we have in place that would be a Tier 1 intervention. So, basically, we went
through a process of building out our tier system using the data (what would be a Tier 2
intervention, Tier 3) for each student.” Frequently identified is the need to engage the whole
context of how the student is struggling especially circumstances that may be coming from the
home.
Meetings with the student and parents were important to communicate identified risk factors and
how the school could approach mitigating risk. Often, it is paired with a behavior contract that
formalizes the intervention plan through agreement with the student and family. Three
respondents noted during this process that the EWS data was beneficial to share with parents
about how their student was progressing in attendance, behavior, and academics based on EWS
progress indicators.
Having the identification completed, or at least in a way that priorities could be established, users
responded that they could focus on the kinds of supports that are necessary for each student. For
example, some students may receive supports from multiple tiers. This occurs in the context of
the same student being pulled out of class for Title 1 small group instruction (Tier 2) while
participating in a mentoring program where the student checks in with faculty every morning
(Tier 3).
A principal summarizes the kinds of interventions in their school (which are common in other
schools that were interviewed): “We have some one-on-one support in the classroom with a Push
In model with paraprofessionals. We have Pull Out support in a traditional fashion in which kids
are pulled out for support one on one and in small groups (quieter least restrictive setting). We
have tutoring support especially in Title 1 and Special Education where they work on specific
skills, gaps, and skill deficits to help build that background so that they can remain in an
inclusive classroom and receive core instruction. Sometimes, it’s not traditional. We are using a
Check and Connect system using an intervention aid where our kids check in early in the
morning.” The point of the highest level, mentoring, involves relationship building and fostering
a sense of belonging in the school community as well as providing structured support for
academic goals.
Another principal describes that communication is key to the process of assigning interventions.
A focus is made on early interventions. He describes how he talks to staff to get perspectives on
each student’s circumstances. He engages the families in the process, something frequently done
with the support of EWS data. Building relationships is also important, for example, when
finding the student a mentor who is the right fit and defining which resources are available for
each student.
Also important was the involvement of external agencies on the MTSS team, such as law
enforcement and social services. Two respondents remarked that the intimacy of their schools
allowed for close relationship by multiple actors and allowed interventions to focus on family
needs. One of the interventions proposed by both educators was to focus on attendance and
creating supports that would bring students to school by providing transportation. In doing so,
one district administrator commented on how he drove a small school bus to pick up families that
were not on an established bus route. Such creativity in support was seen as crucial in
establishing relationships and encouraging engagement at school and with parents.
Regardless of the level of support, the end goal remained clear – to get students back on track so
that much of their time can be spent with core instruction. Respondents reported that they focus
on MTSS in the context of the inclusive classroom where students can get back to core
instruction as their primary support. And some educators noted that EWS data was used to
provide measures for this transition to core instruction.
Effectiveness of the Local EWS
There are complexities in measuring the effectiveness of the EWS model. Reportedly, the model
is a success among educators that use the data, meet business requirements, tie data to an
intervention, and foster the development of a data culture surrounding dropout prevention. Data
was used as intended among many schools. The scale of which differs by school. This is seen in
progress monitoring, addressing the issue of students that were missed, and acknowledging
perception of success of local implementations.
Data that does not yield results is a sign of the inefficiency of the system. One mark of this is the
number of non-completers that were not identified by the tool. At a different level are those
students that are identified by the system in situations where local data did not agree. Most users
remarked how they used EWS data to verify local, qualitative, findings. For example, two
schools noted that they use the diagnostic tool only after cataloguing insights from their faculty.
Understanding how each data set may or may not agree is important.
Comments ranged about the accuracy of the EWS numbers. Accuracy addresses the reliability of
the tool. Many educators use the data to confirm local data. Sometimes there is a disjuncture:
“Sometimes you know the numbers are not correct. For example, a model student gets flagged. It
would be nice if we could go back and check the reports, like a complete breakdown of where
they were. It would be helpful if we were provided more specific data on each of these factors”
(Principal). Users would like the ability to track students over time and use this tracking to gauge
the accuracy of reports. Users report that having the longitudinal data in a data display was not
important. Rather, a download with data on dropout probability and risk factors for all the times
data was uploaded for the students.
Other times there are very few surprises. One area many respondents focus on is mobility data
and how the simple fact of a transition to a new school may yield change in at risk status.
Respondents point to the fact that they do not place emphasis on such students that have been
flagged that way. One principal noted that he would like to have a focused FAQ on the impact of
mobility on the EWS model. Other users responded that they wished they had EWS analysis
without the mobility factor.
When asked the question about whether they were able to identify students that the EWS missed,
most respondents returned to the theme of the accuracy of the model. On one hand, a principal
noted that “I think this helps us with those kids that are sleepers. Maybe not in every area. It does
put them on our radar.” Local data can also reveal students that are facing challenges. A principal
noted that some students missed by the system have challenges that are just emerging and may
not be reflected in the data. By cross checking both sources of data, one principal remarks,
“Without an EWS we wouldn’t have been able to identify those situations like we needed to.”
Respondents commented that the reliability between local data and the EWS was fostered by
updating the data in the EWS and running the reports at regular intervals. This longitudinal
analysis was seen as key to verifying accuracy. This happens when examining if and how a
student is at risk is identified over time.
A crucial element to determining level of adoption is the degree to which progress monitoring is
occurring. Data informed follow up is evidence of a tie between data and intervention. This
proved to be an indicator which differentiated schools and also a sign of the development of a
data culture. Presence of follow up procedures showed promise that the process involving
interventions was occurring in an effective manner. Most respondents to the interview reported
that they did not have an established process with progress indicators, even in the context of the
MTSS team. They did however stress they had procedures for follow up. This was seen in the
survey data where most educators stated that they had a follow up process with students that
receive an intervention (23). An interesting trend occurs based on school size. Educators from
schools with less than 500 students more frequently report that progress monitoring does occur
(p=.037). It is possible that the finding points in the direction of the importance in building
relationships among faculty and students, especially with student engagement.
Educators interviewed commented on the possibility of using the EWS in an MTSS team to track
the outcomes of an intervention. They acknowledged that progress monitoring is part of the
process of creating and refining goals for each student. Each goal is tied to progress monitors, for
example, variation in the overall drop out probability of a students. This may be combined with
triggering events that would warrant a change in the intervention. This tracking is tied to a
reward system where students are recognized for their achievements. Regardless of the use of the
data, there was an emphasis placed on longitudinal data and tracking.
In three cases, respondents identified that a student advocate or case manager was critical to the
student’s success, especially with identifying trends in the data and advocating for student
supports among stakeholders. This advocate, who may or may not be a mentor, guides the
student throughout the dropout prevention process, represents the student in front of the MTSS
team, and ensures that basic needs are being met in the home. Having a single point of contact
regarding the support process was seen as crucial.
Respondents recapped their school level processes by noting success. Survey respondents claim
that the intervention suggestions made on GEMS are helpful (20) indicating a tight coupling of
data to intervention. Nonetheless, in the interview data we see that there is not a tight coupling in
survey participants about the primary outcome: whether students graduate or go on to the next
grade.
None
Less than 25%
4%
7%
All
Between 26% and
4%
50%
15%
Greater than 75% Between 51% and
55% 75%
15%
None Less than 25% Between 26% and 50% Between 51% and 75% Greater than 75% All
The majority of survey respondents indicated that greater than 75% of their students go on to
graduate or proceed into the next grade (59%). This finding is confirmed in the interviews where
the outcome of the EWS program reportedly led to an average of at least 75% of more of the
students staying in school and graduating.
Interview respondents took a cautious approach in defining success as local programs are a work
in progress. Some respondents claim that the MTSS process is still in its infancy especially in
cases where the MTSS team did not meet during COVID. Others with more established
intervention frameworks respond that they made progress on the dropout prevention model and
experienced many successes; however, they are looking to revise the system to meet current
needs and priorities. There is an everchanging risk profile of students in these schools.
Respondents focus on the need for the MTSS model to establish a vigilant 360-degree vision of
the student and their emerging needs.
Conclusions
Models for dropout prevention have focused on crisis management (addressing those issues that
may frequently pop up, or as one Assistant Principal suggested the children’s game), long term
plans for student and family engagement, and intervention plans and contracts crafted together
with the student and family. Relationship building is frequently mentioned in the data. So is the
development of a data culture, longitudinal analysis, and progress monitoring. At the core of
Task 2 is the research question; What is the degree of implementation of the EWS program in
participating schools? We address this question by focusing on the level of adoption, mediating
and moderating factors to the implementation, how the EWS worked at the school and state
levels, a spotlight on the tie between the diagnostic tool and interventions, and a discussion of
relevant data about the effectiveness of EWS implementation in schools. These sub questions
and themes are addressed by data on implementation in our data warehouse, interview data from
15 schools (32 participants), and survey data from 29 EWS stakeholders.
Our findings indicate that the degree of implementation is dependent on the level of adoption.
Schools with high adoption tend to have established procedures linked to the MTSS teams or
school-based intervention teams, formal and informal dissemination of EWS results, a clear tie
of the data to intervention, and explicit follow up procedures to an intervention (progress
monitoring). Variation between high adopters did occur based on progress monitoring, however,
issues such as vision, data culture, and opinions surrounding OPI outreach remained the same.
Professional development is recommended to address this variation among high adopters.
Schools with medium adoption have few of these traits. One thing noteworthy of these schools is
they tend to not have MTSS processes or a team focused on dropout prevention. Moreover,
dissemination of the data occurs among a limited number of stakeholders, involves informal
practices, and data use surrounding intervention is limited to early identification. Professional
development should focus on the topic of data use and the development of a data culture linking
data to intervention. They have the tool, however there are questions surrounding data use that
OPI outreach can address. In many cases, stakeholders continued to do what they would have
done in the absence of an EWS such as gathering data from multiple sources surrounding
attendance, behavior, and academics. In these cases, EWS served as a reference and a means to
verify local data.
The EWS program did experience challenges with scaling up. There were 132 schools that
started the EWS process, however only uploaded data a handful of times (low adoption).
Interview evidence from these schools suggests that at least one stakeholder found value in a
EWIS, and they uploaded data for testing purposes only. Archival data suggested that the
motivations (economic disadvantage, locale, and student demographics, graduation rates,
attendance, and achievement) were similar between medium to high adopters and low adopters
suggesting that the warrant to participate in the EWS, or any EWIS, is similar.
There were many significant differences in an analysis of these variables comparing non-
adopters to EWS adopters. These indicated that schools in the non-adoption category have less
economic disadvantage, are more likely from rural areas, have higher graduation rates, have
higher satisfactory attendance rates, and higher rates of proficiency. Moreover, they are
predominantly elementary schools. Hence, the risk profile for schools that showed interest in the
EWS (adopters) is different than the rest of the schools in the state that showed no interest.
Interviews indicated that the business rules surrounding EWS share part of the blame,
predominantly access and sharing of the data. Most comments focused on access issues, although
comments did mention data sharing. Schools that stuck with the data sharing process, read
tutorials, and engaged the OPI research analyst for assistance viewed in hindsight that the
process was straightforward and uncomplicated. Thus, when business requirements were
transparent and simple, users more frequently experienced success.
COVID has brought challenges in many areas. Many schools are only recently reengaged with
formal processes for dissemination of the data. Reengagement should be a focus of professional
development for high adoption schools. This can be achieved in group settings through
conference presentations or a meeting with associations such as the Montana chapter of the
Association for Supervision and Curriculum Development.
There are already signs of reengagement with the diagnostic tool. However, this reengagement
occurs in the presence of alternatives. An open question is how many of these high adopters will
migrate to different EWIS? The answer present in the interview data is that two pilot schools
were no longer using the EWS and instead were using a vendor EWIS. There were many reasons
cited for this change, most apparently that the EWIS was integrated into their student
information system. This integration allows for the consistent use of real time data, is updated
daily, and provides a system of notifications which are sent to school users about the status of
their students. Moreover, this EWIS enables collaboration among all members of the school
community, including teachers. Instead of discouraging this engagement OPI should focus
professional development accordingly. This could allow the EWS to focus on those schools
whose student information system does not have a EWIS.
As revealed in the interviews there are a variety of mediating and moderating factors that
determined the types of implementations and the variation in implementation within each type of
schools. The primary factor seen in the data was the presence of a MTSS team or a team of
educators working toward dropout prevention. Vision and the formation of a data culture are
important factors. Survey data indicate the degree of spread of the data culture highlights the
importance of informal and formal dissemination of the data working in tandem with
intervention teams and faculty engaged in the actual work of the intervention, primarily
mentoring. The role of the OPI is important. Targeted professional development that focuses on
medium and high adoption schools would encourage this spread. One defining characteristics of
this spread would be for teachers’ use of the data. One respondent suggested that OPI draft
recommendations for this local process of scaling up by providing tutorials directed toward
teacher use of the tools.
Practices involving the tool indicated variation among high adopters and variation between
medium and high adopters. Often, they have different definitions of what triage means. Triage
means identifying thresholds or triggering events to start, monitor, or reassess an intervention
(O’ Cummings et al., 2015). OPI support is seen as crucial. The core of these factors is how the
district finds value in the data and what they decide to do with the data. This process of finding
value in the EWS tool was seen as dependent on OPI professional development and outreach.
Responses from the interviews indicated that there was value found in many of the school level
models and the support of the OPI was a catalyst. Users reported the system was reliable and that
reports were viewed as accurate. The design of the online tool was praised. The data sharing
process was seen as efficient. The system does indeed work as intended.
In this study we focused on the tie to intervention. Schools that had vision from leadership and
had made strides in the development of a data culture surrounding dropout prevention tended to
focus on the linkage between data and intervention, specifically the thresholds established for the
triggering event to provide a student a specific intervention. These schools all had a MTSS team
or a school-based intervention team. In these schools there was value placed on addressing the
issue of graduation, the diagnostic tool, and the ability to follow through with the intervention
process.
Data on the effectiveness of the tool begins with the key indicator: progress monitoring. Early
identification indicates the willingness to use the tool. Progress monitoring and the willingness to
adjust the intervention strategy is a sign of a well-developed data culture. Unfortunately, few
districts indicated that they do this step in the interviews. This does not mean that follow up is
not occurring. Indeed, the survey did indicate a high degree of follow up. Rather it indicates a
lack of integration of data in the process. Additional ways beyond early identification to use the
data are important. There is a relatively narrow use of the data. Only a handful of users found the
tool useful for grant writing for programs involving dropout. When they did find it valuable it
was in the context of progress monitoring. More frequently, are the incidence of staff making
reports made to the school board and to district leadership narrowly focused on early
identification and triage.
Schools were hesitant to discuss the success of their processes. They were more than willing to
discuss individual successes. Many survey and interview respondents cited that 75% of identified
students went on to graduate or go on to the next grade. However, by and large interview
respondents expressed the need to adjust their processes and make them more relevant to the
years after COVID. The tenor of the responses focused on the need to reshape the data use model
to address the remaining 25% that tends to have an ever-changing risk profile.
This study faced two limitations. First, the sample size of the survey. Due to circumstances
beyond the control of the research team, OPI could only solicit stakeholders twice to take the
survey. This policy limited state emails to stakeholders. Moreover, many stakeholders opted out
of the Gov Delivery system. Given these factors, it is our best estimate that the response rate was
approximately 15-20% among users that received the email. The impact of the low sample size
includes the difficulty of making comparisons between groups of stakeholders, for example, pilot
school users versus non pilot school users). In a series of 36 Chi Square analyses of trends
between groups, we only found three analyses to be significant. The second issue is that the
survey and interview occurred in 2022. We maintained a focus on the 2015 – 2019 period.
Nonetheless, there were issues extrapolating findings due to this gap. These schools did provide
a lens on how these strategies played out during the 2015-2019 period. Schools recognized the
need and benefits of a EWIS, acted upon that need, sought out a tool, tested the tool and decided
if it met local requirements. Once this value was found they took advantage of the tool.
OPI had success when focusing on high adoption. This high adoption occurred among Pilot
schools and schools adopting at the beginning of the 2015 – 2019 period. Time with the EWS
program was seen as crucial to normalize practices surrounding dropout and the benefits of the
tool. As well as the time it will take to continue the process. OPI should measure its success over
the long term, through involving a variety of stakeholders, and developing a data culture at the
state level surrounding dropout. The 2023 school year is pivotal to reengage with EWS schools.
In fact, archival data shows that the risk profile of these schools (both low and medium to high
adopters) includes high rates of economic disadvantage, low per pupil expenditures, low
graduation rates, low attendance rates, and mixed outcomes for achievement. There is identified
need in these schools.
In fact, many of the themes identified in the report can inform OPI outreach. The primary of
which is making longitudinal data more accessible and focus on developing a data culture at the
state level. This engagement can start now by mirroring the process of many high adopters: since
dropout is a gradual process that can be identified early, school level processes should focus on
gradual change that targets the need to address graduation and student success. There is evidence
that this process of engaging with the data does indeed become more intense over time. This is
seen in the survey data when pilot schools report significant more time spent using EWS data in
comparison to their peers. It may indeed take time, and an understanding of scale, capacity, and
priorities to make the EWS successful in these schools.
Recommendations
Longitudinal Data
Longitudinal data was seen as crucial to identifying interventions, modifying interventions once
in place, and creating thresholds for students to discontinue supports. The principal
recommendation among users was to create ways to manage longitudinal data with reporting
based on each risk factor and dropout probability. This is suggested as a revision to the student
summary report. Ideally, this would contain data from participating elementary and high schools.
Longitudinal data is needed for progress monitoring. Respondents request professional
development on how to use the tool for progress monitoring and identify what are triggering
events involved in the process. This process should focus on the work of student advocates,
mentors, and teachers, specifically how EWS data can be used for evaluation of individual
student needs.
OPI Outreach and Professional Development
Interview respondents discuss how the outreach by OPI was of good quality. However, they
commented on the need for more presentations and workshops about the EWS. These could
occur through organizations such as School Administrators of Montana or at conference like the
OPI Summer Institute. The latter was cited as the most obvious place to begin due to the linkage
with the MTSS. Respondents from high adoption schools discussed how they wanted to focus
less on the use and requirements of the tool and more on the process of identifying students at
risk, establishing thresholds for support, and processes for progress monitoring of students.
Respondents also commented about process issues, specifically of issues involved in
dissemination. It was also suggested that there should be additional downloads for the student
detail report. The purpose of these downloads would be to readily distribute student profile and
longitudinal data. A respondent requested an export of a batch of student detailed reports in a pdf
for each grade and the school. The goal is to get longitudinal data and data from the detailed
report in the hands of teachers.
Frequently mentioned was school and administrator use of the EWS data to establish and
monitor goals for the school community. Many educators found that using the EWS data for
evaluation of MTSS teams and for grant writing was a beneficial use of the data and something
that was not available elsewhere. Respondents requested additional professional development
that discusses ways EWS data can factor into school evaluation and grant writing.
Interventions
One finding from this study is that many schools are in the process of developing a data culture
surrounding the MTSS process. In professional development activities, they claimed that OPI
should create a clear tie between the EWS and MTSS intervention strategies. Specific supports
could focus on the establishment of local thresholds for triggering and monitoring an
intervention.
Access and Business Requirements
The most frequently discussed obstacles to intervention occurred in the context of logging in.
This issue was seen as more important than issues with upload. Respondents desired a single
login for all OPI applications. Moreover, this credential should not expire every three months.
Users also remarked on the process of gaining access. Users claimed that getting access for all
stakeholders, including teachers, would be an arduous process involving district administrator
consent for each individual. A streamlined process of accessing and sharing the data was seen as
needed like how a SIS in Montana provides access to their EWIS. The goal would be for a
school administrator to assign access.
One user commented on the need to streamline uploads for all schools with different student
information systems. He said all the local data provided through the upload process should be
available in Infinite Campus’s State Edition. Ideally this would occur monthly. Another
respondent described that since her school is both an Elementary and a 7-8 building, the system
requires you to access students from one school at a time. She commented that it is difficult to
artificially separate her students by grade level when they are in the same building, with the same
leadership, and receiving the same interventions.
Risk Factors
Many users commented about mobility and how it increases incidence of ‘false alarm.’ OPI
should clearly define why mobility is in with the model and what is the contribution of mobility
to the model (FAQ). Discussions about follow up, or progress monitoring, raised many issues
about integrating EWS data into this process.
Layout of EWS Reports
One counselor was very direct about her main issue with the EWS: the scale on the data displays
(risk factors) is not consistent. Consistency allows for easier communication. The user remarked
that the data displays are misleading since the scale on the various risk factors vary by display.
This was important when sharing data with another user. A first glance, trend lines of the data
displays suggest variation in a certain direction. However, upon closer inspection, the scales
between the risk factors differ from each other making trends for one risk factor (e.g.,
attendance) not comparable to other risk factors.