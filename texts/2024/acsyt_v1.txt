What do we mean when we talk about professional development in formative assessment?
A systematic review.
Alexa von Hagen1*, Alejandra Balbi2, Micaela Bonilla2, Camila Arbildi2
1Competence Centre School Psychology Hesse, Goethe University Frankfurt;
2 Department of Education, Universidad Católica del Uruguay, Uruguay
Abstract
The objective of this systematic review was to examine professional development for teachers in
formative assessment. 23 eligible study reports were included in this review. Both qualitative and
quantitative data were summarised. The evidence base was diverse, reflecting the various
learning objectives set for the professional learning, and multiple approaches researchers have
taken to measure thir impact on FA practices and student learning outcomes. Practical activities
emerge repeatedly as essential components of the programs and subject-specific approaches are
preferred. The diversity found highlights the complexity of the investigated topic and offers
insights into the different aspects that should be considered in future research.
Introduction
In the educational sphere, teacher training emerges as a central challenge and,
simultaneously, as key to educational transformation. Fullan (2002) convincingly argues that
teacher training faces intrinsic problems but also represents the most promising solution for
reforming education. This ambivalence has been widely recognized, which identifies teacher
training as a driver of educational change. However, there is notable variability in how different
nations conceptualize and facilitate this professional development.
Formative Assessment (FA), a pedagogical approach noted for its effectiveness and low
implementation cost, offers an opportunity to enhance the quality of teaching and learning. Many
educational systems worldwide consider FA practices to be needed to ensure high-quality
instruction for all students and have developed actions to encourage and strengthen its
*Corresponding author: alexavh@gmail.com
implementation. This is largely because extensive literature has investigated the effects of FA on
students' achievement, and several meta-analyses and systematic reviews have suggested that FA
can positively affect learning (Black & Wiliam, 1998; Graham et al., 2015; Hattie & Timperley,
2007; Kingston & Nash, 2011; Klute et al., 2017; Lee et al., 2020). However, attempts to
implement FA are challenging (Lee et al., 2020; Schildkamp et al., 2020; Wisniewski, et al.,
2020). Knowledge and skills of FA are necessary in other to implement it effectively, which calls
for the need for effective professional development (Yan et al., 2021; Schildkamp et al., 2020).
This study aims to conduct a systematic review that provides an understanding of
research dedicated to the professional development of teachers within the framework of FA. We
have identified problematic areas regarding professional development and the integration of
formative assessment practices in educational contexts that will be critically examined in this
section. Drawing from this exploration, we will formulate specific research questions that will be
addressed through a comprehensive analysis of relevant literature. With this approach, we aspire
to contribute significantly to the understanding and improvement of professional development
strategies in education.
Formative Assessment
Formative assessment is understood or conceptualized in different ways: as a set of
specific yet interconnected strategies (Hawe & Parr, 2014) and as a dynamic and recursive
process that is organically integrated into everyday pedagogical practice (Donaldson, 2015)
In any of the approaches, FA is the process through which evidence on students’current
level of understanding is interpreted, and used by teachers or students to make decisions that
have an actual impact on learning (Wiliam & Thompson, 2008). The Wiliam and Thompson
(2008) model is one of the most referenced by researchers, which takes the three key questions
proposed by Hattie and Timperley (2007): Where are we going? Where are we now? How do we
get there? and proposes five key strategies: clarifying learning goals, eliciting evidence,
providing feedback, involving and encouraging students to collaborate by participating in peer
feedback processes, and making students accountable for their learning (Black & Wiliam, 1998;
Wiliam & Thompson, 2008).
Additionally, FA goes beyond the traditional conception of assessment as a teacher’s job
and postulates that students and their peers also play an essential role in informing themselves
and each other about the three processes (Wiliam & Thompson, 2008). There is agreement that
FA requires a change in traditional classroom dynamics, where assessment is used to understand
students’ learning process to help them move forward. Also, a key aspect of FA is that it requires
making students accountable for their learning, promoting a student-centered pedagogy (Wiliam
& Thompson, 2008; Schildkamp et al., 2020).
However, defining FA or assessment for learning is not an easy task and an absolute
consensus is far from being reached, especially in relation to the specific practices that the
construct involves (Bennett, 2011; Dunn & Mulvenon, 2009). Lack of consent has led to
methodological complexities in the implementation and evaluation of professional development
initiatives focused on FA.
In this context, a key challenge is translating FA practices and principles into learning
outcomes for professional development. For instance, two meta-analyses examining the
prerequisites for effective FA among educators reveal that possessing both knowledge and skills
is essential (Yan et al., 2021; Schildkamp et al., 2020). However, this category encompasses a
diverse array of elements. Some studies emphasize the importance of the ability to employ
various assessment strategies for collecting data on students' learning (Gottheiner & Siegel,
2012). Others highlight the significance of skills such as critically analyzing and interpreting
assessment results (Lee et al., 2012) and utilizing knowledge to develop appropriate actions in
response to students' needs, such as reviewing content or making curriculum adaptations
(Gottheiner & Siegel, 2012; Lee, 2011; Schildkamp & Visscher, 2009). Furthermore, additional
research suggests that the skill of providing students with timely, relevant, and constructive
feedback significantly influences the success of formative assessment (Aschbacher & Alonzo,
2006; Fox-Turnbull, 2006; Lee, 2011). Ultimately, considering the critical interplay of
knowledge, skills, and diverse strategies identified in relevant research, a central hurdle lies in
what different professional learning initiatives prioritise.
A second significant challenge involves the decision between adopting generalistic or
disciplinary approaches in FA. Traditionally, FA models and strategies were crafted broadly,
without taking into account the particular subject being taught (Bennett, 2011). However,
contemporary perspectives emphasize the importance of developing models that account for the
unique procedures and approaches within each subject area (Bennett, 2011). For example, within
the field of mathematics education, it is proposed that successful FA necessitates a synthesis of
general principles, strategies, and techniques, along with an understanding of the specific
cognitive domain of mathematics learning (Balbi et al., 2022; Burkhardt & Schoenfeld, 2019). In
this context, determining the prioritization of either of these approaches in professional
development becomes a relevant consideration.
Lastly, the challenge of defining FA brings methodological difficulties in assessing its
effectiveness and summarising results across studies (Bennett, 2011). The operationalization of
FA for measurable outcomes remains a nebulous area because they are highly dependent on the
definition adopted. The dynamic and context-dependent nature of FA implementation adds
complexity to the measurement process, making it challenging to establish uniform criteria for
evaluating its impact. Consequently, the current literature reveals a significant gap in establishing
clear and universally applicable operational definitions for FA practices, hindering the
development of comprehensive assessment tools (Bennett, 2011). Additionally, it impedes the
synthesis of research findings because the average effect sizes derived from a meta-analysis may
lack significance when combined across studies that exhibit substantial variations in variable
measurement (Lipsey & Wilson, 2001; Kingston & Nash, 2011). Addressing this methodological
ambiguity is paramount for advancing our understanding of the effectiveness of FA in
professional development contexts.
Professional Development
Teacher effectiveness has been recognized as one of the most decisive factors influencing
student learning (Klassen & Tze, 2014; Darling-Hammond & Youngs, 2002; Staiger & Rockoff,
2010). There is a growing awareness that teachers need to be supported and encouraged in their
professional development throughout their careers. Continuous teacher professional development
has been explored as a potential way to improve educational quality and the conditions under
which professional development can be effective are being researched (Darling-Hammond et al.,
2017; Yoon et al., 2007; Desimone, 2009).
Specific aspects of professional development programs (PDPs) design that are believed to
enhance teacher learning have been described in the literature (Desimone, 2009; Garet et al.,
2008; Bardi et al., 2016). However, some controversies persist. For example, Opfer & Pedder
(2011) raised questions considering there are reports in the literature of teachers participating in
professional development with all the characteristics of effectiveness described, yet not
experiencing significant learning or change.
Some approaches aim to investigate effective practices and consistently emphasize the
importance of professional development focusing on quality content knowledge, which is
supported by strategies that encourage teachers to research or solve problems within their
practice and involve ongoing peer collaboration (Garet et al.,2001). It has been suggested that the
most potent development occurs in real-time situations within actual schools, addressing genuine
problems and decisions faced by educators daily (Crow, 2008, p. 43). However, these approaches
have been questioned because they appear to emphasize learning activities and overlook broader
curriculum policies, either at the school or regional level (Opfer & Pedder, 2011).
Other approaches have been more critical and encourage the inclusion of variables from
the school and regional context. A review indicates that for professional learning to be effective,
it must occur at three interrelated and parallel levels: student, teacher, and organization.
(Cordingley, 2022). It is considered that curricular policies at the school or regional level that
hinder teacher professional learning can impede or affect the impact of more effective practices.
For example, a significant barrier has been identified in the form of conflict with work schedules
and family responsibilities which may be overcome if organizations rather than individual
initiatives drive professional development (Badri, et al, 2016; Castro et al., 2024). In the specific
field of FA, it has been identified that contextual factors including the support provided by
school administration and external policies regarding initiatives and resources, may promote or
obstacle teachers’ FA implementation (Brink & Bartz, 2017; Crichton & McDaid, 2016; Yan et
al., 2021).
A definitive relationship has yet to be established between the methodologies and
principles underpinning professional development and the efficacy of these programs (Opfer &
Pedder, 2011). Furthermore, the required methodologies may be influenced by the established
learning outcomes and content knowledge. Also, it is important to recognize that the
effectiveness of professional development can be contingent upon contextual factors and the
characteristics of the teacher. This underscores the crucial need for a systematic review to
comprehensively examine the diverse facets of professional development in FA and the dynamic
interplay between methodologies, contextual factors and teacher characteristics to address the
challenge of designing effective professional development in the field.
A second challenge in the domain of professional development revolves around assessing
its impact. Studies examine how professional development affects teacher variables, and some
also explore its influence on student outcomes. The evaluation of the impact of professional
development programs involves various factors, such as teachers' beliefs, reactions to
professional development, acquired knowledge, satisfaction, and near-transfer variables, like
changes in teaching practices (Erdas Kartal et al., 2018). When assessing student impact, the
focus is often on academic achievements, though some research delves into socioemotional
factors or perceptions of the learning environment. Moreover, the absence of a widely
agreed-upon framework for FA and its practices contributes to the complexity of this challenge.
The present study
Policy makers, researchers and educational organisations are investing resources in
developing PDP to promote FA practices. However, to the best of our knowledge, there is no
systematic investigation of the characteristics of PDP on FA. Understanding the characteristics of
professional development programs is crucial since the effects of such programs on teacher and
student outcomes appear to depend on their features (Sasson & Miedijensky, 2020; Garet et al.,
2001, Lindvall & Ryve, 2019). This systematic review aims to address previously identified
challenges, focusing on evaluating the existing evidence concerning PDPs aimed at enhancing
teachers' FA practices.
The first research question examines the definition and scope of the FA concept. Given
the potential theoretical ambiguity of FA, it is crucial to analyse which dimensions and learning
outcomes have been emphasized in academic research, identifying the aspects of the FA
construct that have been addressed. The second question focuses on the implementation elements
of PDPs that have been identified as problematic in the literature. This includes the specific
context of professional development, the mode of implementation, and critically, the
methodological strategies employed. The third question addresses the challenge of measuring
PDPs' impact on teacher and student outcomes.
(1) What learning outcomes are emphasized in professional development programs about
formative assessment?
(2) What strategies are employed for professional learning on formative assessment?
(3) How do professional development programs on formative assessment impact teachers'
and students' outcomes?
Methods
Before beginning data collection, we pre-specified the research questions, methods and data
analysis plan for this study in a pre-registered protocol and submitted it to PROSPERO on
September 30th 2019 (von Hagen et al., 2020).
Search strategy
The following search strategy was used in this study: (Teach* OR Educat*) AND (Professional
AND (development OR training OR intervention OR coaching) AND ("Formative assessment"
OR "Assessment for learning" OR "curriculum-based assessment"). Not all of the information
sources we consulted offered options for advanced searches including boolean operators,
wildcards, quotation marks and parentheses. Therefore, in some cases we used a simplified
version of this search strategy. Furthermore, whenever the user options of the information source
allowed it, we limited our search to references published from the year 2000 onwards and written
either in English or Spanish, as these are the languages all authors of this review are proficient
in. Finally, when possible, we filtered search results based on the occurrence of our search terms
in the title and abstract of the reference, to narrow down the number of irrelevant hits.
Information sources
We searched the following databases between November 23rd and December 1st of 2019, and
29th of december 2022: PsycINFO, PsycARTICLES, MEDLINE, PubMed, ERIC, ProQuest
dissertations, Fuente Académica Premier, Database of the Centro de Ciencias Humanas y
Sociales [Center for Human and social sciences] of the Consejo Superior de Investigaciones
Científicas [Superior Counsel of scientific investigations] in Spain and Red de información
educativa (Redined) [Web on educational information]. We also hand-searched the target
journals Assessment for Effective Intervention, Educational Assessment and Educational
Assessment, Evaluation and Accountability.
Eligibility criteria
We used the PICO acronym to define the eligibility criteria for this review and formulated
Signaling question Inclusion
1. Does the study focus on primary or secondary school teachers as the participants of the
YES
professional development intervention?
2. Does the study report on a professional development program targeted to improve
YES
teachers' formative assessment practicesa?
3. Did the study measure teachers' formative assessment practices either through (a)
measures directed at the teachers themselves (e.g. questionnaires, assessment of conceptual
YES
knowledge, etc.) or (b) external measurement of teachers formative assessment practices
(e.g. classroom observations, students perception of formative assessment practices, etc.).
4. Is this an intervention study with pre-post outcome measurement?
YES
Note. aMain focus should be on improving teachers’ formative assessment practices.
Selection process
After importing the results of our search to Covidence (Covidence, 2019), the software
automatically detected and excluded exact duplicates. Two reviewers independently screened
excluded a study report only if at least one of the exclusion criteria was clearly fulfilled.
Disagreements between both reviewers were resolved through a third independent reviewer
judgement.
Next, we downloaded full texts of the references that had been included so far and
imported them into Covidence. Once again, two reviewers evaluated the eligibility of each full
text based on the information provided in the methods and results section referring to the
way as for the title and abstract screening stage. Reasons for excluding studies at this stage were
registered in Covidence and are reported in the supplemental materials S4.
Data extraction
the information provided by the original studies. If inconsistencies arose, we discussed them
Data items
General report information
Full reference
In text citation
Publication year
Publication type
Publication language
Country of study
Teacher characteristics
Educational level in which teachers work
Primary school
Secondary school
School subjects that teachers instruct
Subject specific (math, chemistry, etc.)
General
Career stage of teachers
Less than 5 years of professional experience
More than 5 years of professional experience
Characteristics of educational institutions teachers work at
At risk (e.g., vulnerable socio-economic status, special populations such as students with learning
difficulties or behavioural problems, etc.)
Regular
Characteristics of the professional development program
Presence of a control group:yes vs. no
Yes
No
Modality of intervention
Face to face meetings
Online sessions
Mixed
Number of modules
Less than three
Three to five
More than five
Intended learning outcomes
Activities
Conceptual inputs
Collaborative reflection
Practical activities - Simulated
Practical activities - Real-life
Total amount of time (in hours)
Characteristics of the outcome measurement
Level of measurement
Measures directed at the teachers themselves (e.g., assessment of conceptual knowledge, etc.)
External measurement of teachers’ formative assessment practices (e.g., classroom observations)
Instruments
Quantitative (e.g., questionnaires, assessment tasks, etc.)
Qualitative (e.g., interviews, focus groups, etc.)
Type of teacher knowledge measured
Declarative knowledge (e.g., explain different components of formative assessment, etc.)
Procedural knowledge (e.g., analysis of case studies, classroom observation, etc.)
Self-knowledge (e.g., self-efficacy to implement formative assessment practices)
Type of student learning outcome measured
Academic performance (e.g., school grades)
Socio-emotional dimensions (e.g., motivational questionnaires, classroom climate observations,
etc.)
Perception of teachers’ formative assessment practices (e.g., student evaluations, focus groups,
etc.)
Results
Study selection
Flowchart of search and selection process
Study characteristics
Concerning general study characteristics, the available evidence includes peer-reviewed
(n = 18) and grey literature (n = 5) study reports published from the year 2005 to 2022. Although
our pre-specified inclusion criteria were set out to include articles published in Spanish, all
articles that met eligibility criteria were published in English. Studies were conducted in the U.S.
(n = 17), Cameroon (n = 1), China (n = 2), Azerbaiyán (n = 1), Serbia, (n = 1), and Nigeria (n =
1).
included in this review.
Teacher characteristics
Variable N %
Educational level in which teachers work
Primary school 9
39,13
Secondary school 10
43,48
More than one category 4
17,39
No information 0
Teachers’ career stage
Less than five years of professional experience 2
8,7
More than five years of professional experience 2
8,7
More than one category 9
39,13
No information 10
43,48
Characteristics of educational institutions teachers work at
At riska 1
4,35
Regular 6
26,09
More than one category 6
26,09
No information 10
43,48
Note. aFor example, vulnerable socio-economic status, special populations such as students with learning difficulties
or behavioural problems, etc.
(1) What learning outcomes are emphasized in professional development programs about
formative assessment?
To answer the research question, we identified the learning outcomes that were stated when they
were made explicit in the study; or alternatively considered the contents of professional
articles. It can be observed that the dimension of providing feedback to students is present in
72.27% of the studies. Similarly, it is interesting to highlight that the least frequent purposes
described in our studies: implementing rubrics, learning progressions, and integrating FA with
summative assessment.
Learning outcomes of the professional development programs about formative assessment
(2) What strategies are employed for professional learning on formative assessment?
We were interested in learning about effective strategies, but also about the design in each
formation. That's why we made the decision to report the results of this objective in three ways.
First, we report whether professional development programs focused on specific and disciplinary
content (versus being generalist) and whether they considered it relevant to involve the
community of the educational center to which the teachers belonged. Second, we generated an
inductive classification of the strategies: (1) conceptual inputs, for example written materials (i.e.
Gade, 2016; Gotch et al., 2021), video-conferences (i.e. Gilson, 2009) and lectures (i.e. Randel et
al., 2016; Zhai, 2018); (2) practical activities, distinguishing between (2.1) simulated problems
(i.e. Yin et al. 2015; Tomasevic et al. 2021) and (2.2) real-life classroom problems (i.e. Gotch et
al. 2021; Martin et al., 2016; Thiede et al. 2018); and finally, (3) collaborative reflection, for
example, discussions (i.e. Vendlinski & Phelan, 2011), forums (i.e. Martin et al., 2016), group
problem-solving (i.e. Beesley et al., 2018).
were delivered in the studies included in this review.
Characteristics of professional development programs
Variable N %
Subject specific or general approach
General 9
39,13
Subject specific 14
60,87
Math 9
39,13
Biology 1
4,35
Chemistry 3
13,04
Physics 1
4,35
Modality of interventiona
Face to face 17
73,90
Online 2
8,70
Mixed 2
8,70
Activitiesa
Conceptual inputs 19
82,61
Collaborative reflection 17
73,91
Practical activities 21
91,30
Simulated 10
43,48
Real-life 20
86,96
Contextual factors
School involvementb 14 60,87
Leaders involvementc 6 26,09
Note. aCount represents explicit reporting in the studies, '0' denotes absence or lack of information. b For example:
Wylie & Lyon (2015), Yan (2021). c For example: Oluwaseyi & Hajiyeva (2022); Lyon et al. (2018).
Overall, the total duration of professional development programs ranged from two
(distributed over 4 weeks - Dudek et al., 2019) to 64 hours (Zhai et al., 2018) with most
programs entailing between 20 and 50 hours.
(3) How do professional development programs on formative assessment impact teachers'
and students' outcomes?
included in this review.
Characteristics of outcome measures and design
Variable N
Presence of a control groupa
Yes 4 17,39
No 19 82,61
Instruments
Quantitative 19 82,61
Qualitative 9 39,13
Type of teacher knowledge measured
Declarative knowledge 5 21,74
Procedural knowledge 15 65,22
Self-knowledge 11 47,83
Type of student learning outcome measurede
Academic performance 5 21,74
Socio-emotional dimensions 1 4,35
Perception of teacher's formative assessment practices 1 4,35
To address this research question, we planned to conduct two overall meta-analyses to
estimate the overall impact of professional development programs on (a) teachers' formative
assessment practices (main outcome) and (b) students' learning outcomes (additional outcome).
However, in pre-registration we stated that we would only follow this plan if available data (at
least two studies - Littell et al., 2008) allowed us to pool effect sizes together and merge studies
according to the conceptual framework of this review. The above-mentioned conditions were not
fulfilled in this review because there were scarce studies which used an intervention-control
research design and they were conceptually very heterogeneous, as they encompassed measures
directed at the teachers themselves (e.g., questionnaires), as well as external measurements of
teachers' formative assessment practices (e.g., classroom observations) and tapped into different
types of knowledge, including declarative, procedural and self-knowledge on formative
assessment practices. Therefore, we decided to summarise the data narratively to avoid the
common mistake of “mixing apples and oranges” (Borenstein et al., 2009, p. 379) when
computing overall effects from such a heterogeneous evidence-base.
In total, four studies out of 23 included studies (17.39%) used an intervention-control
group design which allows a more careful investigation of the impact of professional
development programs on teachers' formative assessment practices (Randel et al., 2016; Schütze
covers nine effect sizes ranging from g = 0.02 for external measure of procedural knowledge
Hedges
(Randel et al., 2016) to g = 31.39 for teacher-directed measure of self-knowledge (Randel et
Hedges
al., 2016). Inspection of effect size ranges seems to indicate a higher impact of professional
development programs on teachers’ formative assessment practices when teacher-directed (from
g = 0.21 to g = 31.39) measurements were used, as compared to external measurements
Hedges Hedges
(from g = 0.02 to g = 0.03). Another pattern that we observed is a trend towards a
Hedges Hedges
higher impact when declarative (from g = 0.21 to g = 0.63) and procedural (from g
Hedges Hedges Hedges
= 0.02 to g = 2.12) knowledge was measured, as compared to self-knowledge (g =
Hedges Hedges
31.39). However, these trends need to be interpreted with caution due to the limited number of
datapoints (i.e., nine effect sizes) based on information collected with very heterogeneous
measurement instruments.
Summary of quantitative evidence-base with intervention-control group design
Reference N of N of g Level of Type of knowledge Further details of measurement instrument
Hedges
experimental control measurement
group group
Teacher- External Declarative Procedural Self-
directed
Randel et al. (2016) 178 231 0.21 X X Test of Assessment Knowledge developed by Randel et al. (2016)
Randel et al. (2016) 178 231 0.02 X X Adaptation of the Assessment Work Sample, an assignment work
sample instrument originally developed and validated at the
National Center for Research on
Evaluation, Standards, and Student Testing (CRESST;
Aschbacher, 1999; Clare, 2000; Clare, Valdes, Pascal, &
Steinberg, 2001; Matsumura, Garnier, Slater, & Boston, 2008;
Matsumura, Patthey-Chavez, Valdes, & Garnier, 2002).
Randel et al. (2016) 178 231 31.39 X X Teacher reports of student involvement in assessment
Schütze et al. (2017) 17 24 0.03 X X Instructional feedback practice
Schütze et al. (2017) 30 37 0.63 X X Declarative feedback knowledge
Schütze et al. (2017) 30 37 0.23 X X Ability to generate feedback in a test situation
Thiede et al. (2018) 60 65 1.83 X X Absolute judgement accuracy for students’ performance on a
math conceptual understanding test
Thiede et al. (2018) 60 65 2.12 X X Absolute judgement accuracy for students’ performance on a
math skills test
Vendlinski & Phelan 27 33 0.28 X X Teachers’ conceptual knowledge maps
(2011)
A final set of nine studies provided evidence based on qualitative measurements of the
impact of professional development programs on teachers’ formative assessment practices (see
development programs to strengthen formative assessment practices using diverse data sources
such as interviews (n = 5), classroom and/or workshop observations (n = 4), written teacher
reflections (n = 3), open-ended questions in written surveys (n = 3), logs or assessment portfolios
(n = 3) and group discussions (n = 1).
Authors analysed this data by identifying prominent themes, codes or topics and
illustrated their findings with sample participant responses. This evidence can be grouped into
Overarching topics identified in qualitative evidence-base
Types of Knowledge
The first overarching topic we identified consisted of themes, codes and topics related to
different types of knowledge that teachers seemed to have acquired as a result of their
participation in a professional development program. This evidence can be grouped into themes,
codes and topics related to a) conceptual, b) procedural and c) self-knowledge and/or attitudes
about formative assessment practices.
Conceptual Knowledge. Two studies reported qualitative evidence on the impact of
professional development programs on teachers’ conceptual knowledge concerning formative
assessment (i.e., Akom, 2010 and Zhai et al., 2018). For instance, Akom’s (2010) study revealed
details on different aspects of formative assessment that teachers mentioned they had learned.
Some of the themes identified in this study were ‘formative assessment cycle/differences with
summative assessment’, ‘use of self and peer assessment’; ‘existence of other assessment
methods other than oral questioning’. One of the sample participant responses mentioned by
Akom (2010) in this context illustrates changes in teachers’ conceptual knowledge:
I have learned the differences between formative and summative assessment. In addition
to this I have understood that if formative assessment is properly done, summative
assessment becomes much easier, with impressive results. I have also learned that there
are different types of questions such as person-centered, subject-centered, and
process-centered and these can be open or closed. Open, personcentered questions are
best because they show the diversity in student thinking and can be most useful in the
classroom. I have also learned that the feedback I give students should be non-judgmental
as this [judgmental feedback] can discourage some students and make them feel they
know nothing or feedback like 'very good' can make some students not to listen to further
explanations (p. 108).
Procedural Knowledge. Seven studies reported qualitative evidence on the impact of
professional development programs on teachers’ procedural knowledge concerning formative
assessment (i.e., Akom, 2010; Decker & Feijs, 2005; Gade, 2016; Gilson, 2009; Martin et al.,
2016; Zhai et al., 2018;Abell & Sevian, 2021 ). For example, Decker and Feijs (2005) identified
the following themes while analyzing changes in classroom practice: “analysis students work and
strategies, scoring and grading, record keeping”, “instructional embedded assessment, more
observations, more discussion, different role of homework”, “less tests, less quizzes, less
homework checks”; “critique own assessment instruments, use of other formats”, “more levels,
assessment pyramid” and “emphasis on student responsibility” (p. 242). Sample participant
responses highlight how difficult it can be to integrate formative assessment into practice for
teachers, despite having acquired conceptual knowledge on the matter: “Um. I have tried to add a
little bit more high-level questions on my exams but I need more practice on it. I really, really do,
in the creation of those test questions” (Decker & Feijs, p. 246).
Self-knowledge and/or attitudes. Five studies reported qualitative evidence on the
impact of professional development programs on teachers’ self-knowledge and/or attitudes
concerning formative assessment (i.e., Akom, 2010; Decker & Feijs, 2005; Gade, 2016; Gilson,
2009; Zhai et al., 2018). For example, Gilson (2009) found that the professional development
program increased teachers’ ‘positive feelings toward formative assessments’ in relation to three
themes. First, teachers’ attitudes seemed to have shifted in favour of adopting formative
assessment practices, as this sample participant response underlines: “I believe that formative
assessments are a valuable tool in keeping track of student learning and growth. I will be using
them continuously from now on. They tell me if my students understand what they need to be
learning” (Gilson, 2009, p. 27). Second, commitment towards their role as teachers seemed to
have renewed as explained by another participant of Gilson’s (2009) study in the following
quote:
My grade team is focused on the right place — educating our students. We're trying to use the
formatives to teach our students self-monitoring and better communicate to parents where
their kids stand. We're in it to do it efficiently and well, and we're working together (p. 28).
Finally, participating in the professional development program seemed to have helped
teachers gain the necessary confidence to engage in formative assessment practices, as another
participant in Gilson (2009) expressed:
This experience has given me the confidence to create and use the assessments. I am even
comfortable creating rubrics. I feel much more confident about creating and administering the
formative assessment. The kids seem to like to know how they are doing in the process as
well (p. 29).
Components of Formative Assessment
The second overarching topic we identified referred to themes, codes and topics focusing
on different components of formative assessment for which study authors reported changes in
teachers’ practices as a result of participation in a professional development program. Overall,
this evidence provided information on four componentes: (a) formative assessment cycle, (b)
collection of assessment information, (c) teacher feedback and (d) students’ involvement.
Formative Assessment Cycle. Three studies reported qualitative evidence on the impact
of professional development programs on teachers’ practices concerning the formative
assessment cycle (i.e., Akom, 2010; Gade, 2016; Wylie & Lyon, 2015). For example, Wylie and
Lyon’s (2015) findings revealed changes in teachers’ management of learning expectations as
this sample participant response showcases:
I provide students the main topic for this section and they use whiteboards to develop
questions or statements that will become the main learning intentions for the current subject.
This helps students understand what we are trying to learn about in class and gives them some
ownership in the process … We were studying hurricanes and one of the students came up
with the question of why do hurricanes spin. As a class, with guidance, we refined the
question to ‘What factors cause a counter-clockwise rotation of hurricanes in the northern
hemisphere?’ … As the teacher I give students the main subject to develop learning intentions
around and then help guide the revision of the question when it is presented … I revised
which areas I focused my lesson on (p. 150).
Collection of Assessment Information. Six studies identified themes, topics and codes
that pointed towards changes in teachers’ practices to collect assessment information (Akom,
2010; Gade, 2016; Gilson, 2009; Martin et al., 2016; Talkmitt, 2013; Abell & Sevian, 2021).
Findings by Akom (2010), for example, include “open, person centered questioning”,
“non-judgmental comments”, “self/peer assessment” and “wait time” as themes that emerged
when collecting data on “specific assessment aspects to use”. One of the participants’ in this
study described this type of impact as follows:
I have really learnt much about questioning - that there are different types of questions:
subject-centered, person-centered, process-centered, and others questions, and also how to go
about asking them. I also understood, and saw that person-centered questions are the most
important of questions to be asked and that they can be open or closed. Open ones are more
effective and most useful in the classroom. Since I have learnt a lot about effective
questioning I am going to apply the various ways of questioning, and try as much as possible
to focus on open person-centered questions (p. 136).
Teacher Feedback. Evidence from for studies focused on changes in teachers’ feedback
practices (i.e., Akom, 2010; Gilson, 2009; Wylie & Lyon, 2015; Abell & Sevian, 2021). For
instance, Wylie and Lyon (2015) identified an impact on teachers’ abilities to provide formative
feedback, as underlined in the following sample participant response:
I assign work to the students. When they are finished I check their work. It is a very quick
check because I circle any question they did not get the correct answer to. Then I give it back
to them and they fixed the circled problems … I am able to get a good idea of what [mistakes]
the student is making over and over again. At the same time, the technique requires students
to have a deeper knowledge in order to find and fix a mistake then if I tell them what the
problem is … As I am checking I usually talk to the kids and say things like, ‘oh, you need to
watch your signs’ or ‘I’ve noticed you’re having a hard time adding and subtracting the
negatives.’ That gives them a little help (p. 154).
Students’ involvement. The same three studies as above also identified themes, topics
and codes on changes in teachers’ practices concerning students’ involvement (i.e., Akom, 2010;
Gilson, 2009; Wylie & Lyon, 2015). In this context, Gade (2016), for example, talked about
‘student ownership’ and described this impact on teachers’ practices based on participant
expressions such as the following: “They [students] need ‘to do’ in order to understand, and I
need to facilitate their level of engagement by designing better lesson implementation for that
‘doing’ to happen more” (p. 135).
Sources of Support
The third overarching topic we identified is based on themes, codes and topics from five
studies that provide information about sources that teachers perceive as supportive to change
their formative assessment practices (i.e., Akom, 2010; Dekker & Feijs, 2005; Gade, 2016;
Gilson, 2009; Talkmitt, 2013). For example, evidence from Dekker and Feijs (2005) included
“colleagues”, “lead teachers”, the “leadership team”, the “curriculum materials” and
“background materials used”, as well as the professional development program team itself and
their website (i.e., the “CATCH team” and “CATCH website”), “workshops”, “summer
institutes” and “conferences” in this category (p. 242). Especially the possibility to collaborate
with colleagues was mentioned as an important source of support by many teachers, for example,
in Dekker and Feijs (2005):
Talking with different teachers. What works good for them may also help me or maybe what I
am using might help them. Just networking with math teachers throughout this cluster and
throughout the district’. Another participant in this study underlined this idea with the
following quote: “I think the more you get together and talk about it the more it keeps you, it
certainly keeps you involved in what your goals are supposed to be with this (p. 249).
Difficulties
Three studies also reported evidence on difficulties teachers’ experienced to increase
formative assessment practices in their classrooms (i.e., Akom, 2010; Martin et al., 2016;
Talkmitt, 2013). For example, teachers’ in Akom’s (2010) study enumerated a long list of
“possible obstacles/difficulties in using formative assessment” such as “large class size”, “lack of
teaching resources”, “lack of teacher skills”, “lack if co-operation from other teachers and
hierarchy”, “apprehensiveness and difficulty in changing or adapting to new concepts such as
person-centered questioning, wait time, etc.” and “lack of knowledge in areas such as inquiry,
group formation and classroom management”. In contrast, Talkmitt’s (2013) results focus on the
“use of time” as the main difficulty teachers’ experienced by teachers to change formative
assessment practices, as showcased in this sample participant response: “I don’t even have
enough time in my room where I can just breathe and organize” (p. 116).
Knowledge Transfer
Finally, we identified an overarching topic related to “knoweldge transfer” that was only
present in the data reported by Dekker and Feijs (2005), but provides important insights into the
impact of professional development programs on teachers’ formative assessment practices. More
specifically, Dekker and Feijs (2005) used the expression “CATCH ideas are travelling” (p. 250)
to group participant expressions that indicated an multiplicatory effect of the professional
development program CATCH that teachers in their study participated in. One of the teachers in
their study, for example, explained that this newly acquired knowledge could also be applied to
other subject areas, not only the one directly targeted in the professional development program:
“But you know, what is really cool about this whole assessment thing? It can work in science
too” (Dekker & Feijs, 2005, p. 251). Another participant provided evidence of knowledge
transfer to other fellow teachers that had not participated in the professional development
program: “Well, I share my ideas with the other teachers that teach math here as well as when I
teach math education courses at XX and XX. I always use the CATCH philosophy for
assessment” (Dekker & Feijs, 2005, p. 251).
Sample and data source characteristics of qualitative evidence-base
Reference N Brief description of Data sources
participants
Open-ended Group Classroom Written teacher Logs or Interviews
questions in discussions and/or reflections assessment
written surveys workshop portfolios
observation
Akon (2010) 8 Science secondary X X X X
school teachers
Dekker & Feijs 12 Middle school math X
(2005) techers
Gade (2016) 6 Techers of English, X X X
social studies, math,
science, and service
learning
Gilson (2009) 5 Third grade teachers X X
Martin et al. 148 Primary grades X X
(2016) Teachers
Talkmitt (2016) 4 Teachers from grede X X
1 to 12 (English
Language Arts,
mathematics, science
and social sciences)
Wylie & Lyon 202 Math and science X X
(2015) teachers
Zhai et al. (2018) Physics teachers X X
Abell & Sevian 13 Middle and high X
(2021) school science
teachers
Efficacy of professional development programs on teachers’ formative assessment practices
to improve students’ learning outcomes
From the 23 studies included in this review, six (26.08%) measured the impact of
professional development programs on teachers’ formative assessment practices on students’
learning outcomes using quantitative research methods (Adewoye, 2018; Beesley et al., 2018;
Furtak et al., 2016; Randel et al., 2016; Thiede et al., 2018; Zhai et al., 2018). Unfortunately,
none of the studies with qualitative measurements collected data at the students level. While five
of the above-mentioned quantitative studies focused on academic outcomes, only one study (i.e.,
Adewoye, 2018) collected information on socio-emotional outcomes. Furthermore, we only
found data to compute effects sizes for three studies covering five effects.
On one hand, Randel et al. (2016) provided information to compute an effect of g =
Hedges
0.23 based on a comparison of students’ of teachers that participated in a professional
development program as compared to students of control group teachers. Data was collected
using the Colorado No Child Left Behind Assessment (CTB/McGraw-Hill, 2009). On the other
hand, Furtak et al. (2016) and Zhai et al. (2018) reported student data from pre-post-test designs
of teachers attending a professional development program that allowed us to compute four
effects. More specifically, for Furtak et al. (2016) we averaged student data from nine individual
teachers collected through the Daphne Assessment of Natural Selection (Briggs et al., 2006),
which consists of 17 ordered multiple-choice items about natural selection in a variety of plant
and animal contexts, and obtained an effect of d = 1.19. For Zhai et al. (2018) separate data on
AV
three classes (class 1, 2 and 3) reflected effects from d = 2.17 to d = 2.89 based on a multiple
AV AV
choice learning progression assessment on content knowledge referred to the concept ‘energy’.
No effect size calculations were possible for the remaining three studies although the
mentioned the inclusion of student learning outcome measurements for the following reasons: a)
we found no report of descriptive data for Beesley et al. (2018), b) only post-test, but no pre-test
data was reported by Adewoye (2018) making it impossible to compute d and c) we could not
AV
find data on the N of students corresponding to the teacher control and intervention group in
Thiede et al. (2018) turning it difficult to calculate g
Hedges.
Discussion
This study aimed to conduct a systematic review to provide an understanding of research
dedicated to the professional development of teachers in FA. Overall, 23 study reports met the
eligibility criteria for this review, representing the available evidence to address our research
questions. This evidence base is highly heterogeneous, showcasing the variety of approaches
scholars have chosen to design, implement and measure the impact of PDPs on FA. This
heterogeneity underscores the complexity of the topic under investigation and offers insights into
different components that need to be considered in future research. We identified critical areas
which will be examined in this section. Our goal is to significantly contribute to the
understanding and improvement of professional development strategies in education. The
discussion structure focuses on the findings that allowed us to answer the three research
questions posed.
(1) What learning outcomes are emphasized in professional development programs about
formative assessment?
As pointed out by Popova et al. (2022), the wide variety of program types and the fact
that each scholar independently decides which variables are more relevant to their study, has
resulted in almost as many types of programs as there are programs themselves. Our review
found evidence to support this claim in the specific field of FA.
This systematic review has unveiled a diversity of purposes, ranging from broad concepts
of FA (Beesley et al., 2018; Martín et al., 2016) to the use of rubrics (Adewoye, 2018) and
progress monitoring toward goals (Lyon et al., 2018). The diversity may point to the
heterogeneous pedagogical practices that could be involved when operationalizing FA, which
depends on the theoretical background and may also vary depending on the pedagogical
knowledge being taught (Bennett, 2011, Rakoczy et al., 2017).
One key finding is the prevalence of “providing feedback to students” (69,56%) as a
learning objective. It is well-known that providing effective feedback is a challenging aspect of
FA. For instance, in Gade's study (2016), teachers acknowledged a tendency to provide excessive
feedback, potentially limiting students' learning opportunities. This balance is crucial for
effective feedback.
Thiede (2018) suggests that the teacher's effectiveness in providing feedback depends on
their mastery of the specific subject matter being taught. The ability to identify where the student
is in their learning process, where they are headed, and how they can progress requires a deep
understanding of the learning progression in that discipline. Learning progressions describe the
pathways that students are likely to follow as they learn about basic disciplinary ideas and
practices (Corcoran et al., 2009). Indeed, Lyon's study (2018) found that while mathematics and
science teachers significantly increased their use of FA in similar ways, the specific methods they
chose to implement FA in their daily teaching varied between the two disciplines. This suggests
that mastery of specific content and the ability to apply it in FA strategies are crucial, and
assessment practices must be tailored to the particular characteristics of each content area.
We observed a disparity in the representation of purposes such as “student engagement
and participation” and “co-evaluation and collaboration”. However, a key aspect of FA is to
instill accountability in students for their learning, thereby fostering a student-centered pedagogy
(Wiliam & Thompson, 2008; Schildkamp et al., 2020). While assessment was traditionally seen
as solely the teacher's domain, FA models advocate for the active involvement of students and
their peers in the learning process (Wiliam & Thompson, 2008). Consequently, implementing FA
necessitates a fundamental shift in classroom dynamics, a change that can pose significant
challenges (Elwood, 2006; Schildkamp et al., 2020). For example, Balbi et al. (2022) highlighted
that co-evaluation was one of the least common FA practices among mathematics teachers,
attributing it to challenges in altering power dynamic. Additionally, a study from our review
justifies the absence of impact on learning due to students' limited participation in classroom
assessment (Randel, 2016). All in all, our study contributes a pertinent finding: there is a notable
deficiency in professional development opportunities for teachers to acquire skills to engage
students in their own and their peers’ assessment.
(2)What strategies are employed for professional learning on formative assessment?
First, we prioritized distinguishing between generalist and disciplinary approaches of FA.
The findings revealed a leaning towards disciplinary specialization, with 60,87% of the studies
focused on this approach. Additionally, out of the nine studies which used generalist approach,
five were in primary education contexts (Dudek et al. 2019; Gade, 2016; Gilson 2009; Wylie &
Lyon 2015; Yan, 2021), where disciplinary specificity may be less relevant. This distribution of
approaches is consistent with existing literature. Some authors suggest PDPs focused on the
pedagogy of specific subjects are likely to be the most effective because different contents
require different pedagogies (Kennedy, 2019; Villegas-Reimers, 2003; Popova et al., 2022).
Furthermore, Darling-Hammond et al. (2017) emphasizes the importance of specific content
knowledge for the effectiveness of professional development.
However, we also acknowledge perspectives that advocate for a more nuanced approach,
such as that of Wiliam (2019), who argues for finding a balance between general formative
assessment and domain-specific assessment. Though necessary we believe general approaches
may be insufficient for effective professional development in FA because the ability to interpret
students' thinking and provide precise feedback on how they can progress in their learning is
grounded in detailed and profound knowledge of the content being taught.
Second, we analized intervention modalities and activities. Although there is a great
heterogeneity and there are no two studies which use the exactly the same approach, some
patterns can be acknowledged which allowed as to construct inductive categories of activities
which were widely used in the evidence-base.
The studies reviewed commonly included practical activities (91,30%), either real-life or
simulated, which refer to the promotion of active learning. Consistently, literature points out that
the best practices in PDPs are those that are practice-based (Koellner et al., 2022) and promote
insight among teachers by solving real-life problems in a communiy of learning (Kennedy, 2019;
Popova et al., 2022). These are approaches that promote strategic knowledge that focuses on
interpreting events and adapting teaching. Strategies developed through practical aproaches are
usually more flexible and sensitive to unique circumstances and more responsive to differences
among students (Kennedy, 2019). In relation to the theory proposed by Sims et al. (2023) we
believe it is important to distinguish between simulated practical activities which relate to the
purpose of developing a technique by providing practical support and modelling of exmplar
situations; and real-life linked to embedding practice providing ongoing guidance and assistance
to ensure its effective implementation of the technique.
Studies widely provided conceptual inputs (82,61%) such as used lectures, video
conferences and written materials which may be pointing at the importance that need to be given
to content knowledge (Desimone, 2009). This conceptual inputs are related to the mechanism of
instruction proposed in Sims et al. (2023) model, which involves offering clear guidance on how
to execute a teaching technique effectively which helps remove uncertainty about the steps
needed to utilize a method.
Specifically, in the context of FA, two systematic reviews conducted by Schildkamp et al.
(2020) and Yan et al. (2021) underscore the indispensable nature of knowledge and skills. We
believe the prevalence of conceptual inputs may be liked to the critical role of content knowledge
in facilitating effective implementation of FA practices. A solid conceptual understanding of the
pedagogical approach is important for educators to successfully employ FA strategies in their
teaching practices (Schildkamp et al. 2020; Yan et al. 2021). Additionally, practical activities,
both real life and simulated, are essential for hands-on learning for skill and technique
development.
Activities wich reflect collaboration between teachers during de PDP were highly
prevalent (73,91%). Collaboration has been acknowledged as a key component of effective PDPs
(Desimone, 2009; Garet et al., 2001). Collaboration among teachers within the context of PDPs
facilitates the sharing of insights, experiences, and resources, which can lead to deeper
understanding and implementation of new instructional strategies or approaches. It is worth
noting that there may be overlap between practical activities and collaborative reflection. For
example, Beesley et al. (2018) mathematics teachers reviewed student work samples and used
them as fundation to discuss and receive peer support about the implementation of FA in
everyday mathematics classroom. For instance, practical social support has been identified as a
mechanism which fosters the development of technique, because it entails organizing guidance
from fellow teachers on how to put a teaching method into practice (Sims et al., 2023).
Finally, we wanted to synthesize whether the inclusion of the educational community in
PDPs is a distinctive factor. Continuous professional learning in the workplace is crucial for
teacher development and instructional improvement, which ultimately benefits student
performance. Yan and Panadero (2021) affirm that it is not adequate to consider only personal
factors without taking into account the broader context that influences teachers' formative
assessment practices and that without school support, teachers are less likely to be interested in
implementing FA. In fact, Randel (2016) reported that teachers who expressed difficulties in
continuing to implement FA argued that district initiatives were taking away their time. In our
review, 66.67% of the programs involved the school community, indicating an integrated
approach that connects professional development with the school and regional context.
Despite our initial intentions, we encountered limitations in our ability to conduct certain
statistical comparisons due to the lack of sufficient papers with intervention-control group
designs. Specifically, we were unable to statistically compare general versus disciplinary
approaches, explore the moderating role of mechanisms and activities involved, or investigate
the influence of contextual factors. Moving forward, we recommend that future research
endeavors aim to address these limitations by employing robust study designs that allow for
more rigorous statistical analyses and provide more solid conclusions regarding the effectiveness
and impact of different approaches to FA within PDPs.
(3)How do professional development programs on formative assessment impact teachers' and
students' outcomes?
One of the significant findings of this study pertains to the research methods employed to
investigate the influence of PDP on teachers’ FA practices and students’ learning outcomes.
Initially, during the pre-registration phase of this project, we anticipated finding impact
evaluations primarily approached through efficacy or effectiveness studies, as is common within
the realm of evidence synthesis. However, we intentionally kept the eligibility criteria broad, not
confining them solely to quantitative research. As a result, in addition to nineteen quantitative
research studies, our review identified five eligible qualitative studies and another four eligible
mixed methods studies.
This expanded approach to gathering evidence allowed us to obtain a more
comprehensive understanding of the impact of PDP on teachers’ FA practices and students’
learning outcomes. The inclusion of qualitative and mixed methods studies enabled a deeper
exploration of the subjective experiences and perspectives of teachers involved in PDP
initiatives. For example, nuanced aspects such as sources of support, difficulties and knowledge
transfer experienced by teachers participating in PDP on FA only became evident through the
qualitative research pieces included in this review. This information would have been missing in
a purely quantitative assessment of the available evidence on this topic. While the decision to
include both numerical data and qualitative narratives to examine the impact of PDP on teachers’
FA practices clearly introduced a great amount of heterogeneity, we believe that by leveraging
the strengths of both research approaches, a richer and more nuanced understanding of the topic
was achieved.
Taking a closer look at the available quantitative evidence, another important finding is
that only four out of 19 included studies used an intervention-control group design which allows
a more careful quantitative investigation of the impact of professional development programs on
teachers' formative assessment practices. Another eleven quantitative studies reported evidence
from single group pre-post-test designs, which are appropriate to document changes in teachers’
practices, but not to ascertain that these changes are a result of the target PDP.
This pattern is consistent with previous research on the impact of teacher PDP in general,
not solely focused on formative assessment practices. For example, Yoon et al. (2007) underline
the “paucity of rigorous studies that directly assess the effect of inservice teacher professional
development on student achievement in mathematics, science, and reading and English/language
arts” (p.iii). In their review, on “how teacher professional development affects student
achievement” (p.iii), Yoon et al. (2007) only found nine out of 1,300 initially identified reports
(0.69%) that met What Works Clearinghouse evidence standards and represented randomised
controlled trials or quasi-experimental studies. Also, Gersten et al. (2014) identified 643 studies
that measured the impact of professional development interventions related to maths in grades
K–12 and found that only 47 of them (7.31%) “used a relevant design (randomised controlled
trial or quasi-experimental design)” (p. 4) to “examine whether mathematics professional
development approaches cause improvements in student mathematics proficiency” (p. 14). One
reason to explain the scarcity of studies with an intervention-control group relates to the
challenges associated with implementing this type of design in educational settings (Connolly et
al., 2018). For instance, Styles and Torgerson (2018) discuss challenges related to recruitment
and retention of participants, contamination between participants of intervention and control
groups when they belong to the same institution, and assessment of the fidelity of
implementation in ecological contexts.
Additionally, consistently with Popova et al. (2022) assertion about the wide diversity
observed in the field of study, we also found heterogeneity in the variables chosen as outcome
measures. Four studies assessed declarative knowledge, nine focused on procedural knowledge
and eight on self-knowledge. While eleven studies included measures directed at teachers
themselves, eight used external measurement of teachers FA practices such as classroom
observations of students' perception of formative assessment practices carried out by their
teachers. Although heterogeneity responds to the intricate nature of the subject under
examination, it is also considered problematic specifically because it prevented us from
statistically summarising the results across studies. Although this may reflect the lack of
consensus in the definition and operationalization of the construct, in general, there appears to be
no consensus on the appropriate way to measure the impact of PDPs (Desimone, 2023).
Irrespective of the subject, measuring the quality of PDPs and their impact on practice seem to
pose challenges. There is a lack of clarity when talking about effectiveness of different PDPs on
whether this should be measured on teacher outcomes or the resulting impact on student learning
(Desimone, 2023). We suggest it is crucial to develop a more coherent framework for assessing
the impacts of PDPs on both teacher practices and student outcomes. This could involve the
adoption of standardized measures and more rigorous research designs.
Finally, although impacting on students’ learning is one of the ultimate goals of teachers’
professional development, it remains challenging to demonstrate how these programs can lead to
improvements in student outcomes (Yoon et al., 2007; Desimone, 2023). In the evidence-base
identified in this review, six studies estimated the impact of PDP on teachers’ FA on students’
learning outcomes. Our study found that students' learning is often operationalized as
achievement in standardised testing or grades (Beesley et al., 2018; Furtak et al., 2016; Randel et
al., 2016; Thiede et al., 2018; Zhai et al., 2018), and socioemotional outcomes were only
measured in one study (Adewoye, 2018). In general, there is a gap on the evidence on how PDPs
on FA can impact on students outcomes. We suggest further research focus on bridging this gap
and incorporate socio-emotional outcomes, as we argue they may be as important given
achievement refers more to the summative than formative aspects of assessment.
Conclusion
Our analysis of 23 study reports revealed a highly heterogeneous evidence base,
highlighting the diversity of approaches used to measure the impact of PDPs. Despite the
challenges posed by this heterogeneity, our review provided valuable insights into the topic and
identified key findings.
First, our review highlights a heterogeneity of learning objectives within professional
development programs, with prevalence of teaching how to provide effective feedback to
students.
Second, a preference for discipline-specific approaches is observed in professional
development programs, emphasizing the importance of contextualizing content. Practical
activities, both real and simulated, emerge as essential components, promoting active and
strategic learning that enables teachers to adapt their teaching practices to unique circumstances
and effectively respond to differences among students.
Last, we observed a variety of research methods employed in the studies, including
quantitative, qualitative, and mixed-methods approaches. The inclusion of qualitative and
mixed-methods studies allowed for a deeper exploration of the subjective experiences and
perspectives of teachers engaged in PDP initiatives. We found that a limited number of studies
utilised an intervention-control group design which is consistent with previous research on
teacher professional development.
Limitations
We encountered several limitations during the course of this review that merit
acknowledgment. Initially, our pre-registered plan was tailored towards conducting
meta-analytical and moderation analyses. However, due to the limited availability of
intervention-control group studies, we chose to deviate from the protocol to ensure a substantive
contribution to the field. Firstly, our intention was to encompass quantitative studies while not
excluding mixed-methods research. Nevertheless, during data collection, we encountered a
significant amount of qualitative information. This necessitated decisions on how to effectively
integrate it into our analysis to enhance comprehensiveness. Secondly, although we initially
registered the learning outcomes of the programs, we later opted to create inductive categories to
better communicate the richness of the data. Thirdly, while there is a growing body of research
advocating for a disciplinary-specific approach to educational feedback, as opposed to a
generalist one, our review initially aimed to focus solely on mathematics teachers, aligning with
the broader scope of our project. However, we ultimately decided to include this information in
the data extraction. We believe that future studies could benefit from incorporating these
perspectives since pre-registration.
Supplementary materials
Database