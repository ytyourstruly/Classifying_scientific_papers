NOTE: This manuscript is currently under review. This is not the final version.
Version Date: 29 December 2024
BAYES FACTORS CANNOT PROVE THE NULL 1
Bayes factors cannot provide evidence for the null hypothesis
Eric C. Fields
Department of Psychology, Westminster College, New Wilmington, PA
Author Note
I thank Nathaniel Delaney-Busch, Ryan Daley, and Angela Gutchess for comments on an
earlier version of this manuscript.
Correspondence concerning this manuscript should be sent to Eric Fields, Dept.
Psychology, Westminster College, 319 South Market Street, New Wilmington, PA 16172. Email:
fieldsec@westminster.edu
BAYES FACTORS CANNOT PROVE THE NULL 2
Abstract
It is becoming increasingly common in the psychology and neuroscience literature to use Bayes
factors to provide evidence in favor of the null hypothesis. The most common approach reports a
Bayes factor that compares the null hypothesis to a ‚Äúdefault‚Äù alternative. This approach can show
that the data is more likely to have been generated under the null hypothesis than under the
specific alternative tested, but it cannot provide evidence for the absence of an effect. In fact,
analyses using ‚Äúdefault‚Äù alternatives can return evidence in favor of the null hypothesis when the
data is consistent with medium to large effects. I show that common interpretations of these Bayes
factor analyses are misleading, and I provide examples and simulations that help to understand
what a Bayes factor in favor of the null does and does not tell us. I argue that Bayes factors have
no special ability to help us understand null effects when compared to frequentist methods such
as equivalence testing and confidence intervals.
Keywords: Bayes factor (BF), Bayesian statistics, frequentist statistics, null effect, null
result, significance testing, null hypothesis significance testing (NHST), equivalence tests,
confidence intervals
BAYES FACTORS CANNOT PROVE THE NULL 3
Introduction
Most psychologists‚Äô and neuroscientists‚Äô statistical education centers around null
hypothesis significance testing (NHST). In NHST, we test a ‚Äúnull hypothesis‚Äù of no effect (e.g.,
there is no correlation between the tested variables; there is no difference between the tested
conditions; etc.). We learn in statistics courses that we can reject this null hypothesis and conclude
that there is an effect when p < .05. If we reject the null hypothesis when it is actually true, this is
a false positive or Type I error. But if we do our analysis correctly (the assumptions of our test are
met, we don‚Äôt p-hack, etc.), 5% or less of all tests will generate a false positive.
However, when p > .05, this only means that we don‚Äôt have enough evidence to reject the
null; it does not mean we have evidence that the null is true. We may fail to reject the null
hypothesis when it is false, which is a false negative or Type II error. Unlike the probability of a
false positive, the probability of a false negative is unknown (because we need to know the
population effect size to calculate the Type II error rate), but is often substantially larger than the
probability of a false positive (Bakker et al., 2012; Brysbaert, 2019; Cohen, 1992; Szucs &
Ioannidis, 2017). As a result, we usually say only that we ‚Äúfail to reject‚Äù the null when p > .05, not
that we accept the null. That is, non-significant results are inconclusive, and NHST can only
provide evidence against the null, not for it.
In recent years it has become common to claim that, unlike NHST and other frequentist
approaches, Bayesian statistics can provide evidence for the null hypothesis. In particular, Bayes
factors (BF) are said to be able to provide evidence both for and against the null. In this paper I
argue that the widespread adoption of Bayes factors to provide evidence for the null rests on a
misunderstanding of what a BF tells us, and that, insofar as any statistical method can provide
evidence ‚Äúfor the null‚Äù, frequentist methods such as equivalence tests and confidence intervals
1 In the long-run, a false positive will occur in 5% of all tests where the null hypothesis is true. Assuming the
null is false in at least some tests, less than 5% of all tests performed will generate false positives. Thus,
when statistical analysis is conducted properly and reported transparently, false positives should be
relatively rare.
BAYES FACTORS CANNOT PROVE THE NULL 4
are just as effective as Bayes factors and may be less prone to misinterpretation.
What is a Bayes factor?
The BF is a model comparison technique: It says how much more likely the observed data
is to have been generated by one model versus another model. In the case of the Bayesian
approach that has become a common alternative to NHST, the BF compares a model
representing the null hypothesis to a model representing the alternative hypothesis. However,
these terms are not quite being used in the way they are used in standard frequentist testing.
In NHST, the null hypothesis says that the effect size is 0, whereas the alternative says it
is not 0. For the t-test case, we can use Œ¥ (the Greek letter delta) to represent the value of the
Cohen‚Äôs d effect size in the population: The null is Œ¥ = 0 and the alternative is Œ¥ ‚â† 0. It is possible
to calculate the likelihood of a particular observed effect size given that Œ¥ = 0. But it is not possible
to calculate the likelihood given simply that the population effect is not zero (because the likelihood
will be different for different non-zero effects). Because NHST only directly tests the null
hypothesis, this is not a problem in classical hypothesis testing. But for the Bayes factor‚Äôs model
comparison, we need to use a specific alternative.
In the approach that has become popular for analyzing null effects in psychology (Rouder
et al., 2009), a null model (Œ¥ = 0) is compared to a ‚Äúdefault‚Äù alternative. This default alternative is
not a single effect size (as the null model is). Instead, it is a probability distribution over all possible
population effect sizes. In the default models that are widely used, most of the probability is on
relatively large effects. For example, for the t-test case, the most commonly used alternative is a
Cauchy distribution over the Cohen‚Äôs d effect size with a scale of 0.707, meaning half the
probability is on effects greater than Œ¥ = 0.707, and there is substantial probability on very large
2 Cohen‚Äôs d is simply the difference in the means of the two conditions being compared divided by the
standard deviation:
1 2
ùëÄùëÄ ‚àí ùëÄùëÄ
This puts the difference between the conditions in a standardized unit (like a Z-score).
ùëëùëë =
ùë†ùë†
BAYES FACTORS CANNOT PROVE THE NULL 5
Because it compares two models, a Bayes factor in favor of the null only tells us that the null
is more likely than the particular alternative it is compared to: It is as much evidence against the
alternative model as it is evidence for the null model. Thus, it doesn‚Äôt provide evidence for the
null in the sense that most researchers trained in classical null hypothesis testing would
understand that claim: as evidence that there is no effect. In fact, for any observed effect, there is
some alternative model that is less likely than the null. Unless the observed effect is exactly zero,
there is also some alternative that is more likely than the null. That is, regardless of the results
obtained in a study, we can provide evidence either for or against the null model depending on
which alternative model we compare it to. Because the BF only tells us that the null model is more
likely than a specific alternative, a Bayes factor in favor of the null tells us very little unless we
understand the models being compared and this model comparison addresses a question that is
relevant for the data it is applied to.
How are Bayes factors being used in psychology?
Unfortunately, the usage of Bayes factors that has become common in psychology usually
employs default alternative models without justifying this choice or considering the alternative
model as part of the interpretation of the Bayes factor. To illustrate the point, in October 2024, I
examined the list of all papers citing Rouder et al. (2009) according to Google Scholar. I sorted
by publication date and examined the first 10 (most recent) papers that reported original research
3 The math involved in calculating a Bayes factor is complicated (or, at least, it involves calculus), but the
calculation can be understood at a conceptual level without any advanced math. Assume I have the results
of a study with a particular observed effect size d and sample size N. If I draw many random samples of
size N from a population where the effect size is Œ¥ = 0, how often will I observe an effect size close to my
actual observed value of d? Call this proportion L : the likelihood of the data under the null model. Now
imagine that I draw a random effect size value, Œ¥, from a Cauchy distribution with a scale factor of .707 (see
step process many times. How often will I observe an effect size close to the actual observed value of d?
Call this proportion L : the likelihood of the data under the alternative model. The Bayes factor in favor the
alternative is then L / L ; the Bayes factor in favor the null is L / L . These ratios give the relative likelihood
1 0 0 1
of the observed data for each model. If we assume that the prior probability of each model is equal, this
likelihood ratio also gives the relative posterior probability of the models.
BAYES FACTORS CANNOT PROVE THE NULL 6
and that were published in a peer reviewed psychology journal (see supplementary materials for
a list of papers examined and additional details). Only four of these papers fully described the
alternative model used to calculate the Bayes factors (many simply said they used ‚Äúdefault‚Äù
alternatives without specifying which model this entails). Nine papers apparently used the default
alternative with a default scale factor; the 10th did not give enough description to know whether
defaults were used. None of the 10 papers gave any justification for their choice of alternative
beyond describing it as a ‚Äúdefault‚Äù. None of the papers gave any indication that they considered
the specific distribution used as the alternative model in their interpretation of the Bayes factor.
This brief exploration of the literature matches the general sense I get as a reader of the
literature and regular peer reviewer of manuscripts. It is my impression that many researchers
who have used the default Bayes factor do not fully understand the alternative model they are
using. The idea that there is a ‚Äúdefault‚Äù alternative seems to suggest to many researchers that the
choice of alternative model is not one they need to carefully consider and justify. In fact, as noted
above, in some papers that report Bayes factors, the models compared are not even specified in
the manuscript. This makes it impossible to evaluate the information provided by a Bayes factor.
Whatever value Bayes factors may have in data analysis, I would argue that the idea that
there is a default choice for the alternative model is a mistake. It simply encourages researchers
to use Bayes factors mindlessly without careful consideration of the models they are comparing
(and what conclusions the comparison of these models can support). Because the value and
interpretation of the BF depends on the alternative used, the alternative needs to be a model that
is sensible to test for the data it is applied to. As Dienes (2019) argues, ‚Äúthere is no such thing as
a default theory, so there cannot be a default model of H [i.e., the alternative hypothesis]‚Äù (p.
366).
4 To be clear, even the developers and proponents of the widely used default Bayes factors have argued
against using a single default alternative for all research questions. For example, Rouder et al. (2016) say,
‚ÄúTo help researchers with the task of specifying alternative models, we recommend a set of default models
that are broadly applicable for many situations . . . We do not recommend a single default model, but a
BAYES FACTORS CANNOT PROVE THE NULL 7
What conclusions do Bayes factors support?
Given the widespread use of default Bayes factors, it is important to understand what
conclusions we can draw from them. Does the default Bayes factor give a reasonable answer to
a question that the average researcher wants to ask of their data? Does a default Bayes factor in
favor of the null allow for the kind of conclusions that most researchers want to draw in light of a
null result? I my view, answer to both of these questions is generally ‚Äúno‚Äù.
To better understand what Bayes factors tell us, it is useful to examine some examples.
Below I describe a simple example of an experiment with a small effect to demonstrate the logical
flaw of concluding that there is evidence for the absence of an effect simply because the null
model is more likely than a default alternative. I then use simulations to examine the behavior of
Bayes factors across different known conditions. The code for all calculations below can be found
in the supplementary materials.
An example: The default Bayes factor is biased against small effects (in small samples)
Assume that we are comparing two conditions in an independent samples design with a
sample size of 40 in each condition. The results show d = 0.2. Under Jacob Cohen‚Äôs (1992) widely
used rules of thumb, d = 0.2 is the canonical ‚Äúsmall‚Äù effect. If we calculate a Bayes factor with the
default alternative, we see a BF = 3.04 in favor of the null. A common rule of thumb in the literature
is to consider BF > 3 to provide evidence of one hypothesis over the other, so by currently popular
standards, we would conclude that we have shown modest evidence in favor of the null
hypothesis.
collection of models that may be tuned by a single parameter, the scale of the distribution on effect size.‚Äù
(p. 7) That is, the Cauchy distribution over effect sizes is to be considered a default, but researchers should
choose the scale factor for this distribution. They also make clear that the default distribution (even with
flexibility for the scale) may not be appropriate for all situations (p. 8). However, the software that
implements these methods (e.g., JASP and the BayesFactor R package) include a default distribution and
a default for the scale factor, and most users of this approach seem to stick to this default without justifying
the use of this particular alternative.
5 Rouder et al. (2012), citing Jeffreys, suggest that ‚Äúodds greater than 3 be considered ‚Äòsome evidence,‚Äô
odds greater than 10 be considered ‚Äòstrong evidence,‚Äô and odds greater than 30 be considered ‚Äòvery strong
evidence‚Äô for one hypothesis over another‚Äù (p. 228). Kass & Raftery (1995), also citing Jeffreys, label BF >
3.2 ‚Äúsubstantial‚Äù evidence, BF > 10 ‚Äústrong‚Äù evidence, and BF > 100 ‚Äúdecisive‚Äù evidence (p. 777).
BAYES FACTORS CANNOT PROVE THE NULL 8
Our best single estimate of the population effect is Œ¥ = 0.2. This is a small effect, but many
interesting effects in psychology and neuroscience are small (Abelson, 1985; Funder & Ozer,
2019; Gotz et al., 2022), and most researchers would probably not consider Œ¥ = 0.2 small enough
to be irrelevant for most applied or theoretical purposes (but see Anvari et al., 2023; Primbs et al.,
2023). Thus, d = 0.2 is prima facie evidence against the null, even if we define the null as
‚Äúpractically 0‚Äù rather than literally 0. It is weak evidence against the null, and it would not be
significant in a conventional hypothesis test. But even before we apply any inferential statistics,
we know that this cannot be clear evidence for the null.
Of course, d = 0.2 is just the sample estimate of the effect size, and with 40 observations
in each group, this estimate is fairly noisy. This effect is not significant in a conventional hypothesis
test because it could be plausibly generated under the null hypothesis with this sample size. But
the uncertainty this represents goes both directions: d = 0.2 is just as likely to come from a
population where Œ¥ = 0.4 as Œ¥ = 0. A 95% confidence interval says that the population effect might
be as large as Œ¥ = 0.64. This would conventionally be considered a medium-large effect. Note
that a Bayesian credible interval with an uninformative prior would give similar results, so this is
not an issue of Bayesian vs. frequentist statistics.
Simply put, d = 0.2 with n = 40 in each group cannot reasonably be taken as evidence for
the absence of an effect. However, the Bayes factor in favor of the null in this example is not
wrong if we understand the question the BF is answering. It simply tells us that a sample with d =
0.2 is more likely to have been generated by the null model than by the default alternative model.
6 It is possible to obtain a Bayes factor in favor of the null when the data are consistent with even larger
effects. For example, for the independent samples design with d = 0.12 and n = 19 in each condition, BF =
3.01 in favor of the null, but the 95% CI shows that the data is consistent with Œ¥ = 0.76. This would
conventionally be considered a large effect. These examples are closely related to what is commonly called
‚ÄúLindley‚Äôs paradox‚Äù (Lindley, 1957): It is easy to construct examples where a standard hypothesis test
rejects the null, but a Bayes factor provides strong evidence in favor of the null.
7 It is worth noting explicitly that Bayes factors are only one method for assessing evidence and evaluating
claims within the Bayesian framework, and different Bayesian analysts have different assessments of their
value. For a discussion of different Bayesian metrics, see Makowski et al. (2019).
BAYES FACTORS CANNOT PROVE THE NULL 9
But this is not evidence for the absence of an effect. It is evidence that an alternative model that
puts most of the probability on large effects does not predict samples with small effects (see also
Schimmack, 2015; Simonsohn, 2015).
With a different alternative model, the Bayes factor would give a different answer. For
example, assume that we compare the null to an alternative represented by a normal distribution
with a mean of 0.2 and a standard deviation of 0.1. This alternative model predicts a small positive
effect, but with some uncertainty around the exact effect size. The Bayes factor in this case (using
parameters from the example above) is 1.36 in favor of the alternative. Of course, this calculation
only demonstrates the obvious: An observed small effect is stronger evidence for a hypothesis
that says the population effect is small than it is for a hypothesis that says there is no effect in the
population (the evidence in favor of the alternative is weak because it is difficult to distinguish
small effects from the null hypothesis without large sample sizes, a point I return to below).
Using simulations to understand the properties of the Bayes factor
We can further understand how to interpret the default BF through simulations that show
how it performs across repeated samples from a population with a known effect size (see also
Lakens, 2016; Simonsohn, 2015). For each simulation below, 100,000 samples were drawn in
independent samples design with a given population effect size, normally distributed data, and
equal variances across the two conditions. For each simulated sample, the standard frequentist
independent samples t-test was calculated, and the Bayes factor was calculated using the
materials. Readers can modify the script to further explore the performance of the BF with different
parameters.
in each group. When the null hypothesis is true, the default Bayes factor returns evidence in favor
the null hypothesis of BF > 3 in 64% of studies. However, when the null is false and the true
BAYES FACTORS CANNOT PROVE THE NULL 10
population effect is Œ¥ = 0.2, 47% of studies will return a BF in favor of the null and only 7% of
samples will give BF > 3 in favor of the alternative. Again, because the alternative mostly predicts
large effects, the BF is biased in favor of the null when the true effect size is small (with small
sample sizes). Even with a population effect of Œ¥ = 0.5, conventionally considered a medium-sized
effect, 9% of samples will give BF > 3 in favor of the null, while less than half of samples will give
BF > 3 in favor of the alternative. In contrast, the frequentist t-test rejects the null in 60% of
samples.
This is worth dwelling on: Given a relatively common sample size in psychology and a
medium-sized effect, the standard frequentist test would correctly reject the null in 60% of studies,
while in 40% of studies we would be left with inconclusive results (assuming we don‚Äôt incorrectly
interpret a non-significant test as evidence that the null is true). The default BF approach would
only correctly reject the null in 42% of studies, would leave us with inconclusive results in 49% of
studies (BF < 3 in favor of either hypothesis), and would lead us to incorrectly conclude in favor
of the null in 9% of studies.
A possible criticism of the results above would be that BF > 3 is not strong enough
of BF > 10 is used as the standard of support for one hypothesis over another. With the
parameters used in these simulations, this eliminates cases where the Bayes factors indicates
evidence in favor of the null when the null is actually false (although it is certainly possible to
obtain BF > 10 in favor of the null when it is false). However, it also eliminates the ability to provide
evidence for the null when it is actually true. In addition, it significantly reduces the proportion of
studies where the BF will correctly provide evidence for the alternative hypothesis. For example,
with a medium-sized effect (Œ¥ = 0.5), the frequentist t-test rejects the null in 60% of samples, but
BF > 10 in favor of the alternative is only obtained in 24% of samples.
The default alternative model used in the simulations above places half the probability on
BAYES FACTORS CANNOT PROVE THE NULL 11
Cohen‚Äôs d effect sizes greater than 0.707. Given that many effects in psychology are small, this
may not be a reasonable alternative model. In a discussion of statistical power, Brysbaert (2019)
suggests Œ¥ = 0.4 as a default (to use in a power analysis) when little is known about the size of
of 0.4 as the alternative model (half of the probability is on effects below Œ¥ = 0.4 and half of the
probability is on effects larger than this). With the parameters used in this simulation, this
eliminates studies where the BF suggests evidence in favor of a false null hypothesis. However,
it also eliminates the ability to provide evidence for the null hypothesis when it is true. In addition,
with this alternative model, the ability to provide evidence against the null decreases, and the
proportion of studies where the BF is inconclusive increases substantially unless the effect size
is large. Of course, the proportion of inconclusive studies could be decreased with a larger sample
size. But with small sample sizes, it is simply not possible to distinguish the null hypothesis from
small observed effects.
To be clear, none of these simulations show a flaw in the math of Bayes factors; the results
simply need to be interpreted correctly. The BF is not comparing the null and alternative as those
terms are understood in standard frequentist testing. It is comparing the null to a specific
alternative model. That the null is more likely than this specific alternative is not evidence for the
null per se, because there is always some other alternative model that will be more likely than the
null (unless the observed effect is exactly 0), and there is always some alternative model that will
be less likely than the null. If we understand the alternative model being tested and it is a model
that makes sense to test in the context of the research, the fact that the null model is more likely
than this alternative can tell us something important. Otherwise, a Bayes factor in favor of the null
tells us very little.
8 It should be noted that some studies use a Cauchy with a scale factor of 1 as the default alternative. This
was the alternative originally suggested by Rouder et al. (2009). Because this alternative has even more
probability on large effects, it should be obvious that it would only exacerbate the problems discussed here.
BAYES FACTORS CANNOT PROVE THE NULL 12
An alternative to Bayes factors: Using confidence intervals to find an upper bound on the
effect size
A simple alternative to the Bayes factor for interpreting null effects is to use a confidence
interval to find the upper bound on the effect size (Hoenig & Heisey, 2001). When we are unable
to reject the null hypothesis, we know that an effect size of 0 is consistent with the data and will
be in the interval given by the CI. But the CI also shows us the largest effect size that is consistent
with the data. If this upper bound is a large effect, this tells us that both null effects and large
effects are compatible with the data, so our results are relatively inconclusive. If the CI contains
only small effects, we have not proven an absence of an effect, but we can conclude the effect is
not large. If the CI only contains effects sizes close enough to 0 to be practically and/or
theoretically indistinguishable from the absence of an effect, then we have shown that there is no
meaningful effect (even if we have not proven that the effect size is exactly 0). This approach is
simple, easy to implement for a wide range of statistical models and analyses (with a wide range
of statistical software), and relatively easy to interpret.
Defining the largest effect size that would be practically or theoretically equivalent to 0 and
determining whether all effects larger than this are excluded by the CI is a form of equivalence
testing, although equivalence is usually implemented with hypothesis tests rather than confidence
intervals (Lakens et al., 2018). Although confidence intervals and equivalence testing are
frequentist approaches, the basic point here is not about frequentist vs. Bayesian statistics: region
of practical equivalence (ROPE) testing employs essentially the same approach described here,
but with Bayesian credible intervals rather than frequentist confidence intervals (Kruschke, 2018).
Another alternative is theoretically informed Bayes factors (Dienes, 2014, 2019). In this approach,
9 The CI will always include non-zero effects, and no other method can prove than an effect is exactly equal
to 0 in the population (Harms & Lakens, 2018). Thus, a crucial issue for interpreting null effects is
determining the smallest effect that is meaningful for theory, application, or practical consequences. This
has often been called the ‚Äúsmallest effect size of interest‚Äù (SESOI). Determining the SESOI is often not
easy, but a variety of methods have been proposed to help make this determination (Anvari & Lakens,
2021; Dienes, 2019; Riesthuis et al., 2022).
BAYES FACTORS CANNOT PROVE THE NULL 13
the null is compared to an alternative model that represents what is predicted by a theory. If the
BF shows that the null is more likely than the alternative, we haven‚Äôt provided evidence for the
absence of an effect, but we have provided evidence against the theory. All of these approaches
can be useful to help us better understand null effects (Harms & Lakens, 2018). When used and
interpreted correctly, these different approaches will usually give a similar answer.
Given that most researchers are taught to calculate and interpret confidence intervals as
part of their statistical education, whereas Bayes factors are a less familiar approach, why have
Bayes factors become the go-to approach for analyzing null effects instead of the confidence
interval approach outlined above? One possibility is that the CI approach will generally reveal that
medium to large effects are compatible with the data unless sample sizes are quite large, whereas
misinterpretation of Bayes factors has led to the belief that evidence can be obtained for the null
hypothesis in small samples.
observed effect of d = 0 for various sample sizes in the independent samples and paired samples
designs. This value gives the upper bound in the confidence interval if the observed effect is 0 (of
course, in actual data the observed effect will rarely if ever be equal to 0, so in practice the upper
bound will be at least a little larger than this). In small samples, we can rule out large effects. If
we consider any effect d > 0.2 to be meaningful, we will need over 200 observations in each group
(400 total) for an independent samples design and likely over 100 observations in the paired
samples design to establish that there is no meaningful effect. Of course, many interesting effects
in psychology and neuroscience are small (Abelson, 1985; Funder & Ozer, 2019; Gotz et al.,
10 In a letter to R. A. Fisher (the father of the null hypothesis significance test), the pioneering Bayesian
Harold Jeffreys suggested that ‚Äúit would only be once in a blue moon that we would disagree about the
inference to be drawn in any particular case, and that in the exceptional cases we would both be a bit
doubtful‚Äù. In another context Jeffreys said, ‚ÄúI have applied my significance tests [i.e., Bayes factors] to
numerous applications that have also been worked out by Fisher‚Äôs, and have not yet found a disagreement
in the actual decisions reached‚Äù (both quotes are taken from van Dongen et al., 2019, p. 335). For a recent
example and discussion of various inferential philosophies applied to the same data, see van Dongen et al.
(2019).
BAYES FACTORS CANNOT PROVE THE NULL 14
2022), so we may need to rule out even smaller effects (but see Anvari et al., 2023; Primbs et al.,
2023). If we think that an effect must be d < 0.1 to be too small to be relevant, we need samples
of over 750 in each group (1,500 total) in the independent samples design and probably over 400
observations in the paired samples design to conclude that there is no meaningful effect. Given
the difficulty of obtaining such large samples in many research areas, these results are
discouraging: They show that null results will often be inconclusive.
The bottom-line is that it requires large samples to prove that a population effect is trivially
small. This is true regardless of statistical approach. In fact, the frequentist t-test, Bayesian t-test,
frequentist confidence interval, and Bayesian credible interval are all calculated using likelihoods
from the same underlying probability distribution. Regardless of approach, large samples are
required to reduce sampling error to the point that null effects can be distinguished from small
effects. The default Bayes factor is able to provide evidence in favor of the null in small samples
because it compares the null to a model that expects effects to be large much of the time.
Oversimplifying a bit, in a small sample, a BF in favor of the null with the default alternative simply
tells us that the effect is probably not large (see also Simonsohn, 2015). But a confidence interval
can tell us the same thing in a way that most researchers will probably find easier to understand.
When we use Bayes factors to compare the null to a model that expects effects to be small, large
sample sizes are required to distinguish these models, just as they are required to rule out small
effects with a confidence interval.
Conclusions
The title of this paper states that Bayes factors cannot provide evidence for the null
hypothesis. But this problem is not specific to Bayes factors: No statistical method can provide
evidence for the null hypothesis if by this we mean evidence that the effect size is exactly equal
to 0 in the population (Harms & Lakens, 2018). Finite sample data is always subject to sampling
error, which means observed data will always be consistent with some non-zero effect in the
BAYES FACTORS CANNOT PROVE THE NULL 15
population. What is possible is to provide evidence that the null hypothesis is more likely than a
specific alternative or that the effect size is within some specified range of 0. Unfortunately, in
either case, distinguishing the null hypothesis from small effects requires large sample sizes.
The Bayes factor calculates the relative evidence for the null compared to some specific
alternative. Researchers who wish to use this approach should understand, describe, and justify
the alternative model used. In my view, it will rarely be well-justified to use any default alternative
model; instead the alternative should be a model that makes sense to test in the context of the
research. Regardless of the alternative model used, a Bayes factor can never provide evidence
for the null hypothesis in the sense that most traditionally trained researchers will understand this
claim: as evidence for the true absence of an effect. Journal editors and reviewers should not
allow such claims (or claims that are likely to be misinterpreted in this way).
Although Bayesian statistics can be used to understand null effects when calculated and
interpreted properly, it is not true that they have any special ability in this regard. Frequentist
equivalence tests and confidence intervals can be just as effective at telling us what we can
conclude from null results. For researchers who don‚Äôt have any particular preference for a
Bayesian approach, I would recommend analyzing null results by using a confidence interval to
find an upper bound on whatever effects size is most meaningful and easy to interpret for a given
dataset. I think this approach is generally less like to be misunderstood and misinterpreted by
authors and readers alike.
BAYES FACTORS CANNOT PROVE THE NULL 16
Œ¥ = 0.00 Œ¥ = 0.20 Œ¥ = 0.50 Œ¥ = 0.80
t-test p < .05 0.05 0.14 0.60 0.94
Evidence for the null
BF > 3 0.64 0.47 0.09 0.00
BF > 10 0.00 0.00 0.00 0.00
Evidence for the alternative
BF > 3 0.02 0.07 0.42 0.87
BF > 10 0.00 0.02 0.24 0.72
BF inconclusive 0.35 0.46 0.49 0.13
population effect size. Shown are the proportion of simulated studies where the BF and p-value
met various thresholds for different population effect sizes. The BF is taken to be inconclusive
when BF < 3 for both the null and alternative. The alternative for the Bayes factor is a Cauchy
distribution with scale = 0.707. See text for simulation details.
BAYES FACTORS CANNOT PROVE THE NULL 17
Œ¥ = 0.00 Œ¥ = 0.20 Œ¥ = 0.50 Œ¥ = 0.80
t-test p < .05 0.05 0.14 0.60 0.94
Evidence for the null
BF > 3 0.00 0.00 0.00 0.00
BF > 10 0.00 0.00 0.00 0.00
Evidence for the alternative
BF > 3 0.02 0.07 0.44 0.88
BF > 10 0.00 0.02 0.24 0.72
BF inconclusive 0.98 0.93 0.56 0.12
known population effect size. Shown are the proportion of simulated studies where the BF and p-
value met various thresholds for different population effect sizes. The BF is taken to be
inconclusive when BF < 3 for both the null and alternative. The alternative for the Bayes factor is
a Cauchy distribution with scale = 0.4. See text for simulation details.
BAYES FACTORS CANNOT PROVE THE NULL 18
sample size independent paired (r = 0.2) paired (r = 0.5) paired (r = 0.8)
20 0.62 0.54 0.44 0.31
30 0.51 0.44 0.36 0.25
40 0.44 0.38 0.31 0.22
50 0.39 0.34 0.28 0.20
75 0.32 0.28 0.23 0.16
100 0.28 0.24 0.20 0.14
200 0.20 0.17 0.14 0.10
300 0.16 0.14 0.11 0.08
400 0.14 0.12 0.10 0.07
500 0.12 0.11 0.09 0.06
750 0.10 0.09 0.07 0.05
1,000 0.09 0.08 0.06 0.04
2,000 0.06 0.05 0.04 0.03
5,000 0.04 0.03 0.03 0.02
error (half width) for a confidence interval around d = 0 for different sample sizes. For the
independent samples design, the sample size for each condition is given (i.e., the total sample
sizes would be twice this value). For the paired samples design the d measure of effect size is
av
shown (Lakens, 2013). The margin of error for the paired samples design depends on the
correlation between the two conditions.
BAYES FACTORS CANNOT PROVE THE NULL 19
the t-distribution with one degree of freedom. It has a general bell curve shape, but the tails of the
distribution are much heavier than the normal distribution (i.e., there is more probability on large
values). The mean and variance of the Cauchy distribution are undefined, but the distribution that
is widely used in the default Bayes factor is centered around 0 and has a scale factor of 0.707.
This means half the probability is on values larger than 0.707 and half the probability is on values
various benchmarks. Effect size labels are from Cohen (1992) and Sawilowsky (2009).
BAYES FACTORS CANNOT PROVE THE NULL 20