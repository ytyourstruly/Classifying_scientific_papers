Using facial EMG to track emotion during language
comprehension: past, present, and future
1 1 1
Jos J.A. van Berkum , Marijn Struiksma & Björn 't Hart
Utrecht Institute of Linguistics OTS, Utrecht University, Utrecht, The Netherlands
August 12, 2020, version 2d
To appear in Grimaldi, Shtyrov & Brattico (Eds.).
Language electrified: Techniques, Methods, Applications, and Future
Perspectives in the Neurophysiological Investigation of Language. Springer
Correspondence: Prof. Dr. Jos J.A. van Berkum, UiL OTS, Utrecht University,
Trans 10, 3512 JK Utrecht, The Netherlands j.vanberkum@uu.nl
Abstract
Beyond recognizing words, parsing sentences, building situation models, and other cognitive accomplishments,
language comprehension always involves some degree of emotion too, with or without awareness. Language
excites, bores, or otherwise moves us, and studying how it does so is crucial. This chapter examines the potential
of facial electromyography (EMG) to study language-elicited emotion. After discussing the limitations of self-
report measures, we examine various other tools to tap into emotion, and then zoom in on the electro-
physiological recording of facial muscle activity. Surveying psycholinguistics, communication science, and other
fields, we provide an exhaustive qualitative review of the relevant facial EMG research to date, exploring 55
affective comprehension experiments with single words, phrases, sentences, or larger pieces of discourse. We
discuss the outcomes of this research, and evaluate the various practices, biases, and omissions in the field. We
also present the fALC model, a new conceptual model that lays out the various potential sources of facial EMG
activity during language comprehension. Our review suggests that facial EMG recording is a powerful tool for
exploring the conscious as well as unconscious aspects of affective language comprehension. However, we also
think it is time to take on a bit more complexity in this research field, by for example considering the possibility
that multiple active generators can simultaneously contribute to an emotional facial expression, by studying how
the communicator's stance and social intention can give rise to emotion, and by studying facial expressions not
just as indexes of inner states, but also as social tools that enrich everyday verbal interactions.
VAN BERKUM, STRUIKSMA & 't HART
1. Introduction ................................................................................................................................ 3
2. Measuring emotion ..................................................................................................................... 3
2.1 Why self-report is not enough ................................................................................................................... 3
2.2 Measuring emotion via facial electromyography ....................................................................................... 5
3. A review of facial EMG research on affective language comprehension ....................................... 6
3.1 Single-word studies ................................................................................................................................... 9
3.2 Phrase and sentence studies .................................................................................................................... 12
3.3 Discourse studies ..................................................................................................................................... 15
3.4 Some general observations ..................................................................................................................... 19
4. The fALC model ......................................................................................................................... 21
4.1 Language comprehension at multiple levels ............................................................................................ 22
4.2 Emotion-based evaluation ...................................................................................................................... 24
4.3 Emotion as simulation ............................................................................................................................. 24
4.4 Emotional mimicry and other factors ...................................................................................................... 25
4.5 Using the model ...................................................................................................................................... 26
5. Challenges and opportunities .................................................................................................... 27
Acknowledgments ........................................................................................................................ 30
References .................................................................................................................................... 30
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
1. I
NTRODUCTION
One of the most interesting features of us humans is that we can communicate through language. We
do so seemingly effortlessly, for many, many hours a day, as we for example engage in conversation,
check our social media, read a book, or listen to the news. The speed of language processing is
astounding, with an average reading speed of 3-5 words per second for adults, depending on the
language at hand (1), and an estimated average speaking -- and hence also listening -- speed that is
only slightly lower and that can go up dramatically, depending on circumstances (2). Psycholinguists
interested in language comprehension have long realized that to understand this rapid and intricately
orchestrated skill, they need measures that can keep track of reading or listening as language input
unfolds. It is for this reason that the research field makes heavy use of temporally sensitive measures
like self-paced reading, eye tracking, electroencephalography (EEG), and magnetoencephalography
(MEG).
The use of such measures has contributed a lot to our understanding of the many cognitive aspects of
language comprehension that psycholinguistics has traditionally been interested in, like word
recognition, syntactic parsing, or situation model construction (see 3, 4, 5, and this volume, for
reviews). However, language researchers have recently also begun to ask questions about the
relationship between language and emotion (e.g., 6, 7, 8, 9, 10, 11, 12, 13, 14). This new focus in
language research raises an important question: what measurement tools should we use to keep track
of the emotional aspects of language comprehension as the words are coming in? How can we
measure, during reading or listening, the emotional impact of, say, a piece of gossip, a good joke, a
political slogan, a medical information leaflet, or an engaging bit of narrative fiction?
In this chapter, we address exactly this question. After examining the various options for measuring
emotion during language comprehension, we focus on one particularly promising method: facial
electromyography, the electrophysiological recording of facial muscle activation. We explain the
method, provide an exhaustive review of the language-relevant facial EMG research to date, present
a conceptual model for future EMG research in this domain, and discuss the challenges and
opportunities ahead. Our focus in all of this will be largely conceptual, for a methods-oriented review
of facial electromyography in the language sciences and beyond, see 42 in this volume.
2. M
EASURING EMOTION
2.1 Why self-report is not enough
At first sight, emotion seems to be about how people feel. Emotion is therefore often measured by
asking people about this conscious experience, either via an open-ended "how do you feel" question,
or, more commonly, via a constrained self-report format. For example, researchers can ask
participants in their study to rate the presence and intensity of specific emotions, such as joy, shame,
or compassion (e.g., via the Geneva Emotion Wheel, 15). They can also ask participants to rate their
conscious experience on one or more emotion-relevant dimensions. An often used dimension is
VAN BERKUM, STRUIKSMA & 't HART
subjective valence, the degree to which the emotion, or the elicitor of that experience, is experienced
as positive or negative (cf. the smiley-based valence scale used to assess customer satisfaction).
Ratings can also be collected on two orthogonal dimensions, often crossing subjective valence and
subjective arousal, the experienced degree of bodily activation (e.g., SAM, 16), but sometimes also
crossing experienced positivity and experienced negativity (the Affect Matrix, 17). Furthermore,
researchers can systematically ask people to report on various other things, such as what triggered the
emotion (18), or the amount of activity they feel in different parts of the body (19).
These various self-report methods can all help explore how people feel. However, they are not
particularly useful for tracking emotion as linguistic input unfolds, for two very different reasons. On
the practical side, researchers simply cannot ask for a self-report at each word or sentence, at least
not without seriously disrupting the processes under study. The second and much more fundamental
reason is that, just like with other domains of the human mind, many aspects of emotion are simply
inaccessible to consciousness, at least initially (13, 20, 21, 22, 23, 24). In everyday life, people use the
terms "emotion" and "feeling" in interchangeable ways. However, research has shown that emotion
can sometimes do all of its work 'behind the scenes', nudging us without us being aware of the stimulus
or of what it brings about (e.g., 25, 26). Furthermore, even in the case where conscious experience or
"feeling" does arise, there is a lot more to the emotion at hand than what people have conscious access
to -- feeling is usually just the tip of the iceberg, and important motivational, cognitive, physiological
and expressive aspects of an emotion will inevitably escape self-report. How to tap into those?
Motivational and cognitive aspects. Because emotions always come with particular motivations
('action tendencies', 27), and can also for example affect the speed and focus of cognitive processing
(e.g., via emotion-induced attention), familiar behavioral measures developed within cognitive
psychology and psycholinguistics, such as two-choice response time, self-paced reading and eye
tracking can in principle be used to pick up on these aspects of language-induced emotion,
independent of whether those are accessible to consciousness or not. For example, it is possible to
detect very subtle approach or avoidance tendencies when reading words with strong positive or
negative valence (28, 29). Also, because motivational and cognitive aspects of emotion require neural
computation, brain measures such as EEG, MEG and fMRI can also help track these aspects of
emotional responding during language processing, independent of whether people are aware of them
or not (e.g., 30; see 31 and 7 for reviews).
Physiological and expressive aspects. One highly convenient feature of emotions, however, is that they
are "fundamentally embodied" (32), or, more poetically, "play out in the theatre of the body" (33), in
two interesting ways. First, because emotions usually prepare the body to act (e.g., to approach or
avoid, to explore, embrace, or hide), they come with various rapid physiological changes that prepare
for and support the intended actions, such as changes in heart rate, sweating, or hormone levels.
Although brain measures might be able to pick up on the neural control or neural feedback associated
with these physiological changes, the changes themselves can be measured as well (see 34, for a
review of the utility of recording cardiovascular, respiratory, skin conductance, and other physiological
parameters). And they can be measured not just when actually facing, say, a hairy spider, or a loved
one, but also when reading or hearing about them (e.g., 35; see 36 and 37 for reviews).
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
Second, emotions are often expressed in the face, in the voice, and in the movements or posture of
the rest of the body (see 38 for review), sometimes deliberately, and sometimes involuntarily. Such
expressions probabilistically inform others about how we evaluate things and what state we are in, as
well as about what we might do, right now, or in the future (39, 40). Responding to an insult with an
angry face and voice, for example, informs the offender and/or others that we probably dislike seeing
our rights or properties infringed upon, that we may well strike back in some way or the other, and
that it is therefore probably wise to back off. Responding to a sad story of a friend with a
compassionate face and voice tells him or her that we are probably concerned with his or her fate, and
that we might well be willing to help. And responding to a mistake with a facial expression and posture
of embarrassment or shame informs others that we probably agree we messed up, feel bad about it,
and are somehow committed to not let it happen again. Although the specific ingredients of emotional
expressions need not all have evolved for communication originally (41; see also 42), it is clear that like
other primates, we express our emotions to others, and that doing so plays a crucial role in solving the
many problems of social life (43). Again, brain measures might be able to pick up on the required neural
control. But just like the physiological embodiment of emotion, the expressive embodiment itself can
also be measured. One way to do so is by recording facial EMG.
2.2 Measuring emotion via facial electromyography
Facial expressions are generated by contracting and/or
relaxing specific subsets of the ~30 muscles in the human
face. Facial muscles are not only involved in expressing
emotion, but also needed for other things, such as eating
and drinking, improving visual acuity, and speaking. During
non-invasive facial electromyography, all such emotional
and/or non-emotional facial activity can be recorded with
high temporal resolution, by means of small electrodes
1 displays standard electrode positions for recording EMG
from specific facial muscles, including, for example, the
corrugator supercilii (involved in frowning), the levator labii
superioris (involved in wrinkling the nose to express
disgust), and the zygomaticus major and orbicularis oculi
(involved in smiling; see 44, 42, 45).
courtesy of Anton van Boxtel (see also 44).
Facial EMG has been used to study many things, including the facial correlates of physical and cognitive
effort (e.g., 46, 47, 48, 49), and the patterns of lip movement during articulation (e.g., 50, 51).
However, building on earlier work that used facial EMG to track covert articulation during reading,
Schwartz and colleagues (e.g., 52) were the first to use EMG to keep track of emotion: when
participants were asked to imagine happy, sad, and angry situations, the researchers observed
increased corrugator activity during sad and angry emotional imagery as well as decreased corrugator
VAN BERKUM, STRUIKSMA & 't HART
activity during happy emotional imagery. Importantly, Schwartz and colleagues also videotaped the
participants while they were engaging in emotional imagery, and reported that the patterns of activity
recorded via EMG "were not readily detected on the overt face". In line with physiological knowledge
about the imperfect coupling of facial muscle activation and visible skin movement (see 53), this
showed that facial EMG can track very subtle, non-overt changes in emotional state.
Following up on this clinical work, Cacioppo and Petty (54) pioneered the use of facial EMG to track
emotional state in a verbal communication setting, as part of a larger social-psychological research
program that explored attitude-relevant processing. In the critical study (experiment 2),
undergraduate participants listened to pro- and counter-attitudinal messages (e.g., on alcohol use)
while EMG was recorded over the corrugator to assess the degree of frowning, and over the
zygomaticus and depressor anguli oris (upper and lower cheek muscles) to assess the degree of smiling
(to track covert pronunciation, the mentalis also was recorded). In contrast to Schwartz et al. (52), who
related their EMG findings to specific emotions, Cacioppo and Petty looked for EMG traces of
emotional valence, the degree to which different stimuli elicited positive or negative emotion (which
participants may or may not be aware of). Their results suggested that both the corrugator and the
zygomaticus were sensitive to message valence, in the expected direction: increased corrugator
activity to negative stimuli, and increased zygomaticus activity to positive stimuli. Several follow-up
studies confirmed that facial EMG over these and other facial muscles could help assess the valence of
language-induced emotional states, and do so even when no overt changes were visible in the face
(see 55 for review). As such, this early work paved the way for other attempts to track language-elicited
emotion by means of facial EMG. In the next section, we review that work.
3. A EMG
REVIEW OF FACIAL RESEARCH ON AFFECTIVE LANGUAGE COMPREHENSION
To explore the potential of facial EMG for our purpose, comprehension studies that explicitly asked
about the language-emotion interface within the field of psycholinguistics are obviously relevant.
However, as illustrated by the pioneering work by Cacioppo and Petty (54), psychologists interested in
the neighboring field of communication have also been using facial EMG to track emotion to language,
albeit without foregrounding a language processing question. We therefore queried the literature to
find emotion-oriented facial EMG-research with a psycholinguistic or (verbal) communication focus.
Our interest was in examining the types of questions addressed, the procedures by which they were
addressed (e.g., which electrodes were measured, and why), and the characteristic findings obtained.
Psycholinguistically framed research was obtained by querying Scopus with:
TITLE-ABS-KEY ( language OR discourse OR text OR story OR stories OR narrative OR conversation OR sentence OR phrase OR word OR lexical OR speech OR prosody )
AND
TITLE-ABS-KEY ( comprehension OR recognition OR perception OR interpretation OR reception OR processing ) AND
TITLE-ABS-KEY ( "facial EMG" OR electromyography OR zygomaticus OR corrugator OR frontalis OR levator OR orbicularis OR depressor OR mentalis ) AND
TITLE-ABS-KEY ( emotion OR valence OR affect OR sentiment )
Research on communication was obtained by querying Scopus with:
TITLE-ABS-KEY ( communication ) AND
TITLE-ABS-KEY ( "facial EMG" OR electromyography OR zygomaticus OR corrugator OR frontalis OR levator OR orbicularis OR depressor OR mentalis ) AND
TITLE-ABS-KEY ( emotion OR valence OR affect OR sentiment )
The resulting entries were screened for whether they explored facial EMG responses to emotionally relevant linguistic stimuli
presented to non-impaired adult participants, and complemented with ad-hoc leads derived from the obtained papers that
also passed the above screening. All queries were conducted on May 4, 2019, without setting any date ranges.
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
mostly in journal articles, but also, occasionally, in published dissertations. We grouped these
experiments on the basis of whether they involved single isolated words, phrases and sentences, or
multi-sentence discourse -- a standard psycholinguistic criterion that is convenient for now, but to
which we will briefly return later.
and zygomaticus major (ZM) was sensitive to a language-driven emotion manipulation (yes, no, undecidable, or not
measured/evaluated (-)), whether one of the two muscles was sensitive while the other was not (CS>ZM: CS sensitive, ZM
not sensitive; CS<ZM: CS not sensitive, ZM sensitive; CS=ZM: CS and ZM both sensitive, or both not sensitive), and whether
any other facial muscles were recorded from. The undecidable case is a study that used a CS-ZM composite measure.
CS emotion- ZM emotion- Recorded other
Study Comparison
sensitive? sensitive? muscles
Arias, Belin, & Aucouturier (2018) yes yes CS=ZM
Arndt, Allen, & Greenberg (2001) yes -
Bartholow, Fabiani, Gratton, & Bettencourt (2001) yes -
Baumeister, Foroni, Conrad, Rumiati, & Winkielman (2017) yes yes CS=ZM
Bayer, Sommer, & Schacht (2010) yes -
Borgeat, Elie, Chaloult, & Chabot (1984) - - frontalis
depressor anguli oris,
Cacioppo & Petty (1979, study 2) yes yes CS=ZM
mentalis
Cikara & Fiske (2012, study 1) - yes
Durso, Geldbach, & Corballis (2012) yes no CS>ZM depressor anguli oris
Fiacconi & Owen (2015) yes yes CS=ZM
Fino, Menegatti, Avenanti, & Rubini (2016) yes yes CS=ZM
Fino, Menegatti, Avenanti, & Rubini (2019) yes yes CS=ZM
Fino (2014, study 3) yes no CS>ZM orbicularis oculi
Foroni (2015) - yes
Foroni & Semin (2009) yes yes CS=ZM
Foroni & Semin (2013) - yes
Gavaruzzi, Sario, Giandomenico, Rumiati, Polato, De Lazarri, & Lotto (2018,
yes -
study 2)
Hietanen, Surakka, & Linnankoski (1998) yes - orbicularis oculi
Kätsyri, Ravaja, & Salminen (2012) no yes CS<ZM orbicularis oculi
Krumhuber, Tsankova, & Kappas (2016) no no CS=ZM levator labii superioris
Künecke, Somme, Schacht, & Palazova (2015) yes no CS>ZM
Kunkel (2018, study 2, control experiment) yes no CS>ZM levator labii superioris
Kunkel (2018, study 2, discourse experiment) yes yes CS=ZM levator labii superioris
Kunkel (2018, study 3, exp 2, control experiment) yes no CS>ZM
Kunkel (2018, study 3, exp 2, discourse experiment) no no CS=ZM
Larsen, Norris, & Cacioppo (2003) yes yes CS=ZM
Larsen, Norris, McGraw, Hawkley, & Cacioppo (2009, study 4) yes yes CS=ZM
Larson, Hazlett, Chaparro, & Picard (2007) yes no CS>ZM orbicularis oculi
Lee & Potter (2018) yes yes CS=ZM
Leshner, Bolls, Gardner, Moore, & Kreuter (2018) yes -
Levy, Harmon-Jones, & Harmon-Jones (2018, study 2) yes -
VAN BERKUM, STRUIKSMA & 't HART
Livingstone, Forde Thompson, & Russo (2009) undecidable undecidable
Lucas, Sánchez-Adam, Vila, & Guerra (2019) yes yes CS=ZM orbicularis oculi
Magnée, Stekelenburg, Kemner, & de Gelder (2007, study 1) yes yes CS=ZM
frontalis, orbicularis
Morriseau, Mermillod, Eymond, van der Henst, & Noveck (2017) no yes CS<ZM
oculi
Neumann, Hess, Schulz, & Alpers (2005, study 1) yes yes CS=ZM
Neumann, Hess, Schulz, & Alpers (2005, study 2) yes no CS>ZM
orbicularis oculi, levator
Niedenthal, Winkielman, Mondillon, & Vermeulen (2009, study 1) yes yes CS=ZM
labii superioris
orbicularis oculi, levator
Niedenthal, Winkielman, Mondillon, & Vermeulen (2009, study 2) yes yes CS=ZM
labii superioris
Ravaja, Aula, Falco, Laaksonen, Salminen, & Ainamo (2015) no yes CS<ZM
Ravaja, Kallinen, Saari, & Keltikangas-Järvinen (2004) yes no CS>ZM orbicularis oculi
Ravaja, Saari, Kallinen, & Laarni (2006) yes no CS>ZM orbicularis oculi
't Hart, Struiksma, van Boxtel, & van Berkum (2018) yes -
't Hart, Struiksma, van Boxtel, & van Berkum (2019) yes -
't Hart, Struiksma, van Boxtel, & van Berkum (submitted) yes -
Thomson, Mackenzie, Leuthold, & Filik (2016) yes yes CS=ZM
Topolinsky, Likowski, Weyers, & Strack (2009) yes yes CS=ZM frontalis
Topolinsky & Strack (2015) yes no CS>ZM frontalis
Tuisku, Ilves, Lylykangas, Surakka, Ainasoja, Rytövuori, & Ruohonen (2018) yes -
Van Leeuwen (2017, chapter 5) yes -
Wassiliwizky, Koelsch, Wagner, Jacobsen, & Menninghaus (2017, study 1) yes no CS>ZM
Weis & Herbert (2017) yes yes CS=ZM
Wexler, Warrenburg, Schwartz, & Janer (1992) yes no CS>ZM
Wise, Kim, & Kim (2009) yes -
Zhu & Suzuki (2018) yes yes CS=ZM
= nr of CS&ZM-studies
muscle sensitive to language-driven emotion manipulation = 45 26 22 with effects in both
muscles
= nr of CS&ZM-studies
muscle not sensitive to language-driven emotion manipulation = 5 14 12
with effects in CS only
= nr of CS&ZM-studies
undecidable = 1 1 3
with effects in ZM only
muscle not measured or not evaluated = 4 14
= total number of
total number of studies reviewed = 55 37
studies with CS & ZM
Facial EMG can in principle also be useful when assessing the role of emotion during language
production (14). However, echoing a general bias towards comprehension in psycholinguistics, the
interest in how emotion affects production lags far behind the interest in how it affects comprehension
(e.g., 56). Also, speaking involves facial muscle activation as part of articulation, which greatly
complicates the use of EMG to keep track of speaker emotion. Therefore, while psycholinguists
interested in language production have used EMG to study articulation (e.g., 50, 51), they have to our
knowledge not used it to tap into emotional factors -- even in stuttering research, where emotion has
clear relevance, EMG recording focuses on articulation only (e.g., 57, 58).
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
Our review of studies that used EMG to track emotion during language comprehension does not cover
facial EMG studies that use verbal materials as a vehicle but whose description does not satisfy the
language- and communication-oriented keyword searches in this review (e.g., research with 'vignettes'
in social or moral psychology). We also do not cover media-psychological EMG research involving
cinematic narrative (e.g., 59) or advertisements with emotionally toned non-verbal materials (e.g.,
music, 60), work that includes language but does not manipulate verbal materials in ways that are
useful to those interested in language processing. Finally, we ignore EMG studies that, in the trail of
the pioneering emotional imagery EMG work by Schwartz et al. (52), explored the emotional
consequences of thinking about linguistically presented materials after reading or hearing it, of
imagining reading or listening to a text, or of imagining agreeing or disagreeing while reading or
listening to a text (see, e.g., 61, 62, 63, 64).
3.1 Single-word studies
Manipulations of lexical emotional contents
Following up on the early Cacioppo and Petty research (54), as well as on work that had observed
increased lip EMG to words when evaluated on valence (46, 47), Larsen, Norris and Cacioppo (17) were
the first to directly examine word-elicited positive and negative emotion by means of EMG.
Participants read a series of words that spanned a preestablished broad unpleasant-pleasant valence
rating continuum, while EMG was recorded over the corrugator and the zygomaticus muscles. The
same EMG-participants also saw a series of pictures and heard a series of sounds (e.g., animal noises,
alarms, laughter) and after each stimulus, rated how positive and negative they felt about it. When
average EMG responses were plotted against the (ranked) average valence ratings provided by
participants, a very important result emerged: for pictures, sounds, and words, corrugator EMG
tracked subjective stimulus valence in a virtually linear way, with stimuli that had been rated more
zygomaticus EMG-response was more complex: for pictures and sounds, it tracked subjective stimulus
valence in a more U-shaped way (elevated zygomaticus activity for positive stimuli, but also for very
negative stimuli), whereas for words, the zygomaticus was not reliably sensitive to subjective valence.
These results, which were corroborated by a later reanalysis on absolute (rather than ranked) valence
ratings (65: study 4) suggested three important things: (a) corrugator EMG is a useful index of the
valence of word-induced emotion, (b) corrugator valence effects to words, although smaller, are
qualitatively similar to those obtained with pictures and sounds of concrete objects or events, and (c)
relative to the corrugator, the zygomaticus is somewhat less suitable for tracking the valence of word-
elicited emotion.
Other research also points in this direction. For example, in two recent EMG studies (66 control
experiments of studies 2 and 3), corrugator and zygomaticus activity was recorded to written single
words and pictures, focusing on negative versus neutral valence, and using two different secondary
tasks. Whereas zygomaticus activity did not change in response to valence, corrugator activity was
significantly higher to negative compared to neutral words, both when participants were asked neutral
questions about the words after each trial (study 3, e.g., did the word have three syllables? Was the
animal depicted a sheep?), and when participants were asked to rate the degree to which the same
VAN BERKUM, STRUIKSMA & 't HART
words emotionally moved them (study 2; in the latter case, the additionally measured levator labii
superioris was also sensitive to valence). Interestingly, across the two studies, picture-induced effects
displayed the same pattern of results. These findings converge with the Larsen et al. (17) results, and
also show that word-elicited emotional responding does not critically depend on the use of an
emotion-focused secondary task.
and zygomaticus results from
three facial EMG studies with
single words. Panel A:
corrugator and zygomaticus
responses to words ordered
along a valence rank
dimension (left = most
negative, right = most
positive; each dot represents
a word with a particular
valence rank; inset shows the
same data for pictures).
Panel B: corrugator and
zygomaticus responses as a
function of time to emotion
adjectives and emotion
expression verbs of positive
or negative valence. Panel C:
corrugator and zygomaticus
responses as a function of
time to names of loved ones,
compared to neutral familiar
and unfamiliar names. Note
the rapid differential EMG-
responses in Panels B and C,
and, in Panel A, the different
way in how corrugator and
zygomaticus activity tracks
stimulus valence. See text for
further explanation. Panels
A, B, and C reprinted with
permission from 17, 67, and
35 respectively.
The latter idea also received support from a study in which participants were merely asked to read
single words, without any additional task, while corrugator and zygomaticus EMG was recorded (67,
associated with negative emotions (e.g., "frowning", "annoying") elicited a rapid relative increase in
corrugator activity, as well as a rapid relative decrease in zygomaticus activity, both clearly within 750
2C), seeing the names of loved ones led to increased zygomaticus activity and reduced corrugator
activity, relative to seeing control names of unknown or (neutrally rated) familiar persons.
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
Of course, we cannot exclude that facial behaviour in these studies was in part affected by perceived
implicit task demands, an issue to which we'll return later. However, in several EMG studies with
masked stimuli, words that were acoustically hidden in other stimuli to such an extent that participants
did not consciously detect their presence also generated reliable EMG-responses, visible in frontalis
muscle EMG with sex-related words (68), as well as in corrugator (but not zygomaticus) EMG for other
types of valenced words (69; see 70, for a related observation when using written words). Such findings
cannot easily be ascribed to task demands.
As might be expected, task and stimulus parameters do sometimes influence language-induced
corrugator and zygomaticus EMG. For example, in contrast to studies where task effects did not
emerge (66 control experiments of studies 2 and 3), another study (71) did generate a task effect:
valence-dependent responses to concrete and abstract written words were obtained in corrugator and
zygomaticus EMG (as well as in orbicularis oculi and levator labii superioris EMG) when participants
were asked to judge whether the referent of each word was associated with an emotion, but not when
they were asked to judge whether the word was printed in upper- or lower-case. Using the same
emotion-focused judgement task, another study (72) revealed a native language advantage: a valence-
dependent corrugator effect only emerged to written words in one's native language (L1) and not to
words in a familiar foreign language (L2), and although a valence-dependent zygomaticus effect
emerged in both L1 and L2, the effect emerged later and ended sooner in L2. Finally, one study (73)
revealed a concreteness effect: the corrugator was sensitive to the valence of written words in the
case of concrete words, but not in the case of abstract words -- the zygomaticus did not show any
effects in this study. The source of these various task and stimulus type effects, as well as their coming
and going across studies, is as yet unknown.
An interestingly different approach is illustrated in a study where participants were asked to indicate
whether written words were positive or negative by deliberately activating the corrugator or
zygomaticus, and do so as rapidly as possible (74). Relative to participants instructed to use the
corrugator for negative valence and the zygomaticus for positive valence, participants that were
instructed to (incongruently) use the corrugator for positive valence and the zygomaticus for negative
valence were some 100-150 ms slower in contracting their corrugator or zygomaticus, with the
interference emerging at around 500 ms after word presentation. This rapid Stroop-like interference
effect indicates that participants simply cannot stop words from automatically and rapidly activating
valence-congruent facial muscles. Also, when the participants of a follow-up study were simply asked
to contract one particular muscle as soon as a word appeared on the screen, regardless of valence
(e.g., always the zygomaticus in the first half of the experiment, and always the corrugator in the
second), the corrugator muscle was again contracted more slowly to positive than to negative words
(no such congruency effect was obtained for the zygomaticus). In all, and consistent with the masked
word results, this suggests that words can involuntarily trigger valence-congruent activation of the
corrugator and, in one study, the zygomaticus. In related written word research (75), a valence-
dependent EMG response did not only emerge much earlier in the corrugator than in the zygomaticus,
but this early response was also very difficult to suppress, again testifying to a degree of rapid
automaticity.
Manipulations of prosodic emotional contents
VAN BERKUM, STRUIKSMA & 't HART
All studies discussed so far focused on the impact of word meaning. However, words can also be
spoken with a particular emotional prosody. In an EMG-study that explored the impact of this (76),
participants listened to the single word "Sarah" spoken with an angry, neutral, or mildly positive
prosody, while EMG over the corrugator and the orbicularis oculi was measured to obtain an index of
negative and positive emotion respectively. Compared to angry as well as neutral prosody, positive
prosody led to reduced corrugator EMG. Positive prosody also reliably increased orbicularis oculi EMG
relative to neutral prosody, but unexpectedly, angry prosody did so too. So, although clear effects of
emotional prosody could be observed in corrugator EMG, the orbicularis oculi results are complex. We
return to prosody when examining EMG-research with phrases and sentences.
In all, facial EMG research with single spoken or written words suggests that word-elicited emotional
effects can emerge involuntarily and very rapidly, in both input modalities, and in ways that are
consistent with the idea that we communicate our affective response to the world around us by means
of such things as a frown, or a smile. Of course, although single-word paradigms provide conceptually
interesting methodological options (e.g., masked presentation, a comparison to pictures of objects, or
other forms of precise stimulus control), everyday language use goes well beyond presenting single
words to other people. So what about the use of EMG to track emotional responses to bigger chunks
of language? In the remainder of this review, we first examine comprehension research with phrases
and sentences that are not part of a wider discourse, and then turn to research with larger discourse
stimuli.
3.2 Phrase and sentence studies
Manipulations of lexical emotional contents
In one of the first EMG studies focussing on the emotional aspects of sentence comprehension (81),
participants were asked to read negative and neutral sentences word by word as their corrugator
activity was being measured, and to make a semantic correctness judgement after each sentence. The
corrugator EMG elicited by a range of negative and neutral sentence-final verbs was larger for very
negatively rated verbs than for neutrally rated verbs, and very rapidly so (significant in the 300-600 ms
after verb onset). The negatively valenced verbs involved in this comparison also had higher arousal
ratings than the neutral verbs, but a separate comparison, low- and high-arousal words that were
matched on valence did not differ in corrugator activity. This suggests that, at least in this study, the
corrugator effects of subjective valence are not due to a subjective arousal confound.
Pursuing a wider question about embodied language processing, several EMG studies followed up on
earlier single-word research (67) by placing the same written emotion expression verbs (e.g.,
EMG-studies can also examine the emotional impact of single words by measuring EMG to subsequently presented other,
non-language stimuli, for which the words serve as context. For example, there is evidence that the very rapid mimicry of
emotional facial expressions measured through facia EMG, and emerging in the signal at around 300 ms after face
presentation already, can be modulated by subliminally presented congruent or incongruent word primes (77). Emotionally
relevant words can also modulate the affective EMG responses to odors (78), as well as the EMG-measured startle blink
response to loud sound bursts (using the orbicularis oculi muscle, see 79; 80; 35).
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
"frowning", "smiling") in a minimal sentential context, again without an additional secondary task.
Focusing on the zygomaticus only, one of these studies (82) revealed higher zygomaticus activity to
sentences like "I am smiling" than to sentences like "I am frowning", already significant around 400-
600 ms after (whole) phrase presentation onset. This finding resembles the original single-word result.
A similar but delayed and somewhat weaker pattern emerged with proficient L2 speakers (83),
comparable to the attenuated EMG effects for L2-speakers when reading single words (72). In related
research (84), corrugator and zygomaticus EMG was recorded to examine the impact of minimal
emotional expression or emotional state sentences (e.g., "Mario smiles", "Mario enjoys"), in a
paradigm in which participants also had to provide a sentence likeability rating. Corrugator activity was
significantly higher to descriptions of negative states and actions than to descriptions of positive states
and actions already from 300-600 ms onwards, and zygomaticus activity was significantly higher to
descriptions of positive states and actions than to descriptions of negative states and actions from 900-
1200 ms onwards. Finally, in a study that compared 1st- and 2nd-person perspectives (85), verb
valence affected the corrugator and zygomaticus in the expected direction in 2nd-person phrases
("your fear", your joy"), but not in 1st-person phrases ("my fear", "my joy"), a puzzling result that the
authors related to the social embedding of language, and that requires further examination.
Sentences also allow for interesting other manipulations, such as negation. Two studies (82, 83)
explored the impact of negation in L1 and L2 respectively. In L1, zygomaticus activity was higher to "I
am smiling" than to "I am not smiling", from about 200-400 ms after sentence presentation onset,
revealing surprisingly rapid effects of the negation. Furthermore, this negation effect disappeared in
L2, which echoes the native language advantage in facial EMG sensitivity reported before (72) for single
words. Without going into the details here, the full pattern of results obtained in these negation
studies (82, 83) does not indicate that negation simply inhibits word-elicited muscle activity -- as also
testified by the complex pattern of EMG results obtained in another study (85), with phrases like "no
fear" or "no joy", the impact of negation seems much more complex. We return to this later in this
chapter.
Another option afforded by sentences is to manipulate the identity of the agent involved. In a study
on enjoyment over other people's misfortune (Schadenfreude) that paired sentences like "got soaked
by a taxi driving through a puddle" with pictures triggering various types of person stereotypes (86),
negative event descriptions increased zygomaticus activation only for pictures suggesting an enviable,
"cold and competent" person. Furthermore, in a study involving names of well-known ingroup or
outgroup politicians (e.g., Italian versions of "Clinton smiles" or "Trump smiles", 87) the corrugator and
zygomaticus strongly responded to positive and negative expression or state descriptions involving
ingroup politicians, but less so, and sometimes not at all, to similar expressions involving outgroup
politicians. In a related study (88: study 3), comparable results were obtained for the corrugator, with
somewhat more ambiguous findings for the zygomaticus and the orbicularis oculi. Finally, in a study
where participants read sentences like “Mark is furious when <reason>” or “Mark is happy when
<reason>” without an additional task (89: study 3), corrugator activity increased at "furious" compared
to "happy" when a previous picture (and accompanying verbal label) had characterized Mark as a good
person, but not when it had characterized him as a bad person. In line with the negation and
perspective studies discussed before, all this suggests that facial EMG is sensitive to much more than
just local word meaning, and that combinatorial meaning and affiliation-based evaluation matter as
well.
VAN BERKUM, STRUIKSMA & 't HART
Manipulations of prosodic emotional contents
Three spoken-sentence studies have used corrugator and zygomaticus EMG to explore the impact of
affective prosody. In one study (90), participants were asked to rate the 'smiliness' of sentences whose
prosody had been digitally modified to convey the speaker was smiling, neutral, or 'unsmiling', while
their EMG was being measured. From about about 800 ms after speech onset, corrugator activity was
significantly higher to 'un-smiling' prosody than to smiling prosody, and from about about 1100 ms,
zygomaticus activity was higher to smiling prosody than to 'un-smiling' prosody. In a study on
emotional singing (91), corrugator and zygomaticus EMG was recorded as musically trained
participants listened to particular sentences (e.g., "Grass is green in summertime") sung with a happy,
sad, or neutral intention. A composite EMG measure revealed emotion-congruent responses in facial
EMG as sung sentences were heard. In the third study (92: study 1), participants saw a fearful or happy
face while hearing a neutral prepositional phrase spoken with a fearful or happy prosody. Relative to
adding a happy voice, adding a fearful voice to a face increased corrugator activity when the face was
a fearful one, but not when it was a happy one. Also, relative to adding a fearful voice, adding a happy
voice to a face increased zygomaticus activity when the face was a happy one, but not when it was a
fearful one. This suggests that when the additional emotional prosody channel is consistent with the
facial expression shown, the corrugator and zygomaticus can faithfully track the added affective load.
Manipulations of processing complexity
In addition to exploring the impact of emotional contents, facial EMG can also be used to study the
potentially affective impact of processing complexity, emerging when sentences stop making sense or
express very implausible meanings. In one of the studies already reviewed (81), semantic anomalies of
the type typically explored in N400-research did not increase corrugator activity. However, in a study
on the corrugator-indexed affective consequences of inconsistency (93), such anomalies did increase
corrugator activity, in the 1000-2000 ms after critical word onset. Furthermore, in research on the
corrugator, zygomaticus and frontalis effects of surprise (94), surprising trivia statements increased
corrugator activity in a 0-6000 ms latency range, relative to unsurprising controls. Also, a fluency-
oriented study of corrugator, zygomaticus and frontalis activity to semantically coherent or incoherent
word triplets (95) showed that, compared to coherent triplets like salt - deep - foam (all sea-related),
viewing incoherent triplets like dream - ball - book led to significantly higher corrugator and frontalis
activity, as well as lower zygomaticus activity,st in the 1500-3000 ms after critical word onset. These
various corrugator effects perhaps all reflect non-emotional effort (49; see also 42). However, because
processing dysfluency signals a problem, these effects might also reflect negative emotional valence.
In all, and in line with single-word research, EMG-studies with spoken or written sentences reveal
consistent and very rapid valence-dependent facial EMG responses, with the earliest reported
significant effects emerging already around half a second after the relevant contents has been
presented. Also, facial EMG effects induced by words such as "smile" or "frown" can be modulated by
the specific sentential context, as in the case of negation, perspective, and character properties.
Although such modulation has sometimes led to complex data patterns, particularly in the case of
negation, it does reveal that there is more to language-driven facial EMG than just embodied local
word meaning. We will see similar things as we examine, in the next section, the extant facial EMG
research with multi-sentence written and spoken discourse.
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
3.3 Discourse studies
In addition to the pioneering Cacioppo and Petty study (54) on pro- and counter-attitudinal texts (see
section 2.2), our literature search generated three clusters of EMG-studies involving multi-sentence
discourse: studies involving moral and other social transgressions, media-psychological questions, or
various other topics.
Moral and other social norm manipulations
Morality is a rich source of human emotion, and several research teams have examined facial EMG
responses to morally loaded narratives. In a pioneering study (96) on the corrugator effects of social
unexpectedness, participants read stories that first induced a strong positive or negative trait inference
for some character, and then continued with a word-by-word description of trait-consistent or -
inconsistent positive or negative character behavior. Relative to descriptions of good behavior,
descriptions of bad behavior already led to significantly higher corrugator activity in the 100-300 ms
after the critical word, with an additional corrugator activity increase if the bad behavior was trait-
inconsistent. The zygomaticus data were deemed suspect, and therefore not analyzed.
In a study on the impact of social norm transgressions in stories (97), EMG activity was recorded in the
corrugator and the levator labii (with zygomaticus recording to control for smiling artefacts only).
Participants read texts that did or did not involve a social norm violation (e.g., refusing to shake an
extended hand, using somebody else's toothbrush). Relative to neutral ones, stories with social norm
transgressions elicited stronger activity in the levator labii, but not in the corrugator. The absence of a
corrugator effect might be due to the fact that, in contrast to the preceding study (96) as well as the
studies discussed below, this study imposed multiple secondary tasks (subsequent imagery, and three
ratings), a possibly effortful situation that could have driven the corrugator to its upper activity range
throughout the study. Also different from other transgression studies, EMG was averaged over 60
seconds of text reading, much of which did not involve critically different descriptions of people and
events -- this can lead to phasic effects being 'washed out'.
In the above research, participants knew they would be required to recall or rate the stories after
narrative descriptions of bad character behavior (e.g., "Mark accelerates through the puddle on
purpose to create a big splash and soak the pedestrian") consistently increased corrugator activity
within a second after presentation, relative to descriptions of good character behavior (e.g., "Mark
slows down to avoid the puddle, making sure he doesn't soak the pedestrian"). Furthermore, when
bad or good events subsequently befell the same character, corrugator activity rapidly increased at
negative state descriptions (e.g., Mark is frustrated because..." followed by a reason) compared to
positive state descriptions (e.g., Mark is happy because...") in cases where the character had displayed
good behavior before, but not when he or she had displayed morally bad behavior. This apparent
insensitivity to the state of bad characters was observed in both studies, and also replicated in a third,
sentential study (89: study 3, see section 3.2). In line with the results of 96 as well as the other sentence
work discussed before (86, 87, 88: study 3), this character-dependent corrugator effect reveals that
verbal expressions with "frustrated" or "happy" do not merely drive corrugator activity as a function
of word-induced embodied language processing -- an issue to which we return in section 4.
VAN BERKUM, STRUIKSMA & 't HART
stories in which the main character first did something moral or immoral and subsequently experienced something good or
bad; the signals reveal large corrugator increases to immoral behavior, as well as an apparent insensitivity to the subsequent
emotional state of immoral characters. Panel B: Corrugator EMG to sentences that did or did not end with a :P emoticon.
Panel C: Corrugator and zygomaticus EMG to the punchline of spoken jokes, compared to non-joke controls. Panel A is a
different rendering of the data reported in 98 (upper part) and 99 (lower part), both adapted from 89. Panels B and C
reprinted with permission from 100 and 101 respectively.
Two final transgression studies (reported in 66) compared facial EMG activity as participants read
descriptions of immoral versus moral actions embedded in wider stories. Without an additional task
that focused attention on emotion (66 study 3), the corrugator and zygomaticus did not respond to
moral transgressions However, when readers also had to rate the degree to which the story "had
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
moved them" after every trial (66 study 2), descriptions of morally objectionable actions did lead to
higher activity in the corrugator and levator labii, as well as, unexpectedly, in the zygomaticus. The
latter effect may be related to other observations that strong negative content can sometimes increase
activity of this muscle (e.g., 17). The two studies considered together illustrate the potential impact of
a secondary task, with the same materials. Note, though, that as shown by two other studies already
transgressions without a secondary task, in a replicable way. Future research will need to determine
why these various passive reading studies led to different results.
Media research manipulations
Most of the moral transgression studies were conducted with psycholinguistic questions in mind, as
such often focusing on the details of word-by-word processing. However, in the field of media
psychology, various researchers have used facial EMG to assess the emotional impact of different types
of (verbal) messages, or of the same messages conveyed through different media. For example, several
health communication studies have reported complex valence-dependent corrugator effects that vary
as a function of particular types of patient narratives (102, 103), or as a function of different styles of
veterinarian communication (104).
Other media-psychological EMG-studies have explored the processing of valenced news texts
presented in different formats (105, 106, 107, 108, 109), sometimes reporting increased corrugator
activity to negative as opposed to positive news (107, 108), as well as increased zygomaticus and/or
orbicularis oculi activity to positive as opposed to negative news (107, 105). Interestingly, in one study
with corrugator and zygomaticus recording (106), higher zygomaticus activity was elicited by positive
news in comparison to negative news when the news involved companies with a good reputation, but
when news concerned companies with a bad reputation, negative news elicited higher zygomaticus
activity, a finding that is reminiscent of various studies discussed before (86, 88: study 3, 87, and 98,
99, 89: study 3) observations that it is not (just) the text as such that elicits particular facial EMG effects.
In the same study (106), negative comments posted on-line to the abovementioned news messages
elicited a corrugator activity increase, but no zygomaticus activity change, as compared to positive on-
line comments.
Finally, in a media study on in spoken radio advertisements (110), negative, neutral and positive words
embedded in these advertisements elicited corrugator differences that faithfully tracked word valence
and emerged within 500 ms after spoken word onset. Effects also emerged in the zygomaticus, but
they did not faithfully track the valence dimension, with positive words eliciting the highest, neutral
words eliciting the lowest, and negative words leading to an intermediate level of zygomaticus activity
(cf. 17; see also 66 study 2, discourse experiment).
Other manipulations in discourse
A recent study (100) explored the combined processing of irony and emoticons by asking participants
to read texts with a final sentence that praised or criticized somebody in a literal or ironic way (the
latter derivable from context), and that was or was not accompanied by a tongue-face emoticon (:P).
The irony-induced EMG-effects were complex, and require conceptual replication as well as somewhat
VAN BERKUM, STRUIKSMA & 't HART
more detailed interpretation than provided in the paper. However, the simple presence of an emoticon
corrugator), possibly as a sign of higher enjoyment in emoticon-containing communication, or,
alternatively, some form of affective resonance with the communicator stance being signaled.
Two studies used facial EMG to explore the emotional impact of jokes. In the first study (101),
participants heard jokes which they subsequently rated on funniness. Relative to the final words of
non-joke control stories mixed with the jokes in the same session, joke-final 'punch line' words very
rapidly increased zygomaticus activity, and reduced corrugator activity, with clear effects emerging
zygomaticus, orbicularis oculi and frontalis EMG was measured as people heard and rated jokes told
by speakers of the same or different political affiliation. Joke-elicited EMG was not sensitive to speaker
status during the joke, but within 4-5 seconds after the joke had ended, the zygomaticus and
orbicularis oculi were more activated if the joke had been told by an ingroup speaker than by an
outgroup speaker, and activity in these smiling-related muscles correlated strongly with the funniness
ratings.
One study used corrugator EMG to explore the affective consequences of speech overlap in
conversation (113: chapter 5). Participants overheard synthetic spoken dialogues between a man and
a woman that sounded like muffled conversation coming through a room wall. Specific words could
not be identified, but people could hear who was speaking as well as his or her specific intonation and
rhythm. After each mini-dialogue, participants were asked to rate the degree of affiliation heard in the
conversation. Speakers whose response overlapped with the previous speaker were not only rated as
less affiliative than those whose 'waited their turn', but their speech also reliably increased corrugator
activity. Whether this corrugator effect reflects increased listening effort for more complex stimuli,
negative emotion to less fluent processing, or negative emotion to overheard interruptions remains to
be explored. Also, whereas speakers whose speech rhythm was 'out of sync' also elicited lower
affiliation ratings, this did not increase corrugator EMG.
Three remaining studies explore other aspects of language comprehension. In a study on the
emotional impact of spoken poetry (114), those fragments of poetry that evoked chills or goosebumps
also strongly and consistently increased corrugator (but not zygomaticus) activity, possibly reflecting
the 'bittersweet' mixed-emotion state of being moved. In a study on the aesthetic effects of text layout
(115), poorer typographic designs led to elevated corrugator activity, but not to effects in zygomaticus
or orbicularis oculi activity. Finally, in a study where the corrugator, zygomaticus, and depressor anguli
oris were recorded to explore the facial correlates of confusion during story reading (116), corrugator
and depressor anguli oris EMG contained information that better predicted confusion than self-reports
did, testifying to the fact that EMG can pick up on responses that are not consciously accessible.
Strikingly, follow-up research (111) in which the same materials were played to two vegetative state (coma-like) patients
revealed qualitatively comparable, if substantially weaker, zygomaticus and corrugator responses in one of the patients.
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
3.4 Some general observations
Although the number of language- or communication-relevant EMG-experiments found, 55 in all, is
somewhat lower than we had expected, it is clear that since the pioneering Cacioppo & Petty study
published in 1979 (54), language-elicited emotion has been tracked by means of facial EMG to address
a wide variety of research questions, in psycholinguistics, media psychology, and other fields. Stepping
back from the individual studies, what general observations can we make? We discuss a few trends in
the findings as well as in the conceptual and methodological approach.
Few replications and many different types of analysis. We must begin with an important
methodological observation. Most of these studies can be seen as conceptual replications of each
other in the sense that they all demonstrate the utility of facial EMG to assess language-driven
emotion. However, there have as yet been virtually no attempts to systematically replicate specific
EMG-findings in the reviewed domain of inquiry. Also, the studies reviewed differ greatly in how the
data were analysed, in terms of baselining (width of interval, subtraction or % change), in statistical
testing (in a single latency range or a series, boundaries chosen, t-tests, simple condition by time
ANOVA's, or growth curve analysis), in whether tests compared post-stimulus levels to the pre-
stimulus baseline or to a post-stimulus control condition, and in whether raw, difference, or composite
scores were entered into statistical testing. None of this is unrepresentative of current
psychophysiological research, where there is often no single best way to analyse the data. However,
published results obtained under conditions of many experimenter degrees of freedom probably
contain a non-trivial proportion of false positives (see, e.g., 117 for the case of EEG). Therefore, as in
other domains, future EMG-work on language-induced emotion should greatly benefit from
preregistration and systematic replication.
A valence approach to emotion. In virtually all of the EMG-studies reviewed, emotion is framed in terms
of valence, with researchers characterizing their stimuli and/or the emotional responses to those
stimuli as positive or negative. In line with this framing, most studies recorded activity in the corrugator
supercilii and/or the zygomaticus major, the two facial muscles broadly perceived to be the most useful
of the two muscles (e.g., 98, 99, 82, 83), and other studies sometimes also recorded from additional
muscles deemed sensitive to positive or negative valence, such as the orbicularis oculi (involved in
smiling), or the levator labii superioris (involved in disgust). Very few studies classified their stimuli as
potentially evoking specific emotions and recorded from specific facial muscles to assess those
emotions (e.g., the zygomaticus for Schadenfreude, or the levator labii superioris for disgust, see 86).
To some extent, this may result from a dimension-oriented theoretical perspective on emotions.
However, we suspect that the predominance of corrugator and zygomaticus recording mainly reflects
the impact of influential early EMG research (such as 54, or 17), and the fact that there is simply less
knowledge, and more debate, on how to tap into specific emotions by means of EMG (cf. 118, 38).
Rapid valence effects. Of the studies that reported onset-relevant statistics, several clearly indicate
that the effects of valence can emerge in the EMG-record within only a few hundred milliseconds after
critical language input. This holds for single words (e.g., 67), for words embedded in a sentence (e.g.,
81, 84), and for words embedded in a discourse (e.g., 96). Several studies that provided time-course
information without reporting onset statistics numerically confirm that valence-dependent EMG-
VAN BERKUM, STRUIKSMA & 't HART
effects can emerge rapidly in the signal (e.g., see figures 2C and 3A, 3B and 3C). These observations
are consistent with what we know about the speed of language processing and emotional responding,
and, perhaps more interestingly, show that such rapid effects do not just emerge for depictions and
sounds of concrete objects, situations, and events out there in the world, but also for arbitrary signs
used to communicate about those things. It is too early to say if language- or other symbol-elicited
EMG effects lag behind those of non-symbolic stimulus types (see 66 control experiments of studies 2
and 3, for first observations).
Corrugator-dominated effects. Our review also indicates that although the corrugator and the
zygomaticus are the muscles most often used to assess the impact of language-driven emotion
verbal materials and tasks used here: of the 37 studies recording from both muscles, 92% reported
emotion-related effects in the corrugator, while only 68% reported such effects in the zygomaticus
(with also some unexpected effect direction reversals in the latter). The reasons for this differential
sensitivity have been discussed in detail elsewhere (e.g., 17, 42). For one, different muscle types are
involved: whereas the zygomaticus is a phasic muscle that is often inactive and that can in those cases
only increase its activity, the corrugator is a tonic muscle that is always 'on', and that can as such easily
whereas the evidence suggests that the corrugator tracks valence in a relatively monotonic way, with
increasing activity as one moves from strongly positive via neutral stimuli to strongly negative stimuli,
zygomaticus activity can be increased by both positive and very negative stimuli, relative to neutral
stimuli (66 study 2 discourse experiment, 17, 110), presumably because very negative stimuli can lead
to, for example, wry smiles, grins of embarrassment, or disgust-induced raising of the cheeks. This
non-monotonic (and non-linear, 65) behaviour of the zygomaticus need not be problematic when
contrasting positive and neutral stimuli only, but it may well get in the way when contrasting either of
those to negative stimuli.
Task effects as well as automaticity. There is some evidence that secondary tasks, imposed on
participants above and beyond the primary task of reading or listening to language, can have an impact
on language-driven emotional EMG-responses. This is not surprising, as such tasks can easily change
the focus of participants. The more important question is whether the observation of language-driven
emotional EMG-responses critically depends on the presence of a secondary task. As we have seen in
this review, the answer is no. For example, consistent and replicable EMG-effects have been observed
in studies where participants had no other task than to read (98, 99, 89: study 3), and consistent EMG-
effects have also been obtained in situations where participants were supposed to deliberately use
another muscle (74) or were not aware of the stimulus at all (68, 69). Such observations echo EMG-
results with other types of stimuli (e.g., 25, 26), and testify to a very important property of facial EMG:
the electromyographic recording of facial muscle activity does not only pick up on consciously
controlled facial expressions, but also on involuntarily generated ones.
Reading out involuntary emotion. Although usually not made explicit, the studies reviewed share an
important assumption, which is that facial EMG indexes inner emotional states that are automatically
Related, smiles may serve a number of different functions, such as displaying enjoyment, affiliative intentions, or
dominance (see 119, and references therein).
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
generated. For example, we found no studies that used facial EMG to study the deliberate generation
of a facial display as a social tool used in, e.g., conversation. As such, the current language-relevant
facial EMG research is closer to a basic emotions perspective on emotion (e.g., 120) than, say, to a
social constructionist or 'behavioural ecology' perspective on emotion (e.g., 121). We offer this
observation as a fact rather than a verdict. In the minimal social context of a laboratory study where
stimuli are presented to solitary individuals for reading or listening only, with a experimenter that is
trying to be as absent as possible, it is in fact not unlikely that the facial responses picked up on via
EMG are indexing relatively involuntary inner emotional states. And since much of human behaviour
is generated involuntarily, rather than under conscious control, such results can be highly relevant to
understanding affective language use. At the same time, and echoing a more general 'passive
comprehension' bias in psycholinguistics, language- and communication-oriented facial EMG research
has yet to begin to explore emotion in socially richer arenas of language use, where both involuntary
facial expressions and deliberate, strategic facial displays play an important role.
Tracking immediate emotion only. As might be expected from studies with a time-sensitive biosignal
measure, all of the research reviewed focused on relatively immediate emotional responses, usually
unfolding within half a second to a a few seconds after the relevant language materials. Understanding
real-time responding is important, for theoretical as well as practical reasons, and such responses may
also be correlated with later, more stable emotional evaluations (e.g., as part of attitudes). However,
there is no principled reason why facial EMG recording cannot also be used to actually probe those
more stable emotional results (e.g., via the delayed presentation of single 'probe' words, or phrases,
that relate to an attitude object). As with real-time data, the advantage would be that our knowledge
of emotional impact is not limited to what participants can report in an attitude or valence rating
survey, but is assessed in a broader fashion.
Complexity in what drives emotion. The EMG-evidence reviewed suggests that language can very
rapidly elicit emotion during comprehension, in at least partly automatic ways. However, the triggers
for such emotion are complex. For convenience, we have organized our review in terms of familiar
chunks of language structure: a single isolated word, a single isolated phrase or sentence, and a multi-
sentence discourse. But merely concluding that all three can elicit emotion is rather superficial. After
all, we know from psycholinguistics and pragmatics that language comprehension is a very complex
process involving the (near-)simultaneous integration of many different features, at various levels of
analysis. Also, emotional state or emotional expression descriptions (e.g., "happiness", "frowning")
probably elicit emotion in a different way from how other words (e.g., "weekend", "dying", or the
name of a loved one) elicit emotion. Although the various facial EMG-studies discussed all have their
own local theoretical context, what is currently missing is a coherent theoretical framework that makes
explicit the many fundamentally different ways in which language can give rise to emotional facial
expressions during comprehension. In the next section, we provide such a theory.
4. T ALC
HE F MODEL
The model we present is a version of the Affective Language Comprehension or ALC model (12, 13),
slightly modified and extended to better conceptualize the potential sources of facial EMG effects
VAN BERKUM, STRUIKSMA & 't HART
psycholinguistic and pragmatic ideas on what is needed to comprehend language (e.g., 122, 123) with
knowledge of how emotion and its facial expression can be triggered, automatically or deliberately,
without or with conscious awareness. In the model, we draw various distinctions that we consider to
be critical to understanding language-driven facial EMG-effects. We first discuss these distinction
(sections 4.1 to 4.3), and then briefly conceptualize some of the empirical EMG-findings in terms of
the model (section 4.4).
this may also hold for various evaluations drawn on the right of them (arrows omitted for simplicity). Potential facial feedback
loops are not explicitly marked. Further thoughts can be triggered rapidly by representations generated by each of the other
cognitive components of language comprehension. S = impact of emotion simulation; M = impact of emotional mimicry; E =
impact of emotion-based evaluation; O = impact of other factors. See text for further explanation.
4.1 Language comprehension at multiple levels
In everyday language use, understanding an utterance like "Mark is angry" requires processing at
several fundamentally different levels, described briefly below (see 12, 13, for details), and illustrated
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
(1) Recognizing and parsing the signs: To understand "Mark is angry", you'll need to retrieve relevant
information stored in long-term memory for the words "Mark", "is", and "angry", and combine that
information with other relevant cues (e.g., linguistic prosody, a declarative, interrogative, or
imperative sentence form) in accordance with the grammar of English. The result, sometimes referred
to as 'timeless sentence meaning', specifies that, in some to be determined context, time, and place,
an agent exists whose name is Mark and whose current state includes anger. What this means in the
current context is to be determined at another level.
(2) Modelling the situation referred to by the communicator: For successful comprehension, you also
need to work out which specific (real or fictional) "Mark" the communicator is talking about, and
somehow model this particular person as being in a state of anger now, as part of a larger model of
the situation under discussion (situation model; 124). In most instances of everyday language use, a
lot will already be in place in that model. For example, if "Mark is angry" is not the first utterance in a
text or conversation, you may already have modelled where Mark is, who else is around and what you
know about them, what has happened before, and what social norms are in place. Situation models
constructed during communication are extremely rich, and individual utterances usually only add or
modify bits of the model.
(3) Modelling the communicator, which involves at least two important aspects:
(3a) Inferring the communicator's stance: To understand "Mark is angry" in everyday language use, it
is important to model details about the communicator too. For example, when somebody utters "Mark
is angry" in a conversation, it is not just important to work out what he or she is referring to, but also
what his or her stance (feeling, attitude, orientation) on the matter is. Here, facial, vocal, and other
bodily cues matter a lot. For example, the affective prosody with which “Mark is angry” is produced
can easily betray the speaker's stance (although not necessarily what the stance is about). Inferring
stance can also be crucial when comprehending written language (e.g., a WhatsApp message, an
internet blog, an advertisement, or a political statement), where, in the absence of facial, vocal and
other bodily cues, other cues can carry the relevant information (e.g., reply speed, capital letters,
exclamation marks, emoji).
(3b) Inferring the communicator's social intention: Knowing which Mark is angry and how the speaker
feels about this is is usually not enough, though, as there is often a deeper question to be answered
(123): what is it that the communicator wants you to do, know or feel, by producing this utterance in
this particular way? Is the intention to just let you know Mark is angry, share his or her feelings over
this, or persuade you to do something? For example, does the speaker want you to share in
amazement over this fact and as such pull you closer, or does he or she want to tell you that you
messed up? Answering these questions is not just important when conducting a spoken or texted
conversation, but also in such cases like overhearing a conversation or listening to a lecture, as well as,
for example, when reading an advertisement, political statement, or blog.
(4) Further thinking: Finally, there are countless things that the communicator did not intend to convey
but that you will nevertheless rapidly infer from the signs, the situation model, and/or the
communicator's stance and social intention, consciously or unconsciously. For example, the use of
overly complex words ("Mark is antagonized") can signal professional deformation or a disregard for
VAN BERKUM, STRUIKSMA & 't HART
the addressee, the situation referred to can reveal a misanalysis on the part of the communicator, and,
say, a contemptuous tone of voice and social intention to gossip can betray other things, such as that
you're talking to an arrogant person, or to one that is compulsively trying to speak badly about people.
Although perhaps not strictly part of language comprehension, these further thoughts can emerge
rapidly, as you are reading or listening to the utterance at hand.
4.2 Emotion-based evaluation
The second critical part of the fALC model is the assumption that all of these various representations
retrieved or constructed during language comprehension can trigger emotion, as internal stimuli
appraised as relevant to the concerns of the comprehender. For example, at the sign level, you may
have an unconscious positive or negative response to the name "Mark", not because what it refers to
now, but simply because the name itself has a negative connotation for you, for whatever other
conscious or unconscious reason. At the situation model level, learning that this particular Mark is
angry can trigger all sorts of emotional responses, depending, in part, on your relationship to him
(partner, child, friend, competitor, etc.). At the stance level, hearing compassion, contempt, anger or
glee in a speaker's voice may elicit all sorts of emotion, either with or without you being aware of it.
At the social intention level, inferring that the speaker is saying things about Mark to get closer to you
may flatter or worry you. And as for further thinking, realizing that the speaker is kind, or not such a
good friend after all, can also elicit rapid emotion. The various different strong or subtle emotional
responses elicited as you are evaluating the utterance at all these levels can all express themselves in
such things as corrugator or zygomaticus activity, and presumably do so simultaneously.
In addition, and in line with research on fluency (e.g., 125), the model captures the assumption that
the ease or difficulty of processing can elicit evaluative emotion during language comprehension,
conceptually -- but not necessarily empirically -- independent of what that processing is about. Think
about negative emotion to words that are hard to pronounce, to sentences that are hard to parse, or
to texts that are poorly composed or hard to read or hear (e.g. because of poor typography or too
much background noise).
4.3 Emotion as simulation
In the above routes from language to emotion, the comprehender's emotion systems are used for their
original purpose, evaluation, in ways that are comparable to how directly perceived objects, situations
and events, as well as the fluency of one's own actions, can elicit emotion. However, language is special
because it can also refer to emotional states and expressions and can recruit the comprehender's
emotion systems as part of doing so. Embodied language processing studies have indicated that
reading or hearing a word can lead to a simulation of concrete experiences with what the word denotes
or refers to, i.e. the neural (and sometimes also bodily) re-instantiation of relevant perceptual, motor,
and other experience-induced processes and states associated with what the concept or phrasal
combination of concepts is about (126, 127, 128, 129, 130, 124). For example, reading action words
like ‘kick’ or ‘pick’ leads to activation of the motor cortex involved in actually realizing the described
movements (131, 132), and reading phrases such as “he saw an eagle in the sky” leads to a perceptual
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
simulation of the described situation (133, 134). In line with these observations in other domains, the
corrugator and zygomaticus EMG-effects induced by words such as "angry" or "I am smiling" are
usually interpreted in terms of emotion simulation, rather than as reflecting emotion-based evaluation
(see, e.g. 67, 82, 87).
According to the fALC model, emotion simulation -- the use of one's own emotion systems to construct
mental representations of emotion or components thereof -- can in principle occur in at least three
different components of language comprehension. First, when reading "Mark is angry", people may
simulate anger as part of retrieving stored information about the word "angry" from the mental lexicon
-- to the extent that such retrieval-based simulation recruits facial muscle motor control systems (along
similar lines as in, e.g., the abovementioned Pülvermüller et al. study), the resulting neural activity
could lead to a bit of frowning. Second, comprehenders may simulate anger as part of imagining, i.e.,
constructing a vivid representation of an angry Mark in their model of the situation being talked about
-- to the extent that such situation model simulation recruits facial muscle motor control systems, this
could again lead to a bit of frowning. Third, in cases where a communicative act contains partial or
indirect cues to strong communicator stance, emotion simulation may also occur as part of modelling
the latter. For example, when a text message from a familiar person suggests strong emotion via its
'tone' (i.e., via choice of words or typography), readers may imagine the plausibly associated facial or
prosodic expressions as part of their model of the communicator's current stance. Also, when hearing
a familiar person smile over the phone, listeners may use their emotion systems to model other, e.g.,
visual, aspects of the communicator's stance. Whether any of this actually occurs in any particular
situation is an empirical issue, of course (see 67, and other embodied language processing research
for relevant evidence). Critically, however, and illustrated by the fact that people can enjoy somebody
else's negative emotion (as in Schadenfreude), simulating emotion as part of the retrieval of a lexical
concept from long-term memory and/or as part of modelling the affective state of protagonists or
communicators is conceptually very different from using one's emotion system to evaluate things. The
to facial motor systems activation, and, ultimately, EMG-recorded facial muscle activity.
4.4 Emotional mimicry and other factors
Apart from making explicit multiple levels of language comprehension and the difference between
evaluation and simulation, the fALC model makes two additional assumptions. One is that when
comprehenders vividly imagine what, e.g., an angry Mark looks like, or what the communicator's
emotional expression might be when he or she is not directly seen, it is theoretically possible that they
are also involuntarily 'contaminated' by the resulting mental image, in the same way in which actually
seeing a person with a strong facial expression can cause observers to automatically mimic that
expression. The idea of mimicking facial expressions that we have ourselves imagined may sound far-
fetched, but may become somewhat more intuitive when we consider the impact of, say, "Trump
smiled broadly", "Leonardo di Caprio looked really concerned", or similar sentences involving friends
or loved ones. Importantly, the idea follows naturally from two well-established facts. First, there is
considerable evidence that people partly involuntarily mimic visible emotional expressions of other
people (see 135 and 136, for review; see also 42), and such visual mimicry must involve representations
in the visual perception system (e.g., areas V1 and V2). The second fact is that the same visual
VAN BERKUM, STRUIKSMA & 't HART
perception system is thought to be recruited when people imagine, rather than perceive, a visual scene
(e.g., 137, 138). These two facts together open the door to an interesting hypothesis, which is that
people may involuntarily mimic facial expressions that they themselves have constructed as part of
making sense of language (see 87 for a mimicry-based interpretation of language-driven facial EMG
effects). This self-cued facial mimicry hypothesis is not easy to test, and perhaps impossible to
experimentally separate from the language-driven emotion simulation that is needed to bootstrap the
process. Nevertheless, in the absence of evidence against the idea, we think it is important to include
The second additional assumption is that many non-emotional other factors can potentially act as
systematic confounds or sources of noise, such as the increased muscle tension induced by physical
effort (48), facial muscle activity in the lower half of the face during covert articulation (46, 47), the
selective relaxation or activation of specific facial muscles to improve auditory or visual perception, or
the effort-induced steady increase in corrugator activity during an experimental session (see 42 for
review). The boundary between emotional and non-emotional EMG effects will not always be clear, as
in the case of increased corrugator activity associated with an error (139) -- is that part of an emotional
response, or is it a conceptually separate index of orienting and effort? However, many clearly non-
of potential confounds and sources of noise.
4.5 Using the model
The core idea of the fALC model is that language-induced facial EMG-responses can arise for a wide
variety of interestingly different reasons. As such, the model can provide a new perspective on the
studies reviewed. For example, it points to the possibility that facial EMG-effects induced by negation
or person (cf. "I am not smiling", "you are smiling") need not just involve the contextual inhibition or
tuning of lexical-conceptual emotion simulation (as supposed in 82, and 85), but can also involve
simulation as part of situation model construction, and, in sufficiently contextualized cases, perhaps
even a subtle evaluative response to the situation modelled. Situation model evaluation is clearly also
a relevant possibility to consider for facial EMG effects involving whether an in- or outgroup member
"got soaked by a taxi" (86), whether an in- or outgroup member "... is smiling" (88: study 3, 87),
whether a good or bad Mark "... is angry" (98, 99, 89: study 3), or whether negative news concerns a
bad-reputation company or a good-reputation one (106). Furthermore, to the extent that the verbally
described situation is rich enough to allow people to vividly imagine the emotional expression of a
Although regular perception-driven facial mimicry is itself also often described as a form of simulation (e.g.,
119), such perceptual simulation is conceptually different from simulation involved in language-driven imagery -
- the notion explored here is that the latter may give rise to the former. Also, although one can argue that
emotional facial mimicry is a low-level form of emotional evaluation (the 'emotional contagion' or 'affective
resonance' part of empathy; 140), there is some debate as to whether such mimicry is truly mediated by 'deep'
emotional states or on a par with other, non-emotional behavioral mimicry (such as unconsciously adopting
somebody else's sitting posture or walking rhythm). In the fALC model, therefore, we currently prefer to separate
mimicry from evaluation.
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
protagonist (e.g., because it involves the facial expression of famous politicians, 87), we should
consider the possibility that the EMG effects obtained might reflect facial mimicry as well.
Over and above situation-level representations, the model reminds us of the fact that some of the
facial EMG-effects reported in the literature, such as the emoji findings reported in 100, as well as the
affective prosody findings reported in 90, 76, and 92 can to an unknown extent reflect emotion that is
triggered as a function of the communicator's inferred stance or social intention -- which also raises
further questions about their specific proximal cause (emerging via evaluation, simulation, and/or,
perhaps, mimicry). The model makes explicit that the facial EMG-effect reported to unexpected or
incoherent materials (115, 116, 93, 94) might reflect an evaluative response to disfluent processing.
And the model can for example help us be more specific about what particular secondary tasks can
bring about (e.g., a "sentence likeability" rating task probably leads people to rate particular situations,
not sentences; cf. 84).
The fALC model can help think about past results but also about future experiments. Consider the
replicated in another paradigm (89: study 3). The most interesting finding is that whereas "Mark is
frustrated" increased corrugator activity relative to "Mark is happy" when Mark had displayed morally
good behavior before, it did not increase corrugator activity when Mark had displayed morally bad
behavior. By itself, this already shows that simple emotion simulation at the conceptual or situation-
model level cannot account for the findings. The fALC model, however, suggests a number of
alternative possible accounts. One is that the apparent non-responsiveness of readers to the emotions
of a bad character reflects the simultaneous use of their emotion systems to simulate and evaluate
the events described, with the two forces counteracting for bad characters. For example, reading
about a “frustrated” bad character might increase frowning because of simulation, while
simultaneously decreasing frowning because of some fairness-based evaluation (cf. Schadenfreude).
A second possibility is that readers are somehow less inclined to simulate the state of a bad character,
an idea that fits with the emerging realization that language-driven simulation is not an all-or-none
concept but depends on all kinds of contextual factors (128, 141, 142, 132, 129, 130). A third possibility
is that readers are less likely to automatically mimic the simulated emotions of bad characters, an idea
that is consistent with reports that facial mimicry can be diminished by disliking or disidentifying with
the other person, by the desire to disaffiliate from somebody, and, related, by classifying him or her
as an outgroup member (135). And fourth, it could be that readers do not evaluate -- i.e., care about -
- what happens to bad characters at all. Some of these hypotheses might be more plausible than
others, and some are very hard to test. For now, however, the point is that the fALC model can help in
systematically defining the options, and as such facilitate the search for what is actually happening in
this particular situation.
5. C
HALLENGES AND OPPORTUNITIES
We have reviewed facial EMG-research on affective language comprehension and proposed a model
of how language can lead to changes in facial muscle activity. What are the challenges and
opportunities ahead of us? One opportunity is obvious: facial EMG can help track conscious and
unconscious emotional responses to language as it unfolds. That is good news, because facial EMG
VAN BERKUM, STRUIKSMA & 't HART
extends the methods repertoire for tracking emotion in unique ways. For example, relative to EEG,
MEG or fMRI, it can tap into somebody's emotional state without requiring lengthy electrode
attachments sessions (EEG), expensive and relatively inaccessible equipment (MEG, fMRI), and the
complex analyses that comes with high-dimensional data (all three). Also, whereas skin conductance
provides information about undirected arousal only, with fairly low temporal resolution, facial EMG
provides directional (valence) as well as intensity information, with a higher temporal resolution. And
because EMG can detect facial muscle activity even when this is too weak to lead to observable skin
movement (53), the measure is more sensitive than automated facial image analysis (such as Face
Reader), an important property when working with language stimuli in a laboratory setting (42).
Whether facial EMG should be the method of choice naturally depends on the research question: if
the latter is about feeling only, for example, self-report is the way to go. However, in affective language
comprehension research, facial EMG clearly occupies a unique and attractive niche.
We see several important challenges for facial EMG research in this domain. First, although the fALC
challenge to facial EMG research in psycholinguistics and other fields: we're dealing with a very
complex situation here. When exploring single written words presented without any additional context
in the lab, this complexity is still relatively restricted. However, as soon as we enrich the materials or
the context, many other options open up. For example, as soon as language is spoken, stance-elicited
emotion can enter the picture, even with a single word. Also, as soon as sentences really describe
something, situation model simulation and evaluation enter the picture, and as we move to bigger
chunks of discourse, such as private or professional conversations, advertisements, government
messages, or blogs (to name just a few), emotions elicited by inferred social intentions enter the
picture too. Although the complexity made explicit in our model is perhaps a little disheartening, it
also points to exciting new questions that as yet have not been tackled. Research on those questions
might confirm some of the predictions of the fALC model, but -- just as attractive -- might also eliminate
some of the proposed routes from language to facially expressed emotion.
The second challenge involves using facial EMG research to assess specific emotions. Although
emotion and emotional expression is part and parcel of our biological heritage, there has been much
debate over how reliable the information provided by facial, vocal and other bodily cues to emotion
actually is (see, e.g., 38, 118, 39). What that debate has made very clear is that, in contrast to what
early basic-emotions research suggested (e.g., 143), particular emotions do not map onto particular
facial, vocal, or other bodily expression in a simple one-to-one fashion, or one that is completely
invariant across occasions. The various aspects of a particular emotion are probabilistically linked (see
144, 39), which means that particular appraisals and motivational, physiological, cognitive and
behavioral responses typically but not invariably cohere with each other. For example, although anger
tends to increase the probability of frowning, to such an extent that the cue becomes an informative
cue, not every instance of anger does so. Because our body is used for many things besides expressing
emotions, facial, vocal, and other bodily cues are often ambiguous, an ambiguity that we can only
resolve by taking the context into account too (e.g., what is the situation, who is involved, what cultural
constraints are in place?).
Now, emotional expressions are reliable and specific enough to do considerable work in our daily lives.
We routinely pay attention to how people look, sound, and move to infer specific emotional states and
FACIAL EMG AND AFFECTIVE LANGUAGE COMPREHENSION
the associated intentions -- all that would not happen if facial, vocal, and other embodied emotional
cues are fundamentally unreliable, or only signal valence. However, the problem is that much of the
rich context that allows us to effectively disambiguate facial signals in real life is not present in the
average laboratory experiment. One way to address this problem is to resort to high-dimensional
signal measurement (38), so that the simultaneous recording of many different facial muscles,
autonomous nervous system parameters, and other potentially relevant bodily cues can provide
additional constraints. The other, and often more feasible, option is to design experiments in which
the materials, tasks, and other details of the setting constrain interpretation to such an extent that the
activity of even just one or two facial muscles becomes sufficiently unambiguous and informative.
The final challenge also relates to the multi-interpretability of facial expressions. It has been argued
that emotional expressions conveyed through the face, the voice, and the rest of the body provide a
more 'honest' indication of emotion than what people say, because such embodied multi-channel
expressions often arise involuntarily, are not easy to suppress, and, as rich and complex patterns, are
relatively hard to generate deliberately (145, 39, 40). At the same time, however, real-life experience
shows that people can deliberately generate emotional expressions that we take at face value. Actors,
for instance, do so routinely, and all parents of young children will recognize the utility of being able
to make an emotional facial expression that is at odds with one's inner state. Moving beyond such
anecdotal observations, careful research has shown, for instance, that some facial expressions are only
produced when a particular audience is around (e.g., 146, 147, 148), and that, more generally, we all
use facial expressions as social tools (121). Related, there is discussion as to what 'honest' really means
(see, e.g., 119). All this testifies to the complexity of using facial EMG when studying language
comprehension. As long as nobody else is around, and the experiment is designed such that
comprehenders do not feel that making certain facial expressions is a desirable thing to do, facial EMG
may predominantly tap into 'inner' emotional states, generated for one or more of the reasons laid
out in the fALC model. However, all that may change when an interlocutor or other audience enters
the arena, because in such more social situations, facial expressions can and often will be used
strategically, under the expressers’s full control and deliberate designed to have a particular impact
on one’s current interlocutor or audience (121, 146). This means that as soon as we abandon the classic
solitary-comprehender paradigm to explore comprehension in a more social context, facial EMG will
most certainly not just ‘tap into inner emotional states’, but will also pick up on strategic facial displays
that, although probably often motivated by some emotion, are not themselves direct reflections of
that emotion (nor part of language comprehension proper).
These are all non-trivial challenges. However, rather than just avoiding all this complexity, it is much
more interesting to address it. For example, by increasing the richness of our verbal stimuli, we can
use facial EMG to explore the impact of stance signals and social intentions. By designing EMG-studies
that independently control emotion simulation, emotional evaluation, and perhaps mimicry, we can
try to tease those factors apart. And by manipulating the social context, we can use facial EMG to
explore the exact conditions under which language-induced emotional expression arises, as an
involuntary response to the stimulus and its context, as a deliberate social tool, or both. Language-
driven emotion is all around us, in our personal lives, and in the big world out there. Facial EMG
provides a unique tool to help us understand how such emotion comes about, as well as how we
express it to influence others.
VAN BERKUM, STRUIKSMA & 't HART
Acknowledgments
Partly supported by NWO Vici grant #277-89-001 to JvB. We thank Anita Eerland and Mirko Grimaldi for
comments on an earlier version of this chapter, the members of the UiL OTS Language & Communication
generously sharing his expertise when we embarked upon our first EMG-project. Correspondence:
j.vanberkum@uu.nl.