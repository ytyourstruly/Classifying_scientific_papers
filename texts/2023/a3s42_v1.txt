ERN ACROSS VERSIONS OF FLANKER
A Registered Report of a Two-Site Study of Variations of the Flanker Task:
ERN Experimental Effects and Data Quality
Peter E. Clayson*1, Harold A. Rocha1, Julia B. McDonald1, Scott A. Baldwin2, and
Michael J. Larson2,3
1Department of Psychology, University of South Florida, Tampa, FL
2Department of Psychology, Brigham Young University, Provo, UT
3Neuroscience Center, Brigham Young University, Provo, UT
*Corresponding author at: Department of Psychology, University of South Florida, 4202 East
Fowler Avenue, Tampa, FL, US, 33620-7200. Email: clayson@usf.edu
ERN ACROSS VERSIONS OF FLANKER 2
Abstract
Error-related negativity is a widely used measure of error monitoring, and many projects are
independently moving ERN recorded during a flanker task towards standardization,
optimization, and eventual clinical application. However, each project uses a different version of
the flanker task and tacitly assumes ERN is functionally equivalent across each version. The
routine neglect of a rigorous test of this assumption undermines efforts to integrate ERN findings
across tasks, optimize and standardize ERN assessment, and widely apply ERN in clinical trials.
The purpose of this registered report was to determine whether ERN shows similar experimental
effects (correct vs. error trials) and data quality (intraindividual variability) during three
commonly-used versions of a flanker task. ERN was recorded from 172 participants during three
versions of a flanker task across two study sites. ERN scores showed numerical differences
between tasks, raising questions about the comparability of ERN findings across studies and
tasks. Although ERN scores from all three versions of the flanker task yielded high data quality
and internal consistency, one version did outperform the other two in terms of the size of
experimental effects and the data quality. Exploratory analyses of the error positivity (Pe)
provided tentative support for the other two versions of the task over the paradigm that appeared
optimal for ERN. The present study provides a roadmap for how to statistically compare
psychometric characteristics of ERP scores across paradigms and gives preliminary
recommendations for flanker tasks to use for ERN- and Pe-focused studies.
Keywords: event-related brain potentials (ERPs); performance monitoring; multilevel location-
scale models; intraindividual variability; ERP psychometric reliability; data quality
ERN ACROSS VERSIONS OF FLANKER 3
1. Introduction
Error monitoring is of high interest due, at least in part, to its fundamental role in a range
of cognitive domains that are relevant to everyday function. For example, neural mechanisms of
error monitoring are studied in relation to attention, working memory, learning, and decision
making, and clinical researchers study the role disruptions in these mechanisms play in
psychopathology. The error-related negativity (ERN) component of the scalp-recorded event-
related brain potential (ERP) is a commonly used measure of early error monitoring that is
relevant for cognitive control and learning (e.g., Larson & Clayson, 2011; Larson et al., 2014),
and ERN shows considerable promise in its clinical application as a neural biomarker of anxiety
and of risk for anxiety disorders (e.g., Hajcak et al., 2019). Despite considerable interest in ERN
and thirty years of work since its first observation (Falkenstein et al., 1991; Gehring et al., 1993),
paradigms for recording ERN lack standardization. The diversity of tasks used for eliciting ERN
represents a barrier to understanding its functional significance in basic and applied work
(Clayson et al., 2021e). Therefore, the aim of the current study was to determine whether ERN
recorded during three versions of a flanker taskâ€”each of which has been separately
standardized, optimized, or proposed to support normative databasesâ€”show similar
experimental effects (error vs. correct) and data quality.
The general consensus among theoretical explanations of ERN is that it indexes a
performance-monitoring process that supports adaptive adjustments in behavior (Weinberg et al.,
2015). Despite this general consensus, there remains interpretative inconsistency about between-
person and between-group differences in ERN amplitude (Clayson et al., 2021e). For example,
larger (Themanson et al., 2008) and smaller ERN (Pontifex et al., 2011) are interpreted as signs
of better cardiorespiratory fitness relating to better performance monitoring, and similar
ERN ACROSS VERSIONS OF FLANKER 4
interpretative inconsistencies are seen for ERN in depression (for meta-analyses, see Clayson et
al., 2020; Moran et al., 2017) and for ERN sex differences (e.g., Larson et al., 2011). Larger
ERNs are often viewed as â€œstrongerâ€ responses, but smaller ERNs are also viewed as â€œmore
efficientâ€ depending on the research question or sample (Clayson et al., 2021e; Larson &
Clayson, 2011). These interpretative inconsistencies might reflect a failure to understand the
functional significance of ERN.
The development of cohesive theories that can explain the disparate findings between
ERN and other variables (e.g., individual differences) are undermined by a failure to understand
how task differences might impact ERN. There is no consensus regarding how ERN data should
be recorded, processed, and analyzed, and subtle differences in analysis pipelines can alter
relationships between ERN and external variables. For example, the ERN-worry relationship
might be primarily related to ERN recorded during right-handed, but not left-handed, errors, and
ERN-worry relationship is stronger for right-handed errors to horizontally oriented stimuli than
to vertically oriented stimuli (Lin et al., 2015). The importance of which paradigm is used for
recording is readily shown in meta-analytic work. For example, the task used for recording ERN
moderates the relationship between ERN and obsessive compulsive disorder (Mathews et al.,
2012; Riesel, 2019), psychosis (Martin et al., 2018), internalizing symptoms (Pasion & Barbosa,
2019), and externalizing symptoms (Pasion & Barbosa, 2019). It is possible that disparate
findings between ERN and other variables are at least partially due to task-related factors that
impact the functional significance of ERN.
ERN reflects a manifestation of cognitive, affective, motivational, and motor processes
(Gehring et al., 2012), and the contribution of any process to an observed ERN score could be
stronger or weaker during different tasks used to elicit ERN. Speeded, forced-choice reaction
ERN ACROSS VERSIONS OF FLANKER 5
time tasks are among the most widely used for eliciting ERN (Gehring et al., 2012; Larson et al.,
2014; Olvet & Hajcak, 2008), and examples of these tasks include the Eriksen flanker task
(Eriksen & Eriksen, 1974), Stroop task (Stroop, 1935), Simon task (Simon, 1969), and Go/NoGo
task (Donders, 1969). In versions of the flanker task, participants respond to a central target
stimulus that is flanked by distractors, such as arrows or letters that can queue the same response
as the central target stimulus (e.g., congruent trial: <<<<< or HHHHH) or a different response
(e.g., incongruent trial: >><>> or SSHSS) from the central target stimulus. Flanker tasks can
vary across several features, such as the proportion of congruent to incongruent trials, the total
number of trials, participant response windows, or the duration of inter-trial intervals, but
instantiations of each version of the task are still categorized using the unitary name: flanker
task. A researcher typically selects features of the paradigm that are believed to lead to a high
number of participant errors, because some high-performing participants make relatively few
errors. The number of recorded trials is closely tied to the data quality and internal consistency of
ERN scores, but the paradigmatic features of the flanker task that lead to robust experimental
effects (correct vs. error) and high data quality and internal consistency are virtually unknown.
Task differences in psychometric internal consistency and data quality might also
contribute to inconsistent findings across studies. Internal consistency partially characterizes data
quality by providing an index of the relationship between within-person variance (i.e.,
intraindividual variability) and between-person variance (i.e., interindividual variability). Data
with high internal consistency have low within-person variance relative to between-person
variance, and high internal consistency is necessary to justify examining individual differences.
When referring to the data quality of an ERP mean, we refer to the amount of error for a
given ERP scoring procedure, which is measured by within-person, between-trial variability
ERN ACROSS VERSIONS OF FLANKER 6
(Clayson et al., 2021c; Luck et al., 2021). Although internal consistency is only partially
impacted by measurement error, studies of internal consistency can shed light on potential
factors that might impact data quality. In a meta-analysis of ERN score internal consistency, the
task used for recording moderated internal consistency estimates (Clayson, 2020). Low
psychometric internal consistency of ERP scores can lead to erroneous statistical inferences
(Clayson & Miller, 2017b) and reduced statistical power (Boudewyn et al., 2017; Kolossa &
Kopp, 2018; Loken & Gelman, 2017). Therefore, task differences in ERN psychometrics might
contribute to the observation of inconsistent between-group/between-person differences in ERN.
Taken together, there are at least three reasons to advocate the development of a
standardized paradigm for recording, processing, and analyzing ERN. Task differences might 1)
contribute to interpretive inconsistencies across studies, 2) impact the relationship between ERN
and external correlates, and 3) moderate the psychometric internal consistency and data quality
of ERN scores, which can compromise the validity of inferences. The flanker task (Eriksen &
Eriksen, 1974) is among the most widely used paradigms for recording ERN, and this is likely
due to the ERN yielding robust experimental effects (i.e., amplitude differences in error vs.
correct trials) and high estimates of internal consistency when the flanker task is used. When
ERN tasks were compared within the same sample of participants, the flanker task showed
stronger experimental effects than the Stroop task, numerically higher estimates of internal
consistency than the Stroop task or Go/NoGo task, and required numerically fewer trials to
achieve the same level of internal consistency than the Stroop or Go/NoGo tasks (Meyer et al.,
2013; Riesel et al., 2013). Therefore, the flanker task is a good candidate for examining
experimental effects in ERN and determining whether differences in task characteristics
contribute to changes in ERN.
ERN ACROSS VERSIONS OF FLANKER 7
Separate studies have implicitly advocated the flanker task for recording ERN, but each
study used a different version of the task. A version of the flanker task was proposed as a part of
a larger canon of standardized ERP tasks (Kappenman et al., 2020). A large ERN data set that
was recently published as normative used another version of the flanker task (Imburgio et al.,
2020), and at least two studies have tried to optimize this version of the flanker for internal
consistency (Klawohn et al., 2020; Sandre et al., 2020). A processing pipeline for ERN recorded
during yet another version of the flanker task was optimized for maximizing experimental effects
and data quality (Clayson et al., 2021b). Each project moved ERN towards standardization,
optimization, and eventual clinical application and therefore supported the use of the flanker task
in basic and applied work. However, two untested and critical assumptions could compromise
these efforts. First, it is unclear whether ERN is functionally equivalent across these three
versions of the flanker task, and second, it is unclear whether each flanker task produces
numerically similar ERN scores. These assumptions need to be rigorously tested to substantiate
the comparability of ERN recorded from different flanker tasks so that ERN findings from one
version of the flanker task can inform decisions about ERN from other versions.
The neglect of a rigorous test of the assumption of functional significance across versions
of the flanker task undermines efforts to integrate ERN findings across flanker tasks, optimize
and standardize ERN assessment, and widely apply ERN in clinical trials. Standardization1 is
necessary to ensure comparability of scores across datasets (e.g., controls vs. patients);
otherwise, spurious interpretations could be drawn based on comparisons because deviant scores
1 We are not suggesting that all studies must use a certain or any version of the flanker task when
investigating ERN. It is reasonable to adapt a given task to the research question. Our hope is to
determine whether task differences that might be overlooked when using an umbrella term for a
paradigm (flanker task), which could undermine efforts to interpret ERN findings within and
across studies and lead to spurious conclusions.
ERN ACROSS VERSIONS OF FLANKER 8
could be due to differences in administration of tests (Bigler & Dodrill, 1997)â€”i.e., differences
could be due to the use of different versions of the flanker task. The present study will determine
whether ERNs are functionally equivalent across three versions of the flanker task. Although the
present study could identify a version of the flanker that outperforms the others based on
experimental effects or data quality, this outcome should not be interpreted as an unequivocal
endorsement of a single task, but rather such an outcome would highlight the need to consider
task differences in theoretical explanations of ERN and when integrating ERN findings across
studies and across healthy and clinical samples.
If different versions of the flanker task elicit a functionally similar ERN, it would be
expected that participants would show similar ERN means and variances across each version of
the task. Otherwise, numerical differences raise questions about the comparability of
psychometric reliability and validity of the measurements across tasks; such measurement issues
can lead to spurious and misleading inferences, particularly for inferences about the functional
significance of ERN. For example, if three different assessments of depression symptoms were
administered to a group of participants and each assessment yielded different scores, the
psychometric characteristics of scores from those three different assessments would be called
into question. It is possible that score variability could be due to poor psychometric internal
consistency or possibly differences in content validity. In fact, these exact psychometric issues
were recently highlighted in a review of depression measurement, which emphasized that
measurement issues are impeding progress in treating depression (Fried et al., 2022). The
fundamental psychometric principles relevant to measures of depression from this example are
just as relevant for ERN and other ERPs. Therefore, the present study investigated whether
ERN ACROSS VERSIONS OF FLANKER 9
measurement issues could compromise inferences based on studies of different versions of the
flanker task by examining numerical differences in means and variances of ERN scores.
The present registered report of a two-site study compared experimental effects and data
quality across the three versions of the flanker. Multilevel location-scale models were used to
simultaneously consider the differences in ERN amplitude average scores and intraindividual
variability between the three tasks. Multilevel models are well suited for analyzing ERP data,
which are often characterized by variability in trials retained for averaging following artifact
rejection (Volpert-Esmond et al., 2021). The use of multilevel models leads to more efficient
parameter estimates than the arithmetic solutions, because information is partially pooled across
participants (e.g., Gelman, 2006; Gelman et al., 2012). Multilevel location-scale models
simultaneously estimate the impact of predictors on average scores (i.e., location) and within-
person variance (i.e., scale; Walters et al., 2018; Williams et al., 2019; Williams et al., 2021).
Therefore, whether tasks show comparable mean and variance structures can be statistically
evaluated within the same model to determine whether flanker tasks show similar experimental
effects (correct- vs. error-trial amplitudes) and data quality (within-person variance). This
provides the capability to determine whether one task shows better psychometric characteristics
(i.e., data quality, internal consistency) than another. A recent examination observed notable
intraindividual variability in ERN amplitudes using location-scale models (Clayson et al., 2022),
and estimates of within-person variance from these models can be used to calculate estimates of
subject-level internal consistency (Clayson et al., 2021c).
The present two-site project supports efforts toward the standardization (Kappenman et
al., 2020), optimization (Clayson et al., 2021e), and eventual clinical application (Imburgio et al.,
2020) of ERN by determining whether ERN scores are functionally and psychometrically similar
ERN ACROSS VERSIONS OF FLANKER 10
across three versions of the flanker task. ERN scores from each task were compared based on
experimental effects (i.e., correct vs. error amplitude) and data quality (i.e., within-person
variance). We predicted that flanker tasks would show similar experimental effects and data
quality. If ERNs are numerically similar, this finding would support the functional similarity of
ERN across the three flanker tasks. However, if one version showed larger experimental effects
or higher data quality (i.e., smaller within-person variance) than the other(s), then this would
indicate that ERN across different versions of the flanker task is not functionally equivalent.
Conversely, if one version showed inferior experimental effects or data quality, this would
indicate a need for paradigm optimization or to potentially abandon further use of the task.
2. Method
2.1. Transparency and Openness
The present manuscript is a registered report; we implemented recent recommendations
for open science practices in psychophysiological research (Garrett-Ruffin et al., 2021; Paul et
al., 2021). Specifically, all study materials, including de-identified participant demographic and
summary data, that are needed for reproducing the proposed analyses and replicating the study
code, and de-identified raw electroencephalogram (EEG) data are hosted on OpenNEURO.
2.2. Participants
To improve the generalizability of the study findings, the present study was conducted
across two sites: Brigham Young University (BYU) in Provo, UT, USA, and the University of
South Florida (USF) in Tampa, FL, USA. Each site followed identical study protocols, which are
ERN ACROSS VERSIONS OF FLANKER 11
provided in the supplementary material on OSF. All participants provided written informed
consent prior to study participation, and the informed consent included a clause describing the
data that are publicly available and the related risks. Participants who did not consent to data
sharing were not allowed to participate. A total of 125 participants from BYU and 65 participants
from USF were expected to participate in the study a priori, with the goal of retaining at least
150 total participants for analysis following attrition and artifact rejection procedures (see
Experimental Task and ERP Recording and Reduction sections). These sample sizes were
selected based on projections of resource constraints (Lakens, 2021) and expectations from the
sensitivity power analyses, which are shown below (see Power Analysis section). Our sampling
approach involved accounting for data attrition. If a total of 150 participants were not successful
in making it through the data processing pipeline2 (see below), then additional participant data in
batches of 10 participants at BYU and 5 participants at USF would be collected to reach the final
goal of at least 150 participants with data included in the analyses.
Participants were recruited from undergraduate courses at each university and were
compensated with course credit. Participants were eligible to participate in the study if they were
over 18 years of age. Exclusion criteria included any self-reported psychiatric or neurologic
diagnosis, psychoactive medication use, uncorrected visual impairment, and limited fluency in
English.
2.3. Coordination Across Sites
2 Participant data were processed until the number of single trials that survived artifact rejection
could be identified. If participants were excluded based on the data-processing criteria described
in the Electrophysiological Data Recording and Reduction section, then additional participant
data would be collected until a sample size of at least 150 total participants was reached.
ERN ACROSS VERSIONS OF FLANKER 12
Each site followed the data collection protocol posted on OSF. MJL oversaw data
collection at BYU, and PEC oversaw data collection at USF. Each study site used a similarly
configured 129-channel geodesic sensor net (described below), and researchers at each site
followed the same protocol for the application of the net to minimize the likelihood of potential
site differences in scalp topography of EEG signals. Estimates of data quality and internal
consistency (including between-trial variance and error variance), the number of retained trials,
and raw ERP means will be reported separately for each site to characterize the collected data.
2.4. Experimental Task
Each participant completed all three versions of the flanker task in a single recording
session, and the order of the tasks was counterbalanced across participants. Each flanker task was
presented using the E-Prime 3.0 software (Psychology Software Tools, Pittsburgh, PA), and
these tasks are posted on OSF. Participants completed 24 practice trials prior to beginning the
experimental task to ensure adequate understanding of instructions and familiarity with each
version, and participants responded using a computer mouse during each task. Details on each
flanker task are presented below and a summary of the differences between each version of the
used to optimize data processing (Task A; Clayson et al., 2021b), to provide normative ERN
scores (Task B; Imburgio et al., 2020), or to disseminate as a standardized paradigm (Task C;
Kappenman et al., 2020).
2.4.1. Flanker Task A. This version of the flanker task followed the procedures outlined
in Clayson et al. (2021b). Each trial comprised a congruent (<<<<< or >>>>>) or an incongruent
(<<><< or >><>>) stimulus. Participants were instructed to respond as quickly and accurately as
possible with a right-hand key press. The participant responded by making a button press on the
ERN ACROSS VERSIONS OF FLANKER 13
keyboard that corresponded to the direction of the target stimulus (i.e., middle arrow; â€˜jâ€™ key for
a left response using the index finger of the right hand; â€˜kâ€™ for a right response using the middle
finger of the right hand). Flanker stimuli were presented for 100 ms prior to the onset of the
target stimulus, which remained on the screen for 600 ms. The response window was 1,600 ms.
If the participant responds after 1,600 ms, the trial was counted as an error of omission. The
inter-trial interval (ITI) varied pseudo-randomly between 800 ms, 1,000 ms, and 1,200 ms. Three
blocks of 300 trials (900 total trials) were presented; congruent and incongruent trials were
randomly presented within each block (135 congruent trials and 165 incongruent trials per
block). Participants controlled the duration of breaks in between blocks. Unlike Tasks B and C,
no performance-based feedback was presented.
2.4.2. Flanker Task B. This version of the flanker task followed procedures outlined in
Imburgio et al. (2020). Two different trial lengths were used across the three lab sites in
Imburgio et al., and the long version was chosen for the present study. Each trial comprised a
congruent (<<<<< or >>>>>) or an incongruent (<<><< or >><>>) stimulus. Participants were
instructed to respond as quickly and accurately as possible by pressing a left (index finger) or
right (middle finger) button press with their right hand. The entire stimulus was presented for
200 ms, and the response window was 2,000 ms. If the participant responded after 2,000 ms, the
trial was counted as an error of omission. The ITI varied randomly between 1,000 and 2,000 ms,
and a white fixation cross was presented during the ITI. Participants received self-paced breaks
after each block. Trial order was random for each participant. Participants completed 11 blocks
of 30 trials (330 total trials), with congruent and incongruent trials randomly and equiprobably
presented within each block (15 congruent trials and 15 incongruent trials per block).
Performance-sensitive feedback was provided during the breaks between blocks: â€œPlease try to
ERN ACROSS VERSIONS OF FLANKER 14
be more accurateâ€ was shown when performance is 75% correct or lower and â€œPlease try to
respond fasterâ€ when performance was above 90% correct; otherwise, â€œYou are doing a great
jobâ€ was presented.
2.4.3. Flanker Task C. This version of the flanker task followed the procedures outlined
in Kappenman et al. (2020). Each trial comprised a congruent (<<<<< or >>>>>) or an
incongruent (<<><< or >><>>) stimulus. Participants were instructed to respond as quickly and
accurately as possible with a right-hand button press; the index finger was used for a left
response, and the middle finger was used for a right response. The entire flanker stimulus was
presented for 200 ms, and the ITI varied randomly between 1,200 and 1,400 ms. A response will
be collected for the duration of the ITI. Ten blocks of 40 trials (400 total trials) were presented.
The distribution of congruent and incongruent trials was equal within each block (20 congruent
trials and 20 incongruent trials per block), and trials were randomly presented within each block.
Each block of trials began with the sequence: â€œReadyâ€, â€œSetâ€, and â€œGoâ€. Participants controlled
the length of the rest breaks between blocks. Performance feedback was provided at the end of a
block. When the block-wise accuracy was above 90%, â€œTry to respond a bit fasterâ€ was
presented. When the block-wise accuracy was below 80%, â€œTry to respond more accuratelyâ€ was
presented. When the block-wise accuracy was between 80% and 90%, â€œGood job!â€ was
presented.
2.5. Electrophysiological Data Recording and Reduction
EEG was recorded from 128 passive Ag/AgCl scalp sites using a 129-channel hydrocel
geodesic sensor net and Electrical Geodesics, Inc. (EGI; Eugene, OR) amplifier system (20K
nominal gain, delta-sigma sinc lowpass filter with a half-power cutoff of 2,000 Hz) at both
recording sites (BYU, USF). The sensor layout of the hydrocel net is shown in Clayson et al.
ERN ACROSS VERSIONS OF FLANKER 15
(2011). EEG was online referenced to the vertex electrode, and a ground sensor located at PCz.
EEG was digitized continuously at 500 Hz with a 24-bit analog-to-digital converter. Impedances
were maintained below 50 kï—. The raw de-identified data from BYU were transmitted to USF,
where all data were processed and analyzed as outlined below.
Data processing steps were based on the optimized pipeline for ERN recorded during a
flanker task3 (Clayson et al., 2021b). Data were processed offline using open-source MATLAB
software packages. Continuous EEG was filtered offline using a sixth-order (36 dB/oct) infinite
impulse response (IIR) Butterworth filter with half-amplitude cutoffs at .01 and 15 Hz
implemented in ERPLab v8.02 (Lopez-Calderon & Luck, 2014). Response-locked epochs were
extracted separately for each participant using a temporal window from 400 ms prior to the
participantâ€™s button press to 800 ms following a button press. Eye blinks and horizontal and
vertical saccadic eye movement were removed from the segmented waveforms using
independent components analysis (ICA) implemented in the ERP PCA Toolkit v2.95 (Dien,
2010). For the ICA procedure, epoched EEG data from all channels were processed through a
binary version of EEGLabâ€™s v2021.0 runica function called binica (Delorme & Makeig, 2004).
Any ICA components that correlated at .9 or above with the scalp topography of a blink template
or at .8 or above with the scalp topography of vertical and horizontal saccade templates were
removed from the data. Templates that were created by the present authors using the collected
3 The data-processing pipeline was optimized using a study of ERN recorded during Task A.
Although a data-processing pipeline must be chosen, this could be a potential issue for
comparing findings across tasks. However, the observation of task differences would still
meaningful because ERN processed through the same pipeline would ideally be robust against
variations in data processing. Furthermore, de-identified raw data will be uploaded as part of the
project, and researchers could examine the impact of alternative data-processing pipelines.
ERN ACROSS VERSIONS OF FLANKER 16
data and templates that were automatically generated by the ERP PCA Toolkit were used for
ocular artifact correction.
Following artifact correction, channels with more than a 100 Î¼V step within 100 ms
intervals, a voltage difference of 300 Î¼V through the duration of the epoch, or an absolute
correlation with the nearest six neighboring channels that fall below .4 were marked as bad for
the epoch. Channels that were marked as bad for more than 20% of epochs were considered
globally bad. Bad channels were interpolated using spherical splines (Perrin et al., 1989), but if
more than 10% of channels were marked bad for an epoch, the entire epoch was rejected. EEG
was then rereferenced to an average reference scheme, and the period from 400 ms to 200 ms
prior to participant response was used for baseline adjustment.
Following our a priori registration, participants were excluded if fewer than 4 correct
trials or 4 error trials were retained from each flanker task. Simulation studies indicate that the
multilevel location-scale models yield sufficient power (> .80), good coverage, and minimal-to-
zero bias with as few as 10 observations per person (n = 25) for effect sizes smaller than those
expected for the proposed study (Walters et al., 2018). In the smallest case, a conservative
threshold of 4 correct and 4 error trials would result in 24 observations per person (4 trials/2
events/3 tasks), which should lead to robust variance estimates (most participants will likely have
1,000+ total observations for statistical analysis).
Response-locked ERPs were then analyzed. To reduce the biasing effects of background
EEG noise on ERP measurements, ERN was scored using a time-window mean amplitude
approach (Clayson et al., 2013; Luck, 2014). Regions of interest (ROIs), which are created by
averaging data across multiple electrodes, were used to provide improved signal-to-noise ratio
over data from a single electrode (Baldwin et al., 2015; Huffmeijer et al., 2010), and these ROIs
ERN ACROSS VERSIONS OF FLANKER 17
are consistent with ERN research from our lab (e.g., Larson et al., 2015; Larson et al., 2013).
ERN was extracted as the average activity over fronto-medial sites (6 [FCz], 129 [Cz], 7, 106;
for electrode configuration, see Clayson & Larson, 2013) and quantified using a time-window
mean amplitude (average activity from 0 to 100 ms).
2.6. Data Analysis
Summary statistics are reported separately for each study site and for the combined
dataset. Descriptive statistics include demographic characteristics (age, sex, race/ethnicity,
handedness), number of trials retained for averaging, and ERN amplitudes.
Multilevel location-scale models were used to examine trial-level ERN data (Walters et
al., 2018; Williams et al., 2019; Williams et al., 2021), and these models have successfully been
applied to other ERN datasets (Clayson et al., 2021c; Clayson et al., 2022). Multilevel models
are well suited for ERP data (Volpert-Esmond et al., 2021) and accommodate the nested nature
of the proposed dataset (i.e., trials nested within participants). Location-scale models relax the
assumption of fixed residual variance across participants used in traditional multilevel models
(i.e., location only), simultaneously model the mean and variance structures of the dataset, and
provide more robust parameter estimates against potential model misspecifications than
traditional multilevel models (Leckie, 2014; Leckie et al., 2014; Walters et al., 2018). The use of
multilevel location-scale models facilitated a comparison of the amplitude across tasks (e.g., are
average ERN scores for participants similar across tasks?) and a comparison of the within-person
variance in amplitude across tasks (e.g., is intraindividual variability similar across tasks?).
Person-specific variance estimates from the models were used to characterize subject-level
internal consistency (Clayson et al., 2021c; Clayson et al., 2022; Williams et al., 2020).
ERN ACROSS VERSIONS OF FLANKER 18
There were two models of interest for the present analyses, and both models included all
predictors for the location and scale portions. Model 1 included a predictor for event type
(correct, error), varying slopes for event type within persons, and varying intercepts for persons.
A global intercept was not included. Model 2 included the following additional predictors: a
predictor for task (A, B, C) and an event type by task interaction. Model 1 and Model 2 are
described below using Wilkinson notation for transparency (Wilkinson & Rogers, 1973), and
model code is posted on OSF.
Model 1: Single-Trial ERN ~ 0 + Event + (0 + Event | Participant) (Location)
Sigma ~ 0 + Event + (0 + Event | Participant) (Scale)
Model 2: Single-Trial ERN ~ 0 + Event:Task + (0 + Event | Participant) (Location)
Sigma ~ 0 + Event:Task + (0 + Event | Participant) (Scale)
Bayesian models were fit within R (R Development Core Team, 2021) using the package
brms (BÃ¼rkner, 2017, 2018), which is a front end wrapper for Stan (Stan Development Team,
2021). The priors for fixed effect estimates, including the event-type intercepts, used a normal
distribution with a mean of 0 and a standard deviation of 3. All variance components were
estimated using a Studentâ€™s t prior distribution with 10 degrees of freedom, a 0 location
parameter, and a 2 scale parameter. An LKJ prior distribution with a shape parameter of 3 was
used as the prior for the correlation between the varying intercepts and slopes effects of event
type (correct, error) within persons (Lewandowski et al., 2009). A shape = 3 parameter can be
considered mildly informative, putting more weight on an identity matrix. Similar location-scale
models and priors have been successfully used on models of ERN data independent of the
proposed project, and they consistently yielded good convergence and robust parameter
estimates (Clayson et al., 2022).
ERN ACROSS VERSIONS OF FLANKER 19
Leave-one-out cross-validation (LOO-CV) was used to compare fits of Model 1 and
Model 2 (Vehtari et al., 2016), and LOO-CV was implemented in the R package loo (Vehtari et
al., 2020). Conceptually speaking, LOO-CV compares the predictive accuracy of models. LOO-
CV was used over a comparison of the posterior distributions of each parameter due to expected
posterior dependencies (i.e., ERN is expected to be collinear across levels of the task predictor).
Once an optimal model was identified (Model 1 or Model 2), parameter estimates for that
optimal model were interpreted using the 95% credible intervals of the posterior parameter
estimates. All parameter estimates for Model 1 and Model 2 are reported. Summary statistics for
raw and conditional means are reported for correct- and error-trial ERN amplitudes for each task.
Site-related differences were also tested using two additional models. Model 3 included
all predictors of Model 2 with the addition of a site fixed effect (BYU, USF) on the location and
scale portion of the model. Model 4 included all predictors of Model 3 with the addition of
interactions with the site fixed effect (e.g., Wilkinson notation: Event:Task:Site) on the location
and scale portions of the model. If Model 3 or Model 4 appeared to be the best fitting model
based on LOO-CV, then site differences would then be considered in the interpretation of effects,
and either Model 3 or Model 4 would be used as the best fitting model.
For the sake of comparability to other ERN studies, estimates of group-level
psychometric internal consistency and data quality (standardized measurement error [SME]) are
reported separately for each site and for the combined dataset. Generalizability theory was used
to calculate dependability (ï¦), which is a measure of internal consistency analogous to
coefficient alpha from classical test theory (see Baldwin et al., 2015; Brennan, 2001; Clayson et
al., 2021a; Clayson et al., 2021c; Shavelson & Webb, 1991). Dependability as a function of the
number of trials needed for a stable average ERN as a function of event (correct, error) and task
ERN ACROSS VERSIONS OF FLANKER 20
was examined using the ERP Reliability Analysis (ERA) Toolbox (Clayson et al., 2021d;
Clayson & Miller, 2017a). For computation of the SME, arithmetically derived estimates of the
standard error of the mean were used to characterize data quality of ERN scores as a function of
event and task (Luck et al., 2021).
2.6.1. Primary Test of Hypothesis. We predicted that flanker tasks would show similar
experimental effects and data quality. This prediction would be supported in two instances4: if
Model 1 shows improved fit over Model 2 or if Model 1 and Model 2 show comparable fit.
Model 1 would be considered to fit the data better 1) if the difference in expected log
Ì‚ Ì‚
predictive density (ğ‘’ğ‘™ğ‘ğ‘‘ ) was greater than 4 and 2) the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was greater than
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
Ì‚ Ì‚
2 times the standard error of ğ‘’ğ‘™ğ‘ğ‘‘ (SE(ğ‘’ğ‘™ğ‘ğ‘‘ )).
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
Model 1 would be considered to have comparable fit to Model 2 1) if the difference in
Ì‚ Ì‚ Ì‚
ğ‘’ğ‘™ğ‘ğ‘‘ was less than 4 and/or 2) the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was less than 2 times the SE(ğ‘’ğ‘™ğ‘ğ‘‘ ).
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
If Model 1 were to show comparable fit to Model 2, then this would be taken as evidence
supporting Model 1 over Model 2 due to principles of parsimony: if the two models were to show
similar fit, then the model with fewer predictors would be considered optimal.
Our prediction would not be supported if Model 2 showed improved fit over Model 1.
This would occur 1) if the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 4 and 2) the difference in
ğ‘™ğ‘œğ‘œ
Ì‚ Ì‚
ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 2 times the SE(ğ‘’ğ‘™ğ‘ğ‘‘ ).
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
2.6.2. Manipulation Check. Each task was expected to show larger ERN during error
trials than during correct trials. If no task showed this pattern of effects, then this outcome-
neutral test would suggest that the data would be inconclusive. Therefore, this manipulation
4 If site differences are observed, the primary test of the hypothesis will compare Model 3
(instead of Model 1) and Model 4 (instead of Model 2).
ERN ACROSS VERSIONS OF FLANKER 21
check was necessary to support the comparison of ERN across tasks. Data for all tasks would
pass the manipulation check if the 95% credible interval of the posterior parameter estimates of
error trials for the location portion of the model did not include any of the 95% credible interval
of the posterior of correct trials for the location portion of the data from Model 1.
2.6.3. Statistical Power Analysis. A detailed explanation of the a priori sensitivity
analyses is posted on OSF along with the data and code for reproducing the analyses. Briefly, the
statistical model for Model 1 was fit on ERN data from Clayson et al. (2021b) to estimate
expected parameters. Then, 50 new datasets were simulated using these parameter estimates
within the brms package. Data from three tasks with the expected number of trials retained for
averaging were each simulated from the same posterior parameter estimates for each participant.
Therefore, these datasets represented examples of a â€œnullâ€ model wherein ERN was identical
across the three tasks. Task-specific effects were then systematically adjusted in the simulated
datasets (e.g., differences in mean and/or variance structure in one of the three tasks).
Differences between only one task and the other two were examined to provide a conversative
estimate of sensitivity to task differences, because if each task were to show a unique mean and
variance structure then task differences in Model 2 would be more easily observed than if only
one task showed differences.
For each simulation of task-specific effects, Model 1 and Model 2 were fit to the dataset,
and LOO-CV was performed on the observed model fits. For example, a dataset was altered so
correct- and error-trial ERN from Task A showed a .1 ÂµV difference from ERN recorded during
Tasks B and C; this would be consistent with a main effect of task. Model 1 and Model 2 were fit
on the data, and the model fits were compared using LOO-CV. If ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 4 and
ğ‘™ğ‘œğ‘œ
ERN ACROSS VERSIONS OF FLANKER 22
Ì‚ Ì‚
the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 2 times the SE(ğ‘’ğ‘™ğ‘ğ‘‘ ), then the model comparison
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
was considered adequately powered to detect a .1 ÂµV difference for that simulation.
When 90% of the model comparisons of the 50 model comparisons showed improved
model fit for Model 2 over Model 1, then the data were considered adequately powered (> .90) to
of 150 participants.
2.7. Deviations from Preregistration
Although not a deviation per se, the Stage 1 Report did not clearly specify which
response device would be used for each task. Participants used a computer mouse to respond
during all three versions of the flanker tasks to make for consistent motor responses across tasks.
The reporting of some demographic information was not preregistered, but these data are
collected as part of routine lab procedures. In an effort to adhere to newly published
recommendations for describing the diversity and representation of study participants (Gatzke-
Kopp et al., 2023; Kissel & Friedman, 2023), additional demographic information was reported.
3. Results
Initial sample size was 190 participants. However, 18 participants were excluded for
having fewer than four correct trials or four error trials for each task or endorsing exclusion
criteria after providing informed consent. The final sample size collapsed across sites was 172
participants with an average age of 20 years (SD: 4, range: 18 to 58). Other summary
Grand average waveforms are shown in Figures 1 and 2. Summary information for ERP
ERN ACROSS VERSIONS OF FLANKER 23
shown separately for each site in the online supplementary material.
3.1. Manipulation Check
Model 1 was used to test whether ERN was larger during error trials than during correct
trials. The posterior parameter estimates for error trials from the location portion of the model
yielded a 95% credible interval (CrI: 0.01, 0.65), which was below and did not overlap with the
95% CrI for correct trial (2.52, 3.09). Therefore, ERN showed the expected experimental effects,
with larger (i.e., more negative) ERN during error trials than during correct trials, and the
manipulation check was passed.
3.2. Model Comparisons
We predicted that flanker tasks would show similar experimental effects and data quality
(see section 2.6.1). However, Model 2 showed improved fit over Model 1 because the difference
Ì‚ Ì‚ Ì‚
in ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 4 (âˆ†ğ‘’ğ‘™ğ‘ğ‘‘ = 690.4) and the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was greater than 2
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
times the SE(ğ‘’ğ‘™ğ‘ğ‘‘ ), which was 39.1 (see section 2.6.1). Site-related differences were also
ğ‘™ğ‘œğ‘œ
tested, but Model 4 did not show improved fit over Model 2. Although the difference in ğ‘’ğ‘™ğ‘ğ‘‘
ğ‘™ğ‘œğ‘œ
Ì‚ Ì‚
was greater than 4 (âˆ†ğ‘’ğ‘™ğ‘ğ‘‘ = 13.92), the difference in ğ‘’ğ‘™ğ‘ğ‘‘ was smaller than 2 times the
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
SE(ğ‘’ğ‘™ğ‘ğ‘‘ ), which was 7.34. Taken together, our prediction was not supported, and Model 2
ğ‘™ğ‘œğ‘œ
was interpreted below to determine which task(s) showed the largest experimental effects and
highest data quality.
3.3. ERN Model Interpretation
material on OSF. For the location portion of the model, correct-related negativity (CRN; correct-
ERN ACROSS VERSIONS OF FLANKER 24
trial ERN) scores were smallest (i.e., least negative) for Task B (95% CrI: 3.02, 3.59 ï­V), and
then increased in size for Task C (95% CrI: 2.77, 3.33 ï­V) then Task A (95% CrI: 2.23, 2.80
ï­V). ERN was larger (i.e., more negative) for Task A (95% CrI: -.15, 0.48 ï­V) than for Tasks B
(95% CrI: 0.12, 0.76 ï­V) and C (95% CrI: 0.09, 0.73 ï­V), but ERN amplitudes were similar
across Tasks B and C. For difference scores, ï„ERN (error minus correct) was largest (i.e., most
negative) during Task B (95% CrI: -3.17, -2.56 ï­V) and then decreased in size during Task C
(95% CrI: -2.94, -2.34 ï­V) and then during Task A (95% CrI: -2.63, -2.04 ï­V).
For the scale portion of the model, within-person variability was lowest for Task A (95%
CrI: 1.53, 1.59; note that parameter estimates are on the log scale) for correct trials and then
increased in size for Task C (95% CrI: 1.54, 1.61) and then for Task B (95% CrI: 1.57, 1.64). For
error trials, within-person variability was lower for Tasks B (95% CrI: 1.55, 1.63) and C (95%
CrI: 1.55, 1.63) than for Task A (95% CrI: 1.60, 1.67), but similar variability was observed
across Tasks B and C. For difference scores, within-person variability was lowest during Task B
(95% CrI: -4%, 1%; i.e., similar within-person variability across correct and error trials) and then
increased in size for Task C (95% CrI: -1%, 3%; i.e., similar within-person variability across
correct and error trials) and then for Task A (95% CrI: 6%, 9% increase in within-person
variability for error trials over correct trials).
Taken together, experimental effects (ï„ERN) were largest during Task B. Within-person
variability is an estimate of data quality, such that lower scores represent higher data quality.
Task A showed the highest data quality for CRN scores, but for ERN scores Tasks B and C
showed higher data quality than Task A.
3.4. Exploratory Analyses
ERN ACROSS VERSIONS OF FLANKER 25
3.4.1. Error positivity (Pe). Studies of performance-monitoring ERPs often examine Pe,
which is a slow, tonic waveform that follows ERN, peaks between 200 and 400 ms, and is larger
for error trials than correct trials (Nieuwenhuis et al., 2001; Overbeek et al., 2005). Whereas
ERN indexes early error detection, Pe reflects later error awareness (Steinhauser & Yeung, 2012;
Ullsperger et al., 2010; Wessel, 2012; Wessel et al., 2012). Pe was scored as the average activity
between 200 and 400 ms following a participantâ€™s response from six centro-parietal sites (Pz, 72,
67, 77 ,71, and 76). Considering that Pe is often examined along with ERN, exploratory analyses
were performed to determine whether the same pattern of effects was observed for the two ERP
components or whether studies primarily interested in Pe might use a different version of the
flanker than studies primarily interested in ERN. Parallel analyses were performed for Pe scores,
Ì‚ Ì‚
and Model 2 similarly showed improved fit over Model 1 (âˆ†ğ‘’ğ‘™ğ‘ğ‘‘ = 379.6, SE(ğ‘’ğ‘™ğ‘ğ‘‘ ) =
ğ‘™ğ‘œğ‘œ ğ‘™ğ‘œğ‘œ
29.6). The inclusion of study site in Model 4 did not improve model fit over Model 2 (âˆ†ğ‘’ğ‘™ğ‘ğ‘‘ =
ğ‘™ğ‘œğ‘œ
13.3, SE(ğ‘’ğ‘™ğ‘ğ‘‘ ) = 7.3). Therefore, parameter estimates from Model 2 were used to identify the
ğ‘™ğ‘œğ‘œ
task that yielded the largest experimental effects and highest data quality. Parameter estimates
means and credible intervals are reported in the supplementary material on OSF.
For the location portion of the model, correct-trial Pe was smallest during Task B (95%
CrI: -1.18, -.81 ï­V) and then increased in amplitude for Task A (95% CrI: -1.12, -.76 ï­V) and
then for Task C (95% CrI: -.54, -.18 ï­V). Error-trial Pe was largest for Task C (95% CrI: 1.35,
1.88 ï­V) and then decreased in amplitude for Task A (95% CrI: 1.14, 1.66 ï­V) and then for
Task B (95% CrI: 0.82, 1.36 ï­V). For difference scores, ï„Pe was larger during Task A (95%
CrI: 2.04, 2.62 ï­V) than during Tasks B (95% CrI: 1.78, 2.37 ï­V) and C (95% CrI: 1.67, 2.27
ï­V), but ï„Pe amplitudes were similar across Tasks B and C.
ERN ACROSS VERSIONS OF FLANKER 26
For the scale portion of the model, within-person variability of correct-trial scores was
lower for Task C (95% CrI: 1.55, 1.62; note estimates are on the log scale) than for Tasks A
(95% CrI: 1.56, 1.62) and B (95% CrI: 1.56, 1.63). Similar variability was observed across Tasks
A and B. For error trials, within-person variability was lowest for Task C (95% CrI: 1.55, 1.62)
and then increased in size for Task B (95% CrI: 1.58, 1.65) and then for Task A (95% CrI: 1.63,
1.70). For difference scores, within-person variability was higher during Task A (95% CrI: 5%,
9% increase in variability of error trial scores over correct trial scores) than during Tasks B (95%
CrI: .02%, 4% increase in variability of error trial scores over correct trial scores) and C (95%
CrI: -2%, 2%; i.e., similar within-person variability across correct and error trials), but variability
was similar across Tasks B and C.
Taken together, experimental effects (ï„Pe) were largest during Task A. Task C showed
highest data quality for correct trials and error trials.
3.4.2. Mean-Variance Relationships. The use of location-scale models facilitates the
examination of random effects correlations across the location and scale portions of the model
mean-variance relationships (i.e., mean-variance relationships; see also Clayson et al., 2022;
Clayson et al., 2024; Williams et al., 2021). This section describes mean-variance relationships
for ERP component amplitudes. CRN and ERN amplitude were positively correlated (95% CrI:
.42, .64), suggesting that people with large CRN (i.e., more negative) also tended to have large
variability in CRN scores (95% CrI: .23, .48), but ERN was not related to within-person
variability in ERN scores (95% CrI: -.22, .07). CRN and ERN within-person variability were
highly correlated (95% CrI: .91, .96).
ERN ACROSS VERSIONS OF FLANKER 27
Larger correct-trial Pe (i.e., more positive) was related to larger error-trial Pe (95% CrI:
variability (95% CrI: -.31, -.03), and the converse was true for error-trial scores: larger error-trial
Pe was related to more variability in within-person error-trial Pe scores (95% CrI: .13, .43).
Variability in correct- and error-trial Pe scores was highly correlated (95% CrI: .88, .94).
3.4.3. Post-Estimation Contrasts of Reliability. Data quality characterizes person-
specific, between-trial variation in ERP component scores and can be scaled using the between-
person variance to obtain an intraclass correlation coefficient (ICC). This ICC is a â€œtrial-
independentâ€ estimate of internal consistency. Alternatively, the number of trials retained for
averaging can be used to determine dependability estimates (ï¦), characterizing the internal
consistency of the data using a â€œtrial-dependentâ€ measure (see Clayson et al., 2021c). Internal
consistency characterize the adequacy of data quality in light of observed between-person
differences, and high internal consistency is a prerequisite for studies of individual differences
(Clayson, 2024b).
Post-estimation contrasts were used to compare ICC and dependability estimates of ERN
and Pe scores to determine whether scores from one task showed higher internal consistency than
other tasks. Model fits were rerun using zero-intercept models with fixed effects for each
combination of task and event and random slopes for each combination of task and event to
directly estimate between- and within-person variances for each task and event within a single
model (see supplementary material on OSF for more information). This approach facilitates the
direct comparison of internal consistency estimates using the posterior samples from the model
fit.
ERN ACROSS VERSIONS OF FLANKER 28
3.4.3.1. ERN Reliability Contrasts. The ICC and dependability estimates with their
Task A, and ICCs were similar across Tasks A and B. ERN scores showed higher ICCs during
Tasks B and C than during Task A, and ICCs were similar across Tasks B and C. ICC estimates
of ï„ERN were similar across the three tasks.
Dependability estimates, which consider the number of trials retained for averaging, were
higher for CRN recorded during Task A than for CRN recorded during Task B or Task C. CRN
from Task C showed higher dependability than CRN from Task B. Dependability estimates of
ERN were similar across the three tasks. Dependability estimates of ï„ERN were higher for Task
A than for Tasks B or C and were similar across Tasks B and C.
3.4.3.2. Pe Reliability Contrasts. Tasks yielded similar ICC estimates for correct-trial Pe
than for Tasks A or B, and differences in ICCs were not observed between Tasks A and B. ICC
estimates of ï„Pe were similar across tasks.
Correct-trial Pe showed higher dependability estimates during Task A than during Tasks
B and C, and Task C yielded higher dependability estimates of correct-trial Pe than Task B. For
error-trial Pe, only Task C showed higher dependability than Task B, and dependability was
similar across Tasks A and B and across Tasks A and C. For ï„Pe, only Task A showed higher
dependability than Task B, and dependability was similar across Tasks A and C and across Tasks
B and C.
4. Discussion
ERN ACROSS VERSIONS OF FLANKER 29
The present registered report tested the assumption of functional equivalence of ERN
across three commonly-used versions of the flanker task. Contrary to expectations, present
findings failed to support the numerical equivalence of ERN scores across three different flanker
tasks, each of which has been separately standardized (Task C; Kappenman et al., 2020),
optimized (Task A; Clayson et al., 2021b), or proposed to support normative databases (Task B;
Imburgio et al., 2020). These findings undermine the assumption of functional equivalence of
ERN from these three task versions, because if ERNs were functionally equivalent, similar
means and variances across tasks would be expected. Experimental effects (error- minus correct-
trial differences) were largest during Task B. Although Task A yielded the highest data quality of
CRN scores, Tasks B and C yielded ERN scores with higher data quality than Task A, and data
quality of ERN scores is of greater concern than CRN scores due to the small number of error
trials compared to correct trials recorded during ERN paradigms. Therefore, present findings
indicate caution might be warranted when synthesizing ERN findings across the three examined
modified versions of the flanker task.
These task differences should be considered in the context of additional data quality and
psychometric characteristics, such as SME and internal consistency. The root mean square
(RMS) of SME of ERN scores in a study using Task C was 2.04 (Zhang & Luck, 2023) and in a
study using Task B was 1.33 (Clayson et al., in press). The RMS of the SME of ERN recorded
during a Stroop was 1.61 and during a Go/No-go task was 1.84 (Clayson et al., in press). The
range of RMS of SMEs for ERN scores was between .81 and .85 for the present analyses, which
is numerically lower than the mentioned studies, suggesting higher data quality of the observed
scores than of those from other ERN studies that report SME. Group-level internal consistency
estimates scale data quality using the number of trials included in an average score and is more
ERN ACROSS VERSIONS OF FLANKER 30
readily interpretable than SMEs (Clayson et al., 2021c). Group-level internal consistency was
high for ERN scores (.92 to .93) and for ï„ERN scores (.88 to .91), suggesting that ERNs
recorded during any of the three tasks yielded high enough data quality for analysis of individual
differences (see Clayson, 2024b; Clayson & Miller, 2017b). Therefore, given the high data
quality and internal consistency of scores from all three tasks, present outcomes should not be
interpreted as unequivocal endorsement of a single task but rather highlight a need to consider
task differences when synthesizing findings across studies.
Exploratory post-estimation contrasts of dependability estimates and ICCs provide
complementary information to the primary analyses. Dependability can be considered a â€œtrial-
dependentâ€ estimate of internal consistency, whereas ICC can be considered a â€œtrial-
independentâ€ estimate of internal consistency (Clayson et al., 2021c). Dependability estimates of
ERN scores were similar across the three tasks and were highest for ï„ERN from Task A.
However, this was likely due to Task A using more than double the number of trials (900 trials)
than Task B (330 trials) or Task C (400 trials). ERN scores showed higher ICCs during Tasks B
and C than during Task A, and ICC estimates of ï„ERN were similar across the three tasks.
Therefore, when only considering the variance components that contribute to internal
consistency estimates and not the number of trials, Task B or Task C might be a better option
than Task A for studies of individual differencesâ€”although ICC estimates of ï„ERN were similar
across tasks. In terms of task length5, Task B was the shortest (M = 9.6 min.; SD = 0.7 min.),
Task C was about 3 minutes longer on average (M = 12.9 min.; SD = 1.0 min.), and Task A was
the longest (M = 28.6 min.; SD = 0.7 min.)â€”Task A was about three times length of Task B and
5 Time on task was calculated for each task for each participant based on the amount of time between presentation of
the first practice trial and the last experimental trial. These estimates therefore include any breaks, feedback screens,
and actual participant response windows in the estimation of task length.
ERN ACROSS VERSIONS OF FLANKER 31
a little more than twice the length of Task C. Therefore, considering participant burden, high
internal consistency of Task B, and the large experimental effects observed during Task B, Task
B could be the preferred option of the three evaluated flanker tasks.
A comparison of task characteristics might shed light on those features that contribute to
large experimental effects and high data quality of ERN scores. Tasks B and C showed higher
data quality than Task A and are more similar to each other than either task is to Task A (see
presentation of flanker and target stimuli, a longer stimulus presentation length than the other
tasks, shorter average ITI, fewer blocks with more trials than either other task, and more trials
than the other tasks. Tasks B and C primarily differed in the length of the response window, the
length of the ITI, the number of blocks, number of trials per block, and total number of trials. A
possibility could be that the short response windows and ITIs in Task C compared to Task B led
to reduced ERN, as short ITIs are related to reduced ERN (Compton et al., 2017). Another
possibility could be that the numbers of trials recorded were related to decreases in ERN over the
course of the task, resulting in smaller average ERN scores. This possibility is supported by
findings that ERN decreases over the course of a single task (e.g., Clayson et al., 2012; Volpert-
Esmond et al., 2018), possibly related to habituation as errors become less salient (Volpert-
Esmond et al., 2018) or to mental fatigue (Boksem et al., 2006). Future research might consider
identifying the optimal time on task for recording ERN before amplitude changes occur due to
habituation or fatigue.
The use of MLSMs facilitates an evaluation of individual differences in mean-variance
relationships for ERN. CRN is generally considered an index of overall response monitoring
(Weinberg et al., 2012), and the present study showed that smaller CRN related to greater
ERN ACROSS VERSIONS OF FLANKER 32
variability therein. This observed correlation might indicate that individuals with lower response
monitoring failed to consistently engage monitoring processes throughout the task. ERN
amplitudes were not related to within-person variability of ERN scores. These findings are
inconsistent with another study of CRN/ERN mean-variance relationships that showed larger
CRN related to less amplitude variability and larger ERN related to more variability (Clayson et
al., 2022). However, these discrepant findings might be due to the use of a semantic version of
the flanker paradigm in Clayson et al. (2022), potentially highlighting a need to consider other
stimulus features in understanding performance monitoring across flanker paradigms.
Although the present study used three commonly-used versions of a flanker paradigm,
the selected tasks represent only a small sample of the many versions of flanker paradigmsâ€”any
of which might show differences in experimental effects or data quality. Tasks B and C were
quite similar in many task features, such as stimulus type (arrows), congruent/incongruent ratio
similarly used arrow stimuli, a 45%/55% congruent/incongruent ratio, and a 600ms presentation
length. However, versions of all possible flanker tasks vary in many features, such as size and
orientation of stimuli (horizontally or vertically aligned stimulus, flankers surrounding target
stimulus), spacing between stimuli, number of flankers, number of trials, length of inter-trial
intervals, and actual stimuli (letters, types of arrows, faces). Systematically varying experimental
features is necessary to determine the exact characteristics that lead to large experimental effects
and high data quality, and this study points to characteristics of Task B as possibly being
important for further optimization. Furthermore, the present study provides a roadmap for how to
statistically compare relevant metrics.
ERN ACROSS VERSIONS OF FLANKER 33
Exploratory analyses of Pe provided fewer clear recommendations than those for ERN.
Experimental effects were largest for Pe during Task A, but Pe scores recorded during Task C
yielded the highest data quality. Like ERN scores, high group-level internal consistency was
observed across tasks for Pe (.88 to .91) and ï„Pe scores (.86 to .90). Post-estimation contrasts
indicated that Pe scores showed the highest ICCs during Task C, but ICC estimates of ï„Pe were
similar across the three tasks. Dependability estimates of Pe scores were higher for Task C than
for Task B, and Task A showed higher dependability of ï„Pe than did Task B. Taken together,
these findings seem to support the use of Task A or Task C over the use of Task B for studies of
Pe.
Present findings should be considered in the context of limitations. First, only a single
data-processing pipeline was used and different data-processing pipelines can impact the
experimental effects and data quality of ERN scores (e.g., Clayson et al., 2021b) and can lead to
differences in internal consistency (e.g., Clayson, 2020). Multiverse-like analyses can be used to
determine the data-processing decisions that impact experimental effects and data quality across
versions of flanker tasks (Clayson, 2024a; Clayson et al., 2021b). Different paradigms used to
record ERN might be sensitive to different-processing decisions, leading to differences in
optimal data processing pipelines between tasks. Second, present analyses focused on numerical
similarity of scores to conclude functional equivalence and selected â€œoptimalâ€ tasks based
experimental effects and data quality. An alternative approach for selecting an optimal paradigm
could be to choose the paradigm that most strongly relates to external variables of interest, such
clinical group differences, individual-difference characteristics, or behavioral measurements
(e.g., post-error behavioral adjustments). For example, task moderates relationships between
ERN and obsessive-compulsive disorder (Mathews et al., 2012; Riesel, 2019), psychosis (Martin
ERN ACROSS VERSIONS OF FLANKER 34
et al., 2018), and internalizing/externalizing symptoms (Pasion & Barbosa, 2019). Therefore,
certain tasks for recording ERN might be better suited to studying certain external correlates,
which could be used to select an optimal task for carrying forward in that line of work. Despite
these limitations, the present study had notable strengths, including the use of a registered report
format, an analysis of a relatively large sample for an ERP study (see Clayson et al., 2019; Kissel
& Friedman, 2023), collection of data from two sites to improve generalizability, and the use
open science practices (e.g., open data, open code) that support transparency, reproducibility, and
replicability of the work.
The present two-site registered report tested a widespread assumption that ERNs
recorded during three different versions of the flanker task are functionally equivalent by
evaluating the numerical similarity (means, within-person variances) of ERN scores. Task B
(Imburgio et al., 2020) appeared optimal for recording ERN based on the size of experimental
effects and data quality. Exploratory analyses provided tentative support using Task A (Clayson
et al., 2021b) or Task C (Kappenman et al., 2020) for studies of Pe. Further efforts to support
optimization of the task used for recording ERN or Pe might consider systematically varying key
paradigm characteristics to evaluate which characteristics lead to larger experimental effects and
higher data quality.
ERN ACROSS VERSIONS OF FLANKER 35
Acknowledgments
The authors would like to acknowledge Nathan Balls, Eliza Young, Margaret Hancock, Bethany
Hartwell, Rylee Hopkins, Tanner Jensen, Jacob Langston, Alexa Larsen, Abish Lopez, Conner
Monson, Kevin Olpin, Alejandra Perez, Berkeley Runia, Julian Beck Seamons, Emily Wall,
Anna Wheeler, and Kevin Wu for their assistance with data collection.
ERN ACROSS VERSIONS OF FLANKER 36
Summary of Differences in Paradigm Characteristics for Each Flanker Task
Task A Task B Task C
Clayson et al. Imburgio et al. Kappenman et al.
(2021b) (2020) (2020)
Congruent/Incongruent 45% / 55% 50% / 50% 50% / 50%
Flankers â€“ Target ISI (ms) 100 0 0
Stimulus Presentation Length (ms) 600 200 200
Response Window (ms) 1600 2000 ITI duration
Inter-Trial Interval (ms) 800, 1000, 1200 1000 â€“ 2000 1200 â€“ 1400
Block-Wise Performance Feedback No Yes Yes
Number of blocks 3 11 10
Trials per block 300 30 40
Total Number of Trials 900 330 400
Note: The inter-trial interval (ITI) for Task A varied pseudo-randomly between 800, 1,000, and
1,200 ms, and the ITI for Tasks B and C are presented randomly within the specified ranges. The
flankers to target stimulus asynchrony indicates the time between presentation of the flankers
and the subsequent presentation of the target stimulus. ISI = inter-stimulus interval
ERN ACROSS VERSIONS OF FLANKER 37
Summary of Results of Sensitivity Analyses for 150 Participants and 90% Statistical Power
Task A Task B Task C
Location
Main Effect of Task .15 .20 .20
Difference in ERN Only .50 .60 .55
Scale
Main Effect of Task .001 .005 .007
Difference in ERN Only .001 .005 .005
Note: Only one task, the task listed it the column header, was perturbed in each simulation.
Changes to the location portion of the model refer to changes in mean amplitude. Changes to the
scale portion of the model refer to changes in variabilityâ€”the standard deviation of the noise
added to the model that was detected by the model comparisons. When 90% of the model
comparisons of the 50 simulated datasets showed improved model fit for Model 2 over Model 1,
then the data were considered adequately powered to detect differences of that size.
ERN ACROSS VERSIONS OF FLANKER 38
Summary Demographic Information for Participants
Female/Male 97/75
Women/Men/Other 97/74/1
Right-Handed/Left-Handed 158/14
n %
Income
Below $15,000 14 8
$15,000 to $30,000 14 8
$30,001 to $45,000 16 9
$45,001 to $60,000 22 13
$60,001 to $75,000 23 13
Over $75,000 83 48
Race
American Indian/Alaskan 1 1
Asian 17 10
Black/African American 6 3
Hawaiian/Other Pacific Islander 1 1
Multiracial 16 9
White 131 76
Hispanic/Latino/Spanish Origin 35 20
Personal Education
High-School 53 31
Some College 92 53
Bachelorâ€™s or Registered Nurse Degree 2 1
Associate Degree 25 15
ERN ACROSS VERSIONS OF FLANKER 39
Summary Information for Numbers of Trials, Event-Related Potential Amplitude (ï­V), and
Artifact Correction/Rejection Data
Task A Task B Task C
Clayson et Imburgio et al. Kappenman et
al. (2021b) (2020) al. (2020)
M SD M SD M SD
Correct Trials 777 92 260 42 324 47
Error Trials 77 66 57 36 60 41
CRN 2.5 1.9 3.4 2.2 3.1 2.3
ERN -0.1 2.2 0.4 2.6 0.3 2.5
ï„ERN -2.6 2.2 -3.0 2.1 -2.8 2.1
Correct-Trial Pe -0.9 1.3 -1.0 1.4 -0.4 1.5
Error-Trial Pe 1.4 2.0 1.3 2.1 1.8 2.2
ï„Pe 2.3 2.2 2.2 2.1 2.1 2.1
Blinks: Correct (%) 2.4 8.8 1.8 6.8 1.5 5.6
Blinks: Error (%) 2.1 7.8 2.4 9.1 1.4 5.6
Bad Channels: Correct (%) 2.4 1.8 2.9 2.7 2.8 2.1
Bad Channels: Error (%) 2.5 1.8 3.0 2.7 2.9 2.1
Bad Trials: Correct (%) 0.8 1.4 0.4 1.2 1.9 1.7
Bad Trials: Error (%) 0.8 1.7 0.6 1.6 1.0 2.3
Note: Blinks refers to the percentage of trials that were corrected for blink artifact for each
subject. Bad Channels refers to the percentage of channels that were interpolated for each
subject. Bad Trials refers to the percentage of trials excluded for each subject. None of the
participant data showed vertical or horizontal saccades in the trial waveforms, so no trials were
corrected for saccade activity. CRN = correct-related negativity; ERN = error-related negativity;
ï„ERN = ERN minus CRN; Pe = error positivity; ï„Pe = error-trial Pe minus correct-trial Pe
ERN ACROSS VERSIONS OF FLANKER 40
Error-Related Negativity (ERN)
Group-Level Subject-Level Between-Person Within-Person
Task Event Reliability Reliability ï³ ï³ ICC RMS(ğ‘†ğ‘€ğ¸)
A Correct 0.99 (0.99, 0.99) 0.99 (0.005) 1.94 (1.73, 2.17) 5.01 (4.99, 5.03) 0.13 (0.11, 0.16) 0.18
Error 0.92 (0.90, 0.94) 0.89 (0.09) 2.13 (1.89, 2.40) 5.40 (5.33, 5.46) 0.14 (0.11, 0.17) 0.81
Difference 0.91 (0.89, 0.93) 0.07 (0.06, 0.09)
B Correct 0.98 (0.98, 0.98) 0.98 (0.01) 2.20 (1.98, 2.46) 5.08 (5.05, 5.12) 0.16 (0.13, 0.19) 0.32
Error 0.93 (0.92, 0.95) 0.90 (0.07) 2.50 (2.22, 2.82) 5.04 (4.97, 5.11) 0.20 (0.16, 0.24) 0.85
Difference 0.88 (0.85, 0.90) 0.07 (0.06, 0.09)
C Correct 0.99 (0.98, 0.99) 0.98 (0.007) 2.26 (2.03, 2.53) 4.96 (4.93, 4.99) 0.17 (0.14, 0.21) 0.28
Error 0.93 (0.91, 0.94) 0.89 (0.09) 2.39 (2.13, 2.69) 5.09 (5.02, 5.16) 0.18 (0.15, 0.22) 0.85
Difference 0.88 (0.85, 0.90) 0.07 (0.05, 0.09)
Error Positivity (Pe)
Group-Level Subject-Level Between-Person Within-Person
Task Event Reliability Reliability ï³ ï³ ICC RMS(ğ‘†ğ‘€ğ¸)
A Correct 0.98 (0.98, 0.98) 0.98 (0.01) 1.34 (1.21, 1.49) 5.16 (5.14, 5.18) 0.06 (0.05, 0.08) 0.19
Error 0.89 (0.86, 0.91) 0.84 (0.10) 1.77 (1.56, 2.02) 5.52 (5.45, 5.58) 0.09 (0.07, 0.12) 0.89
Difference 0.90 (0.88, 0.92) 0.07 (0.05, 0.08)
B Correct 0.95 (0.94, 0.96) 0.95 (0.02) 1.37 (1.23, 1.53) 5.07 (5.04, 5.10) 0.07 (0.06, 0.08) 0.32
Error 0.88 (0.85, 0.91) 0.83 (0.10) 1.82 (1.60, 2.08) 5.06 (4.99, 5.13) 0.12 (0.09, 0.14) 0.82
Difference 0.86 (0.83, 0.89) 0.06 (0.05, 0.08)
C Correct 0.96 (0.96, 0.97) 0.96 (0.02) 1.46 (1.31, 1.64) 5.00 (4.97, 5.03) 0.08 (0.06, 0.10) 0.28
Error 0.91 (0.89, 0.93) 0.86 (0.10) 2.04 (1.80, 2.31) 4.94 (4.87, 5.01) 0.15 (0.12, 0.18) 0.89
Difference 0.88 (0.85, 0.90) 0.07 (0.05, 0.09)
Note: Task A refers to Clayson et al. (2021b), Task B refers to Imburgio et al. (2020), and Task
C refers to Kappenman et al. (2020). ICC = intraclass correlation coefficient; RMS(ğ‘†ğ‘€ğ¸) = root
mean square of the standardized measurement error.
ERN ACROSS VERSIONS OF FLANKER 41
Estimates from Location-Scale Multilevel Model Predicting Error-Related Negativity Amplitude
(Model 2)
Predictor Estimate SE 95% CrI
Location Portion
Correct Intercept (Task A) 2.51 0.14 2.23, 2.80
Correct: Task B 0.79 0.03 0.74, 0.84
Correct: Task C 0.54 0.02 0.49, 0.58
Error Intercept (Task A) 0.18 0.16 -0.15, 0.48
Error: Task B 0.27 0.07 0.14, 0.39
Error: Task C 0.24 0.06 0.11, 0.37
Scale Portion (SD)
Correct Intercept (Task A) 1.56 0.02 1.53, 1.59
Correct: Task B 0.05 <0.01 0.04, 0.05
Correct: Task C 0.02 <0.01 0.01, 0.02
Error Intercept (Task A) 1.63 0.02 1.60, 1.67
Error: Task B -0.04 0.01 -0.06, -0.03
Error: Task C -0.04 0.01 -0.06, -0.03
Random Effects
Mean Structure
Correct Intercept 1.87 0.10 1.69, 2.09
Error Intercept 2.10 0.12 1.88, 2.35
Variance Structure (SD)
Correct Intercept 0.24 0.01 0.22, 0.27
Error Intercept 0.25 0.01 0.22, 0.28
Note: Estimates of parameters represent the median, and parameters in standard deviations (SD)
units are shown on a log scale. Task A refers to Clayson et al. (2021b), Task B refers to Imburgio
et al. (2020), and Task C refers to Kappenman et al. (2020). SE = standard error; 95% CrI = 95%
credible interval
ERN ACROSS VERSIONS OF FLANKER 42
Pairwise Contrasts for Task-Related Differences from Model 2
Location Portion of Models
ERN Pe
Parameter Median 95% CrI Median 95% CrI
Correct: A - B -0.79 (-0.84, -0.74) 0.06 (0.004, 0.11)
Correct: A - C -0.54 (-0.58, -0.49) -0.58 (-0.63, -0.53)
Correct: B - C 0.25 (0.20, 0.31) -0.63 (-0.69, -0.57)
Error: A - B -0.27 (-0.39, -0.14) 0.31 (0.18, 0.44)
Error: A - C -0.24 (-0.37, -0.11) -0.21 (-0.34, -0.08)
Error: B - C 0.03 (-0.10, 0.16) -0.52 (-0.66, -0.39)
Difference: A - B 0.52 (0.39, 0.66) 0.25 (0.11, 0.40)
Difference: A - C 0.30 (0.17, 0.43) 0.36 (0.23, 0.50)
Difference: B - C -0.22 (-0.37, -0.08) 0.11 (-0.03, 0.25)
Scale Portion of Models
ERN Pe
Parameter Median 95% CrI Median 95% CrI
Correct: A - B -0.05 (-0.05, -0.04) 0.00 (-0.01, 0.005)
Correct: A - C -0.02 (-0.02, -0.01) 0.01 (0.001, 0.02)
Correct: B - C 0.03 (0.02, 0.04) 0.01 (0.002, 0.02)
Error: A - B 0.04 (0.03, 0.06) 0.04 (0.02, 0.06)
Error: A - C 0.04 (0.03, 0.06) 0.08 (0.06, 0.10)
Error: B - C 0.00 (-0.02, 0.02) 0.03 (0.01, 0.05)
Difference: A - B 0.09 (0.07, 0.11) 0.05 (0.03, 0.07)
Difference: A - C 0.06 (0.04, 0.08) 0.07 (0.05, 0.09)
Difference: B - C -0.03 (-0.05, -0.01) 0.02 (-0.001, 0.04)
Note: Pairwise contrasts are shown for Model 2 parameter estimates of correct trial, error trial,
and difference scores. For example, `Difference: A - B` is the pairwise contrast for the difference
scores, subtracting Task B differences scores from Task A difference scores. When the credible
interval (CrI) does not include 0, the font is bolded for ease of identification. Task A refers to
Clayson et al. (2021b), Task B refers to Imburgio et al. (2020), and Task C refers to Kappenman
et al. (2020). ERN = error-related negativity, Pe = error positivity.
ERN ACROSS VERSIONS OF FLANKER 43
Estimates from Location-Scale Multilevel Model Predicting Error Positivity Amplitude (Model
2)
Predictor Estimate SE 95% CrI
Location Portion
Correct Intercept (Task A) -0.94 0.09 -1.12, -0.76
Correct: Task B -0.06 0.03 -0.11, -0.00
Correct: Task C 0.58 0.02 0.53, 0.63
Error Intercept (Task A) 1.39 0.14 1.14, 1.66
Error: Task B -0.31 0.07 -0.44, -0.18
Error: Task C 0.21 0.07 0.08, 0.34
Scale Portion (SD)
Correct Intercept (Task A) 1.59 0.02 1.56, 1.62
Correct: Task B 0.002 <0.01 -0.00, 0.01
Correct: Task C -0.01 <0.01 -0.02, -0.004
Error Intercept (Task A) 1.66 0.02 1.63, 1.70
Error: Task B -0.04 0.01 -0.06, -0.02
Error: Task C -0.08 0.01 -0.10, -0.06
Random Effects
Mean Structure
Correct Intercept 1.18 0.07 1.06, 1.32
Error Intercept 1.66 0.10 1.48, 1.87
Variance Structure (SD)
Correct Intercept 0.20 0.01 0.19, 0.23
Error Intercept 0.21 0.01 0.18, 0.23
Note: Estimates of parameters represent the median, and parameters in standard deviations (SD)
units are shown on a log scale. Task A refers to Clayson et al. (2021b), Task B refers to Imburgio
et al. (2020), and Task C refers to Kappenman et al. (2020). SE = standard error; 95% CrI = 95%
credible interval
ERN ACROSS VERSIONS OF FLANKER 44
Post-Estimation Contrasts for Estimates of Internal Consistency
Intraclass Correlation Coefficients
ERN Pe
Parameter Median 95% CrI Median 95% CrI
Correct: A - B -0.02 (-0.04, 0.01) 0.00 (-0.02, 0.02)
Correct: A - C -0.03 (-0.06, -0.002) -0.01 (-0.03, 0.01)
Correct: B - C -0.01 (-0.04, 0.01) -0.01 (-0.03, 0.01)
Error: A - B -0.06 (-0.10, -0.02) -0.01 (-0.04, 0.02)
Error: A - C -0.04 (-0.08, -0.01) -0.04 (-0.08, -0.01)
Error: B - C 0.01 (-0.02, 0.05) -0.04 (-0.07, -0.002)
Difference: A - B 0.00 (-0.02, 0.02) 0.01 (-0.01, 0.03)
Difference: A - C 0.01 (-0.01, 0.03) 0.00 (-0.02, 0.02)
Difference: B - C 0.00 (-0.02, 0.02) -0.01 (-0.02, 0.01)
Dependability
ERN Pe
Parameter Median 95% CrI Median 95% CrI
Correct: A - B 0.01 (0.01, 0.02) 0.03 (0.02, 0.04)
Correct: A - C 0.01 (0.005, 0.01) 0.02 (0.01, 0.02)
Correct: B - C -0.01 (-0.01, -0.002) -0.01 (-0.03, -0.004)
Error: A - B -0.01 (-0.03, 0.01) 0.02 (-0.01, 0.06)
Error: A - C -0.01 (-0.03, 0.02) -0.02 (-0.05, 0.01)
Error: B - C 0.00 (-0.01, 0.02) -0.04 (-0.07, -0.01)
Difference: A - B 0.03 (0.01, 0.06) 0.05 (0.02, 0.09)
Difference: A - C 0.03 (0.01, 0.06) 0.03 (-0.002, 0.06)
Difference: B - C 0.00 (-0.03, 0.03) -0.02 (-0.06, 0.01)
Note: Pairwise contrasts are shown for the reliability comparisons using the posterior parameter
estimates from the models described in section 3.4.3. For example, `Difference: A - B` is the
pairwise contrast for the difference scores, subtracting Task B differences scores from Task A
difference scores. When the credible interval (CrI) does not include 0, the font is bolded for ease
of identification. Task A refers to Clayson et al. (2021b), Task B refers to Imburgio et al. (2020),
and Task C refers to Kappenman et al. (2020). ERN = error-related negativity, Pe = error
positivity.
ERN ACROSS VERSIONS OF FLANKER 45