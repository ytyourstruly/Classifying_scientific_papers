1 Title: Object-based attention during scene perception elicits boundary contraction in memory
4 Authors: Elizabeth H. Hall (1,2) & Joy J. Geng (1,2)
5 Affiliations:
6 1 – Department of Psychology, University of California Davis; Davis, CA 95616
7 2 – Center for Mind and Brain, University of California Davis; Davis, CA 95618
11 Corresponding Author: Elizabeth H. Hall, ehhall@ucdavis.edu
13 Conflict of interest statement: The authors declare no competing financial interests.
15 Acknowledgements: This work was supported by R01 MH113855 to JJG and a NDSEG
16 fellowship to EHH. We thank Maya Tochimoto, Katherine Foray, and Zoe Hareng for their help
17 scoring the drawings, and Tiffany Kim for her help running the experiment. We’d also like to
18 thank the authors of Boettcher et al. (2018) for sharing their stimuli with us.
Abstract
34 Boundary contraction and extension are two types of scene transformations that occur in
35 memory. In extension, viewers extrapolate information beyond the edges of the image, whereas
36 in contraction, viewers forget information near the edges. Recent work suggests that image
37 composition influences the direction and magnitude of boundary transformation. We
38 hypothesize that selective attention at encoding is an important driver of boundary
39 transformation effects, selective attention to specific objects at encoding leading to boundary
40 contraction. In this study, one group of participants (N=36) memorized 15 scenes while
41 searching for targets, while a separate group (N=36) just memorized the scenes. Both groups
42 then drew the scenes from memory with as much object and spatial detail as they could
43 remember. We asked online workers to provide ratings of boundary transformations in the
44 drawings, as well as how many objects they contained and the precision of remembered object
45 size and location. We found that Search condition drawings showed significantly greater
46 boundary contraction than drawings of the same scenes in the Memorize condition. Search
47 drawings were significantly more likely to contain target objects, and the likelihood to recall other
48 objects in the scene decreased as a function of their distance from the target. These findings
49 suggest that selective attention to a specific object due to a search task at encoding will lead to
50 significant boundary contraction.
Introduction
75 More than 30 years ago, Intraub and Richardson (1989) reported a consistent pattern of
76 errors in memory for scene photographs where people remembered more details than were
77 actually present in the original picture. Since then, this pattern of errors, dubbed "boundary
78 extension," has been replicated in numerous studies (Candel et al., 2004; Chadwick et al.,
79 2013; Chapman et al., 2005; Green et al., 2019; Intraub et al., 2008; Lin et al., 2022; Mathews &
80 Mackintosh, 2004; McDunn et al., 2014; Munger & Multhaup, 2016; S. Park et al., 2007; Patel et
81 al., 2022; Kong et al., 2010; Seamon et al., 2002; Wan & Simons, 2004). In a subsequent study,
82 Intraub, Bender, and Mangels (1992) found a similar, yet inverse pattern of errors in memory -–
83 participants would sometimes remember the boundaries of wide-angle pictures as being more
84 constricted than they originally were. Similar studies showed that contraction and extension are
85 not fixed features of scene images but can be adjusted based on the viewing distance of single
86 images or manipulating the image set (McDunn et al., 2016; Menetirer et al., 2018; Intraub et
87 al., 1992; Chadwick et al., 2013). Building on these studies, we test whether the task-induced
88 distribution of attention during scene viewing contributes to the type of boundary transformation
89 that occurs for an image in memory.
90 A recent resurgence of research on boundary transformations has begun to examine
91 how different image properties influence boundary transformations in memory. Bainbridge and
92 Baker (2020) tested over 1,000 images in 2,000 participants and found that pictures of close-up,
93 central, objects tend to elicit extension while far-away image viewpoints of scenes with
94 distributed objects tend to elicit contraction. This suggests that the distance of the image
95 viewpoint focusing on a single object or an entire scene determines whether scene contraction
96 or extension occurs in memory (Hafri et al., 2022). Going further, J. Park et al. (2021) tested
97 participants on a set of stimuli that varied in both the viewpoint distance and the number of
98 objects in each environment. They reported that the distance transition point at which
99 participants reliably experienced neither contraction nor extension in memory varied by the
100 objects present in the scene. Scenes populated with many small and manipulable objects
101 elicited closer transition points in memory, indicating a bias towards contraction but scenes that
102 contained only a few, large and space-defining objects, like tables and bookshelves, elicited
103 farther transition points in memory. These findings suggest that if participants were focusing on
104 an object present in the scene, their memory would be biased to the best viewpoint to process
105 the object. Whereas, if the participants were focusing on the scene as a whole, their memory
106 would be biased to the best viewpoint to process the scene identity.
107 Consistent with the notion that object processing impacts the degree of boundary
108 extension or contraction, emotional and semantic object properties can also drive boundary
109 transformations. For example, scenes with negatively valent objects, like weapons or graphic
110 injuries, will limit the degree of extension and can even elicit significant contraction effects in
111 memory (Safer at el., 1998; S. A. Christianson, 1984; S.-Å. Christianson & Loftus, 1987; Green
112 et al., 2019; Safer et al., 2002; Takarangi et al., 2016; Wonning, 1994; Ménétrier et al., 2016).
113 This presumably occurs because high valence objects capture attention and focus image
114 processing on a single object. Similarly, images with heterogenous object semantics elicit more
115 contraction in memory, compared to scenes that contain the same amount of shared semantic
116 label objects (Greene & Trivedi, 2022). This effect is most likely due to related objects being
117 automatically attended together leading to more distributed attention over the scene image (Wei
118 et al., 2018; Mack & Eckstein, 2011; Nah et al., 2021; Nah & Geng, 2022). These results
119 suggest that the objects participants attend to during perception is an important factor
120 determining the trend and degree of transformation.
121 Together, the literature suggests that image properties and object content both impact
122 scene memory. One possible explanation for these differences is that the images may lead to
123 systematic differences in how attention is distributed during scene encoding (Intraub et al.,
124 2008). Following along the line of work done by J. Park and colleagues (2021) we hypothesized
125 that instructing participants to find and encode a small object in a wide-angle scene may lead to
126 a shift in memory towards boundary contraction because the target object is misremembered at
127 a more preferential viewpoint for processing its identity, i.e. closer than it originally appeared.
128 Likewise, instructing participants to process and encode the identity of the “scene” within the
129 wide-angle image would lead to participants misremembering the image to a viewpoint that was
130 preferential for processing scene identity.
131 In this current study, we show two groups of participants the same scene images but ask
132 them to either engage in target search, or simply memorize the image. We hypothesize that
133 those engaged in target search will focus attention primarily on the target object (Wu & Wolfe,
134 2022; Young & Hulleman, 2013; Yu et al., 2022), leading to a higher degree of boundary
135 contraction in memory; in contrast, those engaged in scene memorization only will distribute
136 attention more broadly, leading to less boundary contraction. The influence of image
137 composition is kept constant across groups by using the same images, but the goals of the
138 viewer are manipulated by task. After encoding, all participants are given a surprise drawing
139 task in which they are asked to draw as many of the previously seen scenes as possible. Their
140 drawings were analyzed for the objects recalled, their drawn location and size, and the
141 transformation of scene boundaries. Results revealed significant boundary contraction in
142 drawings from participants who engaged in Search, while drawings from the Memorize condition
143 showed equal rates of contraction and extension. Further analyses of boundary contracted
144 Search drawings revealed diminishing memory for objects as a function of distance from the
145 target object, whereas the smaller amount of Memorize drawings that exhibited contraction
146 revealed diminishing memory for objects as a function of the distance from the center of the
147 image. These findings provide evidence that boundary transformations in memory are due to
148 how attention is distributed amongst objects at encoding.
153 Search condition, participants were instructed to find the circled target object.
Method
156 Participants. Thirty-six undergraduate students (26 females, mean age = 19.44, SD = 1.34,
157 range = 18-23 years) participated in the Search condition and thirty-six different students
158 participated in the Memorize condition (27 females, mean age = 19.94, SD = 1.67, range = 18-
159 25 years). Students were recruited from the University of California, Davis through the Sona
160 research pool in exchange for research credit. Participants were native English speakers with
161 normal or corrected vision. We also recruited online scorers to judge the drawings on a variety
162 of metrics. Five-hundred and seventy-nine scorers were collected from Amazon Mechanical
163 Turk and were monetarily compensated. One hundred and sixty-four scorers were collected
164 from the SONA research pool to complete ratings on Testable for course credit. Each participant
165 provided informed written consent in accordance with the local ethics clearance as approved by
166 the National Institutes of Health.
167 Stimuli. The 15 scene images used in this study were initially constructed for an
168 experiment assessing the role of anchor objects on eye movements in visual search (Boettcher
169 et al., 2018). The stimulus images were created with ArchiCAD software version 18 (Graphisoft,
170 Munich, Germany). All images were 1280 wide by 960 pixels tall. Each scene contained a visual
171 search target (e.g., toilet paper). The scenes were selected so that there was no overlap in
172 target objects across the scenes, and each scene could be identified by a unique categorical
173 identifier (i.e., there was only one kitchen in our stimulus set).
174 Apparatus. Stimuli were presented on a ASUS MG279Q monitor with a 60 Hz refresh
175 rate and a spatial resolution of 1920 x 1200 pixels. Participants were seated 60cm away from
176 the screen and a computer running PsychoPy (Peirce, 2007) controlled all stimulus
177 presentations. Eye movements were tracked using an EyeLink-1000 desktop mount, sampling
178 from the right eye at 500 Hz (SR Research, Ontario, Canada).
179 Experimental design. The visual Search group was run before the Memorize group so
180 that scene exposure times from the Search group could be used to constrain viewing time in the
181 Memorize group. All saw the same 15 computer-generated scenes and both groups completed
182 1 practice trial with a scene that was not from the main experimental set. The Search group was
183 instructed to search for and click on a specific target. Each trial, first the target cue word
184 appeared on the screen for 3s, followed by a 1s fixation cross, after which the stimulus image
185 appeared on the screen. Once the image appeared, they were given up to 10 seconds to click
186 on the target with the computer mouse. They were also instructed to memorize the scene in as
187 much detail as possible since their memory for the images would later be tested, though specific
188 details of the memory test were not provided.
189 Participants were then asked to complete the Visual Vivid Imagery Questionnaire which
190 contains questions regarding their ability to visualize images (Marks, 1973). This task was used
191 to limit rehearsal of the scenes and items in memory and an average of 4.56 minutes (SD =
192 1.48 minutes) passed from the end of the eye-tracking phase to the start of the drawing phase.
193 After the VVIQ, participants were instructed to draw as many scenes as they could
194 recall, in no particular order and with no time limit, while their pen movements were tracked on a
195 digital drawing pad. They were provided with 15 sheets of paper each with a 1280 x 960 black
196 frame and were instructed to draw every detail they could remember about the scene within the
197 frame. They were told that the drawings would not be scored on the basis of their drawing ability
198 but would be scored on how accurately they were as a representation of the studied stimulus
199 images. If they felt they could not accurately draw an item in the scene, they were instructed to
200 try to draw the general shape, and they could label anything they felt was unclear. They could
201 use color pencils to add any color they remembered.
202 In the Memorize group, participants were instructed to memorize each scene in as much
203 detail as possible as their memory would be tested later on, and they saw each scene for the
204 average time that the participants viewed the scene in the Search experiment (M = 4.15s, SD =
205 0.96s, MIN = 3.23s, MAX = 6.06s). They were not instructed to search for the target or click the
206 image and did not see the target word before each image but instead saw a blank screen for 3s
207 followed by a 1s fixation cross. An average of 4.22 minutes (SD = 1.33 minutes) passed from
208 the end of the eye-tracking phase to the start of the drawing phase.
209 Eye-tracking analysis. Fixations and saccades were defined from raw eye-tracking
210 data using the Saccades package in R (von der Malsburg, 2015). Fixations could not be
211 determined for one participant from each condition due to poor data quality. We included the
212 drawings from these two participants in analyses but discarded their eye movement data. We
213 computed the percentage of the scene that was foveated by a participant by placing a circular
214 filter with a 1 degree of visual angle radius centered on each fixation. We defined the
215 percentage of the scene that was foveated in a trial as the summed area of pixels occupied by
216 the circular filters divided by the total amount of pixels in the image (Castelhano et al., 2009).
220 to find the cued objects in 15 scene images. In the Memorize condition (N=36) participants memorized
221 each scene for the average amount of time it was viewed by participants in the Search condition. After a
222 delay, both groups of participants had an unlimited amount of time to draw the scenes from memory.
Online scoring procedures
224 The 72 in-lab participants drew 601 scenes from memory. Three scorers, the first author and
225 two undergraduate research assistants, matched each drawing to a scene image. A drawing
226 was considered to be matched to an image if two out of three scorers agreed. If the scorers
227 believed that a participant drew the same stimuli image more than once, the first drawing of that
228 scene was considered a match, and subsequent drawings of the same image were not included
229 in analyses. Drawings that were not matched to an original image by the experimenters were
230 not scored (86 out of 601 drawings, or 13.64% of drawings), leaving 515 drawings for analyses.
231 Of the 86 unmatched drawings, 32 were of the practice trial image. Three different measures
232 were collected for each drawing. The code for these measures was adapted from Bainbridge et
233 al. (2019).
234 Boundary transformation. Forty-four scorers were recruited from the SONA research
235 pool to provide ratings of boundary transformation for each drawing on Testable. Scorers were
236 shown the drawing and the originally viewed stimulus image side-by-side on the screen Scorers
237 were asked whether the drawing was “closer, the same, or farther than the original photograph,”
238 and were told to ignore any extra or missing objects in the drawing. Scorers responded on a 5-
239 item scale: much closer, slightly closer, the same distance, slightly farther and much farther,
240 with the additional option to indicate “can’t tell” if they believed the drawing to be
241 incomprehensible. Seven scorers provided boundary ratings for each drawing and boundary
242 transformation scores for each drawing were calculated by the mean across the ratings normed
243 on a scale of -1 (much farther) to +1 (much closer).
244 Object marking. One hundred and twenty scorers were recruited from the UC Davis
245 SONA research pool to complete an online object marking task on Testable. The purpose of this
246 task was to determine if an object from the original image was included in the drawing or not.
247 Scorers were shown the original image with an object outlined in red using the LabelMe
248 annotations presented next to a drawing. Scorers were asked to indicate if the outlined object
249 was included on the drawing. Scores were collected from three participants per object and an
250 object was determined to be in the drawing if at least two out of three participants agreed that it
251 was present.
252 All objects in the stimulus images were segmented using LabelMe, an online object
253 annotation tool (Russell et al., 2008). There were 360 objects in the stimulus set and each
254 image contained 24 objects on average (SD = 16.99, min = 8, max = 83). Objects were
255 “nameable, separable, and visually distinct items” (Bainbridge et al., 2021). If multiple objects of
256 the same type were touching, these objects were grouped together and given a plural label (e.g.
257 “shampoos”). Object parts (e.g. “tire” on truck) were not segmented, but if an object was visually
258 distinct and could be defined as a separate semantic label it was segmented separately (i.e.
259 decorative “pillow” on a couch). Background segmentations (“grass”, “trees”, “floor”, “walls”,
260 “ceiling”) were not included in analyses (Bainbridge et al., 2019).
261 Object location and size. Five-hundred and seventy-nine scorers were recruited on
262 Amazon Mechanical Turk to complete an online object location task. The purpose of this task
263 was to quantify the location and size of drawn objects. Only objects that had been determined to
264 be present in the drawing by the object marking task were scored in this task. Scorers were
265 shown an original image with an object outlined in red next to a drawing and asked to place and
266 resize an ellipse around the same object in the drawing. Three scorers were asked to locate
267 each object of interest in a drawing. Object location was calculated as the median centroid of
268 the ellipses across the responses. Object size was calculated as the median radii of the ellipses
269 across responses.
272 average, 62.3% of drawings in the Search condition showed boundary contraction, a significantly greater
273 proportion than the 37.7% that showed extension. Only 44.7% of drawings in the Memorize condition
274 showed contraction, while 48.0% showed extension, and there was no significant difference between the
275 proportions. (right) Results of the split-half consistency analyses for each condition. Seven different raters
276 scored the amount of boundary transformation in each drawing. Each set of ratings was split in half, and
277 we calculated the correlation between the average transformation score of each half. The gray line shows
278 the other half of ratings sorted randomly. For both conditions, ratings between groups were highly similar
279 and significantly correlated.
Results
282 Object-based attention elicits more boundary contraction.
283 The main question of this study was whether the patterns of object-based attention used
284 in search would elicit contraction effects in memory above the rate elicited by the image alone.
285 To investigate this, we had an experimentally-naive group of online scorers rate the degree of
286 boundary transformation in the Search and Memorize drawings. We visualized what percent of
288 start, we found that a majority of Search drawings had contracted scene boundaries. On
289 average, 62.29% of Search drawings showed boundary contraction, while only 30.73% showed
290 boundary extension. The results from a chi-square test of independence confirmed that the
291 difference between proportions was significant (χ (1, N = 283) = 28.0, p < .001). Comparatively,
292 only 44.67% of the drawings from the Memorize condition showed boundary contraction, with
293 47.95% of drawings showing boundary extension, and the chi-square test revealed no
294 significant difference between the proportions (χ (1, N = 232) = 0.04, p > .5). The
295 transformations ratings for the Memorize drawings are consistent with the findings of Bainbridge
296 and Baker (2021) who revealed that scene images have a fairly equal probability of eliciting
297 either contraction or extension. To assess the reliability of the ratings we conducted a split-half
299 Boundary transformation ratings were highly consistent across raters’ responses for both the
300 Search ( * = 0.64; p < .001) and Memorize drawings ( * = 0.51; p < .001). We then looked at
⍴ ⍴
302 3b). This analysis allows us to directly compare the effect of the task on memory, as both
303 groups of participants studied the scenes for roughly the same amount of time. Results from a
304 non-parametric Wilcoxon rank-sums test (WRST) confirmed that scene images were
305 significantly more likely to elicit boundary contraction in memory when participants engaged in
306 target Search during the encoding period (N = 15, Z = 2.32, p = .020). Taken together, these
307 findings suggest that object-based attention during scene perception can elicit boundary
308 contraction in memory.
313 transformations. The orange box is the spread of attention, and the black box is what the participant
314 remembers of the scene. When the spread of attention is narrow, boundaries could extend beyond the
315 target region in memory, but because the target is a relatively small proportion of the image, a majority of
316 the scene is forgotten. (b) Plot of the average boundary transformation rating across drawings by image.
317 Each gray line represents one of the 15 scene images. Error bars represent the standard error of the
318 mean. Drawings of scenes done in the Search condition showed significantly more boundary contraction
319 on average. (c) Example drawings of the scenes that elicited the most boundary contraction and
320 boundary extension for the Search (left) and Memorize (right) conditions. For drawings with the most
321 contraction (top), the colored outlines on the images show how much of the scene the participant
322 recalled. For drawings with the most extension, the colored outlines on the drawings show the boundaries
323 of the studied image. Area outside of the boundaries is what was extended in memory.
324 Smaller spread of attention in Search leads to less memory for other objects.
325 Participants exhibited strong task-based influences on their eye movements during study
327 average and exhibited a strong bias to fixate the center of the image. Participants in the Search
328 condition foveated significantly less of each scene on average (N = 15, Z = 3.3, p <.001), with
329 their fixations covering only 3.22% of the scene on average (SD = 0.89). As expected,
330 participants in the Search condition spent significantly more time on the target object (Z = 5.78,
331 p < .001), fixating it 18.97% (SD = 0.09) of trial time on average, while participants in the
332 Memorize condition spent only 3.58% (SD = 0.02) of the trial time looking at the target (from the
333 Search condition).
337 conditions. Heatmaps are scaled to a range of 0 to 1. Examples for 3 of the 15 scenes are shown here,
338 with the target object circled in white and labeled by the target cue. Heatmaps show the general tendency
339 for participants in the Search condition to spend more of the trial fixating the target object, while
340 participants in the Memorize condition tended to show a strong center bias in their eye movements.
342 Given that the target object was more likely to be fixed during encoding in the Search
343 condition compared to the Memorize condition, we next looked to confirm that target objects
345 showed a separate group of online workers each drawing alongside the original stimulus image
346 and asked them to make a judgment as to whether an outlined object on the image was present
347 in the drawing. From these judgments, we found that the target object was present in 80.36%
348 (SD = 16.09%) of Search drawings. In comparison, drawings from the Memorize group,
349 contained the target only 19.39% (SD = 18.85%) of the time (N = 15, Z = 4.50, p < .001),
350 although they included significantly more of the non-target objects in the scene (Memorize: M =
351 29.58%; Search: M = 18.21%; WRST: N = 15, Z = 2.76, p < .006).
353 Distance from the search target predicts memory for non-target objects.
354 We next tested whether objects included in drawings from each group could be
355 predicted by the location of the target, or the center of the image. Finding that objects in the
356 Search condition are more likely to be drawn if they are close to the target would be consistent
358 a logistic regression model on the recall data from drawings done in the Search condition (N =
359 6,165 objects [1,115 objects drawn / 5,050 objects not drawn]). Proximity to the target was
360 defined as the distance from the center of the object segmentation to the center of the target
361 object segmentation, z-scored across objects. We found that proximity to the target object
362 significantly predicted whether an object would be included in the memory drawing (β = -1.51,
363 CI = [-1.58, -1.45], Z = -45.61, p <.001; target proximity: β = 0.11, CI = [0.05, 0.18], Z = 3.33, p =
364 .001). These findings suggest that participants from the Search condition may have maintained
365 a representation of the scene in memory that was focused around the target object.
366 For the Memorize drawings (N = 5,782 objects [1,717 drawn / 4,065 not drawn]), we
367 found that proximity to the target was negatively correlated with a likelihood to include the object
368 in the drawing (β = -0.87, CI = [-0.93, -0.82], Z = -30.01, p <.001; target proximity: β = -0.18, CI
369 = [-0.24, 0.12], Z = -6.07, p < .001); proximity to the center of the image was positively
370 correlated with the object being drawn (center proximity: β = 0.16, CI = [0.10, 0.22], Z = 5.46, p
371 < .001). These results likely reflect the fact that participants in the Memorize condition were
373 objects were placed around the periphery of the scene images, which could explain the negative
374 relationship between target proximity and likelihood of being drawn.
375 Additionally, drawings from both conditions showed a tendency to include large, space-
376 defining objects. Results of a logistic regression further confirmed that object memory was
377 significantly predicted by both object size (n=12,230 objects [3,055 drawn / 9,175 not drawn];
378 β = -0.87, CI = [-0.93, -0.81], Z = -29.60, p <.001; size: β = 0.41, CI = [0.35,0.47], Z = 12.90, p
379 <.001) and the Search condition (β = -0.52, CI = [-0.61, -0.44], Z = -12.05, p <.001). The
380 interaction between size and condition was also significant in the model (β = 0.10, CI = [0.01,
382 drawings show a stronger likelihood of including objects that fall within the first five standard
383 deviations of mean object size. The interaction between object size and condition occurs as
384 object size becomes greater than five standards above the mean, when Search drawings start
385 to show a greater likelihood of drawing the object from memory.
388 condition spent a significantly greater proportion of trial time fixating the target. (b) Plot of the proportion
389 of drawings that included the target object. A significantly greater proportion of Search drawings
390 contained the target object. Each gray line represents one of the 15 scene images. Error bars represent
391 the standard error of the mean. (c) Regression line indicating the probability of recalling an object in the
392 Search condition by its proximity to the target object. (d) Regression line indicating the probability of
393 recalling an object in the Memorize condition by its proximity to the image center. (e) Regression lines
394 indicating the probability of recalling an object by its size and condition. Shaded error bars are the
395 confidence interval bootstrapped across 1000 iterations.
397 Contraction increases the size that target and non-target objects are drawn.
398 The drawings also provided some insight into how participants in the Search
399 represented the targets’ size and location in memory. We asked a group of online workers to
400 provide judgements as to an object's height and width by asking them to draw an ellipse around
401 each object in each drawing. We then defined the height and width of a drawn object as the
402 median radii from a set of three judgements. From these judgments, we found that participants
403 in the Search condition consistently drew the target objects from memory as wider (M = 4.55%
404 wider, SD = 2.18% wider) and taller (M = 5.87% taller, SD = 1.76% taller) than they originally
405 appeared. A paired sample signed-rank test of the ellipse size between target objects of the
406 images and target objects in the Search drawings confirmed that target objects were drawn
407 significantly larger than portrayed in the stimuli images (N = 15, Z = 3.30, p = .001). “Nontarget
408 objects in search drawings were drawn 3.84% wider (SD = 1.97%) and 4.94% taller (SD =
409 2.36%), a smaller increase in size than that shown for target objects. However, the differences
410 in drawn width or height between target and non-target were not significant (width: Z = 0.5, p >
411 .5; width: Z=1.14, p> .5), suggesting that the increase in target object size may be mostly due to
412 the contraction effect.
413 Memorize drawings that included the target object also showed a significant tendency to
414 overestimate the target’s size (wider: M = 2.14%, SD = 1.94%; taller: M = 2.84%, SD = 2.02%).
415 However, results from paired sample signed-rank tests of the average increase in target height
416 and width between conditions confirmed that targets were drawn as significantly taller and wider
417 in memory representations from the Search condition compared to those from the Memorize
418 condition (taller: N=14, Z = 2.44, p = 0.015; wider: N = 14, Z = 3.63, p < .001 [we excluded one
419 scene from this analysis as no drawings from the memorize condition of this scene included the
420 target object]). This result points to the idea that when memory representations for a scene are
421 contracted, objects tend to be remembered as larger (or closer) than they originally appeared.
422 This result points to the idea that when memory representations for a scene are contracted,
423 objects tend to be remembered as larger (or closer) than they originally appeared (Kirsch et al.,
424 2018). The finding that target objects were drawn as even larger in Search drawings compared
425 to Memorize drawings lends support to the idea that these drawings represent memories where
426 the boundaries were significantly contracted around the target object.
431 objects in the original image. (top right) The mean x-axis and y-axis distance between object centroids of
432 the different conditions and object centroids in the original image. Each gray line represents one of the 15
433 scene images. Error bars represent the standard error of the mean. (bottom) Example maps of the
434 average ellipse encompassing the target objects by condition. Target objects are noted above the
435 scenes.
437 The ellipse judgements provided by the online raters also provided a metric for
438 measuring whether participants had accurate memory for the objects’ locations throughout the
440 of three ellipse judgments. Participants in the Search condition drew the target objects close to
441 where they appeared in the stimulus image, with target objects centroids displaced on average
442 by 10.74% (SD = 4.28%) of the scene overall in the x-direction, and by 12.40% (SD = 4.73%) in
443 the y-direction. A paired sample signed-rank test found that there was no significant difference
444 in where the object centroid was located on the x-axis compared to where it was drawn (N = 15,
445 Z = 0.97, p > .1). A paired sample signed-rank test of the drawn and real target centroid location
446 on the y-axis revealed that there was some significant displacement along that axis (N = 15, Z =
447 2.22, p = 0.03). Target objects were displaced slightly more in the Memorize drawings (x-axis: M
448 = 12.51%, SD = 8.71%, y-axis: M = 16.05%, SD = 9.03%), although results from paired sample
449 signed rank tests found that this difference in x and y-axis displacement between conditions was
450 not significant (x-axis: N = 14, Z = 0.46, p >.1; y-axis: N = 14, z = 1.10, p > .1). The significant
451 degree of displacement along the y-axis for target object in the Search condition and the lack of
452 significant displacement along the x-axis is likely due to there being more pixels along the x-axis
453 in the stimulus images (the stimuli 1280 pixels wide x 960 pixels tall). Therefore, we should not
454 rule out that participants in the Search condition had some displacement overall for where the
455 target object was located. However, the changes in location could be seen as slight compared
456 to the magnitude of increase in size in memory for target objects.
Discussion
459 This study sought to provide insight into how scene memory is shaped by selective attention at
460 encoding. Specifically, we looked to see if instructing viewers to constrain spatial attention to
461 target objects through visual search would impact the magnitude and trend of transformation
462 effects. We hypothesized that the tightly constrained focus of attention needed to efficiently
463 select and recognize targets would lead to significant boundary contraction in memory.
464 Consistent with our hypothesis, we found a high rate of boundary contraction in drawings from
465 participants who engaged in Search, above the rate reported in previous studies (Bainbridge &
466 Baker, 2020; Greene & Trivedi, 2022; Hafri et al., 2022; Lin et al., 2022; J. Park et al., 2021).
467 Further, we found that a group of participants who viewed the same scenes for roughly the
468 same amount of time but only to memorize the scenes, had significantly less boundary
469 contraction in their drawings, and in fact showed roughly equal rates of contraction and
470 extension. Taken together these results suggest that focused attention on target objects during
471 encoding can lead to those objects having an exaggerated role in memory, with boundaries
472 contracting around them.
473 Many studies on boundary transformations manipulate the properties of the image
474 stimuli, but rarely give explicit instructions on where to look. In this study, we instructed
475 participants to look for specific targets in the Search condition and found an increased rate of
476 contraction. Target objects were included in four-fifths of all Search drawings and the likelihood
477 of any other object being included was commensurate with their distance from the target. This
478 suggests that participants in the Search condition centered their memories for the scene around
479 the target object leading to scene contraction around the target object. However, without
480 specific features to guide looking behaviors, as in our Memorize condition, viewers tend to show
481 a center bias in their fixations on an image (Tatler et al., 2007; Bindemann, 2010). Although
482 there was overall less contraction in the Memorize condition, contracted drawings showed
483 diminishing memory for objects with increasing distance from the center of the image. This
484 suggests that memory for the scene was constructed around information in the center of the
485 image, although looking was likely highly idiosyncratic. Nevertheless, this overall explanation for
486 our data is consistent with the earliest findings on boundary contraction which showed a loss of
487 peripheral information near the image boundaries (Intraub et al.,1992).
488 Our work is not the first to reveal a significant relationship between attentional spread
489 and boundary transformations in memory (Intraub et al., 2008). For example, manipulating
490 participants to attend to image edges lessens boundary extension (McDunn et al., 2016;
491 Gagnier et al., 2013). Similarly, perceptual object grouping due to semantic relatedness spreads
492 attention and leads to increased rates of extension in memory (Greene & Trivedi, 2022).
493 Inversely, scenes with especially valent objects that capture and hold attention, elicit more
494 boundary contraction (Safer at el., 1998; S. A. Christianson, 1984; S.-Å. Christianson & Loftus,
495 1987; Green et al., 2019; Safer et al., 2002; Takarangi et al., 2016; Wonning, 1994; Ménétrier et
496 al., 2018). These findings show that constraining attention to specific objects at encoding leads
497 to increased rates of contraction, whereas distributing attention tends to lead towards greater
498 extension. Our findings are consistent with these studies and provide further evidence that
499 attention plays a pivotal role in eliciting boundary transformations, even when attention is guided
500 by task demands rather than image properties.
501 Our results also complement work showing that viewers have different rates of
502 transformation in memory as a consequence of whether they attend to scene or object
503 information. For example, boundary extension does not occur without tangible (McDunn et al.,
504 2014) or implied (Intraub et al., 1998) scene background information, and boundary contraction
505 is more likely to occur for images of objects on blank backgrounds (Gottesman & Intraub, 2002;
506 Intraub et al., 1998). Other work on memory for images of individual objects show a pattern of
507 conversion where remembered objects are transformed towards their real-world size (Konkle &
508 Oliva, 2007; Lin et al., 2022), possibly reflecting a closer preferred viewing distance for
509 processing object information (J. Park et al., 2021; Chen et al., 2021; Bainbridge & Baker,
510 2020). Visual processing of objects and scenes operates through different functional pathways
511 (J. Park et al., 2023; Doshi & Konkle, 2023) and this appears to determine the directionality of
512 boundary transformation.
513 More work is needed to determine if attention contributes to other sources of memory
514 transformation or not. For example, natural statistics of scene depth are strongly correlated with
515 the trend and magnitude of transformation (Lin et al., 2022), with images with unnaturally deep
516 depth of field having a higher likelihood of eliciting boundary contraction (Gandolfo et al., 2023).
517 Likewise, judgements of perspective distance have been shown to be highly reliant on patterns
518 of spatial frequency (Brady & Oliva, 2012; Oliva & Torralba, 2001; Oliva et al., 2006; Barron et
519 al., 2021; Lescoart et al., 2015). These results may be related to attentional effects, such as a
520 strong central fixation bias, or they may be due solely to perceptual processes. A better
521 understanding of the multiple possible mechanisms underlying boundary transformations in
522 memory will require further work on how the brain processes peripheral versus foveal, eccentric,
523 object and scene information, such as that being done by Konkle and colleagues (J. Park et al.,
524 2023; Doshi & Konkle, 2023; Julian et al., 2016; Konkle & Caramazza, 2013). Discovering how
525 the neurobiological properties of the visual system contribute to memory transforms will likely
526 relate to the multisource account of boundary extension as a “filling-in” of expected information
527 (Intraub, 1997, 2002, 2010; Intraub et al., 1992; Gottesman & Intraub, 2002; Intraub &
528 Berkowits, 1996; S. Park et al., 2007; Mullaly et al., 2012; Maguire et al., 2016).
529 In summary, we believe that our results set-up an interesting account for how selective
530 attention in visual processing can drive transformation effects. We found that requiring the
531 participants to focus attention in order to identify objects during encoding led memory drawings
532 to show a higher rate of boundary contraction than found in those of participants not required to
533 constrain attention. We propose that attentional guidance in part determines the trend and
534 magnitude of boundary transformation effects in memory, along with other competing factors,
535 such as the distance and image contents. Together these findings contribute to the evolving
536 discovery of what factors influence transformation effects in memory.
538 Open Practices Statement
539 Drawings and code will be made publicly available on the Open Science Framework (link TBD).
541 Analyses on the how search leads to memory for size and location of the target object, and
542 memory for other objects in relation to the target object, were preregistered.
544 References
545 Bainbridge, W. A., & Baker, C. I. (2020). Boundaries Extend and Contract in Scene Memory Depending
546 on Image Properties. Current Biology: CB, 30(3), 537–543.e3.
547 Bainbridge, W. A., Hall, E. H., & Baker, C. I. (2019). Drawings of real-world scenes during free recall
548 reveal detailed object and spatial information in memory. Nature Communications, 10(1), 5.
549 Bainbridge, W. A., Kwok, W. Y., & Baker, C. I. (2021). Disrupted object-scene semantics boost scene
550 recall but diminish object recall in drawings from memory. Memory & Cognition.
552 Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., & Hedman, P. (2022). Mip-nerf 360:
553 Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on
554 Computer Vision and Pattern Recognition (pp. 5470-5479).
555 Bindemann, M. (2010). Scene and screen center bias early eye movements in scene viewing. Vision
556 research, 50(23), 2577-2587.
557 Boettcher, S. E. P., Draschkow, D., Dienhart, E., & Võ, M. L.-H. (2018). Anchoring visual search in
558 scenes: Assessing the role of anchor objects on eye movements during visual search. Journal of
559 Vision, 18(13), 11.
560 Brady, T. F., & Oliva, A. (2012). Spatial frequency integration during active perception: perceptual
561 hysteresis when an object recedes. Frontiers in psychology, 3, 462.
562 Candel, I., Merckelbach, H., Houben, K., & Vandyck, I. (2004). How Children Remember Neutral and
563 Emotional Pictures: Boundary Extension in Children’s Scene Memories. In The American Journal of
565 Castelhano, M. S., Mack, M. L., & Henderson, J. M. (2009). Viewing task influences eye movement
566 control during active scene perception. Journal of vision, 9(3), 6-6.
567 Chadwick, M. J., Mullally, S. L., & Maguire, E. A. (2013). The hippocampus extrapolates beyond the view
568 in scenes: an fMRI study of boundary extension. Cortex; a Journal Devoted to the Study of the
569 Nervous System and Behavior, 49(8), 2067–2079.
570 Chapman, P., Ropar, D., Mitchell, P., & Ackroyd, K. (2005). Understanding boundary extension:
571 Normalization and extension errors in picture memory among adults and boys with and without
572 Asperger’s syndrome. Visual Cognition, 12(7), 1265–1290.
573 Chen, Y. C., Deza, A., & Konkle, T. (2022). How big should this object be? Perceptual influences on
574 viewing-size preferences. Cognition, 225, 105114.
575 Christianson, S. A. (1984). The relationship between induced emotional arousal and amnesia.
576 Scandinavian Journal of Psychology, 25(2), 147–160.
577 Christianson, S.-Å., & Loftus, E. F. (1987). Memory for traumatic events. Applied Cognitive Psychology,
578 1(4), 225–239.
579 Doshi, F. R., & Konkle, T. (2023). Cortical topographic motifs emerge in a self-organized map of object
580 space. Science Advances, 9(25), eade8187.
581 Gagnier, K., Dickinson, C. A., & Intraub, H. (2013). Fixating picture boundaries does not eliminate
582 boundary extension: Implications for scene representation. Quarterly Journal of Experimental
583 Psychology, 66(11), 2161-2186.
584 Gandolfo, M., Nägele, H., & Peelen, M. V. (2023). Predictive Processing of Scene Layout Depends on
585 Naturalistic Depth of Field. Psychological Science, 9567976221140341.
586 Gottesman, C. V., & Intraub, H. (2002). Surface construal and the mental representation of
587 scenes. Journal of Experimental Psychology: Human Perception and Performance, 28(3), 589.
588 Green, D. M., Wilcock, J. A., & Takarangi, M. K. T. (2019). The role of arousal in boundary judgement
589 errors. Memory & Cognition, 47(5), 968–982.
590 Greene, M., & Trivedi, D. (2022). Spatial scene memories contain a fixed amount of semantic information.
592 Hafri, A., Wadhwa, S., & Bonner, M. F. (2022). Perceived Distance Alters Memory for Scene Boundaries.
593 Psychological Science, 33(12), 2040–2058.
594 Intraub, H. (1997). The representation of visual scenes. Trends in cognitive sciences, 1(6), 217-222.
595 Intraub, H. (2002). Anticipatory spatial representation of natural scenes: Momentum without
596 movement?. Visual Cognition, 9(1-2), 93-119.
597 Intraub, H. (2010). Chapter 6 - Rethinking Scene Perception: A Multisource Model. In Psychology of
598 Learning and Motivation (Vol. 52, pp. 231–264). Academic Press.
599 Intraub, H., Bender, R. S., & Mangels, J. A. (1992). Looking at pictures but remembering scenes. Journal
600 of Experimental Psychology: Learning, Memory, and Cognition, 18(1), 180.
601 Intraub, H., & Berkowits, D. (1996). Beyond the edges of a picture. The American Journal of
602 Psychology, 109(4), 581.
603 Intraub, H., Daniels, K. K., Horowitz, T. S., & Wolfe, J. M. (2008). Looking at scenes while searching for
604 numbers: Dividing attention multiplies space. In Perception & Psychophysics (Vol. 70, Issue 7, pp.
606 Intraub, H., & Richardson, M. (1989). Wide-angle memories of close-up scenes. Journal of Experimental
607 Psychology. Learning, Memory, and Cognition, 15(2), 179–187.
608 Intraub, H., Gottesman, C. V., & Bills, A. J. (1998). Effects of perceiving and imagining scenes on memory
609 for pictures. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24(1), 186.
610 Julian, J. B., Ryan, J., Hamilton, R. H., & Epstein, R. A. (2016). The occipital place area is causally
611 involved in representing environmental boundaries during navigation. Current Biology, 26(8), 1104-
612 1109.
613 Kirsch, W., Heitling, B., & Kunde, W. (2018). Changes in the size of attentional focus modulate the
614 apparent object’s size. Vision Research, 153, 82–90.
615 Kong, J.-G., Kim, G., & Yi, D.-J. (2010). Effects of scene inversion on boundary extension. In Journal of
617 Konkle, T., & Caramazza, A. (2013). Tripartite organization of the ventral stream by animacy and object
618 size. Journal of Neuroscience, 33(25), 10235-10242.
619 Konkle, T., & Oliva, A. (2007). Normative representation of objects and scenes: Evidence from
620 predictable biases in visual perception and memory. Journal of Vision, 7(9), 1049-1049.
621 Lescroart, M. D., Stansbury, D. E., & Gallant, J. L. (2015). Fourier power, subjective distance, and object
622 categories all provide plausible models of BOLD responses in scene-selective visual
623 areas. Frontiers in computational neuroscience, 9, 135.
624 Lin, F., Hafri, A., & Bonner, M. F. (2022). Scene memories are biased toward high-probability views.
625 Journal of Experimental Psychology. Human Perception and Performance, 48(10), 1116–1129.
626 Mack, S. C., & Eckstein, M. P. (2011). Object co-occurrence serves as a contextual cue to guide and
627 facilitate visual search in a natural viewing environment. Journal of vision, 11(9), 9-9.
628 Marks, D. F. (1973). Visual imagery differences in the recall of pictures. British Journal of Psychology ,
629 64(1), 17–24.
630 Mathews, A., & Mackintosh, B. (2004). Take a closer look: emotion modifies the boundary extension
631 effect. Emotion , 4(1), 36–45.
632 Maguire, E. A., Intraub, H., & Mullally, S. L. (2016). Scenes, spaces, and memory traces: what does the
633 hippocampus do?. The Neuroscientist, 22(5), 432-439.
634 McDunn, B. A., Siddiqui, A. P., & Brown, J. M. (2014). Seeking the boundary of boundary extension.
635 Psychonomic Bulletin & Review, 21(2), 370–375.
636 McDunn, B. A., Brown, J. M., Hale, R. G., & Siddiqui, A. P. (2016). Disentangling boundary extension and
637 normalization of view memory for scenes. Visual Cognition, 24(5-6), 356-368.
638 Ménétrier, E., Didierjean, A., & Barbe, V. (2018). Effect of contextual knowledge on spatial layout
639 extrapolation. Attention, perception & psychophysics, 80(8), 1932–1945.
640 Ménétrier, E., Didierjean, A., & Vieillard, S. (2013). Is boundary extension emotionally
641 selective?. Quarterly Journal of Experimental Psychology, 66(4), 635-641.
642 Mullally, S. L., Intraub, H., & Maguire, E. A. (2012). Attenuated boundary extension produces a
643 paradoxical memory advantage in amnesic patients. Current Biology, 22(4), 261-268.
644 Munger, M. P., & Multhaup, K. S. (2016). No imagination effect on boundary extension. Memory &
645 Cognition, 44(1), 73–88.
646 Nah, J. C., & Geng, J. J. (2022). Thematic object pairs produce stronger and faster grouping than
647 taxonomic pairs. Journal of Experimental Psychology: Human Perception and Performance, 48(12),
648 1325.
649 Nah, J. C., Malcolm, G. L., & Shomstein, S. (2021). Task-irrelevant semantic properties of objects
650 impinge on sensory representations within the early visual cortex. Cerebral Cortex
651 Communications, 2(3), tgab049.
652 Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial
653 envelope. International journal of computer vision, 42, 145-175.
654 Oliva, A., Torralba, A., & Schyns, P. G. (2006). Hybrid images. ACM Transactions on Graphics
655 (TOG), 25(3), 527-532.
656 Park, J., Josephs, E. L., & Konkle, T. (2021). Systematic transition from boundary extension to contraction
658 Park, S., Intraub, H., Yi, D.-J., Widders, D., & Chun, M. M. (2007b). Beyond the edges of a view:
659 boundary extension in human scene-selective visual cortex. Neuron, 54(2), 335–342.
660 Patel, S. D., Esteves, C. V., So, M., Dalgleish, T., & Hitchcock, C. (2022). More than meets the eye:
661 emotional stimuli enhance boundary extension effects for both depressed and never-depressed
662 individuals. Cognition & Emotion, 1–9.
663 Park, J., Soucy, E., Segawa, J., Mair, R., & Konkle, T. (2023). Ultra-wide angle neuroimaging: insights
664 into immersive scene representation. bioRxiv, 2023-05.
665 Peirce, J. W. (2007). PsychoPy—Psychophysics software in Python. Journal of Neuroscience Methods,
666 162(1), 8–13.
667 Safer, M. A., Christianson, S. Å., Autry, M. W., & Österlund, K. (1998). Tunnel memory for traumatic
668 events. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in
669 Memory and Cognition, 12(2), 99-117.
670 Safer, M. A., Levine, L. J., & Drapalski, A. L. (2002). Distortion in Memory for Emotions: The Contributions
671 of Personality and Post-Event Knowledge. Personality & Social Psychology Bulletin, 28(11), 1495–
672 1507.
673 Seamon, J. G., Schlegel, S. E., Hiester, P. M., Landau, S. M., & Blumenthal, B. F. (2002).
674 Misremembering pictured objects: people of all ages demonstrate the boundary extension illusion.
675 The American Journal of Psychology, 115(2), 151–167.
676 Takarangi, M. K. T., Oulton, J. M., Green, D. M., & Strange, D. (2016). Boundary Restriction for Negative
677 Emotional Images Is an Example of Memory Amplification. Clinical Psychological Science, 4(1), 82–
678 95.
679 Tatler, B. W. (2007). The central fixation bias in scene viewing: Selecting an optimal viewing position
680 independently of motor biases and image feature distributions. Journal of vision, 7(14), 4-4.
681 von der Malsburg, T. (2015). Saccades: Detection of fixations in eye-tracking data. R Package Version
683 Wan, X., & Simons, D. J. (2004). Examining boundary extension in recognition memory for a large set of
684 digitally edited images. In Journal of Vision (Vol. 4, Issue 8, pp. 872–872).
686 Wei, L., Zhang, X., Li, Z., & Liu, J. (2018). The semantic category-based grouping in the Multiple Identity
687 Tracking task. Attention, Perception & Psychophysics, 80(1), 118–133.
688 Wonning, T. S. (1994). Susceptibility to misleading post-event information: Emotionality or uniqueness.
689 Butler University.
690 Wu, C.-C., & Wolfe, J. M. (2022). The Functional Visual Field(s) in simple visual search. Vision Research,
691 190, 107965.
692 Young, A. H., & Hulleman, J. (2013). Eye movements reveal how task difficulty moulds visual search.
693 Journal of Experimental Psychology. Human Perception and Performance, 39(1), 168–190.
694 Yu, X., Johal, S. K., & Geng, J. J. (2022). Visual search guidance uses coarser template information than
695 target-match decisions. Attention, Perception & Psychophysics, 84(5), 1432–1445.