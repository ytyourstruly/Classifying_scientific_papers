PREPRINT – OPEN SCIENCE FRAMEWORK
Electrophysiological markers of language adaptation in joint
production. Evidence from human-robot interaction.
1,2 1,2 1,2 1,2*
Giusy Cirillo , Elin Runnqvist , Kristof Strijkers , Noël Nguyen & Cristina
3*
Baus
1 Aix-Marseille University (AMU) & CNRS, Laboratoire Parole et Langage (LPL), Aix-en-
Provence, France
2 Institute for Language, Communication and the Brain (ILCB), Marseille, France
3 Department of Cognition, Development and Educational Psychology, University of
Barcelona, Spain
Abstract
The present study investigates whether people engage in lexico-semantic processing when
performing a language task with a humanoid robot. A human participant and a robot alternated
in naming pictures of objects belonging to 15 semantic categories, while participants’
electrophysiological activity was recorded. We manipulated word frequency as a measure of
lexical access having half of the pictures associated to high frequency names and half to low
frequency names. In addition, the robot was programmed to give the semantic category name
(e.g., tool for the picture of a hammer) instead of the more typical basic-level name (e.g.,
hammer for the picture of a hammer) for items belonging to 5 categories. Analyses on the
picture-locked activity revealed a comparable ERP associated to frequency both when it was
participant’s turn and robot’s turn to speak. Analyses on the response-locked activity show a
different pattern for the category and basic-level responses in the first but not in the second
part of the experiment, indicating acquisition and adaptation to the lexico-semantic pattern of
the robot. Taken together, our findings provide empirical evidence for 1) the involvement of
listeners’ production system to predict robot’s upcoming words, and for 2) partner-adaptive
behavior supporting comprehension.
Keywords: Joint action; Speech production; Artificial partner; Lexical Frequency; Co-
representation; Lexico-semantic processing
1. Introduction
According to the joint action theory (Sebanz et al., 2006; Sebanz & Knoblich, 2009), among
the key elements allowing activity partners to coordinate and accomplish a joint goal is co-
representation (Baus et al., 2014; Brehm et al., 2019; Gambi et al., 2015; Sahaï et al., 2019;
Sebanz et al., 2003, 2005). Representing a co-actor’s task enables individuals to make
predictions about their partner’s plans and upcoming actions (Baus et al., 2014; Vesper et al.,
2017). This applies to a tennis match as much as to an everyday conversation. In a common
exchange, conversational partners take turns between speaking and listening at extremely fast
rates (at about 200 ms gaps), requiring them to process multiple language levels simultaneously
(Levinson & Torreira, 2015). Predicting the partner’s upcoming speech makes them able to
plan and produce a response even before their interlocutor has actually ended up the utterance
(Corps et al., 2017; Garrod & Pickering, 2015). Those expectations are formed based on
previous experience and shaped by acquired phonological and grammatical constraints as well
as meaning (Ramscar et al., 2013; Signoret et al., 2020). For instance, individuals use semantic
information from the sentence to predict the upcoming verb (Freunberger & Roehm, 2016), or
semantic information of verbs to predict the dependent noun (Maess et al., 2016). Expectations
are also supported by non- linguistic factors, including culture, social dynamics and familiarity
with the interlocutor (Martin et al., 2016; Molnar et al., 2015).
The integrated theory of language production and comprehension described by Pickering and
Garrod (2013) proposes that the ability to process different language dimensions rapidly and
almost automatically comes from the strict coupling between language production and
comprehension. The main idea behind this theory is that interlocutors use their production
system to comprehend language via the formation of production intentions, retrieving all the
relevant language representation levels, as if they were speaking themselves (Dell & Chang,
2014; Pickering & Garrod, 2007, 2013a). This account finds support from other studies showing
how the quality of comprehension is highly dependent on production skills (Huettig, 2015;
Mani & Huettig, 2012). One key aspect of this prediction-by-production process supporting
comprehension (Martin et al., 2018) is that it is subject to a continuous update, and interlocutors
need to select relevant properties of the language environment and adapt to them.
In this work, we implemented a joint picture-naming task to monitor the prediction dynamics
to the lexical and lexico-semantic properties of a robot’s naming pattern using EEG. Robots
have proven to be good task partners in joint production studies (Brandstetter & Bartneck, 2017;
Marge et al., 2022; Wudarczyk et al., 2021). Typical coordinating signals, such as gaze cueing,
facial expressions, hesitation sounds have been found in human-robot interaction as much as in
human-human interaction (Kilner et al., 2003; Loth et al., 2015; Skantze, 2016). At the
behavioral level, for instance, participants have shown to align with robots in terms of their
lexical, syntactic or conceptual choices (Branigan et al., 2010; Cirillo et al., 2022; Iio et al.,
2015) or to facilitate production (Wudarczyk et al., 2021), suggesting that humans are able to
co-represent a robot’s task. On the other hand, little investigation is present in the joint
production research to signal the electrophysiological responses to a robot’s (verbal) action,
while it is often limited to processing physical movements (Cooper et al., 2020). Elicitation of
ERP components associated to prediction typically found in human-human interaction might
signal language adaptation in human-robot interaction.
Besides the presence of a non-human agent, another important aspect that characterizes our
approach is linked to the way co-representation and linguistic alignment (i.e., adaptation at the
language production level) are examined. Our study separates from standard studies
investigating lexical alignment, where participants were monitored on whether they would pick
up the same word uttered by the partner or not (e.g., Brennan & Clark, 1996; Garrod &
Pickering, 2004). Specifically, we were interested to see what happens when we change the
expected (and rather obvious) lexical choice used to refer to an item (e.g., saying ‘apple’ to
refer to the item apple) with a less expected one (e.g., saying ‘fruit’ to refer to the item apple).
How would participants react to this particular choice of the interlocutor? Our study draws from
the joint picture-naming study by Cirillo et al. (2022), in which the responses of a social robot
were manipulated so that, for a regular subset of trials, the robot named the pictures with their
corresponding semantic category label (e.g., saying ‘fruit’ for the picture of an apple) instead
of the basic-level name (e.g., saying ‘apple’ for the picture of an apple). Via this manipulation,
authors were able to show adaptation at the production level, which they referred to as
conceptual alignment. Participants aligned with the robot’s lexico-semantic choices, producing
progressively more category names for the items belonging to the same categories for which
the robot had used category names. The purpose of the present follow-up study is to demonstrate
that this conceptual alignment is a consequence of a progressive adaptation at the level of
prediction affecting comprehension, consequently generating an updating of the production
choices. We therefore focused on exploring brain correlates linked to comprehension.
For this purpose, our design included two important manipulations. First, to establish an index
of lexical co-representation, we manipulated word frequency, having half of the pictures
corresponding to high frequency words and half to low frequency words (following the
procedure in Baus et al., 2014). Second, to establish an index of lexico-semantic adaptation, we
manipulated robot’s responses so that, for a regular subset of trials, the robot named the pictures
with their corresponding semantic category label instead of the basic-level name (following the
procedure of Cirillo et al., 2022). We asked whether 1) human participants engage in
lexicalization processes when the robot prepares to speak and 2) they are able to adapt to the
robot’s lexico-semantic choices. Lexico-semantic patterns denote a language choice that, in
addition to the lexical information, also make use of semantic information. Two different
objects (e.g., a pen and a pencil), can share the same semantic category, such as ‘tool’ or
‘stationery’ (Katz & Fodor, 1963), while being associated to different names respectively (i.e.,
pen and pencil). Choosing to name an object with its category label would therefore guide the
attention on this particular aspect of the object. In this sense, the generic perspective of the
object seen as a piece of the stationery comes to be prioritized.
In joint naming, researchers have highlighted how people co-represent their partner’s task and
speech, as comparable components associated to lexical access are present when both preparing
to speak and preparing to listen to the partner, even though, given the task, no direct interaction
is elicited between the participants. The most striking evidence comes from a joint picture
naming study conducted by Baus et al. (2014), in which participants showed similar amplitudes
of P200/P300 components related to easiness of processing associated to the lexical frequency
of the words for both those trials that required their response and those that required the
response of their partner as compared to no-go trials. Our study connects to Baus et al. (2014)
and rises the question whether lexical co-representation is present also when having a
robot as naming partner.
Our second aim concerning adaptation relates to EEG studies reporting the elicitation of
specific ERP components (e.g., P300, N400, P600) after processing deviant and unexpected
linguistic events (Arbel et al., 2011; Fitz & Chang, 2019; Hodapp & Rabovsky, 2021; Kaan et
al., 2000; Kutas & Federmeier, 2011; Van Petten & Luka, 2012). In our work, the use of a robot
as language partner makes the quest for social effects even more challenging. Our first objective
was to establish whether human participants predict what the robot is going to say, by
monitoring lexical access via the frequency manipulation as in Baus et al. (2014). Secondly,
and novel to the field, we aimed to capture adaptive prediction to the robot’s responses over the
course of the task, possibly evident in a gradual reduction of the amplitude of relevant ERP
components (e.g., P300, P600, N400) or activity over trials (Dikker & Pylkkanen, 2011;
Fjaellingsdal et al., 2020; Hodapp & Rabovsky, 2021; Kutas & Federmeier, 2011; Nieuwland,
2019; Nieuwland & Van Berkum, 2006).
2. Methods
In accordance with Open Sciences Practices data, we made our script analyses and materials
2.1 Participants
Thirty-four participants (14 males; mean age 24.5 years; age range 18–33 years) took part in
the EEG study. All participants were right-handed, had normal or corrected- to-normal vision,
were native French speakers, had no known history of neurological or psychiatric disorders.
Prior to the experiment, participants read the instructions, and gave written informed consent.
All participants were paid for their participation to the experiment. Due to technical problems
related to system synchronization between Furhat and Biosemi (the EEG system), we were not
able to provide results for 6 participants. 4 participants were also excluded because the robot
did not perform the category naming level condition. For 1 participant, the experimental
computer crashed in the middle of the experiment, and we were not able to recover the data.
Our final pool included 23 participants.
2.2 Material and design
Our visual stimuli consisted of 450 pictures of objects belonging to 15 semantic categories (e.g.,
fruits, tools, musical instruments), with 30 items in each category (see Appendix A). 377
pictures were taken from the MultiPic database (Duñabeitia et al., 2018), while the remaining
73 were drawn by a local artist hired for the purpose. We extracted the frequency value for each
item from the online French word database ‘Lexique’ (B. New, Pallier, Ferrand, & Matos,
2001). We averaged the means of the frequency values for words contained in books and words
contained in film subtitles, to account for both spoken and written usage. A median split of the
stimuli resulted in a mean frequency value for the high frequency items of 72.5 occurrences per
million (sd = 108.2, subjective frequency: mean = 3.3, sd= 0.8) and a mean frequency value for
low frequency items of 5.7 occurrences per million (sd = 3.9; subjective frequency: mean 2.3,
sd= 0.8). As further test, we run a survey on 30 students from Aix-Marseille University, who
evaluated how frequently they encountered a list of words in a 5-point Likert scale, where 1
corresponded to ‘very rarely’ and 5 to ‘very often’. This allowed us to have a subjective
frequency value as additional anchor. The audio stimuli consisted of 465 word productions pre-
recorded by the synthesized voice of Furhat, that we distributed across six sets of randomized
naming latencies (M = 930 ms, SD = 250 ms, range = 500–1600 ms). The robot was
programmed to produce the basic-level name (e.g., ’hammer’) for items belonging to 10
semantic categories, and the semantic category name (e.g., ’tool’) for items corresponding to
the remaining five semantic categories.
Our design included three main turn conditions, corresponding to who had to name the pictures:
go trials (participant naming), other-go trials (robot naming) and no-go trials (no one naming).
Overall, 100 (10 items x 10 semantic categories) were assigned to each turn condition. Those
categories corresponded to the 10 semantic categories in which items were named with the
basic-level name by Furhat. 75 additional items (15 items x 5 semantic categories) were named
by the robot with the category name, and the remaining 75 items were included as no-go trials.
We eventually assigned 75 filler items to participants, to have a more balanced naming
alternation across trials. Pictures were presented within a square, of which the color (blue, green
or orange) indicated whether it was participant’s, robot’s or nobody’s turn to speak. Semantic
categories, items and colors were counterbalanced across naming conditions and turn
conditions.
Naming-level condition Turn condition
go other-go no-go
Basic-level 10 items x 10 items x 10 items x
10 categories 10 categories 10 categories
Category-level 15 items x 15 items x
5 categories 5 categories
2.3 Procedure
Each participant was introduced to Furhat and initiated a free conversation (around 5 min) with
him. Participant and robot sat alongside each other in front of a computer screen. They were
given instructions together. The experimenter told them that their task was to name as quickly
as possible the pictures presented inside the square of the color (either blue, green or orange) to
which they were assigned and to remain silent for the rest of the pictures. Before getting started
with the main experiment, participants were able to practice the task with the robot. They were
presented with a total of 15 pictures to familiarize themselves with the three main turn
conditions (go, other-go, no-go). We run the experiment using the OpenSesame software
for a detailed illustration of the experimental timeline): they started with a fixation point (+)
presented in the middle of the screen for 500 ms followed by the picture presentation (2500 ms)
and finally by a white screen (500 ms). The experiment comprised 5 blocks of 125 trials each.
After each block, participants were asked to take a 2-minutes pause.
level condition associated with the robot response. In the basic-level condition, the robot gave the basic-level name
of the object (pear), while in the category condition, he gave the semantic category corresponding to the object
(instrument). The thinking cloud icon corresponds to participant’s tendency to co-represent robot’s trials.
2.4 EEG data acquisition
Electroencephalographic (EEG) activity was recorded continuously by means of a 64-channel
electrode cap (BioSemi Active). The impedance of the electrodes was kept below 20KΩ. Data
was down-sampled offline to 512 Hz and a band-pass filter between 0.1 and 30 Hz was applied.
The correction of ocular artifacts was conducted by Independent Component Analysis (ICA) in
Brain Vision Analyzer 2.0 (Brain Products GmbH, Germany), for which segments containing
artifacts and eye blinks were identified and manually rejected. Afterwards, data was re-
referenced to electrodes placed on the mastoids (TP7 and TP8), segmented, artifact corrected
(voltages above or below 100 microvolts) and time-locked to the onset of both picture and
robot’s response (-100 ms pre- and 700 ms post-stimulus). Cleaned epochs were baseline
corrected to -100 ms pre-stimulus baseline and were averaged separately according to each
condition.
3. Data analysis
We analyzed behavioral responses (for go-trials) and ERP responses (for go, other-go and no-
go trials) of 23 participants.
3.1 Behavioral analysis
We run an error rate and a naming latency analysis on participants’ responses in go-trials,
especially to replicate standard frequency effects as a sanity check for our design. We divided
the experiment into two parts or blocks (block 1 and block 2). This methodological choice was
adopted in order to understand how participants’ responses changed over time, with a sufficient
number of trials. It was used in the response-locked ERP analyses as well. Errors (N = 459)
comprised no-responses, hesitations as well as responses that differed from the target name we
aimed to elicit (e.g., ’boat’ for the picture of a ship). They were fitted into a generalized linear
mixed model (glmer) using the lmr4 package in R Bates et al. (2015), where frequency (low vs
high) was taken as fixed factor while item and participant were taken as random factors. For
naming latencies, the analysis was carried out by fitting a linear mixed effects model (lmer)
with frequency (low vs high) as fixed factor, and item and participant as random factors.
Naming latencies were log-transformed for a better fit to the model, and errors as well as 2.5
standard deviations above or below the mean response time for high and low frequency words
respectively were excluded.
To monitor participant’s adaptation to robot’s conceptual patterns, we run an analysis of the
category responses over time (across the two blocks). We collected a total of 68 responses,
indicating that participants did manifest a certain tendency to reproduce the behavioral pattern
of the robot. We binary coded (0–1) participants’ responses, indicating whether they used or
not a category-related name during each trial. Category names (e.g., tool for hammer; N = 176)
were fitted to a logits regression model with block (1 and 2; we divided the experiment in two
parts) as fixed factor and participant and item as random factors.
3.2 EEG analysis
We conducted separate analyses to distinguish between brain activity locked to the picture
presentation and brain activity locked to the robot’s responses. For the first analysis we were
interested to see the effect of the lexical frequency manipulation across the three turn conditions
(go, no-go and other-go). Based on previous literature investigating the electrophysiological
correlates of frequency, we focused on two-time windows: 150-250 ms and 250-350 ms
corresponding to the P200 and P300 respectively (Baus et al., 2014; Strijkers et al., 2010). In
addition, we analyzed activity in the time window 350-450 for later effects. Go trials were
analyzed separately from no-go and other-go trials. Analysis of go-trials served as a baseline to
test the validity of the frequency manipulation. To do so, we run a linear mixed effects model
for each time- window (Bates et al., 2015), with word frequency (high vs. low frequency) and
region (Anterior, Central, Posterior) (Anterior: AF7, AF3, AF4, AFz, F1, F3, F5, FC3, FC5,
Fz, FC1, FCz, FC2, F2, F4, F6, FC4, FC6; Central: C1, C3, C5, CP3, CP5, Cz, CP1, CPz, CP2,
C2, C4, C6, CP4, CP6; Posterior: P3, P5, P7, PO3, PO7, P1, Pz, P2, POz, P4, P6, P8, PO4;
PO8, O1, Iz, Oz, O2) as predictors and participants as random factor. Other-go and no-go trials
were analyzed together as two different levels of no-go trials. The model included type of turn
(other-go vs simple no-go) together with frequency (high vs. Low frequency) and region
(Anterior, Central, Posterior) as fixed factors and participant as random factor.
For the electrophysiological responses to the robot’s responses, we defined how the category
manipulation affected comprehension by contrasting the two naming levels associated with the
robot’s responses (saying the basic-level name vs. saying the category-level name). In addition,
we compared activity between the two parts of the experiment (two blocks). For this, we fitted
the continuous brain activity to a linear mixed effects model with naming level (basic vs.
category), block (1 vs 2) and region (Anterior, Central, Posterior) as fixed factors and subject
as random factor. We conducted the analysis on one time-window: 400 - 600 ms. This specific
choice was made in order to capture the prediction error we expected to elicit when hearing the
semantic category instead of the more predictable basic-level name. As the manipulation we
performed concerned the semantic level, we based our approach on previous research
suggesting that semantic and contextual patterns are normally integrated later in time compared
to lexical patterns. A higher amplitude of components associated to stimuli deviating from
contextually-induced expectations (e.g., P300, N400 or P600) at the category-level condition
would evidence a processing difficulty when listening to the category name instead of the basic-
level name. In addition, having block as a variable allowed us to monitor how the amplitudes
of those components changed over the course of the experiment, and how different processing
patterns were associated to the two naming conditions respectively.
4. Results
We analyzed behavioral responses (for go-trials) and ERP responses (for go, other-go and no-
go trials) of 23 participants.
4.1 Behavioral results
The analysis on error rates revealed an effect of frequency. Low frequency words were named
less accurately than high frequency words (M LF = 0.29, SD LF = 0.4; M HF = 0.13, SD HF =
0.3: b = 0.52167, s.e. = 0.05688, z = 9.172, p <0.001). The analysis did not reveal any effect of
block (b = -0.04, s.e. = 0.05, t = -0.71, p = 0.473) nor of the interaction between block and
frequency (b = 0.06, s.e. = 0.056, z = 1.06, p = 0.28). The analysis on naming latencies showed
a significant effect of frequency (b = -3.694e-02, s.e. = 7.750e-03, t = -4.766, p <0.001) and
block (b = 1.070e-02, s.e. = 4.613e-03, t = 2.32, p = 0.02), but not for the interaction between
the variables (b = -3.210e-04, s.e. = 4.657e-03, t = -0.06, p = 0.94). Pictures associated with
low frequency words were named slower (M = 1055.28 ms, SD = 292.79) than high frequency
items (M = 995.2 ms, SD = 285.94). From the behavioral measures we were therefore able to
replicate standard frequency effects characterizing lexical speed in picture-naming.
4.2 ERP results
4.2.1 Picture-locked activity: Frequency effects
For go-trials, the analysis performed in the 150-250 time-window (P200) re-
vealed an effect of frequency, with low-frequency items eliciting more positive wave-
forms than high frequency items (b = 0.09589, s.e. = 0.04068, t = 2.357, p = 0.0185). No
interaction with region was observed (p > .05). We found the same pattern in the 250-
350 ms time-window, with a significant frequency effect (b = 1.285e-01, s.e. = 5.484e-
02, t = 2.343, p = 0.0192), and no interaction between frequency and region (p >.05).
The frequency effect, however, disappeared when testing it in the latest time-window
(350-450 ms: b = 0.12755, s.e. 0.06824, t = 1.869, p = 0.0617). Full statistical results for
go-trials are included in the Tables 2, 3 and 4.
For no-go trials (robot’s trials and no one’s trials), the model on the 150-250 ms time-window
did not reveal any effect of frequency (b = 2.231e-02, s.e. = 2.948e-02, t = 0.757, p = 0.44913),
neither an interaction with type of turn (b = 5.256e-02, s.e. = 2.948e-02, t = 1.783, p = 0.07463)
nor region (p >0.05). However, and relevant here, we observed an interaction between
frequency and type of turn in the second time- window (250-350 ms: b = 0.07153, s.e. =
0.03517, t = 2.034, p = 0.042011). When performing the pair-wise comparison contrasting
frequency and type of turn, we found that, for other-go trials, low frequency items elicited more
positive waveforms compared to high frequency items (b = 0.2667, s.e. = 0.09, t = 2.6, p =
0.0370), while no significant frequency effect was reported for no-go trials (b = -0.0195, s.e. =
0.0995, t = -0.196, p = 0.9973). We also found a significant interaction in the time-window 350-
450 ms (b = 7.024e-02, s.e. = 3.483e-02 , t = 2.017, p = 0.04380), confirming the elicitation of
a P300 component associated to processing low-frequency compared to high frequency words
for other-go trials (b = 0.4272, s.e. = 0.0985, t = 4.336, p = 0.0001) and not for no-go trials (b
= 0.1462, s.e. = 0.0985, t = 1.484, p = 0.4472). Full statistical results are included in the Tables
5, 6 and 7. For full pair-wise comparison results, see the supplementary materials.
Fixed effects Estimate Std. Error t value p
(Intercept) 0.24798 0.17563 1.412 0.1719
Frequency1 (Low/High) 0.09589 0.04068 2.357 0.0185 *
Region1 (Posterior/Anterior) 0.51961 0.05627 9.235 <0.001 ***
Region2 (Posterior/Central) -0.28126 0.05624 -5.001 <0.001 ***
Frequency1/Region1 0.01962 0.05627 0.349 0.7274
Frequency1/Region2 -0.08320 0.05623 -1.480 0.1391
in volt) according to Frequency (High vs. Low), Region (Anterior, Central, Posterior) and their interactions.
Fixed effects Estimate Std. Error t value p
(Intercept) -2.695e-01 2.204e-01 -1.223 0.2343
Frequency1 (Low/High) 1.285e-01 5.484e-02 2.343 0.0192 *
Region1 (Posterior/Anterior) 1.446e+00 7.584e-02 19.071 <0.001 ***
Region2 (Posterior/Central) -1.354e+00 7.580e-02 -17.863 <0.001 ***
Frequency1/Region1 -1.468e-03 7.584e-02 -0.019 0.9846
Frequency1/Region2 -8.781e-02 7.579e-02 -1.159 0.2468
in volt) according to Frequency (High vs. Low), Region (Anterior, Central, Posterior) and their interactions.
Fixed effects Estimate Std. Error t value p
(Intercept) 0.07699 0.27914 0.276 0.7853
Frequency1 (Low/High) 0.12755 0.06824 1.869 0.0617
Region1 (Posterior/Anterior) 0.96919 0.09438 10.269 <0.001 ***
Region2 (Posterior/Central) -1.78779 0.09433 -18.953 <0.001***
Frequency1/Region1 -0.01658 0.09438 -0.176 0.8605
Frequency1/Region2 -0.04808 0.09432 -0.510 0.6103
in volt) according to Frequency (High vs. Low), Region (Anterior, Central, Posterior) and their interactions.
the upper panel represent the grand averages for go trials for anterior, central and posterior electrodes respectively.
Electrodes were grouped according to the region (frontal, central, posterior). Red lines represent low-frequency
words (LF) and black lines represent high-frequency words (HF). The lower panel illustrates the topographical
maps representing the frequency effect in the 150-250, 250-350 and 350-450 time windows (low frequency words
minus high frequency ones). Red colors indicate positive differences, corresponding to low frequency words bring
more positive than high frequency words.
Fixed effects Estimate Std. Error t value p
(Intercept) 1.603e-01 1.234e-01 1.299 0.20746
Frequency1 (Low/High) 2.231e-02 2.948e-02 0.757 0.44913
Turn1 (Other/No) 3.488e-02 2.948e-02 1.183 0.23681
Region1 (Posterior/Anterior) 1.240e+00 4.077e-02 30.427 <0.001 ***
Region2 (Posterior/Central) -8.706e-01 4.075e-02 -21.367 <0.001 ***
Frequency1/Turn1 5.256e-02 2.948e-02 1.783 0.07463
Frequency1/Region1 1.168e-02 4.077e-02 0.286 0.77457
Frequency1/Region2 -2.880e-02 4.074e-02 -0.707 0.47977
Turn1/Region1 1.272e-01 4.077e-02 3.120 0.00182 **
Turn1/Region2 -1.961e-01 4.074e-02 -4.812 <0.001 ***
Freauency1/Turn1/Region1 5.435e-02 4.077e-02 1.333 0.18253
Freauency1/Turn1/Region2 -5.258e-03 4.074e-02 -0.129 0.89732
(measured in volt) according to Frequency (High vs. Low), Turn (Other-go vs. No-go), Region (Anterior, Central,
Posterior) and their interactions
Fixed effects Estimate Std. Error t value p
(Intercept) -0.44379 0.15681 -2.830 0.009735 **
Frequency1 (Low/High) 0.06180 0.03517 1.757 0.078938
Turn1 (Other/No) 0.10420 0.03517 2.963 0.003064 ***
Region1 (Posterior/Anterior) 1.60961 0.04864 33.094 <0.001 ***
Region2 (Posterior/Central) -1.29616 0.04861 -26.664 <0.001 ***
Frequency1/Turn1 0.07153 0.03517 2.034 0.042011 *
Frequency1/Region1 0.02894 0.04864 0.595 0.551836
Frequency1/Region2 -0.07796 0.04861 -1.604 0.108806
Turn1/Region1 0.11945 0.04864 2.456 0.014092 *
Turn1/Region2 -0.16701 0.04861 -3.436 <0.001 ***
Freauency1/Turn1/Region1 0.04300 0.04864 0.884 0.376705
Freauency1/Turn1/Region2 0.01160 0.04861 0.239 0.811455
(measured in volt) according to Frequency (High vs. Low), Turn (Other-go vs. No-go), Region (Anterior, Central,
Posterior) and their interactions.
Fixed effects Estimate Std. Error t value p
(Intercept) -5.602e-01 1.737e-01 -3.224 0.00390 **
Frequency1 (Low/High) 1.433e-01 3.483e-02 4.115 <0.001 ***
Turn1 (Other/No) 1.288e-01 3.483e-02 3.698 0.00022 ***
Region1 (Posterior/Anterior) 1.008e+00 4.817e-02 20.925 <0.001 ***
Region2 (Posterior/Central) -8.681e-01 4.814e-02 -18.031 <0.001 ***
Frequency1/Turn1 7.024e-02 3.483e-02 2.017 0.04380 *
Frequency1/Region1 -3.427e-02 4.817e-02 -0.711 0.47684
Frequency1/Region2 -2.842e-02 4.814e-02 -0.590 0.55505
Turn1/Region1 1.495e-01 4.817e-02 3.103 0.00193 **
Turn1/Region2 -2.346e-01 4.814e-02 -4.872 <0.001 ***
Freauency1/Turn1/Region1 -8.228e-04 4.817e-02 -0.017 0.98637
Freauency1/Turn1/Region2 6.914e-02 4.814e-02 1.436 0.15102
(measured in volt) according to Frequency (High vs. Low), Turn (Other-go vs. No-go), Region (Anterior, Central,
Posterior) and their interactions.
pictures. The three figures in the upper panel represent the grand averages for other-go trials for anterior, central
and posterior electrodes respec- tively. Red lines represent low-frequency words (LF) and black lines represent
high- frequency words (HF). In the lower panel, the same grand averages are represented for simple no-go trials.
4.2.2 Response-locked activity: Adaptation to the lexico-semantic pattern
The statistical model on robot’s responses on the time-window considered (400-600 ms)
revealed a significant effect of naming level (b = 1.000e-01, s.e. = 2.744e-02, t = 3.646, p =
0.000269) and, relevant here, of the interaction between naming level and block (b = 1.347e-
01, s.e. = 2.744e-02, t = 4.910, p <0.001). Category-level names elicited more positive
amplitudes compared to basic-level names in the first block (b = 0.4695, s.e. = 0.0776, t = 6.050,
p <0.0001). However, this difference disappeared in the second block (b = -0.0693, s.e. =
0.0776, t = -0.894, p = 0.8082). Those results signal 1) different processing ease associated to
each naming condition respectively (first block) and, especially, 2) adaptation to the lexico-
semantic choices of the robot, that became gradually more expected and accepted by
full pair-wise comparison results see the supplementary materials.
Fixed effects Estimate Std. Error t value p
(Intercept) 2.236e-01 1.520e-01 1.470 0.155587
NamingLevel1 (Category/Basic) 1.000e-01 2.744e-02 3.646 0.000269 ***
Block1 (1/2) -5.339e-02 2.744e-02 -1.946 0.051721
Region1 (Posterior/Anterior) -4.543e-01 3.810e-02 -11.925 <0.001 ***
Region2 (Posterior/Central) 2.880e-01 3.761e-02 7.658 <0.001 ***
NamingLevel1/Block1 1.347e-01 2.744e-02 4.910 <0.001 ***
NamingLevel1/Region1 -2.679e-02 3.810e-02 -0.703 0.481989
NamingLevel1/Region2 -1.304e-02 3.761e-02 -0.347 0.728732
Block1/Region1 4.751e-03 3.810e-02 0.125 0.900750
Block1/Region2 -8.292e-02 3.761e-02 -2.205 0.027516 *
NamingLevel1/Block1/Region1 3.026e-02 3.810e-02 0.794 0.427013
NamingLevel1/Block1/Region2 -1.874e-02 3.761e-02 -0.498 0.618371
electrophysiological activity (measured in volt) according to Naming Level (Basic, Category), Block (1,2),
Region (Anterior, Central, Posterior) and their interactions.
of the brain activity elicited when hearing the robot responses across the two blocks. Electrodes were grouped
according to the region (frontal, central, posterior). Black lines represent the basic-level naming, while red lines
correspond to the category naming. In the upper panel, the grand average is related to the first block, while in the
lower panel it is related to the second block.
5. Discussion
In the present study, we explored the electrophysiological correlates associated to co-
representation and partner-adaptation in language processing when performing a joint picture
naming task with a social robot. Participants took turns in naming pictures with Furhat, a
humanoid robot able to produce speech and embody physical emotions. The pictures displayed
on the computer screen represented objects belonging to 15 semantic categories and were
distributed across three turn conditions: a go condition requiring the participant to name the
pictures, an other-go condition in which Furhat named the pictures and a no-go condition in
which none of them spoke. Half of the pictures corresponded to high frequency names, and half
to low frequency names. Importantly, we manipulated the responses of the robot in a way to
have the robot produce the semantic category name of the object (e.g., fruit for the picture of a
pear) instead of the basic-level name (e.g., pear for the picture of a pear) for a regular
subset of trials (5 out of the 15 semantic categories).
Our aim was twofold. First, we wanted to replicate with the robot previous studies in human-
human interaction showing co-representation towards the partner’s task, and find comparable
electrophysiological patterns when preparing to speak and when preparing to listen to an
interactive partner (Baus et al., 2014). We monitored lexical access via the frequency
manipulation reflected in the different processing ease between low and high frequency words.
Based on previous studies using lexical frequency, we expected that, if participants successfully
co-represented robot’s trials, we would find more positive amplitudes corresponding to the
P200 and P300 associated to low frequency words both in go and other-go trials but not in the
no-go trials. Our second objective was related to the manipulation we performed at the robot’s
naming level. We monitored participants’ brain activity when listening to a less predictable
name in comparison to when hearing the expected, basic-level name across the two main parts
of the experiment. In a previous experiment, Cirillo et al. (2022) showed conceptual alignment
towards the same lexico-semantic choice of the robot. In other words, participants’ production
was affected by the robot’s responses, and this was manisfested by the fact that participants
gradually used the category names of the objects in the same context of the robot. In the present
study, we explored this mechanism at the neural level, and aimed to find traces of this
conceptual alignment in comprehension, by comparing the electrophysiological signals related
to basic and category words in the two blocks. In accordance with research focusing on ERPs
associated with processing of deviant and unexpected (lexical, semantic) linguistic events, we
focused on the participants’ brain activity starting around 300/400 ms and hypothesized a
reduced difference in voltage amplitude between the naming level condition from the first to
the second block. This would be linked to semantic processing of less predictable words in the
category condition but not in the basic-level condition (Fitz & Chang, 2019; Hodapp &
Rabovsky, 2021).
The following main results were observed. At the behavioral level, naming latencies on go trials
were faster and more accurate for high compared to low frequency words, replicating previous
results of picture-naming studies using the frequency manipulation (Almeida et al., 2007; Baus
et al., 2014; Navarrete et al., 2006; Strijkers et al., 2010). At the electrophysiological level, go
trials elicited the expected ERP components associated to lexical frequency, that is the P200
and P300 components (Baus et al., 2014; Strijkers & Costa, 2011). Low-frequency words were
associated to larger, more positive waveforms between 200 and 300 ms after stimulus onset.
The analyses performed on no-go trials revealed elicitation of a comparable activity
distinguishing low versus high frequency words peaking around 370 ms for other-go trials,
while no difference was observed in simple no-go trials. Those results suggest that participants
successfully co-represented robot’s task, and they were actively monitoring his upcoming
words.
When analyzing the electrophysiological activity time-locked to the robot’s responses (for both
the basic-level names and the category names), we observed that category names and basic-
level names elicited different amplitudes in the first block. This difference disappeared over the
course of the experiment. This result indicates adaptation towards the robot’s lexico-semantic
choices.
5.1 Task co-representation with robot: picture-locked activity
Our results show that participants co-represented the robot’s task, and engaged in comparable
lexicalization processing when preparing to name the pictures and when the robot was expected
to name the pictures. This is indicated by the fact that we found a frequency effect for other-go
trials (trails in which the robot named the pictures) but not in simple no-go trials (trials in which
no one named the pictures). Our study therefore confirms Baus et al. (2014)’s results for human-
robot interaction, showing that humans are able to co-represent a robot’s verbal actions and
predict relevant lexical properties of his speech. More broadly, our work relates to the accounts
of co-representation within the joint action theory, suggesting that, under shared attention,
activity partners co-represent each other’s action at different moments in the interaction
(Atmaca et al., 2011; Sebanz et al., 2003, 2005) and use their production system to simulate
what the partner is going to say (Pickering & Garrod, 2007, 2013b). In line also with Baus et
al. (2014), we obtained a difference in latency between go and other-go trials for the frequency
effect. Frequency effects appeared later when the other (human or robot) named the pictures
(around 350 ms) than when the participant named the pictures (200-300 ms). This finding was
originally explained by the authors by referring to inhibitory processes participants engage with
to refrain from responding to no-go trials: in this sense, the speed with which lexical access
starts is modulated by whether a word will be finally uttered or not (Strijkers & Costa, 2011).
Here we introduce an alternative interpretation, for which no-go trials trigger a weaker
representation of the item. The less strong representation associated with preparing to speak
compared to preparing to listen (Pickering & Garrod, 2013a) might have hindered a small delay
in online processing.
5.2 Adaptation to robot’s lexico-semantic choices: response-locked activity
When analyzing participants’ electrophysiological activity locked to the robots’ responses, we
observed more positive waveforms starting around 300 ms after response onset between
category names and basic-level names. Those more positive waveforms suggest that
participants found it harder to process words that did not correspond to the more typical and
more expected basic-level names. In our interpretation, those electrophysiological responses
are an index of what is perceived as a deviant response in human-robot interaction. Importantly,
this difference was not observed in the second block, indicating that participants acquired the
robot’s way of responding and adapted to it. From a linguistic point of view, it indicates that
participants were able to adapt to the interlocutor at the conceptual level, and learned to predict
the lexical choices of the robot. Our work relates to research in linguistic alignment within
human-robot interaction (Branigan et al., 2010, 2011; Pearson et al., 2006; Wudarczyk et al.,
2021), in which humans have been shown to copy a robot’s syntactic structures, and words.
Here we were able to reproduce prediction adaptation towards a robot’s word choices at the
conceptual level (Wudarczyk et al., 2021) in comprehension, and therefore our work is
complementary to Cirillo et al. (2022), where this effect was explored in production.
6. Conclusion
The present study investigated the ERP components associated to co-representation and
adaptation to lexical and lexico-semantic properties using a joint picture-naming task between
a participant and a humanoid robot. Taken together, our results provide evidence for two
important electrophysiological aspects emerging after picture presentation and after response
production respectively. First, human participants were able to co-represent the robot’s verbal
actions. This was demonstrated by the parity of activity associated to processing low versus
high frequency words when preparing to speak and preparing to listen compared to simple no-
go trials. Our work therefore replicates that by Baus et al. (2014)’s in human-robot interaction,
possibly suggesting that co-representation and prediction of one’s partner upcoming speech are
so eradicated in the joint language production dynamics that they are present even when the
partner is a robot. Second, our results reveal adaptation to the response pattern of the robot, who
produced the semantic category name instead of the basic-level name of objects for a regular
subset of trials, complementing the work by Cirillo et al. (2022) at the comprehension level.
The electrophysiological responses of the participants were, in fact, diverging between the
basic-level condition and the category-level condition in the first part of the experiment, while
this difference disappeared in the second part of the experiment. To our knowledge, our work
offers the first empirical evidence on language adaptation in human-robot interaction at both
the lexical and the conceptual level using EEG.
Disclosure statement
The authors report there are no competing interests to declare.