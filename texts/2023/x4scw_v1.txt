HOGAN SINGLE-ITEM SCALES 1
Running Head: HOGAN SINGLE-ITEM SCALES
Development of the Hogan Personality Content Single-Items (HPCS) Inventory
Dustin Wood P.D. Harms Ryne A. Sherman
University of Alabama University of Alabama Hogan Assessment Systems
Michael Boudreaux Graham H. Lowman Robert Hogan
Hogan Assessment Systems Kennesaw State University Hogan Assessment Systems
Publication status: In press at Assessment.
Draft date: September 21, 2023
Author Note
This research was supported by the National Science Foundation under Award No. 2121275 to P. D.
Harms and Dustin Wood. The content is solely the responsibility of the authors and does not necessarily
represent the official views of the National Science Foundation. Resources for using or interpreting
Correspondence concerning this article should be addressed to: Dustin Wood, Department of
Management, University of Alabama, Alston Hall, 361 Stadium Drive, Tuscaloosa, AL 35487, United
States. Email: dustin.wood@cba.ua.edu
HOGAN SINGLE-ITEM SCALES 2
ABSTRACT
The Hogan Personality Inventory (HPI) and Hogan Developmental Survey (HDS) are among the most
widely used and extensively well-validated personality inventories for organizational applications,
however they are rarely used in basic research. We describe the Hogan Personality Content Single-Items
(HPCS) inventory, an inventory designed to measure the 74 content subscales of the HPI and HDS via a
single-item each. We provide evidence of the reliability and validity of the HPCS, including item-level
retest reliability estimates, both self-other agreement and other-other (or observer) agreement, convergent
correlations with the corresponding scales from the full HPI/HDS instruments, and analyze how similarly
the HPCS and full HPI/HDS instruments relate to other variables. We discuss situations where
administering the HPCS may have certain advantages and disadvantages relative to the full HPI and HDS.
We also discuss how the current findings contribute to an emerging picture of best practices for the
development and use of inventories consisting of single-item scales.
Keywords: single-item scales; personality assessment; short scales; retest reliability
HOGAN SINGLE-ITEM SCALES 3
Development of the Hogan Personality Content Single-Items (HPCS) Inventory
The Hogan Personality Inventory (HPI) and Hogan Developmental Survey (HDS; Hogan & Hogan, 2009)
have been extensively validated for predicting job performance in high-stakes settings, where respondent
scores are used for selecting applicants for jobs and other organizational purposes (Hogan et al., 1996).
Hundreds of thousands of people from more than 100 countries complete the HPI and HDS instruments
each year for such purposes, with over 11 million total assessments to date. Despite this, the Hogan
inventories are rarely utilized in basic research. Particularly given the aims of measuring job performance,
leadership potential, and other aspects of how individuals behave in organizational settings, the HPI and
HDS have been designed to measure somewhat different personality content than the most commonly-
used personality inventories in basic research. Although Hogan Assessment Systems offers free
administration and scale/subscale scoring of their assessments for academic researchers, the scoring key is
considered critical intellectual property and is not shared. To prevent reverse engineering of the scoring
key and to protect individual privacy, item-level responses are also not shared. The overall length of the
HPI and HDS instruments may further lead many basic researchers to use other instruments for their
investigations. Ultimately this serves to further increase the misalignment between basic researchers and
practitioners (Kaufman, 2022).
Here we describe the development of the Hogan Personality Content Single-Items inventory, or
HPCS, to address these limitations and to help bridge the gap between assessment tools used in applied
settings and those used in basic research. Within the HPCS, each of the 74 subscales within the HPI and
HDS â€“ which are assessed by 3 to 6-item scales referred to as Homogenous Item Clusters, or HICs â€“ are
assessed by a single item, resulting in an inventory approximately one-fifth the length of the combined
HPI and HDS instruments. We continue by describing some reasons for developing single-item
inventories in general, and then for doing so more specifically to assess the personality content within the
HPI and HDS instruments.
Why Create an Inventory Consisting of Single-Item Scales?
HOGAN SINGLE-ITEM SCALES 4
There are a number of personality inventories that have been designed to consist entirely of
single-item scales. Examples include the Adjective Check List (ACL; Gough, 1960); the California Adult
Q-Sort (CAQ) and California Child Q-Sort (CCQ)(Block, 1961); the Shedler-Westen Assessment
Procedure (SWAP; Shedler & Westen, 2007); the Riverside Situation Q-Sort (RSQ; Funder, 2016) and
Riverside Behavior Q-Sort (RBQ; Furr et al., 2010); and the Inventory of Individual Differences in the
Lexicon (IIDL; Wood et al., 2010). There are also numerous single-item scales that have been developed
to assess more specific constructs, such as the Big Five personality traits (Denissen et al., 2008; Woods &
Hampson, 2005) and associated facets (Soto & John, 2017a), narcissism (Konrath et al., 2014), trait
empathy (Konrath et al., 2018), self-esteem (Robins et al., 2001), and job satisfaction (Wanous et al.,
1997).
Single-item scales are often offered as â€˜scales of last resortâ€™ â€“ or scales that should be used almost
exclusively when corresponding longer measures cannot be administered due to limited time and
resources. For instance, in presenting a 15-item inventory to assess the 15 traits found on the BFI-2 with
an item apiece, the scale developers suggest administering this instrument â€œwould clearly be better than
not measuring personality at allâ€ (p. 77), but use the final sentence of their article to emphasize â€œfor most
studies, however, we recommend administering the full measure due to its greater reliabilityâ€ (p. 79)(Soto
& John, 2017a).
We depart from the â€˜scales of last resortâ€™ stance and describe some positive arguments for the use
of inventories comprised of single-item scales, and how they can be developed and used more effectively
(Block, 1961; Condon et al., 2020; de Vries et al., 2016; Fisher et al., 2016; Matthews et al., 2022). As we
detail, single-item scales are not just â€˜not as badâ€™ as people might think relative to longer multi-item
scales, but have some advantages which are unshared with multi-item scales.
Brevity of Administration
The most salient and widely understood advantage of inventories comprised of single-item scales
to many researchers is simply their practicality. For instance, completing the current HPI and HDS
HOGAN SINGLE-ITEM SCALES 5
instruments requires participants to rate 374 items. Having respondents rate a single item for each of the
74 HICs contained within these instruments would result in an instrument less than one fifth this length.
The advantages of shorter inventories should not be readily dismissed. Since ultimately time is a
limited commodity for both the survey administrators and respondents, the use of shorter inventories
opens opportunities to assess additional content within the survey. Or it can just result in a shorter overall
assessment. This in turn can result in higher quality data in many situations, as longer surveys often
increase careless or insufficient effort responding (Bowling et al., 2022; DeSimone et al., 2015). Shorter
surveys can also open opportunities to collect data from individuals who otherwise would be unlikely to
complete longer assessments at all (Bergkvist & Rossiter, 2007; Dejonckheere et al., 2022). For instance,
it is sometimes recommended to not offer financial incentives when collecting observer reports, as
incentives can increase invalid responding (Vazire, 2006). We may reasonably expect a considerably
higher response rate if such volunteers are asked to complete a survey requiring about 10 to 15 minutes to
complete rather than one requiring closer to an hour.
The Reliabilities of Single-Item Scales are Estimable, and Typically Decent
A major reservation toward the use of single-item scales has been the question of how to estimate
their reliability, , which concerns â€œthe correlation of a measure with itselfâ€ (John & Soto, 2007, p.
ğ‘‹ğ‘‹ğ‘‹ğ‘‹
ğœŒğœŒ
464) or with â€œa test just like itâ€ (Revelle & Zinbarg, 2009, p. 147). Reliability estimates are most typically
formed through internal consistency statistics, such as coefficients alpha (Cronbach, 1951) or omega
(McDonald, 1999; Revelle & Condon, 2019) which largely concern how items within a scale relate to one
another. With single-item scales, of course, there are no other scale items to estimate inter-item
correlations, which makes such internal consistency statistics inestimable.
Some authors have shown how internal consistency-type reliability estimates can be provided for
single-item measures (e.g., de Vries et al., 2016; Denissen et al., 2008; Wanous & Hudy, 2001). However,
we argue that the more important advance has been a recent understanding that retest correlations may be
more appropriate indices of measurement reliability. McCrae and colleagues (2011) noted that the square
root of a measureâ€™s reliability serves as the expected upper-limit of its ability to correlate with other
HOGAN SINGLE-ITEM SCALES 6
variables: . As such, the quality of a reliability indicator can be evaluated in part by how
ğ‘‹ğ‘‹ğ‘‹ğ‘‹ ğ‘‹ğ‘‹ğ‘‹ğ‘‹
ğœŒğœŒ â‰¤ ï¿½ğœŒğœŒ
well the indicator correlates with validity-related criteria, such as self-other correlations or heritability
indices. Various authors have now shown that retest correlations over short intervals better track variation
in validity-related criteria than do internal consistency indices such as coefficient alpha (de Vries et al.,
2016; Henry et al., 2022; Lowman et al., 2018; McCrae et al., 2011).
A major advantage of retest correlations is that they can be estimated for scales of any length â€“
including single-item scales. The reliability of single-item scales is increasingly indexed as the itemsâ€™
retest correlations over short intervals, such as a couple of days or weeks (de Vries et al., 2016; Henry et
al., 2022; Lowman et al., 2018; Matthews et al., 2022; Wood et al., 2010). When retest correlations are
estimated in this manner, they have generally been found to be higher than many researchers may expect.
For instance, Henry et al. (2022) found the 100 items of the HEXACO-100 to have a mean retest
correlation of .65 over 13 days, compared to a mean of .81 for the 25 four-item scales of HEXACO facets
formed from the same items. Similarly, Wood et al. (2010) found the 61 items of the IIDL instrument to
have a mean retest correlation of .62 over 4 days.
Given that the correlations between items within a multi-item scale are frequently in the
neighborhood of .40 (e.g., Clark & Watson, 1995, 2019), the fact that retest correlations of single-item
scales over such intervals may tend to exceed .60 may strike some researchers as surprising. Further, for
many common research purposes, retest correlations estimated over even a couple days should be
regarded as lower-bounds of the scaleâ€™s reliability, as such estimates will underestimate how highly the
scale may correlate with other scales administered within a larger survey due to occasion-specific
variance (Chmielewski & Watson, 2009; Lowman et al., 2018; Wood et al., 2023).
â€œHit the Nail on the Headâ€ Item-Development Strategy
When using single-item scales, it is generally possible to create an item that indicates the intended
construct in a fairly direct and face-valid manner (Matthews et al., 2022; Wood et al., 2010). For instance,
Robins et al. (2001) developed a single-item self-esteem consisting of the item â€œI have high self-esteemâ€;
HOGAN SINGLE-ITEM SCALES 7
and Konrath and colleagues (2014, 2018) developed a single-item narcissism scale consisting of the item
â€œI am a narcissistâ€ and an empathy scale consisting of the item â€œI am an empathetic person.â€ As these
examples indicate, single-item scales can be created to largely serve as a direct self-report of the construct
of interest (Gough, 1960; Wood et al., 2010), which we will call the â€˜hit the nail on the headâ€™ strategy.
This strategy appears to be somewhat implicit or accidental in the development of single-item
measures, and differs a bit from how multi-item personality scales are frequently created. For instance, if
a scale developer is interested in creating a multi-item scale of sociability, they may use items to measure
some of the behaviors more sociable people are more likely to do, such as talk to strangers in lines, make
friends easily in new environments, or to enjoy crowds. In contrast, using the â€˜hit the nail on the headâ€™
strategy may instead involve asking people to report directly on whether they have the trait understood as
the common element of these and other behaviors, either by measuring agreement with a direct self-report
of the trait (â€œare you sociable?â€), or with a description of the traitâ€™s central defining characteristics (e.g.,
â€œdo you enjoy interacting with other people?â€).
This very direct approach could be regarded as problematic when creating a scale consisting of
more than one item, as the resulting items are likely to be highly synonymous or semantically similar to
one another, which scale developers sometimes prescribe avoiding to prevent the creation of â€œbloated
specificâ€ factors (Boyle, 1991; Cattell & Tsujioka, 1964; Oltmanns & Widiger, 2018). However, we
argue this is a valuable strategy when the intention is ultimately to use a single item to measure the
construct â€“ or at least, the individualâ€™s perceived level of the construct. Indeed, the use of direct, face-
valid items is recommended for the assessment of certain constructs (Bergkvist & Rossiter, 2007;
Burisch, 1984; Thurstone, 1928).
Ability to Present the Full Scale as Seen by Respondents
Many issues within behavioral science research involve instances where the label used to
represent the mean or sum of a multi-item scale fails to make clear the specific content seen and rated by
respondents. Differences between scale content and scale labels form the essence of the well-known
jingle and jangle problems (Block, 1995; Gonzalez et al., 2021).
HOGAN SINGLE-ITEM SCALES 8
On the jingle side: scale labels regularly obscure the existence of semantically redundant items
across different scales being correlated. For instance, Nicholls and colleagues (1982) noted that the
relatively large association between commonly used self-report measures of masculinity and assertiveness
could be largely attributed to the fact that many of the items across the two measures were virtually
redundant. Problems of this sort continue to persist, with researchers questioning the meaningfulness of
operationalizations of transformational and charismatic leadership (van Knippenberg & Sitkin, 2013),
Machiavellianism and psychopathy (Miller et al., 2017), job engagement and other job attitudes (Newman
et al., 2010; Newman & Harrison, 2008), psychological grit and Conscientiousness (CredÃ© et al., 2017),
among others. The problem is exacerbated by the regular use of â€œbroadâ€ scales, where a broad range of
specific items are averaged to form a single scale score, which regularly obscures the existence of
semantically similar items forming the scale scores being correlated (MÃµttus, 2016).
On the jangle side: diverging correlations in how different measures with the same label relate to
a variable of interest can be due to systematic differences in item content. For instance, differences in how
various â€˜Extraversionâ€™ scales relate to gender (Costa et al., 2001; Feingold, 1994) have been attributed to
differences in content emphases within the broader Extraversion domain (e.g., assertiveness, positive
affectivity, sociability). More generally, there is considerable variation in the nature of what different Big
Five scales measure, which largely concerns variation in content emphases (Pace & Brannick, 2010).
One way to see whether both jingle and jangle fallacies may be in operation is simply to â€˜eyeballâ€™
the items within the scales to see what content is represented and whether this content is overlapping (e.g.,
MÃµttus, 2016; Newman & Harrison, 2008; Nicholls et al., 1982). Single-item scales provide a means of
making such inspections easier: when a scale consists of a single item, it is possible to simply not create a
separate label for the scale at all â€“ and instead label the scale using the exact item as seen by
participants. This can make it clearer to consumers of the research â€“ including the investigators
themselves â€“ how specific properties of the stimuli rated by the respondents could account for observed
phenomena.
Why Create an Inventory of Single-Item Scales for the HPI and HDS Inventories?
HOGAN SINGLE-ITEM SCALES 9
We continue by describing three additional reasons to create a short inventory focused on the
content found within the HPI and HDS inventories in particular.
Connecting Basic and Applied Personality Assessments
A range of commonly-used brief personality inventories â€“ such as the 60-item BFI-2 (Soto &
John, 2017b), the 60 and 100-item versions of the HEXACO (Ashton & Lee, 2009; Henry et al., 2022),
and the 60-item NEO-FFI (Costa & McCrae, 1992) â€“ were designed to assess core vectors of individual
differences identified from factor analyzing self-ratings of common lexical terms, following an
understanding that these vectors may be particularly socially important (Goldberg, 1993; Saucier &
Goldberg, 2001; Wood, 2015). In contrast, the HPI and HDS instruments were designed specifically to
assess key traits within socioanalytic theory (Hogan, 1982, 1991). Within socioanalytic theory,
individuals are expected to have greater success in groups to the extent that they have more specific traits
that facilitate â€˜getting alongâ€™ and â€˜getting aheadâ€™ in interpersonal contexts. The focus on assessing traits
theorized to facilitate or inhibit success in groups and organizational settings has resulted in the HPI and
HDS assessing many traits that are less emphasized within surveys designed to measure lexically-derived
personality factors â€“ such as competitiveness, conformity, leadership capacity, and mastery orientation.
Although dimensions resembling the Big Five can be extracted from the HPI and HDS instruments
(Hogan, 1996), the greater focus on assessing traits relevant to success in organizational settings could
result in an associated inventory of single-item scales being better positioned to predict such outcomes.
Assessing More Dysfunctional Individual Differences
Research has shown that measures of traits which concern more dysfunctional and antisocial
patterns of behavior often add substantially to the prediction of outcomes such as job performance, leader
effectiveness, and the quality of interpersonal relationships (Harms & Sherman, 2021; Padilla et al.,
2007). Brief measures in this space frequently focus specifically on assessing the â€˜Dark Triadâ€™ dimensions
of narcissism, psychopathy, and Machiavellianism (e.g., Jonason & Webster, 2010; Jones & Paulhus,
2014; Paulhus et al., 2021). In contrast, the HDS instrument (Hogan & Hogan, 2001) was designed to
assess subclinical levels of the personality disorders found in Axis II of the fourth edition of the
HOGAN SINGLE-ITEM SCALES 10
Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; American Psychiatric Association,
1994). Some of these scales are indeed similar to Dark Triad dimensions â€“ for instance, the Bold scale
broadly reflects narcissistic tendencies, while the Mischievous scale is broadly similar to psychopathy.
However, the HDS assesses many additional dimensions in this space; for instance, the Cautious scale
can be regarded as a subclinical variant of Avoidant personality disorder, reflecting tendencies toward
being overly indecisive and risk-averse; and the Diligent scale can be regarded as a subclinical variant of
Obsessive-Compulsive personality disorder, reflecting tendencies toward being perfectionistic and
micromanaging.
The HDS scales have been found to regularly correlate highly (rs â‰¥ .50) with matching
personality disorder scales on instruments such as the MMPI (Hogan & Hogan, 2001). However, they
were not designed for use in clinical settings, but rather to assess dysfunctional or self-defeating patterns
of behavior that may derail oneâ€™s relationships or career. These derailing tendencies are thought to
manifest when individuals are either unable or unwilling to engage in reputation management by
inhibiting their darker impulses.
Better Operationalizing the Distinction Between Identity and Reputation
Socioanalytic theory distinguishes between a personâ€™s identity and reputation â€“ how the person
sees oneself and how the person is seen by others, respectively, and argues that a personâ€™s reputation
should ultimately be the more predictive of their performance (Hogan & Blickle, 2018; Roberts & Wood,
2006). In practice, the HPI and HDS instruments aim to indicate a personâ€™s reputation indirectly through
their responses to self-report items â€“ for instance, a personâ€™s extraversion may be indicated by their self-
reported tendencies to go to parties, speak up in meetings, and talk to strangers â€“ actions which may
reveal how extraverted the person is seen by others even if the person does not personally identify as an
extravert (Hogan & Nicholson, 1988). However, a more direct operation of reputation would be to simply
ask other observers who know the person to describe whether the person is an extravert.
However, the length of the full HPI and HDS instruments has made the collection of observer
reports relatively infrequent. A considerable briefer inventory would make it much easier to collect
HOGAN SINGLE-ITEM SCALES 11
observer reports. In turn, a personâ€™s trait identity can be operationalized more directly as the personâ€™s self-
reported level of the trait, whereas their trait reputation can be operationalized as the personâ€™s mean
reported level of the trait from observer reports (compare to Connelly et al., 2022). This in turn can
increase the ability to see how self-perceptions and observer-perceptions of the traits assessed by the HPI
and HDS instruments differ. For instance, we will explore whether self-ratings show a lower tendency to
track trait desirability estimates than observer-ratings made by friends and family, as has been found in
other personality inventories (Kim et al., 2019), or whether scales concerning more observable traits tend
to have higher mean ratings (Funder & Colvin, 1997; Wood, 2015) and self-other agreement (Funder &
Dobroth, 1987; Watson et al., 2000).
Method
Stage 1: Development of the HPCS Instrument
We created one item for each of the narrow HICs assessed by the HPI and HDS personality
inventories (Hogan & Hogan, 2007, 2009). Each HIC consists of a 3 to 6-item scale designed to measure
a single narrow characteristic. The 7 primary dimensions of the HPI each contain 4 to 8 HICs, totaling 41
subscales, whereas each of the 11 dimensions assessed by the HDS contain scales for 3 HICs, for a total
of 33 subscales. There were thus a total of 74 narrow dimensions that would be represented by one item
each within the new instrument.
A complete list of the HPI and HDS subscales and of the items created to measure these subscales
create the HPCS items below.
Item Format
We created each item to be rated under the general stem â€œIs someone whoâ€¦â€ â€“ similar to stems
used in inventories such as the BFI and BFI-2 (e.g., â€œI am someone whoâ€¦â€; John & Srivastava, 1999;
Soto & John, 2017) and in the International Personality Item Pool (IPIP; Goldberg et al., 2006). This stem
was chosen in part to result in items that could be presented without alteration both for self-ratings and for
HOGAN SINGLE-ITEM SCALES 12
observer-ratings by only making the general stem slightly more specific â€“ for instance â€œI am someone
whoâ€¦â€ for self-ratings, and â€œ[target] is someone whoâ€¦â€ for observer-reports.
Each item was formed to fit a â€œlabel (subclause)â€ format â€“ with a short abstract label followed by
directly from the label given to the HIC within the HPI and HDS technical manuals. We additionally
examined descriptions of HICs within the manuals and supporting guides (Hogan Assessment Systems,
2016) to assist in writing the itemâ€™s subclause in a manner corresponding to the intended emphasis of the
HIC. For instance, within the HPI technical manual high scorers on the Easy to Live With subscale are
described as â€œTolerant and easygoing natureâ€; the item we created to indicate this subscale was â€œis Easy
to Live With (easy-going/tolerant nature; works well with others).â€ We intended the â€œlabel (subclause)â€
format of the items to function somewhat analogous to word (word definitions) linkages within
dictionaries, where the subclause served to help ground and clarify intended meaning of the broader label
through somewhat more concrete terms. We additionally tried to make the label and subclause largely
synonymous to help focus the meaning of the broader item (Wood et al., 2010), and so researchers could
generally report just the label component when presenting study results (as we will illustrate in
subsequent tables and figures) with minimal drift from the meaning participants understood from reading
and responding to the complete item.
Additional Considerations in Item Development
Removal of negations. A number of the HICs involved labels containing negations, such as â€œNot
Anxiousâ€ and â€œNo Hostility.â€ We removed all negations from the items created for this inventory, as
items containing negation terms are thought to be more difficult for respondents to process and rate
correctly, and have been associated with lower levels of properties associated with item reliability and
validity â€“ such as retest correlations and self-other agreement (de Vries et al., 2016; McCrae et al., 2011).
For the HPCS items, we usually simply removed negations found in the HIC labels, which
reversed the direction of high scores on the dimension from desirable to undesirable. For instance, the
HIC â€˜Not Anxiousâ€™ was assessed by the item â€œis Anxious (prone to anxiety or worry)â€. These reversals
HOGAN SINGLE-ITEM SCALES 13
resulted in an increase in the percentage of items in which high scores indicated undesirable traits. This is
often seen as a desirable property of an inventory, in part as inventories with a better balance in the
percentage of items indicating positive and negative characteristics increase the ability to use participant
response patterns such as mean ratings, normativeness, and indices of response similarity over retests as
data quality screens (Henry et al., 2022; Soto et al., 2008; Wood et al., 2017).
Item length. We additionally aimed to keep the total length of the item relatively short to
encourage more frequent reporting of the complete item by researchers, and additionally given that
shorter items likely facilitate both quicker and more consistent responding among participants (de Vries et
al., 2016). Items in the HPCS ranged from 33 to 99 characters, with a mean of 66. As items will
sometimes be shown in results with just the label portion (i.e., by removing the â€˜subclauseâ€™ portion of the
item given in parentheses), we also aimed to keep the abstract label portion of the item particularly short;
the abstract label portion of the HPCS items ranged from 7 to 26 characters, with a mean of 16.
Stage 2: Validational Evidence of HPCS Instrument
We utilized data from two samples to examine properties of the resulting HPCS items, as
described below. Materials administered to Sample 1 were approved under University of Alabama IRB
#22-09-5972. Sample 2 was conducted by a research team at Hogan Assessment Systems, and the
materials administered were reviewed internally by the research team to avoid ethically questionable
research practices.
Sample 1 afforded the ability to estimate item-level retest stability and self-other agreement, to
provide reliability and validity evidence of the measures. Respondents in Sample 2 completed both the
HPCS and HPI and HDS instruments, allowing estimation of the convergent validity with the full
instruments the HPCS was designed to assess. Sample 2 respondents additionally completed measures of
organizational attitudes and experiences, which afforded the ability to examining whether the HPCS items
and corresponding HICs from the full HPI/HDS instruments performed similarity in predicting
organizational criteria. Below, we report how we determined our sample size, all data exclusions, all
manipulations, and all measures in the study.
HOGAN SINGLE-ITEM SCALES 14
Sample 1: Undergraduate Management Students
The HPCS was collected as part of a larger data collection effort in which students in an
undergraduate management course completed one major survey each week for five weeks on the
Qualtrics platform. Students completed the surveys for course participation credit, and completed the
HPCS in both the first and fourth week of the larger data collection.
HPCS self-ratings. Items were rated on a continuous slider scale, anchored from â€œStrongly
Disagreeâ€ to â€œStrongly Agreeâ€, with a scale midpoint of â€œNeither Agree Nor Disagreeâ€, which were
scaled to have scores ranging from 0 to 100. Each of the 74 items was presented in one of five sets, with
four sets containing 15 items, and one set containing 14 items. The order in which participants rated the
five sets was randomized, and the order of the items within each sets was additionally randomized. All of
the HPCS items were rated under the general stem â€œI am someone whoâ€¦â€ A total of 377 students
completed the first administration of the HPCS, and 341 students completed the second administration.
Note that some of the students who completed the second administration did not complete the first.
Participants were excluded if they failed either a response time or response variability screen. We
describe these screens below, and the number and percentage of respondents failing each screen during
the first and second administration of the HPCS.
Low response time. For each of the five pages of HPCS items, the time of the first and last click
on the page was saved. This makes it possible to compute a seconds-per-item index; participants were
excluded if they were indexed as completing any of the five sets of items at a rate of faster than 1-second-
per-item, which has been found to be a valid indicator of careless or inattentive responding for items
consisting of short phrases or sentences (Jaso et al., 2022; Wood et al., 2017). A total of 25 participants
(6.6%) failed the response time screen during the first administration of the HPCS; and 28 (8.2%) failed
this screen during the second administration.
Low response variability; = 20 (5.3%). The percentage of maximum response variability was
ğ‘µğ‘µ
indexed for each participant by this equation: , where indicates the within-person
ğ‘ğ‘ ğ‘ğ‘
2ğ‘ ğ‘  /(ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š âˆ’ ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š) ğ‘ ğ‘ 
standard deviation in scores across the 74 items, and max and min indicate the minimum and maximum
HOGAN SINGLE-ITEM SCALES 15
scale points (here: 100 and 0). This index will range from 0% if participants provide exactly the same
score to all questions to 100% if participants only responded by using the minimum and maximum scale
values (here: 0 and 100) and used each 50% of the time. Participants were excluded if their response
variability was estimated as equal or less than 25% of the maximum possible.
A total of 20 participants (5.3%) failed the response variability screen during the first
administration of the HPCS; and 29 (8.5%) failed this screen during the second administration.
When coded dichotomously (failed screen = 1, passed screen = 0), the two screens were highly
positively correlated both during the first administration of the HPCS (r = .51), and the second
administration three weeks later (r = .52). A total of 33 of the 377 respondents (8.8%) of the first
administration of the HPCS were excluded due to failing at least one screen. A total of 41 of the 341
respondents (12.0%) of the second administration of the HPCS failed at least one screen and were
excluded.
Of the 344 who completed the first assessment, 196 (57%) were male and 148 (43% were female.
On average, participants were = 20.8(1.5) years old; range 19 to 35. The distribution of ethnic
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
identities was as follows: 270 (78.5%) White, 33 (9.6%) Black or African American, 23 (6.7%) Asian, 6
(1.7%) Hispanic, and 12 (3.5%) reported other ethnicity or did not respond to the question.
HPCS peer-ratings. After providing self-reports on the HPCS, participants were asked to
provide the names of individuals who could provide observer reports. Specifically, participants read:
â€œPlease provide the names of up to 6 individuals who know you at least relatively well
and provide their contact information. Try to include at least some individuals who may
know you outside of UA (for instance: parents/siblings, high school friends, coworkers).
They will be contacted only once to provide an independent description of your
personality on the items you just rated.
HOGAN SINGLE-ITEM SCALES 16
Their ratings will also be used for in-class exercises. Their ratings will not be
shared with you, and their participation (or non-participation) will not affect your course
credit.â€
When the individuals who the participants had listed were contacted, they received the following
instructions:
â€œPlease read the following before continuing to the next page.
You have been nominated by [target] as someone who can describe their
personality. As part of the activity, you will be asked to complete a personality survey
describing [target]. The survey usually takes participants about 10 minutes to complete,
and should not take more than 15 minutes.
The responses you provide will be used as part of an in-class exercise which
concerns how much individuals and the people that know them agree about descriptions
of their personality. Please note that [target] will not be able to see your specific ratings,
so please feel free to answer honestly.
As we value your privacy, we will not contact you again. However, we hope you
will agree to help with this exercise!
To begin the activity, please select â€œContinue to surveyâ€ below.â€
The same response time and response variability screens described above were also used to screen
observer ratings; these were again highly correlated (r = .60). However, the percentage of observer ratings
which failed the screens was considerably lower than the self-ratings, with only 5 of the 420 observer
reports (1.2%) failing each screen, and only 7 (1.7%) failing at least one screen, resulting in 413 observer
reports included in the analysis.
Of the 344 participants who completed the first self-report of the HPCS, 200 (58%) had at least
one observer report. More specifically, 86 participants had exactly one observer report, 63 had two, 22
had three, 19 had four, 6 had five, and 4 had six.
HOGAN SINGLE-ITEM SCALES 17
These observers were asked to indicate the nature of their relationship with the participant; the
response options provided were not mutually exclusive and each observer could check multiple options.
46% of observers described themselves as the participantâ€™s friend, 2.9% as their current coworker, 1.9%
as a past coworker, 1.7% as their manager or boss, 7% as their romantic partner, 32.4% as their parent or
guardian, 9.9% as their sibling, 3.4% as an extended relative, 6.3% as their fraternity brother or sorority
sister, and 2.9% as â€˜otherâ€™, which included grandparents, roommates, or teammates.
Sample 2: Hogan Assessment Systems Online Panel
A second sample was collected from MTurk by the research team at Hogan Assessment Systems.
The sample began with 2,051 workers who were invited to â€œcomplete a survey about your attitudes and
behaviors.â€ Workers were required to live in the United States and to have a HIT (Human Intelligence
Task) approval rate to be greater than 95%.
From this initial sample, 407 workers (19.84% of the initial sample) were selected to participate
in a multi-part, yearlong study. In order to be selected, workers had to (1) be employed at least part-time,
(2) work at least 20 hours per week (outside of MTurk), (3) report a valid job title (workers who listed
MTurker, student, or retired were not included), (4) speak English as their primary language, and (5) be
willing to take multiple surveys over the course of one year. In addition to these criteria, workers needed
to correctly answer at least 16 of 48 (33%) true or false questions on a speeded cognitive assessment.
Workers were paid $8.50 per hour to complete study materials over the course of a year.
HPI/HDS assessments. The MTurk participants completed both the full HPI and HDS
instruments in addition to the HPCS, allowing us to test convergent validity with full scales. The HPI and
HDS were completed by almost all participants on a day in early February, whereas almost all
participants completed the HPCS on a day at the end of August, resulting in a measurement interval of
202 days (or 6.7 months) separating these measurements for most participants.
HPCS assessment. A total of 287 participants who had completed the earlier HPI/HDS
instruments went on to complete the HPCS assessment. Of these, 151 (52.6%) were male and 135 (47%)
were female; one participant did not disclose their biological sex. On average, participants were
HOGAN SINGLE-ITEM SCALES 18
= 42.8(11.8) years old; range 21 to 84. The distribution of ethnic identities was as follows: 227
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
(79.1%) White, 24 (8.4%) Asian, 13 (4.5%) Black or African American, 11 (3.8%) Hispanic, and the
remaining 12 (4.2%) reported other ethnicity or did not respond to the question.
Thirteen of the 287 participants (4.5%) were removed from data analyses because they either
responded to the questions too quickly (i.e., less than 1 second per question, on average) or entered an
incorrect sequence of letters and numbers to prove their human identity.
Between Samples 1 and 2, nine HPCS items were modified slightly in consultation with the
research team at Hogan Assessment Systems; the changes were made to clarify the item by removing
difficult terms or to bring the item closer to the understood focus of the HIC it was designed to assess.
The largest change consisted of changing the item â€œis Satisfied (satisfied/content with oneâ€™s
circumstances)â€ to â€œis Dissatisfied easily (has many complaints),â€ as this reversed high scores from
indicating satisfaction to dissatisfaction. More moderate differences included the item â€œis Tough
(interpersonally cold; focused on tasks rather than people)â€ shifting to â€œis Unsympathetic (interpersonally
cold; shows little sympathy for others' problems)â€, and the item â€œis Highly Confident (believes is capable
of accomplishing anything they set their mind to)â€ shifting to â€œis Overconfident (believes is capable of
accomplishing more than realistically possible).â€ We have indicated the 9 items that differed from Sample
The HPCS items were rated on a 7-point scale with scale anchors of â€œStrongly Disagreeâ€ and
â€œStrongly Agreeâ€, and a midpoint of â€œNeither Agree nor Disagreeâ€. Scores were converted to a 0-to-100
percentage of maximum possible (POMP; Cohen et al., 1999) metric to make item means and standard
deviations more directly comparable to Sample 1. The full set of inter-item correlations of the HPCS
items observed in the two waves of Sample 1 and in Sample 2 are provided in the Supplemental Tables
S3-5.
Job/organization experience scales. Respondents from the MTurk sample also completed a range
of measures about their employment experiences, including organizational commitment (Allen & Meyer,
1990), job satisfaction (Eid et al., 2008), task performance (Goodman & Svyantek, 1999), organizational
HOGAN SINGLE-ITEM SCALES 19
citizenship behaviors (Lee & Allen, 2002), counterproductive work behaviors (Fox & Spector, 1999),
burnout (Demerouti et al., 2010), and work engagement (Schaufeli et al., 2006). These measures were
administered 62 days after the full HPI and HDS assessment, and 140 days before the HPCS.
Coding HPCS Item Properties
Following the collection of data from these two samples, the HPCS items were rated for their
desirability and observability. A total of 13 research assistants rated both the desirability and
observability of the HPCS items.
Item desirabilities. Raters were instructed to â€œPlease indicate the extent to which you understand
it as desirable or undesirable to be someone who has the trait belowâ€, on a scale with points labeled 1 =
â€œVery Undesirableâ€, 2 = Somewhat undesirableâ€, 3 = â€œEqually desirable/undesirableâ€, 4 = â€œSomewhat
desirableâ€ and 5 = â€œVery desirableâ€. The average inter-rater agreement was .67, indicating raters showed
very high agreement on which items were desirable and undesirable. The resulting mean was estimated to
have an alpha reliability of Î± = .96, which can be interpreted as indicating the mean would be expected to
correlate .96 with a mean created from a set of 13 new raters. The items coded as least desirable were
â€œFeels Entitledâ€, â€œis Hostileâ€, and â€œFeels Underappreciatedâ€; the items coded as most desirable were â€œis
Mastery-Orientedâ€, â€œis Well-Accomplishedâ€, and â€œhas a Good Memoryâ€.
Item observabilities. Raters were instructed: â€œSome traits refer to attributes that can be easily
observed by an outside observer. Other traits refer to attributes that are much more difficult to perceive
accurately about someone. For each term, consider how easy or difficult you think it is to observe a
personâ€™s level of this characteristic.â€ Ratings were made on a 5-point scale with endpoints labeled with
1 = â€œvery difficult to observe by an outside observerâ€ and 5 = â€œvery easy to observe by an outside
observerâ€, with the remaining points unlabeled. The average inter-rater agreement was .28, suggesting
modest agreement on which items were more and less observable, and the resulting mean was estimated
to have an alpha reliability of Î± = .84, which can be interpreted as indicating the mean would be expected
to correlate .84 with a mean created from a set of 13 new raters. The HPCS items coded as least
observable were â€œFeels underappreciated,â€ â€œis Passive-Aggressiveâ€, and â€œis Manipulativeâ€, whereas the
HOGAN SINGLE-ITEM SCALES 20
items coded as most observable were â€œis Emotionally Volatileâ€, â€œis Entertainingâ€, and â€œis Strictly
Organizedâ€.
Results
Factor Structure of the HPCS
We conducted a factor analysis of the items from the data collected in Sample 1. As there were
two waves in which the HPCS was administered, these ratings were averaged, both to create more stable
estimates of the participantâ€™s self-rating, and to maximize sample size as some participants completed
only the first or second wave, resulting in 403 participants included in the analysis. Note that averaging
scores in this manner will tend to increase eigenvalues somewhat by increasing score reliability. We
conducted a factor analysis using principal axis factoring (PAF) and varimax rotation using the
fa
function of the package in (Revelle, 2017). The first ten eigenvalues were 10.1, 7.1, 6.1, 4.5,
psych R
3.2, 2.4, 1.7, 1.7, 1.5, and 1.4. We focused on the five-factor solution both to compare the nature of the
resulting factors to the Big Five dimensions, and as the six-factor solution did not yield an easily
interpretable additional dimension (the largest loading of the sixth factor in this solution was .445, and
there were only 4 items with loadings above .400 in magnitude).
mapped to the Big Five, however they appeared to differ moderately from the usual Big Five content
dimension, labeled as Volatile, maps loosely to the Big Fiveâ€™s Neuroticism dimension, but with a greater
emphasis on angry affect. The second dimension, labeled as Sociable, maps well to the Big Fiveâ€™s
Extraversion dimension, blending descriptions of liking and feeling confident or comfortable in social
situations. The third dimension, labeled as Diligent, maps loosely to the Big Fiveâ€™s Conscientiousness
dimension, but with greater emphasis on achievement orientation and on ambitious and perfectionistic
tendencies. The fourth dimension, labeled as Sensitive, maps loosely to the Big Fiveâ€™s Agreeableness
dimension, particularly with the emphasis on empathic and caring tendencies, but also placing
considerable emphasis on tendencies to feel guilt-prone, anxious, and fearful. The fifth dimension, labeled
HOGAN SINGLE-ITEM SCALES 21
as Creative, maps loosely to the Big Fiveâ€™s Intellect or Openness dimension, with items reflecting self-
descriptions of being uniquely creative and gifted, but also of being eccentric and risk-taking.
More generally, the factors extracted from the HPCS appeared to have somewhat more
maladaptive content emphases than are usually found in Big Five models or in five-factor extractions of
personality inventories. This is likely due to the HPCSâ€™s greater emphasis on assessing more
dysfunctional traits to represent the content assessed by the HDS. For instance, Conscientiousness veers
toward perfectionism, Agreeableness toward oversensitivity, and Neuroticism toward emotional volatility.
However, some of these content emphases â€“ such as anger and perfectionism â€“ are more emphasized
within the 6-factor HEXACO model of personality (Ashton et al., 2014).
Properties of HPCS Items
Reliability and Validity Evidence of HPCS Items
including means and standard deviations from each sample, three-week retest correlations, self-other
agreement, observer (or other-other) agreement.
Three-week retest stability. The three-week retest stability of the HPCS items ranged from a
low of .36 for â€œFeels Uniquely Sensitive (believes one has unique abilities to understand issues/people)â€
to a high of .77 for â€œhas Math Ability (works well with numbers)â€; = .57 (.09). Note that the
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
three-week retest correlations can be understood as an estimate of the lower-bound of how highly the item
should be able to correlate with other variables administered over shorter measurement intervals â€“ such as
with other measures administered within the same survey (McCrae, 2015; Wood et al., 2023).
Self-other agreement correlations. Self-other agreement correlations for the HPCS items, were
estimated by correlating the personâ€™s self-rating at the first survey session with a single observer rating.
Self-other agreement for the HPCS items ranged from a low of r = .07 for â€œis Directionless (lacks well-
defined beliefs or interests)â€ to a high of r = .49 for â€œLikes Parties (enjoys parties, social gatherings)â€;
= .23 (.09).
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
HOGAN SINGLE-ITEM SCALES 22
Observer agreement correlations. Observer agreement concerns the degree to which ratings
from two different raters of the same target agree tend to be correlated, and are generally expected to
range from a low of 0 (no agreement) to 1 (perfect agreement). This was estimated as the intraclass
correlation of ratings (ICC1) using the function within the package in (Revelle,
statsBy psych R
2017). The observer agreement correlations ranged from a low of r = .04 for â€œis Trusting (unsuspicious of
others' intentions)â€ to a high of r = .46 for â€œis Perfectionistic (exacting and obsessive about work
quality)â€; = .24 (.09).
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
Relationship between matching HPCS and HPI/HDS scales. Within Sample 2, we were able
to estimate the correlations between the single items of the HPCS and scale scores for the corresponding
HICs from the HPI or HDS inventory. All correlations differed significantly from zero (p < .05). Further,
the only negative correlations between HICs and HPCS items were for the six items in which negations
within HIC labels were removed when creating the corresponding HPCS item (e.g., the HIC â€˜No Social
Anxietyâ€™ and the corresponding HPCS item â€œis Socially Anxious (reserved/anxious in social situations)â€.
Consequently we reversed the correlations for these six items before examining properties about the
convergence of HPCS and HIC scales.
The correlations between corresponding HPCS and HIC dimensions ranged from a low of r = .16
for the items â€œis Indecisive (overly reliant on advice; reluctant to act independently)â€ and â€œis Passive-
Aggressive (can act outwardly pleasant while feeling inwardly resentful)â€ to a high of r = .78 for â€œLikes
People (enjoys social interaction)â€, with a mean correlation of = .49(.15) across the 74 items.
ğ‘Ÿğ‘Ÿ
ğ‘€ğ‘€(ğ‘†ğ‘†ğ‘†ğ‘†)
This is despite the fact that the HPCS was administered over six months after respondents completed the
HPI and HDS instruments, which should attenuate the correlation that would have been observed if the
measures were administered more closely together.
Correlations Between Item Properties
We next examined how various estimated properties of the HPCS items were correlated with one
another at the between-item level of analysis. Note that between-item correlations are referenced here
with q to more clearly distinguish â€˜between-itemâ€™ and â€˜between-personâ€™ (or r-)correlations (Cattell, 1952;
HOGAN SINGLE-ITEM SCALES 23
Wood & Furr, 2016). We were particularly interested in exploring whether retest correlations served as
useful indicators of the HPCS itemâ€™s reliability. A common way of exploring this question is to examine
how retest correlations (or other candidate reliability indicators) relate to other item properties (de Vries
et al., 2016; Henry et al., 2022; McCrae et al., 2011; Wood et al., 2018, 2023). These correlations are
|ğ‘ğ‘ğ‘ ğ‘ |
other item properties. Items with larger retest correlations tended to have higher standard deviations (q =
.22), observer agreement (q = .50) and self-other agreement (q = .57). The number of characters of the
HPCS item was negatively associated with the itemâ€™s estimated retest correlation (q = -.28), self-other
agreement (q = -.17), and other-other agreement (q = -.19).
The itemâ€™s retest correlation was also a robust predictor of the itemâ€™s correlation with the
corresponding HIC from the HPI or HDS (q = .41). The estimated correlation between the HPCS item and
corresponding HIC was additionally highly related to the HICâ€™s estimated coefficient alpha (q = .50), and
was negatively related to the HPCS itemâ€™s number of characters (q = -.30) and rated observability (q =
.27). This indicates that the correlation between the HPCS item and corresponding HIC could be strongly
predicted by the estimated reliabilities of both measures, consistent with classical test theory (Lord &
Novick, 1968; McDonald, 1999), and by factors expected to influence their reliabilities. This was also
observed despite the fact that HPCS retest correlations and HIC internal consistencies were estimated in
different samples, which provides some evidence that the estimates of both properties have non-trivial
levels of generality across samples.
Finally, HPCS items with higher retest correlations tended to be associated with HICs which had
higher inter-item correlations (q = .31). Interestingly, this was despite the fact that the HPCS was
developed without sampling actual items from the HICs, and despite the fact that these properties were
estimated in separate samples. Items showing higher levels of both properties included items â€œhas Math
HOGAN SINGLE-ITEM SCALES 24
Abilityâ€, â€œis Academic-Orientedâ€, and â€œLikes Peopleâ€, whereas items with lower levels of both properties
included the items â€œis Satisfiedâ€, â€œis Passive-Aggressiveâ€, and â€œAvoids Troubleâ€.
Consistent with prior research (Goldberg, 1982; Wood & Furr, 2016), the mean self-report for the
item was very highly related to the rated desirability of the item (q = .86). Interestingly, the mean of
observer-reports for the item were even more highly related to the item desirability (q = .94); this
difference was significant by Steiger's (1980) test of dependent correlations (t = 7.78, p < .01). As most
raters that supplied observer reports were friends or family members of the participant, this is consistent
with the meta-analytic finding that observer-reports of personality from close acquaintances tend to more
closely track item desirabilities than self-reports (Kim et al., 2019).
The rated observability of the HPCS items was also highly associated with various item
properties. Items rated as more observable tended to have higher 3-week retest stability (q = .28), self-
other agreement (q = .35), and observer agreement (q = .40).
Dimension-Level Scoring of HPCS
It is also possible to create scale scores of the larger dimensions assessed by the HPI and HDS by
averaging scores for all HPCS items representing HICs within the dimension. This produces three-item
scales for each of the HDS dimensions, and scales ranging from 4 to 8 items for the broader HPI
scales are given, and additionally the correlation between the HPCS dimension-level scales and the
corresponding dimension-levels scales of the HPI or HDS.
Clark and Watson (2019) suggested that for average inter-item correlations in the .40-.50 range
may be preferable for narrow constructs, which could concern the HDS dimensions; whereas inter-item
correlations in the .15-.20 range may be preferable for broader constructs, which could concern the HPI
scales created from HPCS items ranged from lows of .11 and .19 for the HDS Colorful scale and HPI
Prudence scale, respectively, to highs of .70 and .56 for the HDS Skeptical scale and HDS Cautious scale,
respectively, with a mean of .40. This indicates that the scales differed dramatically in the heterogeneity
HOGAN SINGLE-ITEM SCALES 25
of their items, with the three HPCS items corresponding to the HDS Skeptical scale providing particularly
redundant information about participants.
The correlations between the dimension-level HPCS scales and dimension-level HPI and HDS
scales ranged from .42 to .85, with a mean of .64, and were again somewhat higher for HPI dimensions
( = .72) than HDS dimensions ( = .59). These should be interpreted as lower-bound estimates of the
ğ‘Ÿğ‘Ÿ ğ‘Ÿğ‘Ÿ
ğ‘€ğ‘€ ğ‘€ğ‘€
degree to which dimension-level scores on the HPI and HDS can be recovered from simple averages of
HPCS items given the six-month interval separating measurements of the HPI/HDS and HPCS.
Most Redundant HPCS Items
As shown by Wood and colleagues (2023; Henry et al., 2023), it is possible to estimate the
informational similarity of all item pairs in an inventory when the inventory is rated by the same set of
participants twice, via this equation:
ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2 ğ‘‹ğ‘‹2ğ‘‹ğ‘‹1
. 5ï¿½ğ‘Ÿğ‘Ÿ + ğ‘Ÿğ‘Ÿ ï¿½
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ğ„ ğŸğŸ. ğœŒğœŒï¿½ =
ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2 ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2
ğ‘Ÿğ‘Ÿ ğ‘Ÿğ‘Ÿ
This equation estimates how much lower â€˜laggedâ€™ correlations between items X and Y (where
concerns how a personâ€™s first rating on item X correlates with their second rating on item Y) are
ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2
ğ‘Ÿğ‘Ÿ
relative to the retest correlation of the items over the same interval (where and indicate the
ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2 ğ‘‹ğ‘‹1ğ‘‹ğ‘‹2
ğ‘Ÿğ‘Ÿ ğ‘Ÿğ‘Ÿ
retest correlations of X and Y), with indicating the measurement interval â€“ in this case: three weeks
|ğ‘‘ğ‘‘|
between repeating the HPCS. The quantity can be interpreted as a ratio detailing how much lower
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
X and Y correlate with one another than they correlate with themselves when retested over the same
measurement interval. These estimates have the virtues of (1) outperforming raw-score correlations
between the inventories as predictors of whether the items are seen as semantically similar or redundant
by raters, and (2) restoring values of 1.0 as indicating that the two scales provide fully redundant
information.
The full set of estimated retest-adjusted correlations for all pairs of HPCS items is available in
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
HOGAN SINGLE-ITEM SCALES 26
.90 that Wood and colleagues suggested as a lower-bound for beginning discussions of items as
potentially redundant. These item-pairs were (1) the items â€œis Moralistic (follows moral rules /
conventions)â€ and â€œis Virtuous (makes sure to act in a morally upstanding manner)â€, = .993; (2)
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
the items â€œis Emotionally Volatile (easily angered, annoyed, or upset)â€ and â€œis Irritable (easily irritated by
interruptions, requests, or suggestions)â€, = .913; and (3) the items â€œis Empathetic (concerned for
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
others)â€ and â€œis Caring (notices others' moods, appreciates others' needs)â€, = .913. The most
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
negatively correlated item-pair by this index was â€œis Trusting (unsuspicious of others' intentions)â€ and â€œis
Mistrusting (mistrusts people/institutions; alert to perceived mistreatment)â€, = -.862. This indicates
ğ•ğ•ğ•ğ•|ğ‘‘ğ‘‘|
ğœŒğœŒï¿½
that across the 2701 pairs of items within the 74-item HPCS (i.e., ), there were few items that
74 Ã— 73/2
should be flagged as potentially providing redundant information to other items within the set.
Similarity of Associations between HICs and HPCS Items with Other Variables
Finally, we examined whether the HPCS items and corresponding HICs from the HPI/HDS
showed similar associations with other variables. We first reverse-scored the six HICs in which the HPCS
item was scored in the opposite direction. The correlations between the 74 HICs or HPCS items with
correlations, (J. Cohen, 1969). These can be estimated simply by doubling the length of the profiles
ğ¶ğ¶
ğ‘ğ‘
being correlated by appending all reversals of the correlation. For instance: a three-item profile of
becomes as six-item profile of after adding all reversals.
[. 30, .05, .20] [. 30, .05, .20, âˆ’.30, âˆ’.05, âˆ’.20]
The resulting profile correlations have the advantage of being invariant over potentially arbitrary
ğ¶ğ¶
ğ‘ğ‘
decisions of whether to reverse-code certain variables (e.g., to score the HIC as â€œNo Hostilityâ€ versus
â€œHostilityâ€), and in essence index whether the profiles match in how correlations deviate from = 0.
ğ‘Ÿğ‘Ÿ
patterns of associations with the organizational experience variables, with the pattern-vector correlations
HOGAN SINGLE-ITEM SCALES 27
ranging from a low of .83 for organizational commitment and task performance to a high of .88 for
organizational burnout.
We have provided an example of how the HICs and corresponding HPCS items are associated
most important variables of interest to organizational researchers (Hall et al., 1917; Hogan et al., 1996).
HPCS items; for instance, the HPCS item â€œis Self-Controlled (has considerable self-discipline, control
over impulses)â€ showed a .30 correlation with the Task Performance measure, whereas the corresponding
HIC from the HPI, Impulse Control, showed a -.06 correlation. Overall, 6 of the 74 correlations differed
by more than a .30 magnitude across the HPI/HDS HICs and the corresponding HPCS item. However, the
strong positive linear pattern of how the 74 HICs and HPCS items related to task performance as shown
The estimated pattern similarities were slightly smaller for how the HPCS items and HPI/HDS
HICs related to age ( = .79) and gender ( = .70). This can be partially attributed to the smaller range
ğ¶ğ¶ ğ¶ğ¶
ğ‘ğ‘ ğ‘ğ‘
of correlations linking age and gender to either the HPCS items or HICs than observed for the
organizational experience variables. However, these estimates nonetheless crossed thresholds that have
been offered as indicating high pattern similarity (e.g., â‰¥ .60; Lawson & Robins, 2021; McCrae, 2008).
ğ‘ğ‘
We have provided scatterplots showing how the HICs and corresponding HPCS items were associated
magnitude across the HPI/HDS HICs and corresponding HPCS item for either variable: as shown in
others)â€ (r = .21), but lower on the corresponding Empathy HIC from the HPI (r = -.14).
General Discussion
In the present study, we provided evidence that the HPCS functions as a useful inventory for
assessing the content found within the HPI and HDS inventories via single-item scales. The HPCS items
HOGAN SINGLE-ITEM SCALES 28
were found to show decent retest reliability over a period of three weeks, and additionally tended to show
high convergence with corresponding HICs from the HPI and HPS instruments despite being assessed
over half a year apart. The HPCS additionally showed high similarity to the HPI and HPS in terms of how
corresponding scales related to other organizational and demographic variables.
Relative Benefits of HPCS Compared to Full HPI/HDS Instruments
There are some situations in which the HPCS may be preferable to the HPI and HDS. The most
obvious is when respondent time is at a premium; the HPCS consists of about one-fifth the number of
items as the full HPI and HDS instruments. However, there are some other situations in which the HPCS
may be preferred as well.
First, the HPCS is an open instrument which can be used freely. The information needed to
reproduce the full HPCS, including all items as they are seen by respondents, is given within this article
and the Supplemental Materials. We encourage that researchers use the HPCS items as administered in
Sample 2 for research with this instrument (see Appendix A for items that differ across Samples 1 and 2,
Relatedly, the HPCS was designed to facilitate the reporting of item-level results as fully as
possible. As we detailed, each HPCS item was written to have a â€œlabel (subclause)â€ format; the full
allows researchers to have greater clarity about the nature of the scaleâ€™s associations with other variables,
which can otherwise be masked by discrepancies between the scale label and items â€“ a problem which has
been noted as complicating interpretations of personality-outcome associations for decades (Block, 1995;
MÃµttus, 2016; Nicholls et al., 1982; Wood & Harms, 2016).
The HPCS will also be particularly valuable as a tool for collecting observer reports. In research
settings, obtaining observer reports is regarded as a key way to better triangulate the individualâ€™s actual
levels of traits of interest, given how issues related to scale-use, self-presentation, and self-insight can
complicate interpretations of self-report measures (Connelly et al., 2022; McCrae & MÃµttus, 2019;
HOGAN SINGLE-ITEM SCALES 29
Roberts & Wood, 2006). The HPCS serves as a tool that will allow observer assessments to be completed
much more economically, and to better ensure that the somewhat distinct content emphases of the HPI
and HDS instruments are better represented in such assessments.
There are also situations in which the HPCS should not be used relative to the full HPI and HDS
instruments. Perhaps most importantly, the HPCS is not appropriate to use for selection purposes or other
â€˜high-stakesâ€™ assessments (i.e., assessments in which respondent outcomes are determined by their
responses). Although the HPCS was shown here to be highly predictive of corresponding HPI and HDS
instrument less appropriate for such purposes due to the ease of sharing strategies for faking. Further,
although the three-week retest correlations indicate single items of the HPCS to be more reliable than
many researchers might suspect (Tables 3 and 4), the retest correlations for corresponding scales from the
original HPI and HDS instruments will typically be higher. When score reliability is at a premium, this
will serve as another reason to use the original HPI and HDS instruments. It is important to reiterate that
the HPI and HDS assessments are available for academic research purposes with the caveat that item-
level data gathering and scoring be completed by Hogan Assessment Systems. The HPCS instruments are
most appropriate for researchers with testing time constraints or who are interested in item-level analyses.
Future Directions
Further Validational Work
Although we provided evidence of the reliability and validity of HPCS items, further pieces of
evidence would be valuable. First, it would be valuable to estimate retest correlations in additional
samples, and over different retest intervals. It may be particularly important when using single-item scales
to show that estimates of retest correlations generalize across samples â€“ both in terms of their rank-
ordering across different items and in their absolute magnitude â€“ as most investigators will be unlikely to
readminister the inventory twice. Although investigations have found item-level retest correlations to
generalize across samples (Henry et al., 2022; Lowman et al., 2018), more stable estimates of these
properties would be valuable to obtain for the HPCS.
HOGAN SINGLE-ITEM SCALES 30
Additionally, the correlations between HPCS items and corresponding HICs should be
considerably attenuated by the fact that they were administered over six months apart, indicating that the
average correlation of .49 across the 74 items may be a â€˜lower-boundâ€™ of how highly scores should
converge. Administering the HPCS and the full HPI and HDS instruments within a single larger survey
session would provide a better picture of how similarly the inventories function.
It would also be valuable to examine how the HPCS and full HPI and HDS compare in predicting
non-self-reported outcomes, such as how much the person is liked by others, manager ratings of job
performance, objective records of workplace accidents, or credit scores. We expect that this may alter the
picture of how HPCS items and corresponding HPI/HDS scales relate to outcomes to some extent. For
instance, the HPCS item â€œis Perfectionistic (exacting and obsessive about work quality)â€ correlated
with objective performance could be masked when using self-reported performance if perfectionistic
people tend to appraise the quality of their work in an overly harsh manner.
Finally, given that the HPCS is created to be considerably more amenable for completing brief
observer reports, it would be valuable to explore how HPCS observer reports might supplement other
methods of predicting expected performance â€“ including standard HPI/HDS self-reports. Such
assessments could might be expected to improve upon the limited value of letters of recommendation
(Kuncel et al., 2014) as a means of obtaining information from references, by having these references
provide ratings on a common set of scales.
More Direct Comparison to Other Common Short Measures
We have argued that an advantage of the HPCS relative to commonly used short personality
measures of comparable length, such as the 60-item BFI-2 (Soto & John, 2017b), HEXACO-60 (Ashton
& Lee, 2009), and NEO-FFI (Costa & McCrae, 1992) instruments, is that the surveyed content was
selected to include a greater range of dysfunctional content. Specifically, almost half of the HPCS items
(45%) were selected to survey content from the HDS, which was designed to assess traits related to the
HOGAN SINGLE-ITEM SCALES 31
DSM-IV personality disorders, such as tendencies to be cynical, perfectionistic, impulsive, manipulative,
ingratiating, and directionless.
Additionally, responses to inventories consisting entirely of single-item scales may be expected to
communicate more total information about the respondent relative to inventories of similar length
consisting of multi-item scales (Condon et al., 2020; Condon & MÃµttus, 2021). This is because standard
scale development practices â€“ such as selecting items to cross benchmarks for internal consistency within
multi-item scales (e.g., coefficient should exceed .70 or .80), or eliminating items with negligible
ğ›¼ğ›¼
loadings of cross-loadings in factor analyses (DeVellis & Thorpe, 2021; Hinkin, 1998) â€“ should be
expected to introduce a greater level of redundancy. Since none of the items of the HPCS were selected to
â€˜measure the same thingâ€™ as any other, the range of content could be broader.
These two aspects of the HPCS could result in better prediction of other outcomes than other
inventories of similar length. However, it would be valuable to more directly test this through a â€˜good
old-fashioned horse raceâ€™ study, where the HPCS is pitted against other inventories of comparable length
to see which measure can best predict outcomes of interest, using modern cross-validational techniques to
prevent artifactual overprediction (Condon et al., 2020; Rocca & Yarkoni, 2021; Saucier et al., 2020).
More General Procedures for Developing and Using Single-Item Inventories
The present research contributes to an emerging picture of â€˜best practicesâ€™ for both the
construction and usage of inventories consisting of single-item scales (see also Condon et al., 2020;
Matthews et al., 2022; Shedler & Westen, 2007; Wood et al., 2010). We discuss additional themes below.
possible to present part or all of the exact item responded to respondents, which helps to reduce issues
whereby the scaleâ€™s label fails to accurately indicate aspects of the scale driving how it is endorsed and
correlates with other variables. We additionally illustrated how secondary codings of the items, such as
their rated desirability or observability, can help to identify reasons for effect heterogeneity across
indicators of a broader scale or dimension (e.g., MÃµttus et al., 2017, 2019; Revelle et al., 2020), such as
why certain aspects of the broader HPI Adjustment dimension (e.g., whether the person is Anxious, Calm,
HOGAN SINGLE-ITEM SCALES 32
Well-Attached) may have higher self-other agreement than others (e.g., whether the person is Trusting or
Use retest correlations as reliability coefficients. The present research provides further
evidence that test-retest correlations should be regarded as privileged indicators of the testâ€™s reliability
(Guttman, 1945; Henry et al., 2022; McCrae et al., 2011). In the present study, retest correlations were
estimated over a three-week period, which is within the range of about a week to two months that some
have recommended for obtaining dependability estimates, in which the level of trait change is expected to
be negligible (Cattell et al., 1970; Watson, 2004). The present results demonstrated once again that retest
correlations over dependability intervals correlate highly with a range of validity-related properties of the
scale, such as its level of self-other agreement, observer (or â€˜other-otherâ€™) agreement, and the scaleâ€™s
correlation with the corresponding HIC from the original HPI and HDS instruments. Since a scaleâ€™s
reliability places an upper-limit on its ability to relate to other variables, the ability of retest correlations to
track these properties helps to establish these as appropriate and valid reliability estimates.
The results also reinforce the understanding that it is often desirable to attempt to maximize retest
correlations when creating scales (Condon et al., 2020), and indicate some ways this can be done. For
instance, the present study reinforces prior evidence that respondents rate longer items less reliably (de
Vries et al., 2016), and additionally indicates that respondents rate less observable traits (â€œis Mistrustingâ€,
â€œFeels Uniquely Sensitiveâ€) less reliably as well. This indicates that test developers should place
considerable effort into simplifying items such as by shortening items and eliminating jargon to the extent
possible (Graziano et al., 1998; Hogan & Hogan, 2001; McCrae et al., 2005).
indicating the upper-limit of each itemâ€™s ability to correlate with other scales administered three weeks
apart, but should be expected to somewhat underestimate the ability of HPCS scores to correlate with
items administered in the same survey session, due to mood and other relatively transient factors affecting
how people respond to surveys (Chmielewski & Watson, 2009; Wood et al., 2023). This is non-trivial as
much research involves estimating relationships between scales rated within a single testing session. It
HOGAN SINGLE-ITEM SCALES 33
can be valuable to collect same-session retest correlations to address this issue â€“ for instance, by
administering the same inventory twice about 15 minutes apart within a larger survey (Dejonckheere et
al., 2022; Lowman et al., 2018).
Appraising the â€˜hit the nail on the headâ€™ item selection strategy. In selecting HPCS items, we
aimed to measure the scale construct as directly as possible. However, some of the traits that the HPI and
HDS instruments were designed to assess â€“ such as those related to overconfidence, self-enhancement,
and humility â€“ can be regarded as ones where there is arguably a definitional lack of awareness of oneâ€™s
trait level. For such constructs, respondents may need to cut through certain paradoxes or metacognitive
loops to provide valid self-reports. For instance, if someone strongly agrees that they are someone who â€œis
Overconfident (believes is capable of accomplishing more than realistically possible)â€, does this mean
that they recognize that they canâ€™t do as much as they think? And if they do, why are they strongly
agreeing to the item? As we noted, the original HPI and HDS instruments were designed to assess these
and other traits somewhat more indirectly, by assessing more specific beliefs people with these traits may
tend to have, or actions they may tend to do. Other researchers have suggested that such traits may be
usefully thought of and measured as discrepancies between self-reports and other-reports or via related
â€˜componentialâ€™ techniques (Davis et al., 2010; Kwan et al., 2004). It would be worth further exploring the
limits of the direct self-report method used here toward forming valid estimates of traits expected to affect
career success and other socially consequential outcomes.
Inventory-level considerations. Finally, developers of single-item inventories regularly aspire to
create more comprehensive assessments of some domain of content (Furr et al., 2010; Sherman et al.,
2010; Wood et al., 2010). When this is the aim, scale developers may attempt to identify, remove, and
replace items identified as largely redundant to others within the set (Block, 1961). Here, we utilized a
recently developed method of identifying redundant items through the use of retest-adjusted correlations
(Equation 1)(Henry et al., 2023; Wood et al., 2023). Further development of single-item inventories could
use these estimates as a tool for iteratively increasing the breadth and comprehensiveness of the
inventory, by pointing to items that could be replaced or adjusted to provide more distinct nuances of
HOGAN SINGLE-ITEM SCALES 34
information. For instance, the items â€œis Moralistic (follows moral rules/conventions)â€ and â€œis Virtuous
(makes sure to act in a morally upstanding manner)â€ were identified as providing nearly identical
information about participants; in future versions of the HPCS, one of these items could be tweaked to get
at a more distinct meaning or one could be removed to make room for a more distinct construct. If done
effectively, this should decrease the correlations between items to some degree, which in turn should
increase the ability for the set as a whole to predict other outcomes effectively (Altgassen et al., 2023;
Condon et al., 2020).
Further work in developing single-item inventories may also aspire to better balance positive and
negative content within the broader domains measured by the inventory. For instance, most of the broader
tendency could be reduced by using antonymous items for some of the traits of interest. For instance, the
HPCS item labeled â€œis Thrill-Seekingâ€ could be replaced by one labeled â€œAvoids Stimulating Situationsâ€
and the item labeled â€œis Eccentricâ€ could be replaced by one labeled â€œis Ordinaryâ€ to create negative
better disentangle factor-level scores from the raterâ€™s level of acquiescent responding â€“ which concerns
the degree to which a rater is inclined (or â€˜leansâ€™) toward agreeing or disagreeing with any item largely
independent of its content â€“ by increasing the number of item-pairs representing largely antonymous
content (Rammstedt & Farmer, 2013; Soto et al., 2008).
Conclusion
The HPI and HDS personality instruments are among the most well-validated inventories for
predicting employee performance, and differ in non-trivial ways from other personality inventories in the
traits they have been designed to assess. However, they are infrequently used in basic research, due in part
to their length and proprietary nature. In the present article, we have illustrated how the Hogan
Personality Content Single-Items (HPCS) Inventory can be employed to measure the 74 subscales (HICs)
of the HPI and HDS inventories over a total of 74 total items. We detailed that the HPCS items show
respectable test-retest stability, self-other agreement, and observer agreement, and function in a highly
HOGAN SINGLE-ITEM SCALES 35
similar manner to their corresponding subscales within the full HPI and HDS instruments. We have also
detailed how single-item inventories such as the HPCS offer certain benefits for research purposes, such
as the ability to avoid creating distinct scale labels and instead presenting the items as seen by
respondents when reporting results. We expect that the HPCS will serve as a valuable tool in both
research and applied settings.
HOGAN SINGLE-ITEM SCALES 36
Endnotes
To illustrate, this screen would exclude participants that provided ratings only varying across 2 adjacent
points on a 1-to-5 Likert-type scale (e.g., rating all items 3 and 4, or 1 and 2). Such participants would
receive, at most, a value of 25% by this index (with the exact value depending on the percentage of each
of the two values used).
We argue that it is more appropriate to reverse-score multi-item scales (which themselves regularly
contain some percentage of reverse-scored items) than to reverse-score single-item scales. When showing
part or all of an item using the same text as seen by respondents, reverse-scoring single-item scales may
make interpretation of means and correlations involving the scale more confusing to readers, and can also
diminish the ability to see how scale-usage effects such as acquiescence (â€˜yea-sayingâ€™ or â€˜nay-sayingâ€™)
relate to item means and correlations.
Although we argue that correlations are most advisable for indexing profile similarity when some
ğ¶ğ¶
ğ‘ğ‘
variables can be reverse-scored, the correlations reported here differed trivially from other commonly
ğ¶ğ¶
ğ‘ğ‘
used indices of similarity between vectors of correlations, such as simple profile correlations or double-
entry intraclass correlations (McCrae, 2008). These and other pattern similarity statistics can be formed
HOGAN SINGLE-ITEM SCALES 37