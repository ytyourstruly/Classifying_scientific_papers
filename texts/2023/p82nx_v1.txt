EFFECT OF CALIBRATION TRAINING
Effect of Calibration Training on the Calibration of
Intelligence Analysts‚Äô Judgments
***PREPRINT VERSION 1***
Megan O. Kelly1 and David R. Mandel2
1 Defence Research and Development Canada ‚Äì Toronto Research Centre and
University of Waterloo
2 Defence Research and Development Canada ‚Äì Toronto Research Centre and
York University
Correspondence: david.mandel@forces.gc.ca
Funding: This research was supported by the Accelerate Command, Control, and
Intelligence Project Activity #AC2I-019 and Canadian Safety and Security Program
project #2018-TI-2394 under the direction of the second author.
Acknowledgements: We wish to thank Nicole Herz and Daniel Irwin for their
assistance with this research.
Conflict of Interest: Neither researcher has professional or financial connections to
HDR; the course licenses used in this study were procured at regular cost by the
Government of Canada. HDR‚Äôs CPA course was chosen for this study because it has
previously been used by one of the intelligence organizations from which we recruited
participants and that organization had requested an investigation of the CPA course‚Äôs
efficacy. HDR did not provide any support for this study aside from reviewing our
description of the CPA course for accuracy and no member of the team was asked to
review a draft of this article prior to journal submission. None of the raw data from this
study was shared with HDR (nor was it requested).
EFFECT OF CALIBRATION TRAINING
Abstract
Experts are expected to make well-calibrated judgments within their field, yet a
voluminous literature demonstrates miscalibration in human judgment. Calibration
training aimed at improving subsequent calibration performance offers a potential
solution. We tested the effect of commercial calibration training on a group of 70
intelligence analysts by comparing the miscalibration and bias of their judgments before
and after a commercial training course meant to improve calibration across interval
estimation and binary choice tasks. Training significantly improved calibration and bias
overall, but this effect was contingent on the task. For interval estimation, analysts were
overconfident before training and became better calibrated after training. For the binary
choice task, however, analysts were initially underconfident and bias increased in this
same direction post-training. Improvement on the two tasks was also uncorrelated.
Taken together, results indicate that the training shifted analyst bias toward less
confidence rather than improve metacognitive monitoring ability.
Keywords: calibration, training, bias, overconfidence, underconfidence, intelligence
analysts
EFFECT OF CALIBRATION TRAINING
Across many domains, experts are expected to make judgments to support the
decision-making of their organizations. Such judgments may regard the probability that
a given claim is or will become true (e.g., ‚ÄúThere is an x% chance that China will invade
Taiwan by 2030‚Äù) or as the subjective confidence interval for some continuous quantity
(e.g., ‚ÄúWe assess with 90% confidence that between x and y US companies will report
ransomware attacks in 2023‚Äù).1 Calibration is the degree to which confidence coincides
with judgment accuracy (Keren, 1991). An assessor is perfectly calibrated if the
proportion of judgments assigned a given probability equals the proportion that is true
(Lichtenstein & Fischhoff, 1980). Across a series of judgments assigned 80%
probability, 80% should be true, and across a series of interval judgments made with
90% confidence, 90% of true values should fall within the relevant intervals. Provided
the various implications of judgments, (e.g., financial, medical, safety, etc.), the degree
to which an assessor is calibrated (or not) is a vital aspect in understanding the potential
validity or utility of a judgement.
Miscalibration
In contrast to perfect calibration, an assessor may demonstrate miscalibration if
the proportion of judgments assigned a given probability exceeds the proportion that is
true, indicating an overconfidence bias, or falls below the proportion that is true,
Note that Western intelligence organizations typically communicate probabilistic
judgments verbally (e.g., ‚Äúx is likely to occur‚Äù; ‚ÄúWe assess with low confidence...‚Äù),
despite the strong empirical case for using numeric quantifiers (see Dhami & Mandel,
2021; Friedman, 2019; Irwin & Mandel, 2023; Mandel & Irwin, 2021).
EFFECT OF CALIBRATION TRAINING
indicating an underconfidence bias (Lichtenstein & Fischhoff, 1980). In the case of
interval judgments, overconfidence manifests as overprecision with assessors providing
excessively narrow confidence intervals, whereas underconfidence is expressed as
underprecision or excessively wide intervals (Du & Budescu, 2007; Moore & Healy,
2008).
Miscalibration can undermine accurate probability assessment (Mandel &
Barnes, 2014), information processing (Meyer & Singh, 2017; Zacharakis & Shepherd,
2001), and decision-making quality (Berner & Graber, 2008; Biais et al., 2005).
Unfortunately, miscalibration is widespread among experts and non-experts alike
(Arkes, 2001; Bier, 2004; Lichtenstein & Fischhoff, 1980; McKenzie et al., 2008), and is
common across a variety of domains including healthcare and medical science
(Benjamin et al., 2022; Berner & Graber, 2008; Brinkman et al., 2015; Meyer et al.,
2013), geopolitical forecasting (Chang & Tetlock, 2016; Mandel & Barnes, 2018;
Tetlock, 2005), the military (Kelly et al., 1975; Phelps et al., 1980), finance (Biais et al.,
2005; Ben-David et al., 2013; Du & Budescu, 2007), meteorology (Charba & Klein,
1980; Murphy & Winkler, 1984), sports betting (Bum et al., 2018; Ercag & Galic, 2014;
Johnson & Bruce, 2001) and education (Callender et al., 2016; Foster et al., 2017; Huff
& Nietfeld, 2009).
Miscalibration often takes the form of overconfidence (Lichtenstein et al., 1982;
Russo & Schoemaker, 1992) including among corporate executives predicting market
performance (Ben-David et al., 2013), clinicians making medical diagnoses (Berner &
Graber, 2008), oncologists forecasting the efficacy of clinical trials (Benjamin et al.,
EFFECT OF CALIBRATION TRAINING
2022), and geopolitical forecasters (Tetlock, 2005). Oft-cited exceptions include
meteorologists (Charba & Klein, 1980; Murphy & Winkler, 1984), racetrack bettors
(Johnson & Bruce, 2001), and experienced bridge players (Keren, 1987), all of whom
receive timely feedback on their judgments and exhibit excellent calibration.
In the intelligence domain, there is evidence that analysts exhibit both forms of
miscalibration. One early study found that US intelligence analysts exhibited a ‚Äúsmall
but consistent bias towards overconfidence‚Äù when forecasting military-relevant events
(Kelly et al., 1975 as cited in Phelps et al., 1980, p. 11). An analysis of over 2000
analysts found miscalibration driven by underconfidence (Mandel & Barnes, 2014;
2018) and related work found underconfidence in analysts was most pronounced when
accountability pressures were likely to be especially high (Mandel & Barnes, 2014;
Mandel et al., 2014). While overconfident assessments may undermine decision-making
by encouraging excessive risk taking or a misperception of risk, underconfidence can
undermine decision-making by reducing the informativeness of intelligence (Mandel &
Barnes, 2014). Estimates couched in more uncertainty than warranted can dilute the
signal value of those estimates.
Improving Calibration
Given the potential for calibration to signal judgment accuracy, many have aimed
to improve calibration through intervention (Lichtenstein et al., 1982; Lawrence et al.,
2006). For example, social comparison interventions seek to improve calibration by
providing participants with information (real or simulated) about how their peers assess
themselves, how they perform, or how biased they are. Fellner-R√∂hling et al. (2021)
EFFECT OF CALIBRATION TRAINING
found that learning the average bias of peers improved calibration among
underconfident and overconfident participants and had no adverse effect on well-
calibrated participants. In contrast, neither information about peers‚Äô actual accuracy nor
their expected accuracy improved calibration (see also de Carvalho Filho et al., 2001).
Social intervention can also decrease calibration, however, as Pesout and Nietfeld
(2021) found that overconfidence increased when participants were shown top scores
and promised prizes for outperforming their peers in a series of reading comprehension
tasks.
Another approach to improving calibration involves providing performance or
outcome feedback to participants with the intent to help correct biases. Performance
feedback compares actual and expected results, whereas outcome provides the correct
answers and a total score. Several studies suggest that performance feedback can
improve calibration in a variety of judgment tasks (Adams & Adams, 1958, 1961 Benson
& √ñnkal, 1992; Bolger & √ñnkal-Atay, 2004; Lichtenstein & Fischhoff, 1980; √ñnkal &
Muradoglu, 1995; Stone & Opel, 2000; although, see Sharp et al., 1988 and Doussau et
al., 2023 for exceptions), including exam performance in classroom settings (Saenz et
al., 2019) and geopolitical forecasting (Mellers at al., 2014; Moore et al., 2017; Tetlock &
Gardner, 2015). Moore et al. (2017) found that a training intervention that included
providing performance feedback was associated with improved calibration driven by
reduced overconfidence.
There is some evidence that outcome feedback can improve calibration
(Callender et al., 2016; Du et al., 2012; Niu & Harvey, 2022; O‚ÄôConnor & Lawrence,
EFFECT OF CALIBRATION TRAINING
1989; although see Foster et al., 2017). Du et al. (2012) found that combining outcome
and performance feedback was no more effective at improving calibration than outcome
feedback alone. Other studies suggest that providing outcome feedback can precipitate
overcorrection from overconfidence towards underconfidence (Arkes et al., 1987) or
contribute to overconfidence in the form of hindsight bias (Hoch & Loewenstein, 1989),
thus, outcome feedback might be less effective than performance feedback (Lawrence
et al., 2006).
The effects of feedback-centred interventions on calibration demonstrate that
feedback can improve calibration, although not necessarily consistently across contexts.
Successful calibration training may require additional approaches which we aim to
address currently. A third approach to improving calibration is metacognitive training,
which broadly encompasses interventions designed to facilitate reflective thinking, self-
questioning, and awareness of biases relating to calibration (Busby et al., 2018; Jaspan
et al., 2021). While some studies show little evidence of improved calibration (Alpert &
Raiffa, 1982; Emory & Luo, 2022), others demonstrate improvement (Gutierrez &
Schraw, 2015; Huff & Nietfeld, 2009; Kruger & Dunning, 1999; Nietfeld & Schraw,
2002), Nietfeld et al. (2006) found that calibration could be improved by combining
metacognitive training with performance feedback. Metacognitive training can also be
augmented with instruction on specific debiasing strategies, such as techniques that
force subjects to consider alternative hypotheses or generate reasons why their
judgments may be wrong (Busby et al., 2018; Hoch, 1985; Jaspan et al., 2021; Koriat et
al., 1980; Venoitt et al., 2010). Although, elicitation techniques based on similar
EFFECT OF CALIBRATION TRAINING
principles, such as encouraging individuals to consider alternatives to their ‚Äúbest guess‚Äù
have not received much support (Mandel et al., 2020).
Recent years have seen the proliferation of commercial training courses that
draw on the research outlined above as well as from their own samples (e.g., the
Calibrated Probability Assessments course from Hubbard Decision Research, 2019 and
the Superforecasting Fundamentals course by Good Judgment Incorporated, 2022).
The increased availability of these courses coincides with calls for the intelligence
community to explore training aimed at improving judgment accuracy including
calibration skill and better enabling the systematic monitoring of judgment accuracy and
calibration (Chang & Tetlock, 2016; Dhami & Mandel, 2021; Friedman, 2019; Mandel,
2015; Mandel & Irwin, 2021; Rieber, 2004). Intelligence analysts have long received
instruction on analytic techniques meant to improve calibration, but few of these
methods have undergone rigorous empirical testing (Chang et al., 2018). Moreover,
analysts do not receive timely or comprehensive performance, if they receive it at all
(Moore, 2011, Rieber, 2004).
To our knowledge, no study has examined the effect of commercial training on
calibration skill among professional intelligence analysts despite them being experts
who routinely produce probabilistic judgments and who may respond differently to
calibration training than convenience samples (Hubbard, 2014). Moreover, experimental
results from experts in other domains may not generalize to the national security and
intelligence domain. To the extent that commercial training improves calibration skill
among professional analysts, these findings can help inform intelligence organizations
EFFECT OF CALIBRATION TRAINING
(and other expert communities that routinely issue probabilistic assessments)
considering whether to provide calibration training to their analysts.
The Present Investigation
Using a pre-post design, we assessed the effect of a commercial training course
on calibration skill in a sample of Canadian intelligence analysts. The online self-paced
CPA course, Calibrated Probability Assessments (CPA), typically takes 3-4 hours to
complete and comprises six video modules during which participants receive
metacognitive training and techniques to improve their calibration (Hubbard Decision
Research, 2019). Throughout, participants complete calibration tests and receive
outcome and performance feedback. Performance is also visualized alongside the
average performance of past participants, enabling social comparison. Each calibration
test comprises two sets of general knowledge questions corresponding to the two
general types of subjective estimates routinely produced by experts (Hubbard Decision
Research, 2019): One set of binary statements (e.g., ‚ÄúMars is farther away from Earth
than Venus.‚Äù) wherein participants judge whether the statement is true/false along with
a confidence rating of 50-100% and one set of interval estimation questions wherein
participants must provide lower- and upper-bound estimates (e.g., ‚ÄúIn what year was
William Shakespeare born?‚Äù) at 90% confidence.
Over the duration of the course, participants build towards a four-step ‚Äúcalibration
process‚Äù which participants are instructed to apply to each judgment. The general pro-
cess involves (1) choosing an initial estimate (for interval questions, starting with wide
ranges to avoid overprecision), (2) applying the equivalent bet test (meant to quantify
EFFECT OF CALIBRATION TRAINING
uncertainty, Hubbard, 2014, pp. 102-106), (3) assuming the judgment is wrong and giv-
ing plausible reasons as to why (i.e., Klein‚Äôs 2008 pre-mortem), and finally, (4) applying
any recommended adjustments based on feedback after each calibration test.
Given extant calibration training research and that HDR‚Äôs CPA course combines
several evidence-based techniques over a relatively long timeframe, we hypothesized
that training would improve both (1) the calibration of intelligence analysts‚Äô point
estimates and (2) the calibration of their range estimates. Day-to-day, intelligence
analysts may be incentivized to dilute the certainty of their assessments as a blame
avoidance strategy (Gentry 2017; Mandel & Irwin, 2021). However, these accountability
pressures were largely absent during our experiment. Thus, we further hypothesized
that (3) in contrast to results from Mandel and Barnes (2014, 2018), analysts would tend
to exhibit overconfidence during the baseline calibration test.
Method
Supplementary materials including the data, analyses, and other supporting files
are available from the Open Science Foundation (OSF) project page:
Participants
The experiment was completed by 70 Canadian intelligence analysts (34%
female) aged 22 to 73 (M=35.6, SD=11.33) and years of experience in the intelligence
community ranged from 0 to 24 years (M=4.98, SD=5.60). Seventy-three percent of
participants reported prior statistics or probability theory experience, but no participant
EFFECT OF CALIBRATION TRAINING
reported experience with HDR‚Äôs CPA course prior to the study.2 Eighty-six percent of
participants were of civilian rank with the remaining participants reporting as senior
officer (7%), junior officer (3%), junior non-commissioned member (1%), or unspecified
(3%). The highest education attained by participants included doctoral degree (6%),
master‚Äôs degree (54%), undergraduate degree (36%), trade school (non-military; 3%)
and high school or equivalent (1%).
Design
Participants completed both pre-training and post-training experimental tasks and
within each, engaged in both a binary judgment task and an interval judgment task. The
experiment was a fully within-subjects 2 (Training: pre-training, post-training) ÔÇ¥ 2 (Task:
binary choice, interval estimation) design.
Procedure and Materials
Defence Research and Development Canada Human Research Ethics
Committee approved the study prior to its commencement. Participants were permitted
to complete the training and experiments either during or outside of working hours. They
were informed that their participation was voluntary and that there was no remuneration,
but that if they successfully completed the CPA course, they would receive a certificate
of completion (which was, indeed, the case). Pre- and post-training surveys were
administered via anonymous Qualtrics links distributed by the researchers and post-
training was only administered upon the completion of the CPA course. The mean
One participant did not provide a response to this but analyses with or without them
are qualitatively the same.
EFFECT OF CALIBRATION TRAINING
number of days between pre-training and training was 7.96 (SD=2.57; median=7) and
the mean days between training and post-training was 2.20 (SD=2.78; median=1). All
but one participant completed the pre-training survey at least five days before
undergoing calibration training and 87% of participants completed the post-training
survey within five days of their self-reported CPA course completion.3 During
completion of the surveys, participants were unable to view or modify responses
each major component of the training modules. Further description of the CPA course
link for double blind peer review].
Phase Major components
Pre- ÔÇ∑
20 binary choice questions and 20 interval estimation questions
training
Order of question type (binary vs. interval) randomized
Module 1:
Overview and objectives of course
Benchmark test (10 binary + 10 interval estimation questions)
Training:
Module 2:
CPA
Definitions of calibration and overconfidence covered
course
Summary of research on overconfidence
The effects of the course on calibration skill in previous cohorts
Outcome and performance feedback on benchmark test
Module 3:
3 Results are qualitatively the same with and without the 13% of participants who did not report
completing training within 5 days of their self-reported CPA course completion (see supplementary
EFFECT OF CALIBRATION TRAINING
Taught that consistent, unambiguous, and immediate feedback is vital for improving
calibration skill
Introduced to the equivalent bet test for improving skill at subjectively assessing probability
Participants practice equivalent bet test
Complete a second calibration test (10 binary + 10 interval estimation questions)
Outcome and performance feedback on second calibration test
Module 4:
Participants are told that the majority of people applying the equivalent bet test to interval
estimation questions choose Game B and need to widen their ranges but fail to do so
sufficiently
Participants are instructed to start with extremely wide ranges so that they narrow their
estimations as a result of choosing Game A
Introduction to Klein‚Äôs pre-mortem
Review of the updated calibration process:
A third calibration test, this time comprising 20 binary choice + 20 interval estimation
questions
Outcome and performance feedback on this third calibration test
Module 5:
Reminder about wide intervals for interval estimation questions
Shown examples of interval ranges provided by calibrated vs. miscalibrated individuals
Told if their expected performance was not within 2 points of their actual performance on
the 20 binary questions, they were not yet calibrated
Told if their performance on the interval task was not at least 14, they were likely
overconfident as most individuals who are calibrated typically score between 17 and 19 on
the interval questions
Review of the updated calibration process (including do‚Äôs and don‚Äôts)
Completed a series of three calibration tests, each comprising 20 binary choice and 20
interval estimation questions each (outcome and performance feedback provided after
each test)
Module 6:
Review of the calibration process
Misconceptions about applying calibration process to real-world estimates refuted
Encouragement of the tracking and scoring of real-world estimates to remain calibrated
20 binary choice questions and 20 interval estimation questions
Post- ÔÇ∑
Order of question type (binary vs. interval) randomized
training ÔÇ∑
Rating of 71 unique urban agglomerations (UAs) encountered during both pre- and post-
training phases
Pre-training
Participants were informed that they would complete two tests designed to
measure their calibration before and after receiving training. After providing consent,
EFFECT OF CALIBRATION TRAINING
they responded to sets of 20 binary choice and 20 interval estimation questions (set
order was randomized). The question formats reflected those in the CPA course (i.e.,
the training phase) and questions regarded the populations of geographic regions (a
common topic from previous work, e.g., Juslin et al., 1999; Klayman et al., 1999;
Subbotin, 1996; Teigen & J√∏rgensen, 2005)‚Äîspecifically, urban agglomerations (UA)4.
blind peer review].
In the binary choice task, participants responded to true/false statements about
the relative populations of two UAs‚Äîe.g., ‚ÄúNew York-Newark, USA had a larger
population than Tokyo, Japan in 2020‚Äù. For each question, they were also asked to
indicate ‚ÄúHow confident are you that your answer is correct?‚Äù using a slider ranging from
50 (‚ÄúNot confident at all‚Äù) to 100 (‚ÄúAbsolutely confident‚Äù).5 UAs in each pair were drawn
from a list of the most populous UAs in 2020 from the United Nations Department of
Economic and Social Affairs (UN, 2018a). In the interval estimation task, participants
provided lower- and upper-bound estimates for the population of a given UA with 90%
confidence‚Äîe.g., ‚ÄúWhat was the population of Toronto, Canada in 2020?‚Äù.
4 The United Nations Department of Economic and Social Affairs (2018b) defines an urban agglomeration
as ‚Äúthe population contained within the contours of a contiguous territory inhabited at urban density levels
without regard to administrative boundaries.‚Äù Participants were shown this definition at the start of both
question sets.
5 The default position of each confidence slider was 50. Participants who wanted to indicate the default
position as their response still had to click the slider. Whereas HDR‚Äôs CPA course elicited confidence in
10-point increments (except for 95% confidence), our sliders increased in one-point increments.
EFFECT OF CALIBRATION TRAINING
Pre- and post-training questions were matched for difficulty (binary choice task)
and UA familiarity (interval estimation task) based on an earlier unpublished experiment
[Author names redacted for double blind peer review, 2021]. Among participants in the
prior experiment, mean accuracies of two binary questions sets were 69.15% (SD=7.56)
and 68.60% (SD=8.38) with no statistical difference [t(37.60)=0.22, p=.829]. Likewise,
among participants in the same prior experiment, mean familiarities of the two sets of
UAs were 2.45 (SD = 0.71) and 2.46 (SD = 0.75) with no statistical difference [t(37.85) =
0.04, p = .971]. The procedures for equating difficulty and familiarity across pre/post
peer review].
After completing pre-training, participants answered demographic questions (i.e.,
sex, age, education and years in the intelligence community) to characterize the sample
before being debriefed.
Training
The CPA course comprises six video modules and the questions of the course
incorporated a wide variety of general knowledge questions (whereas pre- and post-
training materials focused on the populations of major UAs).6 At the beginning of the
training, participants were presented the course objectives and completed a bench mark
test (10 binary and 10 interval estimation questions). They also learned about
calibration and overconfidence research, and the effects of the course on the calibration
skill in previous cohorts. After that, they received both outcome and performance
6 See examples in Exhibit 5.1 in Hubbard, 2014, p. 96.
EFFECT OF CALIBRATION TRAINING
feedback on their initial benchmark test.
After these initial stages in training, participants were taught key strategies
improve their calibration and completed two calibration tests with both outcome and
performance feedback. One strategy they learned about was the equivalent bet test
where participants are asked to choose between two hypothetical games (Hubbard &
Seiersen, 2016). In Game A, they win $1,000 if their judgment is correct. In Game B,
they spin a wheel with a chance to win $1,000 equal to their stated confidence. If they
prefer Game A, they increase their confidence level, whereas if they prefer Game B,
they decrease their confidence level. The participant replays the equivalent bet until
they are indifferent between Games A and B, indicating their true degree of uncertainty.
The same process is applied to interval estimation questions by treating the lower- and
upper-bound estimates as separate binary choice questions. Participants adjust the
relevant bound while confidence remains fixed at 95%.
After learning the equivalent bet test, participants were introduced to Klein‚Äôs pre-
mortem, which was described as a ‚Äúprospective hindsight approach‚Äù to improving
calibration. The pre-mortem is applied in four steps: (1) participants make their initial
estimate, (2) assume their answer is wrong, (3) explain why their answer is wrong, and
(4) update their estimate accordingly. For example, a participant might estimate the
population of Toronto, assume that their estimate is wrong, identify plausible reasons
why (e.g., lack of familiarity with Toronto), and then widen their range.
Near the end of training, participants completed three calibration tests in series,
receiving feedback after each one and were now familiar with each of the steps in the
EFFECT OF CALIBRATION TRAINING
calibration training process. At the end of training, participants reviewed a final
calibration process: (1) choose initial estimates starting with extremely wide ranges for
interval questions, (2) apply the equivalent bet test, (3) apply Klein‚Äôs pre-mortem, and
(4) apply recommended confidence and interval adjustments. Participants were
reminded to apply each step of the calibration process and to review previous modules
if their performance did not improve over the next sequence of tests. They were also
instructed to avoid backsliding if they initially performed well.
Post-training
The post-training followed an identical procedure to the pre-training except that
after completing the judgment tasks, participants were asked to indicate their familiarity
with the 71 unique UAs encountered during the both phases of the experiment on a
scale from 1 (‚ÄúI have never heard of it prior to this study‚Äù) to 5 (‚ÄúI am very
knowledgeable about it‚Äù). UAs were presented in random order per participant.
Metrics
The current primary dependent measures are miscalibration and bias. In the
binary choice task, miscalibration across the set of items is the absolute value of the
difference between the participant‚Äôs mean confidence and the proportion of correct
responses the participant had (i.e., the participant‚Äôs accuracy rate). Bias is defined as
the arithmetic difference between the participant‚Äôs mean confidence and the proportion
of correct responses the participant had. In the interval estimation task, miscalibration is
defined as the absolute value of the difference between .90 (i.e., 90% confidence) and
the proportion of correct responses (i.e., when the true value falls in the estimated
EFFECT OF CALIBRATION TRAINING
interval), whereas bias is the arithmetic difference between .90 and the proportion of
correct responses. Unsurprisingly, the correlation between accuracy and calibration is
strong for both the binary task [r(68)=.62, p<.001] and the interval estimation task
[r(68)=-.98, p<.001] and likewise for the correlation between accuracy and bias on both
the binary [r(68) =-.70, p<.001] and interval estimation tasks [r(68)=1.00, p<.001].
Results
Analyses were not preregistered but are available along with the data at
As noted in Footnote 2, one participant failed to report: (i) the date of their CPA training,
(ii) whether they had been familiar with such training, and (iii) their familiarity ratings of
the 71 unique UAs at the end of the post-training phase (hence, could not be included in
analyses with familiarity). Another participant consistently reported values in the interval
task well beyond 10 billion (10,000,000,000), which would seem misguided given the
current global estimated population. Despite the atypical responses of the
aforementioned participants, results are qualitatively the same with and without these
individuals. The current results include these individuals where possible in the main text
(analyses without them can be found in the supplementary materials at
ANOVAs are generalized eta squared, ùúÇ , which is more comparable across within- and
between-participant designs and are expected to be smaller than partial eta squared in
repeated-measures designs with multiple factors (Bakeman, 2005; Olejnik & Algina,
2003).
EFFECT OF CALIBRATION TRAINING
Preliminary Analyses
The CPA course is designed to improve calibration (and bias). However, one
would not expect it to improve accuracy on binary answers to general knowledge
questions as in the present binary choice task. In line with this expectation, mean
accuracy in the pre-training version of the binary choice task (M=.80, SD=.12) was not
significantly lower from mean accuracy in the post-training version of the same task (M
=.76, SD =.11). If anything, accuracy was numerically higher before training, though not
significantly [t(69)=1.98, p=.052, d=0.24].
Familiarity
We examined the relation between participants‚Äô rated familiarities with UAs
presented in the post-training phase and their post-training accuracy within each task.
Provided that participants rated the familiarity of the 71 UAs after the post-training task,
analyses examining the influence of familiarity with accuracy are only appropriate for
post-training data to rule out the possibility of history effects for assessments of UAs
presented during pre-training. We computed the correlation coefficient between post-
training UA familiarity and post-training accuracy within each task (for the binary choice
test wherein two UAs were presented simultaneously, the mean UA familiarity was
used) for each participant. The mean correlation between familiarity and accuracy was
.07, 95%CI [.03, .13] in the binary choice task, and -.14, 95%CI [-.18, -.08] in the interval
estimation task (20 participants excluded due 0 or 100% accuracy on the interval task).
The 95% CIs were bias-corrected accelerated bootstrap confidence intervals using
10,000 samples and they indicate a small positive relation between familiarity and
EFFECT OF CALIBRATION TRAINING
accuracy for the binary choice task, but suggest a small negative relation between
familiarity and accuracy for the interval task. One possible explanation for the latter
finding is that as familiarity increases, interval width may decrease. Hence, if
participants are giving more narrow intervals for UA regions they are familiar with, they
may be incorrect and the correct population could be less likely contained by a narrower
interval. Providing potential support for this idea, the mean correlation between post-
training familiarity and post-training interval width was negative but with a small
coefficient of -.11, 95%CI [-.18, -.04].
Demographics
age, education, and years in the intelligence community) and miscalibration and bias
separately for the two tasks. The only significant correlation between a demographic
variable and a dependent measure was between age and miscalibration and for the
interval estimation task in particular. Older participants were better calibrated (i.e., had
lower miscalibration), but note that the effect is small and the significance is uncorrected
for family-wise error (would be not significant with a Bonferroni correction; Dunn, 1961).
EFFECT OF CALIBRATION TRAINING
Bivariate analyses of key demographics and key dependent variables by task.
Note. Bivariate analyses of main variables coloured by task (Pink = interval estimation task; Blue =
binary choice task) where relevant. The upper-right panels present the Pearson correlations (Point
biserial when involving the demographic variable of sex) and significance (*p<.05, **p<.01, ***p<
.001). The diagonal displays (from top to bottom) the proportion of females and males (respectively),
the distribution of age, education level, and years in the intelligence community (demographics not
coloured by task as all individuals completed both tasks) and the distribution of calibration and bias
(coloured by task). The left column presents box plots of the demographic variables by sex and box
plots of the dependent measures as a function of sex and task. The lower-left panels (excluding the
first column) present the bivariate distributions and slopes.
EFFECT OF CALIBRATION TRAINING
Miscalibration
We examined the influences of training and task on participants‚Äô miscalibration
using repeated measures analysis of variance (ANOVA). There were main effects of
2 2
training [F(1,69)=59.48, p<.001, ùúÇ =.13] and task [F(1,69)=73.61, p <.001, ùúÇ =.30].
ùê∫ ùê∫
Miscalibration was greater before training (M=.33, SD=.12) than after training (M=.18,
SD=.08) and greater in the interval estimation task (M=.37, SD=.04) than in the binary
choice task (M=.14, SD=.01). The main effects were qualified by a significant interaction
[F(1,69)=75.56, p<.001, ùúÇ =.17]. Assessing the simple effect of training within task, we
found that training significantly reduced miscalibration in the interval estimation task
[t(69)=8.80, p<.001, d=1.05], but training did not significantly affect miscalibration in the
binary choice task. In fact, in the latter task, miscalibration was worse after training,
although the decrement was not statistically significant, [t(69)=1.80, p=.076, d=0.22].
Bias
We similarly examined the influence of training and task type on participants‚Äô bias
using repeated measures ANOVA. There were main effects of both training
2 2
[F(1,69)=113.44, p<.001, ùúÇ =.20] and task [F(1,69)=335.83, p < .001, ùúÇ =.54]. Bias was
ùê∫ ùê∫
greater before training (M=.21, SD=.11) than after training (M=.01, SD=.10) and greater
in the interval estimation task (M=.34, SD=.01) than in the binary choice task (M=-.12,
SD=.01). These main effects were qualified by a significant interaction [F(1,69)=60.60,
p<.001, ùúÇ =.13]. When assessing the simple effect of training within task, we found that
training significantly increased bias in the binary choice task [t(69)=3.11, p=.003,
EFFECT OF CALIBRATION TRAINING
d=0.37] and significantly reduced bias in the interval estimation task [t(69)=9.83, p<.001,
Training Generalizability
Finally, we examined the generalizability of calibration training across tasks.
First, we computed the standardized difference between pre- and post-training within
each task separately for miscalibration and bias. Then, we tested whether these
standardized differences correlated across task. In fact, these correlations were not
statistically significant: for miscalibration, r(68) = - .06 , p = .638; for bias, r(68) = - .10, p
= .411.
EFFECT OF CALIBRATION TRAINING
Effect of training (pre-training vs. post-training) on each task (binary vs. interval) on both
calibration and bias.
Note: Histograms are sample data with each stroke representing one participant and where bin
width is .10; Box plots are sample data; Group-level means are depicted by the black points and
error bars are bootstrap bias-corrected accelerated 95% confidence intervals using 10,000
samples. Perfect calibration is represented by the horizontal dashed gray line.
EFFECT OF CALIBRATION TRAINING
General Discussion
Calibration is a vital skill component of judgment accuracy (Lichtenstein &
Fischhoff, 1980; Mandel & Barnes, 2014). Commercial calibration training offers one
possible route for organizations that seek to develop the calibration skill of their
employees and tamp down on judgment biases (e.g., overconfidence and
underconfidence) that take the form of miscalibration. Although the prospect of
commercially available solutions to improve judgment quality may be enticing, such
training seldom receives interventions testing by independent researchers.
Presently, we examined the influence of commercial calibration training offered
by HDR in the form of its CPA course on the degree of calibration and bias of
intelligence analysts working across Canadian government departments. Generally, the
CPA course significantly reduced miscalibration and bias. However, the effect of training
on metacognitive performance depended on the task. Calibration performance
increased after training in the case of the interval estimation task, but decreased after
training in the binary choice task such that analysts became more biased (i.e.,
underconfident). Training improved calibration and reduced bias only on the task for
which analysts were initially overconfident (the interval estimation task) while training
exacerbated the extant bias on the task for which analysts were initially underconfident
(the binary choice task). We did not expect one task to yield overconfidence and the
other to yield underconfidence. Instead, the tasks were selected because they reflected
the types of judgments that were featured in the CPA course. The opposing biases
yielded by the task types was fortuitous, however, for it revealed an effect that would
EFFECT OF CALIBRATION TRAINING
likely have otherwise been undetected. Had we only administered the interval
estimation task, training would appear highly effective at improving calibration.
Conversely, had we only administered the binary choice task, training would appear
detrimental to good judgment.
Confidence Reduction
With both tasks yielding opposing biases, the findings indicate that the principal
effect of training is to shift bias toward becoming less confident. After the present
training, individuals initially overconfident become better calibrated after becoming less
confident while individuals who are initially underconfident are at risk of becoming more
underconfident. One might infer from these results that an initially perfectly calibrated
judge will become miscalibrated following training provided an expected shift towards
underconfidence. These findings bear not only on the efficacy of the CPA course, but
raise a fundamental question about all calibration training: does calibration training
reduce miscalibration by improving metacognitive monitoring of confidence in one‚Äôs
judgments, or does it appear to reduce miscalibration because of a simple shift in bias
during a context prone to initial overconfidence? This question is of both practical and
theoretical importance.
If the effect of calibration training is driven by bias shift, then it is vital to know
what, if any, bias is inherent in the judge or what bias might be encouraged by a given
task. A bias-shifting intervention may still be beneficial in terms of accuracy. However,
such benefit will be contingent on conditions remaining the same. Should the judge or
task (or the judge-task interaction) prompt an opposing bias or even an attenuation of
EFFECT OF CALIBRATION TRAINING
the same directional bias, then the debiasing intervention might harm rather than
improve judgment quality. As Chang et al. (2018) and Mandel and Irwin (2023) have
noted, intelligence community methods aimed at debiasing analysts‚Äô reasoning
processes and judgments ought to consider the fact that most cognitive biases are
bipolar (e.g., overshooting as in overconfidence or undershooting as in
underconfidence). Yet, the intelligence community seldom takes bias direction into
account and, quite often, the direction of bias is presumed (e.g., ‚Äúoverconfidence needs
to be reduced‚Äù; Chang et al., 2018; Mandel & Irwin, 2023).
The present findings raise important questions about the causal bases of
calibration training effects. On tasks that yield overconfidence, a reduction in
overconfidence can signal either a genuine improvement in calibration skill (i.e., a better
ability to metacognitively monitor the appropriate level of confidence to assign to a
judgment or choice) or a simple shift in bias. As noted, a simple shift in bias is of
unreliable benefit because any changes that prompt a change in the direction of bias or
that eliminate the bias would lead to impairment of judgment quality. A genuine
improvement in the metacognitive ability to monitor appropriate confidence levels in
judgment should result in confidence reduction where overconfidence is manifested,
and confidence increases where underconfidence is manifested.
Moreover, if metacognitive monitoring were strengthened from training, one
might also expect that changes in performance on the two tasks would be correlated.
However, changes in performance across tasks were uncorrelated in this study,
consistent with previous work showing that calibration skill does not easily generalize
EFFECT OF CALIBRATION TRAINING
across judgment tasks (Keren, 1985 as cited in Keren, 1987; Solomon et al., 1985).
That the effect of CPA training did not appear to generalize between the tasks we used,
even though both tasks drew on similar knowledge (i.e., UA populations), supports a
bias-shifting mechanism rather than genuinely improved metacognitive monitoring of
confidence in judgment. Future research could profitably examine this issue, for
instance, by systematically varying task difficulty in the training and test phases of an
interventions-testing study. If one were trained on difficult items and learned mainly that
they should be less confident, then when presented with easy items, they may be
expected to exhibit underconfidence.
Training Content
It is also important to consider the potential partiality within the training materials
themselves. The content of the current CPA course, in particular, may have caused a
shift in bias towards confidence reduction considering much more attention is paid to
reducing overconfidence than to reducing underconfidence. More direct references to
overconfidence rather than underconfidence were made and training examples focused
on overconfidence. Moreover, a few times throughout training, trainees are told that ‚Äúif
they are like most individuals, they will exhibit overconfidence, especially at first‚Äù and,
the tables used throughout the course to explain degrees of calibration show two levels
of overconfidence‚Äîslightly and extremely, while only showing one level of under
confidence (slightly). Finally, the strategies encouraged throughout the training arguably
promote underconfidence. For example, Klein‚Äôs premortem encourages trainees to think
of why their initial estimate is wrong which might encourage confidence reduction even
EFFECT OF CALIBRATION TRAINING
if individuals are already calibrated or underconfident.
We have not reviewed alternatives such as Good Judgment Incorporated‚Äôs
(2022) Superforecasting Fundamentals course to assess whether there is a similar bias
toward reducing confidence. However, we recommend that organizations seeking to
improve calibration skill review products with this question in mind and also consider
whether they have reason to care more about minimizing one type of miscalibration
over another. If, for instance, they know that their experts are overconfident, then
training that encourages less confidence may still have a desired effect.
Task-Expert Interactions
Note that the present sample of intelligence analysts may be more
underconfident than samples of non-analysts or, for that matter, from other groups of
experts. For instance, Mandel and Barnes (2014, 2018) found that strategic intelligence
forecasts made by Canadian intelligence analysts were underconfident. Forecasts
made by intelligence analysts often bear much similarity to the binary choice task in the
present study, which also demonstrated underconfidence. In contrast, the interval
estimation task has less of a deep-structure resemblance to the typical judgments by
analysts which might explain why analysts were overconfident. The binary choice task
might not prompt underconfidence in non-expert samples, suggesting that careful
attention to potential task-expertise interactions is needed.
Conclusion
This study examined the effect of commercial calibration training on the calibration of
professional intelligence analysts across two judgment tasks and found that there was
EFFECT OF CALIBRATION TRAINING
an overall reduction in miscalibration and bias after training. However, in a task wherein
experts tended to already exhibit underconfidence prior to training, underconfidence
increased rather than decreased. Taken together, the findings indicate that the training
led to a bias shift toward lower confidence rather than an improved ability to
metacognitively monitor confidence in judgment.
EFFECT OF CALIBRATION TRAINING