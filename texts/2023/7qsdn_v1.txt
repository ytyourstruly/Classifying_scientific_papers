NOWLEDGE CUES TO HUMAN ORIGINS FACILITATE
SELF DISCLOSURE DURING INTERACTIONS WITH CHATBOTS
A P
REPRINT
Gabriella Warren-Smith Guy Laban
School of Education Department of Computer Science and Technology
University of Glasgow University of Cambridge
Glasgow, United Kingdom Cambridge, United Kingdom
gabriella.warren-smith@glasgow.ac.uk School of Psychology and Neuroscience
University of Glasgow
Glasgow, United Kingdom
gl538@cam.ac.uk
Emily-Marie Pacheco Emily S. Cross
School of Health in Social Science Professorship for Social Brain Sciences
University of Edinburgh ETH Zürich
Edinburgh, United Kingdom Zürich, Switzerland
School of Psychology and Neuroscience
University of Glasgow
Glasgow, United Kingdom
MARCS Institute for Brain, Behavior and Development
Western Sydney University
Sydney, New South Wales, Australia
ecross@ethz.ch
BSTRACT
Chatbots are emerging as a self-management tool for supporting mental health, appearing across commercial and
healthcare settings. Whilst chatbots are valued for their perceived lack of judgement, they lack the emotional
intelligence and empathy to build trust and rapport with users. A resulting debate questions whether chatbots
facilitate or hinder self-disclosure. This study presents a within-subjects experimental design investigating the
parameters of self-disclosure in social interactions with chatbots in an open domain. Participants engaged in two
short social interactions with two chatbots: one with the knowledge they were conversing with a chatbot and
one with the false belief they were conversing with a human. A significant difference was found across both
treatments, with participants disclosing more to the chatbot that was introduced as a human, as well as perceiving
themselves to do so, perceiving this chatbot as more comforting, and to be demonstrating higher rates of agency
and experience compared to the chatbot that was introduced as a chatbot. However, significant findings also
indicated participants’ disclosures to the chatbot that was introduced as a chatbot were more sentimental, and
they found it to be friendlier compared to the chatbot that was introduced as a human. These results indicate that
whilst cues to a chatbot’s human origins enhance self-disclosure and perceptions of mind, when the artificial
agent is perceived against one’s social expectations, it may be viewed negatively on social factors that require
higher cognitive processing.
Keywords Chatbots · Self disclosure · Human-agent interaction · Communication · Social Perception
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
1 Introduction
users due to their perceived lack of judgement (17). In a
qualitative study investigating the expectations of mental
Chatbots, also known as conversational agents, are artifi- health service users on the application of chatbots in MHC
cial intelligence (AI) programs designed to interact with contexts, participants expressed their desire for a positive
humans by simulating conversation via text or speech (1). chatbot which could build rapport and act in a supportive
Appearing in roles across health communication, symptom and informative manner (17). Current chatbot systems
monitoring, cognitive behavioural therapy, and behavioural were perceived to lack in robotic intelligence and flexibil-
analysis, chatbots offer promising benefits and advances ity, resulting in a reluctance for individuals to consolidate
in the innovation of mental health care (MHC) (2). Ac- their feelings with a chatbot. One participant reported that
cess to MHC is in high demand, yet the provision of these “I wouldn’t talk to the chatbot about things if I was having
services is in short supply globally (3). Barriers to MHC a very bad mental health day, I need a person”. However,
such as perceived stigma, accessibility issues and low- another participant reflected that there are advantages in re-
perceived need (4), contribute to a lack of formal treatment vealing personal information to a chatbot “because it’s not
and unmet healthcare needs. Technological solutions to human and they don’t feel judged. You can be more honest
MHC are rising to meet this provision gap, with an esti- with it.” These contradicting viewpoints open a prominent
mated 10,000 MHC apps commercially available (5), and debate regarding chatbots’ capacity in facilitating, detect-
chatbots emerging as a promising MHC digital system ing, and responding to human emotion. These attributes
(6; 7; 8; 9). Chatbots fulfil MHC provision gaps by being are fundamental in the facilitation of self-disclosure be-
constantly available over lengthy periods of time without tween a client and therapist, an intimacy-building process
experiencing fatigue (6). Users can access chatbots’ ser- involving the sharing of personal and emotional informa-
vices via mobile devices at a desired time and place, a tion (19; 20).
feature which has contributed to their high rates of acces-
Self-disclosure can be defined as exposing one’s true self to
sibility and ease of use (6). Moreover, users often report
others, fostering greater self-knowledge by developing an
high satisfaction rates in various settings due to chatbots’
awareness of one’s emotions that were previously unappar-
language use and anthropomorphic cues (10; 11). In fact,
ent (21; 22), whilst enhancing one’s coping effectiveness
compared to static traditional mobile applications, chatbots
and wellbeing (23). Self-disclosure with a chatbot opens
(see (12)), as well as other cognitive artificial agents (see
new interaction-related parameters, removing barriers to
(13)), can simulate more interactive experiences and elicit
self-disclosure linked to increased vulnerability and fear
richer self-disclosures from users due to their language
of judgement (21; 24; 25; 26). Prior studies have shown
modality; communicating in a socially meaningful way via
that opening up to artificial agents can enhance individuals’
ongoing flowing dialogue.
mood and diminish sensations of loneliness and stress (27).
Chatbots are in the process of integration into MHC. In a Additionally, it has been observed that individuals expe-
scoping review by Abd-Alzareq et al. (14) evaluating 53 riencing negative emotional states (such as lower mood,
studies on the use of chatbots in MHC, 41 unique chatbots increased feelings of loneliness, and higher stress levels),
were identified, with 17 involved in the use of therapy. particularly those who identify as more introverted, tend to
The application of chatbots for therapeutic and counselling disclose more to artificial agents (28). However, chatbots
purposes is gaining much research attention, with chatbots also lack the intimacy of a human partner, and whilst some-
such as Woebot and Wysa gaining increasing numbers of times able to identify or recognise user emotion, they lack
user downloads for the treatment of anxiety and depression the depth and functional ability to reflect and respond to hu-
(15). Nevertheless, there is a major discrepancy regarding man emotion (29). Several studies investigating the trans-
the suitability of chatbots in these contexts, reflected in lation of self-disclosure in human-computer interaction
qualitative studies and survey research addressing percep- (HCI) have identified anonymity and embodiment as facili-
tions of clinicians and MHC patients (e.g.,(2; 6; 16)). tators of self-disclosure to conversational (embodied and
disembodied) artificial agents (e.g., (24; 26; 27; 30; 31)).
Chatbots are often criticised for their lack of emotional
But due to their varying results and a lack of evidence
intelligence and language comprehension (e.g.,(17; 18)),
examining the specific effects of chatbot technology, more
resulting in high user dropout rates and decrease in trust
research is necessary to define the practical challenges and
(4)). Yet, chatbots are also proposed to elicit openness from
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
affordances of emotion-based interaction in chatbot-human (38; 39; 40; 41). For example, Xu (42) found that social
interaction. Whilst chatbot communication may encourage robots with a human voice elicit higher trust in users, in
and elicit rich self-disclosure, it is also considered artificial comparison to robots with a synthetic voice. This research
and distant. Accordingly, the present study evaluates how was then extended by the Media Equation Theory (43; 44),
people respond to chatbots and their artificial nature, com- which asserts that people treat and respond to media in the
pared to disembodied agents that demonstrate human-like same way as they would to another human, particularly
cues and aim to facilitate and introduce a more humane ex- when the media appears human-like. Cues to a chatbot’s
perience. This will enable the study to address the research humanness in human-agent interaction (HAI) have been
question: found to enhance: the natural experience of conversation
(45); user experience in chatbot interfaces (46); and trust
RQ1: to what extent do online users self-disclose to a chat-
and resilience in the design of chatbots (39; 47).
bot that is introduced as a chatbot compared to a chatbot
introduced as a human? Media richness theory is concerned with the ability of a
communication medium in reproducing and communicat-
Furthermore, due to the potential role of chatbots in digital
ing complex messages (48). It predicts that disclosure
MHC, we are also aiming to further understand how people
in HCI is enhanced by media with the capacity to com-
perceive these agents and the quality of both interactions.
municate richer social cues (30). Whereas early research
Hence, our second research question is as follows:
investigated the influence of bottom-up cues such as the
RQ2: to what extent do online users’ perceptions of a chat-
physical objective properties of media (49; 50; 51), recent
bot and their interaction with it differ due to the chatbot
research is concerned with subjective properties such as
being introduced as a human or a chatbot?
attitudes and perceptions of animacy (52).
The present preliminary study investigates parameters of
Nonetheless, experimental approaches investigating the
self-disclosure and perception in interactions with two
relationship between partner identity and self-disclosure in
social chatbots in open domains via a within-subjects ex-
HAI have reached mixed findings. A common approach in
perimental design. Participants took part in short written
examining these variables is to manipulate the interaction
social interactions with two chatbots in two treatments:
between a virtual agent and participant, using deception
one with the false belief that they are conversing with a
to change the perceived identity of the interaction partner.
human; and the other with the knowledge that they are con-
These methods have identified several critical variables that
versing with a chatbot. By analysing and comparing the
facilitate self-disclosure, including identity and anonymity
differences within the two treatments, the study measured
(e.g., (24)), rapport (e.g., (53)), topic sensitivity (e.g., (54)),
self-disclosure in terms of participants’ subjective rates of
and embodiment (e.g., (30)). However, whilst these fac-
self-disclosure, and the number of words disclosed to both
tors are argued to facilitate self-disclosure, contradictory
agents. Moreover, participants perceptions of the agents
results imply they function differently depending on the
and the interaction were measured in terms of perceived
technology applied, with differences appearing between
agency and experience (see (32), comforting responses (see
social robots, disembodied agents, and embodied virtual
(33)), trust (see (34; 35)), and friendliness (see (36)). The
agents.
preliminary results of this study will inform the growing
role and potential of chatbots in digital MHC, strengthen-
2.1 Identity, Anonymity and Rapport
ing their approach to facilitating patients to discuss and
manage their health.
There are a small number of studies investigating interac-
tions with social chatbots in open domains that suggest that
2 Theoretical framework and related work partner identity does not affect self-disclosure (55; 56). For
instance, Ho et al. (55) applied the Wizard of Oz (WoZ)
According to the Computers as Social Actors theory method to compare the belief manipulation effects of a
(CASA) theory, people have a psychological tendency to chatbot and human on self-disclosure, measured in terms
attribute human properties to computers (37). This theory of participants’ factual and sentimental responses. No sig-
has informed the design of social agents to enhance trust, nificant interactions or main effects were observed, with
social interaction quality, and rapport building in HCI (e.g., participants interacting with both agents in the same way.
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
Yet, the disclosure measured was to an external human 2.3 Embodiment
agent rather than to the chatbot the participant interacted
Embodiment refers to the nature of being a living, think-
with. Conversely, some research findings suggest that inter-
ing and feeling body, situated in a physical world (59).
action with embodied artificial agents such as social robots,
It has received much attention in HCI research, concern-
avatars, and virtual humans (computer-generated moving
ing the extent to which the embodiment of an artificial
images of a human) facilitates self-disclosure in compari-
agent is crucial for effective communication with humans
son to a human partner. Lucas et al. (24) applied the WoZ
(60). Laban et al. (30) found embodiment to play an
method to compare the manipulation effects of two condi-
important role in self-disclosure during communication
tions on participant self-disclosure and found that virtual
with artificial (and human) agents that answer to a varying
humans promote willingness to self-disclose information
embodiment. Three agents of varying embodiment were
due to perceived anonymity. Across both conditions, par-
examined, including a human, a humanoid social robot,
ticipants interacted with a virtual human in a clinical inter-
and a disembodied voice assistant. Overall, disclosure dif-
view, answering emotionally provoking questions related
fered by the agent but not by the topic of the interaction,
to their family and personal relationships. Participants dis-
with participants disclosing most to the human partner and
closed more to the agent conveyed as computer-moderated
least to the disembodied agent. These findings sit strongly
(high anonymity) in comparison to the agent communi-
in line with the media richness and media equation theo-
cated as human-controlled (low anonymity). In a later
ries, with participants disclosing most to the agent with the
study, Lucas et al. (53) found that military service mem-
richest communication and social cues. However, Laban’s
bers disclosed more post-traumatic stress disorder (PTSD)
findings apply to physical settings and might also relate to
symptoms to virtual humans than to an anonymised Post-
the agents’ physical presence and physical embodiment.
Deployment Health Assessment (PDHA). This helped de-
This corresponds well to the findings of (61), where partic-
termine the role of rapport in facilitating self-disclosure
ipants may have disclosed more to virtual agents than to
which was the key difference between both conditions.
a static questionnaire because the virtual agent embodied
was richer. Online interactions with disembodied agents
(such as chatbots) or computer-mediated communication
2.2 Topic Sensitivity
(CMC) with human agents might manipulate different psy-
chological mechanisms and trigger different reactions and
Whilst neither study examined the relationship between
behaviour from the ones observed in (30). The findings
disclosure partner and topic, Yokotani et al. (54) found
of (24) show that when the interaction is disembodied, the
that participants disclosed more to virtual humans only
belief mechanisms regarding the partner’s origins take a
when disclosing stigmatising information about sex-related
substantial role in eliciting disclosure. Accordingly, it is
symptoms. For all other topics, participants disclosed more
important to differentiate between clear physical cues of
mental health symptoms to an experienced human psychol-
social embodiment (i.e., stimulus cues), and belief cues
ogist. This differed from a study by Tsai et al. (56) where
that address if the agent is of human or artificial origins
participants disclosed more sexually transmitted disease
(i.e., knowledge cues) (see (62)). Hence, perceptions of
symptoms to a perceived human representative, in com-
artificial agents are not only grounded in physical features
parison to a chatbot. However, the chatbot and human
(i.e., stimulus cues) but are also shaped by prior knowledge
performed comparatively in terms of their influence on
(i.e., knowledge cues) (62).
participant’s behaviour and their perceived usefulness rat-
ings, and participants typed significantly fewer sentences
to the human representative in the embarrassment condi-
tion. These findings are conflicted and may be subject to
unaddressed factors such as perceived animacy and embod-
iment. The information conveyed by the agent should not
only consider the topic’s sensitivity, but also be logical and
free from fallacies (57), moral and attentive (58), all the
while maintaining consistency with its physical or virtual
representation (12; 30).
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
3 Methods
made aware of their rights to leave the experiment if de-
sired. Participants were given the option of withdrawing
Consistent with recent proposals (63; 64), we report how
their data during three weeks after participation. To min-
we determined our sample size, all data exclusions, all
imise distress, participants were debriefed at the end of the
manipulations and all measures in the study . In addi-
experiment about the deception methods, with an explana-
tion, following open science initiatives (e.g., (65)), the de-
tion of why this method was important.
identified data sets, stimuli, and analysis code associated
Participants were prompted to disclose personal informa-
with this study are freely available online . By making the
tion about themselves. Although the topics were neutral,
data available, we enable and encourage others to pursue
they may prompt unexpected experiences of negative emo-
tests of alternative hypotheses, as well as more exploratory
tion, resulting in psychological risks such as depression,
analyses.
increased anxiety, embarrassment, or shame (66). To min-
imise these risks:
3.1 Experimental Design
• (1) The chatbots were programmed to respond to
A within-subjects experimental study was conducted with
participants with empathetic language, without
two treatments (Sam - a chatbot introduced as a human
offering advice on the disclosed information.
vs. Chatbot D12 - a chatbot introduced as a chatbot). The
• (2) The questions were phrased to minimise psy-
two chatbots created for this study were fully automated
chological distress.
and communicated identical content. The chatbots differed
only in their names, the way they were introduced, and
• (3) Participants were provided with a list of the
their response rate (see 3.4:Stimuli for more information).
British Psychology Society (BPS) approved men-
Participants interacted with both chatbots in a randomised
tal health services.
order. Each chatbot asked the participants 4 questions,
including 2 questions about 2 different topics. The order of
3.4 Stimuli
the topics and their allocation per chatbot was randomized
3.4.1 The Chatbots:
using counterbalancing technique.
The chatbot featured in both treatments was a rule-based
3.2 Participants
program designed to simulate a flowing dialogue via text,
developed specifically for this experiment (see (67)). Both
The study included a final sample size of 22 participants.
chatbots asked the same pre-scripted questions and re-
Participant data were excluded due to technical issues in
sponded using the same responses. Each question initiated
the dialogue with the agents (N = 3), and if the researcher
the same pre-scripted response, and was applied identically
observed any major issues in the conversation (e.g., the
across the two chatbot treatments.
participant ignored the questions asked by the chatbots)
(N = 2). Participants were recruited via social media (e.g.,
The chatbot that was introduced as a chatbot was named
Facebook, Instagram, Twitter), and were between the ages
‘Chatbot-D12’ to illustrate that it is a machine/algorithm.
of 23 to 62 (Mean age = 38.4, SD = 13.1), 59% females,
Before the interaction with Chatbot-D12, participants were
41% males. All participants reported they were native En-
introduced to it with the following text: “You are going to
glish speakers and had access to a personal computer and
speak with Chatbot D12, a chatbot programmed to speak
stable internet connection.
with you today.” Chatbot D12 responded immediately to
3.3 Ethics
The chatbot that was introduced as a human was named
University of Glasgow Ethics Committee (reference num- the interaction with Sam, participants were introduced to it
ber 402210230). All participants provided written in- with the following text: “You are going to speak with Sam,
formed consent before participating in the study and were a research assistant at the University of Glasgow. Sam
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
has access to some predefined questions and responses self-reported ten items about the extent to which
that will be used to have a conversation with you”. This they disclosed information to each chatbot on a
priming mechanism was informed by the approach taken scale of 1 (not at all) to 7 (to a great extent).
by (24) in the belief manipulation of the WoZ experiment.
Sam responded to participants with small delays to appear 3.5.2 Perception:
more human-like (<10 words = 1 second delay; 10 - 17
• Agency and Experience: Research into mind per-
words = 1.5 second delay; 18 - 23 words = 2 second delay).
ception entails that agency (the ability of the agent
The study takes the following limitations to rule out the to plan and act) and experience (the ability of the
effects of additional factors on emotional disclosure, whilst agent to sense and feel) are the two key dimen-
ensuring the only variable manipulated is the perception sions when valuing an agent’s mind (32). To de-
of the conversational partner: termine whether a difference in mind perception
emerged between the agents, after each interac-
• (1) Both treatments were counterbalanced to en-
tion participants were requested to evaluate the
sure that the order of the questions, the topics,
agent in terms of experience and agency, after be-
and agent interaction were randomised.
ing introduced to these terms (adapted from (32)).
• (2) Both chatbot names are androgynous to rule Both concepts were evaluated by the participants
out gender effects. using a 0 to 100 rating bar.
• (3) The treatments did not differ in their language • Comforting Responses: To measure the extent to
to rule out linguistic differences as a contributing which participants felt comforted by their interac-
factor. tion with both agents, and the extent to which they
felt that the agents’ responses were comforting,
3.4.2 The Task:
participants answered the comforting response
scale (CRS; (33)), adjusted for the application of
The task included 4 topics, adapted from (30; 27; 68; 69).
artificial agents (see (27)). The scale is compiled
The topics (physical health, work situation, social life,
of 12 self reported items rated on a seven-point
and goals and ambitions) were aimed to elicit rich self-
scale, ranging from 1 (I strongly disagree) to 7 (I
disclosure, address participants’ quality of life (see (70)),
strongly agree). This scale helps to determine the
and encourage deep self reflection (see (27; 68; 69)).
effects of both treatments on participants’ emo-
tional state.
3.5 Measurements
• Trust: To examine the extent to which a differ-
3.5.1 Disclosure:
ence in trust was perceived during interaction in
both treatments, participants rated to what extent
The study collates both subjective and objective measures
they agreed with 12 statements on the Checklist
of self-disclosure to account for the multi-dimensional and
for Trust between People and Automation (73) on
complex nature of self-disclosure (71; 30).
a seven-point scale, ranging from 1 (Not at all) to
7 (Extremely).
• Length of the disclosure is represented via the
number of words disclosed to the agent by the
• Friendliness: This scale was aimed at capturing
participant (see (30; 27; 68; 69)).
how participants perceived the chatbots in terms
• Sentimentality is the ratio of overall sentiment of friendliness and warmth using one item from
(both positive and negative) calculated based on (74) and two items from (75), as suggested by
the combined scores of Vaders’ positive and neg- (55). These items were evaluated on a seven point
ative sentiments (see (72)). scale raging from 1 (not at all) to 7 (extremely).
• Subjective self-disclosure is based on participants’ • Interaction quality: This scale originally aimed
responses to an adapted version from Laban et al. to capture how participants perceived and evalu-
(30) to Jourard’s Self-Disclosure Questionnaire ated interaction with a chatbot using an adapted
(22). In response to each treatment, participants and adjusted version by (76) for a scale by (77).
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
(2) Chatbot D12 - the chatbot that is introduced as a chatbot.
Each interaction included two random items out After each interaction ended, participants rated the agent
of six, except for the mid-session (session 5) and in terms of agency and experience (see (32)), their subjec-
the last session (session 10) that included all six tive self disclosure to each agent (see (30; 22)), the agents’
items of the scale. These items were evaluated on comforting responses (see (33)), the quality of the inter-
a seven point scale raging from 1 (not at all) to 7 action with the agent (see (77; 76), and how trustworthy
(extremely). (see (73)) and friendly (see (55)) the agent was perceived
to be. Participants were also asked to which origins they
• Agent’s origins: To understand the origins (hu- attributed the agents’ behaviour. Finally, the participants
man vs. artificial/machine) to which participants were debriefed about their participation in the study and
attributed the chatbot’s behaviour to, a final step were provided with a list of the BPS-approved mental
of the survey asked participants whether Sam was health services.
a human or a chatbot, and whether Chatbot D12
was a human or a chatbot.
4 Results
4.1 RQ1: to what extent do online users self-disclose
3.6 Procedure
to a chatbot that is introduced as a chatbot
compared to a chatbot introduced as a human?
After being recruited on social media (Facebook, Insta-
gram, Twitter, etc.), participants followed an anonymous
To answer the first research question a doubly multivariate
secure link to participate in the study on Qualtrics. After
analysis of variance was conducted to determine whether
reading the plain language statement and consenting to par-
a difference in subjective (i.e., perceived disclosure) and
ticipate in the study, participants interacted with the two
objective (i.e., length of the disclosure, and disclosure’s
chatbots (Sam and Chatbot D12) in a randomised order. In
sentimentality) measures of disclosure emerged within the
each treatment, the chatbots asked the exact same question
two agents (Sam - a chatbot that is introduced as a human
and responded with the same empathetic responses (e.g.,
vs. Chatbot D12 - a chatbot introduced as a chatbot).
“thank you for sharing that with me”, "I understand", etc.).
The model was found to be statistically significant,
With each chatbot, participants answered four questions re-
W ilk s Λ = .68, F (3,173) = 27.71, p < .001, suggest-
lated to two of four topics (physical health, work situation,
ing that a difference emerged in the combined disclosure
social life, goals and ambitions). Each interaction started
(in terms of subjective and objective disclosure) between
with the participant entering a unique ID to initiate the
the two agents.
ending the interaction, the chatbot explicitly states that the Univariate tests revealed a statistically significant large
interaction is over and that the participant may continue difference in subjective self-disclosure and a medium to
with the survey. large difference in the length of the disclosure between
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
Variable df F p η2
∗∗∗
Subjective Self-Disclosure (1, 175) 52.94 <.001 .23
∗∗∗
Length (1, 175) 26.01 <.001 .13
Sentimentality (1, 175) 6.44 .012 .04
∗∗∗
Agency (1, 175) 15.25 <.001 .08
Experience (1, 175) 5.09 .025 .03
∗∗∗
Comforting Responses (1, 175) 17.37 <.001 .09
Trust (1, 175) 3.48 .064 .02
Friendliness (1, 175) 4.32 .039 .02
Interaction Quality (1, 175) .11 .745 .00
∗ = p < .05, ∗∗ = p < .01, ∗∗∗ = p < .001
the two agents. Participants perceived to be sharing more compared to Chatbot D12 (M = 39, SD = 26.3; M =
information with Sam (M = 3.43, SD = 1.33) compared 26.18, SD = 21.86; M = 4, SD = 1.02). However, partici-
to Chatbot D12 (M = 2.93, SD = 1.46), and participants’ pants found chatbot D12 to be friendlier (M = 3.62, SD
disclosures were longer in the number of words shared = 1.43) than Sam (M = 3.49, SD = 1.31). Finally, there
when disclosing to Sam (M = 24.61, SD = 16.54) than are no differences in perceptions of the interaction quality
tests provided evidence for a statistically small to medium
difference in the sentimentality of the disclosures. Partici-
5 Discussion
pants disclosures to Sam (M = .26, SD = .19) were less
sentimental than their disclosures to Chatbot D12 (M =
The present study sought to examine the effects of convers-
ing with a chatbot introduced as a human, and its impact
on user’s perceptions and self-disclosure. Overall, a signif-
icant difference concerning objective and subjective disclo-
4.2 RQ2: to what extent do online users’ perceptions
sure was found between both treatments, with participants
of a chatbot and their interaction with it differ
disclosing more to a chatbot that is introduced as a human
due to the chatbot being introduced as a human
(Sam) in comparison to a chatbot introduced as a chatbot
or a chatbot?
(Chatbot-D12). Our findings indicate that compared to
To answer the second research question a doubly multi- Chatbot D12, participants perceived Sam with higher de-
variate analysis of variance was conducted to determine grees of agency and experience (see (32)), and as more
whether a difference in perceptions of the agents and the comforting. Interestingly, whilst no significant effects were
interaction emerged within the two agents (Sam - a chatbot observed regarding trust, participants found Chatbot-D12
that is introduced as a human vs. Chatbot D12 - a chatbot to be friendlier than Sam, and their disclosures to it were
introduced as a chatbot). more sentimental.
The model was found to be statistically significant, These results are especially interesting since all but one
W ilk s Λ = .79, F (6,170) = 7.35, p < .001, suggesting participant stated that they belived to be conversing with
that a difference emerged in the combined perception (in a chatbot when talking to Sam. Accordingly we assume
terms of agency, experience, comforting responses, trust, that people behave in a certain way to artificial agents and
friendliness, and interaction quality) across the two agents. perceive these accordingly, based on the way these are
introduced and not particularly based on their actual expe-
Univariate tests revealed statistically significant differences
rience with them. Nevertheless, it should be clear that the
in perceptions of agency (medium to large), experience
results of this study are mainly due to the cognitive effect
(small to medium), comforting responses (medium to
of introducing an agent in a certain way (i.e., knowledge
large), and friendliness (small to medium) between the
cues), rather than it appearing as such (i.e., stimulus cues),
two agents. Accordingly, it can be said that participants
and should be treated accordingly.
perceived Sam to demonstrate higher degrees of agency
(M = 44.60, SD = 25.88), experience (M = 30.05, SD = Our preliminary results have several implications and are
21.32), and comforting responses (M = 4.14, SD = .99) meaningful evidence for people’s psychological and cog-
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
by agent. (3) Mean differences of comforting responses scores by agent.
nitive mechanisms when interacting with artificial agents. tificial agents (or, via CMC interactions) in a controlled
The findings suggest that people may respond automati- manner (see (80; 82; 83; 84)). When interacting with con-
cally to artificial agents using available social cues. Al- versational artificial agents there are clear differences in
though participants suspected that Sam (i.e., the chatbot the manifestation of trust via behaviour (e.g., disclosing
that was introduced as a human) was an artificial agent, the information) and explicit subjective perceptions of trust in
provided information (i.e., the introduction of the agent as the agent. This conflict may be attributed to differences in
a human, having a human name, and responding slower) the cognitive affordances applied in these sort of interac-
still influenced their perception and reaction to the agent, tions (see (34; 35)). People’s behaviour (e.g., disclosing
causing them to act more socially compared to the way they information) during these interactions might therefore be
perceived and reacted to Chatbot D12 (i.e., the chatbot that automatic - processing available social cues and respond-
was introduced as a chatbot). As subjective evaluations of ing accordingly in an automatic manner. When evaluating
disclosures corresponded to objective disclosures, the re- complex social constructs like trust (35) and friendliness
sults lay evidence for how participants’ behaviour aligned (36), people invest more cognitive efforts in processing
well with their subjective expectations of themselves - de- their experiences. Accordingly, when people’s experiences
termining the extent to which they allowed themselves of the agent do not correspond to their expectations they
to disclose more to a fellow human rather than a chatbot, might evaluate the agent more negatively - in the context of
and also, experienced the agent to be more comforting. this study, this was evidenced in terms of trust and friendli-
These results follow the media equation theory (43) and ness. While participants automatically reacted with disclo-
CASA (78), which highlight the reactive way in which sure behaviour and perceptions of mind to social cues (and
users respond to artificial agents’ and medias’ social cues, the belief manipulation) presented by Sam (i.e., the chatbot
particularly when the agent identity and behaviour manifest that was introduced as a human), the dissonance between
human origins and qualities. Furthermore, there results are the way Sam was introduced and their demonstrated ar-
in line with previous results (see (30; 79)) that specifically tificial behaviour affected participants perceptions of the
address how people are relatively objective in their percep- complex social constructs of trust and friendliness.
tions of their disclosure behaviour to artificial agents. The
Whilst the results regarding trust cannot be generalised
present study’s results further contribute to these findings,
beyond this sample, they propose an interesting narrative
demonstrating how these mechanisms also take place in
that is in line with previous results (see (85; 58)), suggest-
text-based interactions with disembodied artificial agents.
ing that in the event of a chatbot’s behaviour mis-aligning
Nevertheless, when participants were required to evaluate with user expectations, users perceive the chatbot as riskier,
the chatbots in terms of trust and friendliness, Chatbot D12 resulting in a sense of resistance against the system. This
(i.e., the chatbot that was introduced as a chatbot) was per- interaction differs from the traditional therapeutic context
ceived to be friendlier and more trustworthy (only within of self-disclosure (21), where the presence of a subjec-
the current sample, as this trend cannot be generalized tive partner is essential for processes such as reciprocal
beyond the sample). The concepts of trust and friendliness sharing and the co-creation of meaning of the disclosed
are deeply rooted in people’s beliefs (80; 81). Accordingly, information (86; 87; 88; 89). Therefore, participants may
our results suggest that people may process information have mistrusted Sam due to the agent acting in an inconsis-
related to trust and friendliness when interacting with ar- tent manner from their expectations of a disclosure partner.
Knowledge cues to human origins facilitate self-disclosure during interactions with chatbots
A P
REPRINT
Moreover, this is in line with related cognitive mechanisms and oversight are not yet clearly established through pol-
that explain the dissonance that occurs when the behaviour icy (99). There is a lack of research addressing the new
of an artificial agent does not correspond with one’s expec- parameters of trust in the context of digital mental health
tations of it due to the demonstrated social cues (see (90)). technology, with the transparency of apps and agents being
Finally, we can state that whilst clear demonstrated social a central concern in the validation of their safety and effec-
cues of an agent’s origins (i.e., knowledge cues (62); e.g., tiveness (34; 35; 96). Researchers and practitioners should
having human name, longer response time, and being intro- therefore consider the correct balance of social cues while
duced as a human) can facilitate self disclosure behaviour avoiding serious deception.
and certain positive perceptions of it, it can also negatively
effect one’s long term use of the agent when processing
6 Conclusions
the information in a more controlled manner. Hence, as
described previously, users may find an agent to be less
The study findings indicate that cues to a chatbot’s human
trustworthy and friendly when it does not follow their ex-
origins enhance self-disclosure, perceptions of mind (i.e.,
pectations. This might eventually effect their behaviour
agency and experience (32)), and comfort (33). However,
and their adaptation to the agent.
when an artifical agent is perceived against one’s social
Overall, the study supports the wider understanding that expectations, it may be viewed as less trustworthy and
social cognition functions differently between humans, ob- friendly. As chatbots continue to be utilised and devel-
jects, and machines (91; 92; 93). Neuroimaging research oped across digital mental health interventions, commer-
on social cognition during HAI and human-robot interac- cial apps, and healthcare services, these findings could
tion (HRI) has signified that whilst human social interac- help strengthen the development of trust and disclosure in
tion engages social motivation and mentalising processes disembodied artificial agents designed for social interac-
in the brain, HAI and HRI recruits additional executive tions in different open domains. Future research should
and perceptual resources (94; 95). In the cognitive recon- identify the role in which disembodied social and anthro-
struction within a human observer, knowledge cues of an pomorphic cues play in the facilitation of self-disclosure
artificial agent are found to be a more significant indicator in HAI, whilst investigate alternative approaches to decep-
than its visual features in shaping one’s interactions with tion in their application to avoid unethical practices in the
an artificial agent (90). In the present study, this is evident application of artificial agents in digital mental health care.
by the effects of knowledge cues to Sam’s human origins
(i.e, being introduced as a human) shaping participant’s
Acknowledgments
perceptions of mind and comforting responses, and facil-
itating greater self-disclosure. These effects are further
The authors gratefully acknowledge funding from the Eu-
supported by media richness theory, which posits that me-
ropean Union’s Horizon 2020 Research and Innovation
dia perceived as rich and socially intelligent elicit stronger
Programme under the Marie Skłodowska-Curie to EN-
social responses during communication (48), which is fur-
TWINE, the European Training Network on Informal Care
ther enhanced by media perceived to be ‘like-me’ (62).
(Grant agreement no. 814072), the European Research
These findings are significant as they suggest that an arti- Council (ERC) under the European Union’s Horizon 2020
ficial agent perceived ‘like-me’ facilitates disclosure and Research and Innovation Programme (Grant agreement no.
comfort. But they open an ethical debate around the use 677270 to EC), and the Leverhulme Trust (PLP-2018-152
of deception in therapeutic practices in the translation of to EC).
this research to digital MHC (96). Research on deception
in psychotherapy is limited, focusing predominantly on