Running head: CULTURALLY DECENTERED PICTURES
Culturally De-Centered Test Design for Pictorial Reasoning Assessments
Joni M. Lakin
Associate Professor, Educational Studies
University of Alabama
Culturally Decentered Pictures 2
Abstract
Although methods have been developed for developing culturally “decentered” assessments that
use languages, such as transadaptation strategies, pictures are often assumed to be culture-neutral
and have not received similar attention. This paper describes CogAT test development in 2007-
2011 on the adaptation of verbal and quantitative reasoning formats that relied exclusively on
pictures for students in grades K to 2. In the course of the test development, we applied cross-
cultural test adaptation practices and developed new resources for cultural fairness reviews that
explicitly addressed cultural loading of pictures and concepts beyond the usual achievement
domains. This paper describes these applications and provides resources for others seeking to
develop culturally decentered, picture-based assessments.
Culturally Decentered Pictures 3
Culturally De-Centered Test Design for Pictorial Reasoning Assessments
In the U.S., English language learner (ELL) students are a heterogeneous population of
students that pose unique challenges to test development and administration. In response to the
challenges, researchers have developed innovative and boundary-pushing solutions that expand
our knowledge of assessment for ELL students as well as the general student population. In this
paper, we present some of these innovations designed to enhance the validity and fairness of
assessments for English language learners (ELLs) in the K-12 testing context. We present a set
of tools we developed for improving the cultural fairness review process for standardized tests,
specifically when the test is heavily reliant on pictures and everyday (not specifically academic)
content.
The assessment context of this paper is a measure of cognitive reasoning abilities for
students in K-12 schools: Cognitive Abilities Test Form 7 (CogAT 7). Reasoning skills are
important predictors of academic success, but are distinguished from measures of achievement
by focusing on common concepts while requiring examinees to find patterns and relatively novel
insights about relationships between concepts (i.e., “going beyond the information given”). In
contrast, achievement tests focus more on school-based learning and reflect past learning
processes rather than eliciting new learning. Using verbal, quantitative, and figural concepts,
ability measures tap into the overall efficiency of cognitive processes and strategies that enable
students to learn new skills and solve problems.
Schools use ability tests like CogAT for a variety of purposes, including gifted/talented
identification and differentiating instruction, often in conjunction with achievement data. Thus,
there is the potential that ability tests can identify untapped potential among all students, raising
the stakes for their cultural fairness. This is particularly true in districts with substantial numbers
of English learners, which in the U.S. comprise mostly Spanish as the native language.
A serious concern in the assessment of cognitive abilities is whether measures of
reasoning that include verbal and quantitative content allow all students to fully demonstrate
their abilities. If the concepts are not equally familiar to all students, then performance is
influenced more by prior learning and less by current reasoning processes. In previous forms of
CogAT, the tasks for young students (grades K to 2) in verbal and quantitative domains required
students to listen to a teacher-read prompt and select the picture demonstrating the best answer to
the question. These item formats required strong receptive language skills in English and resulted
in undesirably strong correlations between the verbal and quantitative scores.
Culturally Decentered Pictures 4
To address these concerns, picture-based reasoning items tapping distinct verbal and
quantitative concepts were developed for a paper-and-pencil assessment with no teacher-read
prompts (only the introductory test directions are read aloud and can be readily translated to new
languages; Lohman, 2012). The goal of this study was to explore the validity and fairness of
picture-based measures of reasoning skills that do not requiring extensive receptive language
comprehension.
Some researchers and test developers feel that presenting only pictorial or nonverbal
content is enough to make a test “culture fair”, free of or minimizing the role of culture in
understanding the test content. However, Ortar (1972) expressed concern about the assumption
that pictures or figures on nonverbal tests were universal. She argued that words are actually
safer, because we are adept at checking their translatability by seeking the help of native
speakers. Pictures are just assumed to translate and “any cultural differences are comparatively
well hidden.” (p 120). In a similar vein, Semin (1972) described the confusion that a simple
picture of a couple sitting in rocking chairs caused for a sample of children in Turkey. The
chairs, the clothes, and the pastimes of the characters were all distinctly U.S. Midwestern (e.g.,
the older white woman in a work dress was knitting). All of the culture-specific features had to
be modified in order for the children to easily recognize the picture and answer the question.
In our work, the non-translatability of pictures was a serious concern. Therefore, we
developed the picture-based items with the same care that a language-based assessment requires
to ensure greater cross-cultural validity. We approached this from the outset with the framework
of of reducing cultural load (van de Vijver & Poortinga, 1992) or “cultural decentering” a test
(Geisinger & McCormick, 2014) with respect to our target population of native English and
Spanish speakers living in the U.S. However, resources for this type of test development were
not available in the literature. Therefore, we adapted existing and developed new approaches to
test development to create picture-based reasoning assessments.
Our process
Our test development process incorporated ideas and processes from the extensive
literature on Universal Design (e.g., Johnstone, Thompson, Bottsford-Miller, & Thurlow, 2008)
and cross-cultural test adaptation (Hambleton et al., 2005; Solano-Flores & Wang, 2015; Tanzer,
2005). Input from students, bilingual teachers, and professional translators were incorporated
into the test development process. Six primary methods were used, which are described in the
sections that follow.
Culturally Decentered Pictures 5
1. Concurrent development of items to create culturally decentered content
2. Decentered and back translations by professional translators
3. Labeling study to assess familiarity of illustrations
4. Bilingual teacher review and item elicitation
5. Additional fairness reviews with “cultural load” rating scale
Concurrent development of prompts
The usual practice of test development in the US is to create a test in English, then
translate the test to Spanish and other necessary languages. This method is fraught with problems
of translating the test, determining the quality of translations, finding a suitable sample of test-
takers for the standardization, and deciding what kinds of interpretations can be made about test
scores (Sireci, 2005). Tanzer (2005) is one of several researchers who have called for test
developers to be proactive in developing multi-language test forms and to build forms
simultaneously.
In some testing contexts, bilingual test developers can be recruited for the project.
However, in small test development teams, this is not feasible. To bring in this expertise to our
small monolingual test development team, we incorporated the support of colleagues with
relevant cultural knowledge for items that would be less familiar to Hispanic students in the U.S.
These team members included a native Spanish speaker from Venezuela, a native Spanish
speaker from Southern California, and another colleague who grew up speaking Spanish with her
originally Puerto Rican family.
These language experts, who were current or former K-12 teachers, reviewed newly
drafted items created by the primary test development team and evaluated how familiar various
concepts would be to students from a similar background to them. They suggested substitutions
that would be more familiar, and, for Sentence Completion items, debated which Spanish
translations would be most familiar to U.S. based Spanish speakers. One notable challenge was
the word for “kite”. Each collaborator knew at least 2-3 words for kite, but none overlapped all
three dialects! When possible, we chose the most neutral or common translation, but we gave
precedence to Mexican dialects, because of the large number of Spanish speaking EL students
from Mexico.
Bilingual teacher review workshops
We realized early in the test development process that decentering content could also be
thought of as identifying “etic” concepts (Geisinger & McCormick, 2014), a term that refers to
Culturally Decentered Pictures 6
concepts or ideas that are shared between two cultures. In contrast to etic concepts, there are
“emic” concepts that are unique to one of the cultures and not shared with the other. Cross-
cultural test development theory allows for emic test content as long as it is equally balanced
between concepts familiar to each represented culture.
With this idea in mind, we sought to identify content that might be more familiar or
derive from the cultural experience of Spanish-speaking U.S. schoolchildren. Our tactic to
identify such content focused on a pool of bilingual educators in our region. A local school
offered a dual-language program where bilingual teachers worked with native English- and
native Spanish-speaking students (primarily from newly arrived immigrant families) with the
goal of teaching academic content while also building bilingual skills in all students.
We held a three-day workshop with our team of four bilingual teachers from this
program, one curriculum professor, and one member of the test development team. We began by
providing teachers with a basic introduction to the test, to test development, and to our goal of
generating a test that was equally familiar and fair to native English and Spanish-speaking
students in the U.S.
We began by reviewing items from our existing item development pool. When a problem
was identified, teachers suggested replacement options or completely new items. We also
dedicated substantial time to brainstorming content that would be uniquely familiar to their
native Spanish-speaking students. Ultimately, we were not able to develop more than a few
viable items from this perspective. As a group, we generated items around music, cultural events
like rodeos, and festival topics. Most of these items were eventually discarded for being too
specific, unfamiliar to other students, or somewhat stereotyping of Mexican-American culture.
Working with more experienced test developers who had the cultural expertise of these students
might be more successful in the future.
Labeling study
As Ortar warned for this type of work, assessing the familiarity of illustrations was more
slippery. One method we developed was a “labeling” pilot study. Essentially, we asked a small
sample of young students to label illustrations they viewed with the goal of verifying that the
images were easily identified by young students. It was not important that they labeled them
exactly as we intended, but that they generally understood what each picture showed (i.e., it was
familiar enough that a child aged 5-8 would recognize it) and were not distracted by specific
details or unfamiliar styles.
Culturally Decentered Pictures 7
Because of their age, children engaged in the task with a parent. The parent asked the
child for their responses and typed them into the relevant box. Two examples are shown below.
In the first example, the indicated picture showed a child coloring. We needed to make sure the
children knew about coloring books and immediately recognized that activity.
Because we recruited some bilingual children, responses could be in English or Spanish.
Originally, we planned to present single images at a time, but we felt that pictures were more
recognizable and specific when placed in context with the other pictures, as they would be seen
in the actual test. Due to time constraints, fewer than a dozen students participated, but we felt
the method held substantial promise for this type of item development.
To further assure that our pictures work for most US schoolchildren, we showed our
pictures to half a dozen knowledgeable reviewers, all bilingual teachers of US school children
with extensive experience with ELL students. They reviewed the pictures and noted which ones
could be problematic for easy recognition by all test-takers. However, they found relatively few
problems. We hope this is because we designed the test from the start with an eye towards
translatability, but it may also reflect the difficulty of the task.
Fairness reviews with Item Cultural Load Scale
It is well known in the testing industry that fairness reviews often pick up idiosyncratic
issues that rarely align with other indicators, such as DIF statistics or even other reviewers. We
wanted to develop a more structured process that could increase the agreement of reviewers on
the level of cultural fairness or cultural load of items.
Past fairness reviews of this measure simply asked reviewers to comment on any issues
they perceived in items. We wanted to expand this process and share our perception of cultural
load of assessments (based on theory and past research reviewed above) with reviewers so they
might be more intentional about the way they reviewed the items for fair content. We hoped that
this process might provide more clarity to the test development team of when items were
Culturally Decentered Pictures 8
problematic and what to do about the issues brought up. Some of the key areas we wanted
reviewers to focus on included:
• Familiarity of content: Items should only require knowledge of content that is already
familiar to U.S. students at the grade level(s) for which the items are intended.
• Familiarity of rules or classification schemes: Students may recognize the individual
pictures in an item but be unfamiliar with the rule, category, or classification scheme
needed to solve the item.
• Quality of art: The pictures should be drawn so that children can easily identify the
concepts represented.
Two examples of culturally loaded items are provided in the Appendix that reflect level 3 or 4
particularly in what kinds of animals are considered part of a typical diet. This item assumes that
smaller birds or songbirds would not be eaten, so the answer is E. This distracts from the focal
concept of what “edible” means. Replacing the smaller bird with a clearly inedible item such as a
toy would improve this item by removing a culturally loaded answer key. Another potential issue
is whether the butter in a butter dish is equally familiar to all students. If they mistook it for soap
or something else, it would unfairly attract them to choose that answer.
In our fairness review, twenty-two reviewers from various backgrounds (including seven
Hispanic school administrators, teachers, or school psychologists) participated. All instructions
and the rating scale are provided in the Appendix. Each panel member provided ratings on each
item (1-4). We flagged items for review when three or more reviewers rated the item at 3 or 4.
All comments also considered and incorporated into item changes. Despite our best efforts, we
still found very low agreement across raters with an intraclass correlation of just .19 (average
measures, ICC type 2 [Two-way random effects]; Landers, 2011). On the one hand, each
reviewer was basing ratings on their own perspective and reference group, so some disagreement
Culturally Decentered Pictures 9
is likely, but the low correlation of ratings is still notable. The average correlation between raters
was just .08.
How did it work?
Zscore(VUSS) Form 6 Form 7
Cohen's
Typical
LEVEL age N M SD N M SD d
K 5/6 1106 -0.67 0.94 634 -0.19 1.02
LEP
0.49
1 7 1143 -0.73 1.05 99 -0.31 0.84
0.44
2 8 1163 -0.70 0.98 199 -0.30 0.78
0.46
3 9 779 -0.65 0.96 215 -0.51 0.86
0.16
K 5042 -0.44 0.91 1922 -0.20 0.95
FRL
0.26
1 6359 -0.42 0.94 447 -0.17 0.93
0.27
2 6187 -0.46 0.94 1503 -0.35 0.88
0.12
A 5613 -0.43 0.87 1351 -0.42 0.86
0.02
1995 -0.70 0.86 1945 -0.18 0.98
Hispanic
0.56
2135 -0.71 0.94 430 -0.23 0.88
0.52
2183 -0.72 0.89 1204 -0.15 1.01
0.61
2201 -0.60 0.92 1205 -0.37 0.87
0.26
Conclusions
Although pictures may be assumed to be culturally universal, it is clear from the literature
that pictures must be evaluated for cultural fairness with at least as much rigor as language. We
presented strategies here that might be used by others in test construction to gauge the suitability
of pictures using examinee and expert reviews. Some of the key points learned from this process
included, first, the need to identify and understand the intended population(s) where the
assessments will be used. Whether culture varies within a testing context (as in the U.S.) or when
a test is transported between cultures, cultural variation needs to be understood at the outset of
test development. Second, test developers should not assume that all illustrations are equal.
While simple geometric shapes may (mostly) universally recognized, illustrations in particular
must be assessed for the content and style of art to determine if all examinees will be equally
familiar with the concept. We found in our work that detail and style of illustrations could vary
substantially across artists and cultures.
Culturally Decentered Pictures 10
Finally, we found that the fairness review process still needs refinement and further
study. Our proposed process is a qualitative improvement on the existing method, but did not
lead to greater reviewer agreement. This begs a fundamental question, do we even seek
agreement across reviewers? Or is it reasonable to expect differences given the diverse
backgrounds of the people selected for sensitivity and fairness reviews? Better constraints and
structure on the fairness review process, along with rigorous quantitative methods such as those
reflected here, will help the field improve the fairness review process beyond a vague qualitative
process to a more effective and structured approach to fairness review. What is absolutely clear
is that pictures, just like words, must be subjected to fairness and cultural review in test
construction.
Culturally Decentered Pictures 11