Are Default Nudges Deemed Fairer When They Are
More Transparent? People's Judgments Depend on
the Circumstances of the Evaluation
Patrik Michaelsen*, Lina Nyström, Timothy J. Luke, Lars-Olof Johansson & Martin Hedesström
Department of Psychology, University of Gothenburg
To avoid concerns of manipulation, nudges should be transparent to the people
affected by the intervention. Whether increasing the transparency of a nudge also
leads to more favorable perceptions of the nudge is however not certain, and may
depend on the circumstances of the evaluation. Across three preregistered
experiments (N = 1915), we study how increased transparency affects the perceived
fairness of a default nudge, in joint vs. separate, and description- vs. experience-
based evaluations. We find that transparency increases perceived fairness of the
nudge in a joint comparison, when the relative benefits of transparency are easy to
see. However, in a real choice-context, with nothing to compare against,
transparency instead decreases perceived fairness. Efforts to make nudges more
ethical may thus ironically make choice architects perceived as less ethical.
Additionally, we find that the transparent default nudge still successfully affects
behavior, that different default-settings communicate different perceived intentions
of the choice architect, and that participants consistently favor opt-in defaults over
opt-out defaults nudges – regardless of their level of transparency.
Keywords: nudging; choice architecture; transparency; fairness; opt-out default
* Corresponding author. patrik.michaelsen@psy.gu.se. Twitter: @ p_michaelsen
Ideally, behavior change interventions will be both effective in changing behavior and perceived
as fair and acceptable by people subjected to them. In the realm of nudges, interventions
influencing behavior by changing cues in choice environments, effectiveness and acceptability
may be in tension. In terms of effectiveness, no interventions outperform default nudges
(Hummel & Maedsche, 2019). However, people count defaults among the least acceptable nudge
types (Hagman, Andersson, Västfjäll & Tinghög, 2015). In the present research, we investigate
whether effectiveness and acceptability can be reconciled by making default nudges transparent
to decision makers. Specifically, we study how fair people judge defaults to be, if fairness
perceptions can be increased by increasing intervention transparency, and whether people’s
judgments depend on the mode by which the nudge is evaluated.
A little more than a decade has passed since Thaler and Sunstein published the seminal
Nudge (2008), a book on psychologically informed behavior change interventions and their
possible usefulness in public policy. Among the many discussions it sparked, a major theme has
been the legitimacy of using behavioral insights to influence people’s behavior (Schmidt &
Engelen, 2020; Sunstein, 2016). A central ethical concern has been that (some) nudges, such as
defaults, are perceived to operate outside of people’s awareness (Bovens, 2009; House of Lords,
2011; Smith, Goldstein & Johnson, 2013). If a nudge affects choices at a level below
consciousness, this may be seen as manipulation, and thus as posing a threat to autonomy.
Traditional forms of policy measures, such as taxation or bans, may at times restrict people’s
freedom of choice more than nudges do. However, since the presence and mechanisms of these
interventions are easily recognizable to people, charges of manipulation are not leveled.
Higher transparency may thus be a key to increasing the ethicality and acceptability of
nudges. Nudges that are not inherently transparent, such as many defaults interventions, can be
made transparent by external means. For instance, written information can be provided that
discloses the presence, intention behind, and expected effect of the nudge. This gives people an
opportunity to spot and comprehend the nudge, making the influence no longer outside of
awareness. How openly and directly available the disclosed information ought to be in order to
avoid manipulation has been debated. On one account, it may suffice with a general disclosure in
public records that certain nudges are in use and to what aim (cf. discussion in Thaler &
Sunstein, 2008, pp. 246-249). A stronger account may require that the nudge is possible to detect
at the time of it exerting influence; either as judged by an average person or by an attentive,
“watchful”, average person (see Bovens 2009, and Ivankovic & Engelen, 2019, which both argue
in favor of the latter).
Not only is addressing ethical concerns important in its own right, but it may be
important in promoting the social interaction taking place between the nudger and the nudgee
(Krijnen, Tannenbaum & Fox, 2017). Even if a nudge is successful in changing behavior, its use
may communicate unwanted elements that render the net effect –broadly construed– lower. For
instance, an intervention perceived as manipulative might negatively affect perceptions of the
choice architect and erode trust in them. Contrarily, a nudge employed in a fair and transparent
way could, even with limited success in changing behavior, signal respectfulness and cultivate
the relationship, which in turn may further future interactions.
A burgeoning survey-literature shows that common nudges receive majority support in a
wide range of countries (Hagman et al., 2015; Jung & Mellers, 2016; Pe’er et al., 2019; Reisch &
Sunstein, 2016; Sunstein, Reisch & Rauber, 2017). A pattern evidenced in several studies is that
people prefer nudges that are transparent and target reflective thinking, as opposed to targeting
subconscious psychological processes. For instance, transparent interventions have been
associated with higher approval ratings, lower threat to autonomy, and lower levels of
paternalism (Davidai & Shafir, 2020; Gold, Lin, Ashcroft & Osman, 2020; Jung & Mellers,
2016). Further, the more the nudge is perceived to target reflective thinking, the more
participants predict choices to feel authentic, and the more willing they are to work for a
company employing the nudge (Felsen, Castelo & Rainer, 2013). As posited at the outset, this
hints at an acceptability problem for default nudges, as they are often perceived as non-
transparent interventions. Indeed, the two default interventions researched by Hagman et al
(2015) – default enlisting of organ donors, and a default to compensate carbon emissions for a
flight – were the only ones to just barely receive majority support, and had more than three
quarters of the sample finding them intrusive to freedom of choice.
However, an important limitation constrains the conclusions we can draw from these
studies about the influence of transparency on people’s perceptions of nudges: Transparency and
the type of intervention have been confounded. Instead of manipulating the level of transparency
for a single nudge intervention, conclusions have been arrived at from comparisons between
transparent nudges on the one hand, and altogether different types of non-transparent nudges on
the other. For instance, in one study warnings and graphic pictures of damaged lungs on cigarette
packages (transparent nudge) were compared to making the filter a larger proportion of the
cigarette (non-transparent nudge). Another study compared information on the benefits of
enlisting as an organ donor (transparent nudge), to default enlisting as an organ donor (non-
transparent nudge). While justified in drawing inferences about groups of nudges, these studies
fail to illuminate how increasing transparency on its own may influence perceptions of a nudge.
Moreover, as the survey-based research paradigm uses descriptions of nudges as the
stimuli for participants to rate, ratings are given without participants actually encountering the
nudges themselves. Similar topics have however recently been approached by studies subjecting
participants to nudges first-hand. By having participants actually make choices in environments
subjected to nudges, these studies provide what may be a better indication of how real world
nudge applications are perceived. In the remainder of this paper, we will refer to the former
approach as a description-based evaluation mode, and the latter as an experience-based
evaluation mode.
In experience-based studies, findings have arguably been somewhat more favorable for
default nudges than in the description-based literature. For instance, in a hypothetical apartment
acquisition scenario, Steffel, Williams, and Pogacar (2016) showed that participants perceived
being nudged towards pro-environmental amenity choices to be fair (vs. unfair). In an actual (as
opposed to hypothetical) charity donation choice, Michaelsen, Johansson and Hedesström (2020)
found decidedly low objection ratings to the choice format (mean ratings 2.6 on a 9-point scale),
even though participants were nudged to give away money. In this study, participants subjected
to the opt-out default (i.e., the nudge) and those subjected to an opt-in default (i.e., defaulted to
keeping the money for oneself) gave virtually indistinguishable ratings of how much they
objected the intervention.
In contrast to the description-based paradigm, experience-based studies have also
investigated effects of increasing the transparency for one and the same nudge. Interestingly
from an ethical viewpoint, transparency has been increased to make the nudge easily detectable
in the moment of exerting influence, by presenting a disclosure statement explaining the nudge
to participants in conjunction with them making their decision. Even such immediate nudge
disclosures have been shown to have a very small, or no, diminishing effect on the effectiveness
of the nudge (Bruns et al., 2018; Loewenstein et al., 2015; Michaelsen, Johansson &
Hedesström, 2020; Steffel et al., 2016; Paunov, Wänke & Vogel, 2019; Wachner, Adriaanse &
De Ridder, 2020).
Increasing the transparency of a nudge also seems to have no or negligible negative
effects on people’s experiences and perceptions of the nudge. For instance, experience-based
studies have found that making a default nudge transparent entailed no or minor increase in
perceived threat to freedom of choice (Michaelsen et al., 2020) and experienced autonomy
(Michaelsen et al., 2020; Wachner et al., 2020). Paunov et al. (2019) further showed that making
a default nudge transparent can lead to lower feelings of being deceived.
At present, it appears unclear exactly why people seem more positive towards nudges in
an experience-based evaluation mode, as compared to in a description-based mode. One
possibility is that the nudge simply is found less intrusive than expected once actually
encountered. Another possibility is that apparent differences between evaluation modes primarily
reflect differences in operationalization and measurement, rather than substantive effects, as few
direct comparisons have been made.
In sum, people seem to perceive nudges differently depending on in which evaluation
mode their judgments are elicited. This has important consequences for how people’s opinions
on nudges should be assessed, as one and the same nudge may be considered acceptable when
judged in one mode, but unacceptable when judged in another. Thus, policy decisions guided by
only a single perspective risk misrepresenting the public perception. To the best of our
knowledge, no studies have systematically investigated the consistency, or lack thereof, of nudge
perceptions across both description-based (nudge rated after reading a description) and
experience-based (nudge rated after making a choice subjected to a nudge intervention)
evaluation modes (but see Bang, Shu & Weber, 2020; Davidai & Shafir, 2020).
It can similarly be expected that increasing the transparency of a nudge is perceived
differently between evaluation modes. Additionally, much is left unanswered as to whether
increased transparency can better the public’s view on nudges, since the dominant, description-
based, research-paradigm for studying people’s perceptions of nudges have confounded level of
transparency with type of nudge.
Thus, in the present paper we set out to accomplish three goals: 1) systematically
compare perceptions of opt-in and opt-out defaults across description- and experience-based
evaluation modes, 2) test whether making one-and-the-same opt-out default nudge transparent
benefits how the intervention is perceived, and 3) investigate whether increased transparency is
perceived differently across different evaluation modes. This is researched in three preregistered
studies (total N = 1915). Specifically, we study people’s fairness perceptions for three choice
formats: an opt-in default, an opt-out default nudge, and a transparent opt-out default nudge, for
one-and-the-same donation choice scenario, and across three evaluation modes. In Study 1 (n =
439), choice formats are compared side by side in a description-based joint comparison format.
Study 2 (n = 424) employs a description-based separate evaluation. Finally, Study 3 (n = 1052)
uses an experience-based evaluation format, where participants assess the choice formats after
having experienced one of them first-hand by actually making a decision in the scenario. Study 3
also provides the highest-powered test to date of whether making a nudge transparent affects its
effect on a consequential (monetary) choice.
Study 1: Description-Based Joint Evaluation
In a joint evaluation format, participants rate either a transparent (explicitly disclosed) opt-out
default nudge side by side with a “standard” (i.e. non-disclosed) opt-out default nudge, or the
standard opt-out default nudge side by side with an opt-in default. Previous research has shown
that when evaluated jointly, transparent (“system 2”) educational nudges receive higher support
and are judged less paternalistic, than non-transparent (“system 1”) opt-out default nudges
(Davidai & Shafir, 2020). Here we extend by keeping the type of nudge constant, in order to
isolate the influence of increased transparency. The preregistration of this study, along with
Method
Participants
paid $0.45 for their time. After excluding participants failing a comprehension check (22.7% of
finished participants failed) our final sample consisted of 439 participants (M = 39.13 years old,
SD = 12.96; 45.8% women). Due to more people failing the comprehension check than expected
(a pilot study, n = 122, had suggested 15%) the final sample was slightly smaller than the 480
expected in the preregistration. A sensitivity analysis showed that the final sample was however
still sufficient to detect differences corresponding to a Cohen’s d of 0.20, with 80% power and α
=.05, in our lowest-powered hypothesis test.
Design, Outcomes and Procedure
Participants’ task was to rate choice formats, meaning ways of designing and presenting a choice
task to a decision maker. Ratings were given in a joint evaluation format. Specifically, each
participant separately rated two choice formats displayed to them simultaneously, side by side.
There were two sets (“pairs”) of choice formats, and each participant viewed only one. Pair 1
consisted of an opt-in default format and an opt-out default format. Pair 2 consisted of an opt-out
default presented with, and without, an accompanying message disclosing the presence and
expected influence of the intervention. The opt-out format without the disclosure message was
identical to the opt-out format in Pair 1. In both pairs, the differences between the two
presentation formats were highlighted in yellow color to facilitate comparison (screenshots are
provided in the Supplementary Materials). In the remainder, we will refer to the opt-out format
without a disclosure as “standard opt-out default”, and the version with a disclosure present as
“transparent opt-out default”. The choice described in the vignettes was the same in all instances,
and concerned keeping or donating a 20¢ bonus payment to a pediatric cancer charity
foundation. Participants did not themselves make the donation choice, their only task was to
evaluate the presentation formats.
The primary evaluation of interest was the perceived fairness of the choice architecture.
Specifically, participants indicated their agreement with the statement “The way in which the
choice was presented was fair” on a seven-point scale, ranging from 1 (Strongly disagree) to 7
(Strongly agree). Ratings were given separately for each of the two choice architectures
displayed. In order to clarify the question’s focus, we instructed participants that “the questions
below concern your perception of how the choice is formatted. We ask you to not judge e.g. that
a bonus was given, or the specific charity cause.”
We posed two confirmatory hypotheses and one open-ended research question:
Hypothesis 1: A standard opt-out default nudge will be perceived as less fair compared to an
opt-in default.
Hypothesis 2: A standard opt-out default nudge will be perceived as less fair compared to a
transparent opt-out default that explicitly discloses the presence and expected effect of the nudge.
Open-ended research question: Will different comparison points affect the absolute fairness
ratings of an opt-out default nudge? In other words, will there be a between-groups difference in
fairness ratings for the opt-out default nudge, depending on whether it was compared with a
standard opt-in default or a transparent opt-out default?
1 The image file containing both choice architectures was displayed at 1200 x 575. A pilot test had indicated this
display size was easy for participants to view, and in the current sample > 98% used a screen resolution wide
enough that no sideways scrolling would be needed.
The questionnaire further included some exploratory measures. We measured participants’
agreement with four statements on perceived intentions behind the choice architect’s selection of
choice format. Specifically, we measured agreements with: “The way in which the choice was
presented was… 1) …intended to influence the likelihood of donating, 2) …intended to suggest
a course of action, 3) ...intended to make choosing easy, and 4) …intended to inform the chooser
sufficiently”. Order of items was randomized, and ratings were given on a seven-point scale from
1 (Strongly disagree) to 7 (Strongly agree).
Additionally, we asked participants to imagine they would be confronted with making the
choice themselves. We then asked 1) which presentation format they would prefer to be
presented with, 2) whether the presentation format would affect how positively they viewed the
choice architect, 3) whether the choice of presentation format would affect their willingness to
work with the choice architect again in the future (7-point bipolar scales, with endpoints
anchored preference for format 1 – format 2), and 4) whether they thought they would choose to
donate the money (separately for each presentation format).
We explained to participants that the purpose of the study was to inform our design of a
future study on MTurk. In the first section of the study, participants were displayed the two
presentation formats and instructed to carefully read them through. Display order, left-right
presentation, for the two formats were counterbalanced. Next followed two pages containing the
outcome variables described above. Each page had the presentation formats displayed at the top
of the page as a reminder. The items on fairness of choice format and perceived intentions of the
choice architect were asked on the first page, remaining items on the second. Lastly, participants
answered a brief demographics section.
Results
Perceptions of fairness
In support of Hypothesis 1, when jointly comparing a standard opt-out default nudge and an opt-
in default, participants perceived the opt-out as less fair than the opt-in, paired t(207) = 9.06, p <
.001, d = 0.79, 95% CI [0.59, 0.99]. Both choice architectures were rated positively on the scale,
with the opt-out default just crossing the neutral midpoint (M = 4.67, SD = 1.82), and the opt-in
situated well above (M = 5.91, SD = 1.21).
In support of Hypothesis 2, when jointly comparing the standard and the transparent opt-
out default nudge, participants perceived the standard opt-out default as less fair than the
transparent opt-out, paired t(230) = 2.57, p = 0.01, d = .22, 95% CI [0.05, 0.38]. Both choice
architectures were rated positively on the scale (standard opt-out M = 5.00, SD = 1.61;
transparent opt-out M = 5.35, SD = 1.64).
For our open-ended research question, the standard opt-out default was perceived as more
fair when displayed next to the transparent opt-out default (M = 5.00, SD = 1.64), than when
displayed next to an opt-in default (M = 4.67, SD = 1.82), independent t(416.08) = 2.01, p = .045,
d = 0.19, 95% CI [0.01, 0.38].
Exploratory analyses
Results for the measures for perceived intentions of the choice architect, and participants’ own
different within each pair (row 1 vs 2; row 3 vs 4), paired t-tests, all ps ≤ .001. Despite being
rated as more fair, the transparent opt-out default was to a higher degree perceived as intended to
influence the chooser than the standard opt-out default was. Presumably, the high level of
perceived intended transparency (“inform the chooser sufficiently”) compensated for other
negative influences on the fairness assessment. Participants’ expectation of whether they
themselves would have donated did not differ significantly between choice formats.
Recall that participants were asked to imagine they were making the decision themselves
for a series of exploratory items. In response to these items, participants’ ratings were similar to
their fairness ratings, thus corroborating Hypotheses 1 and 2. For each of these items,
participants were asked to express their preferences on a 7-point bipolar scale, with each
endpoint of the scale corresponding to a preference for one of the choice formats (or the person
using them). In Pair 1, values below 4 indicate support for the opt-in default format. In Pair 2,
values above 4 indicate support for the transparent opt-out default format. For each item, we
compared the mean rating against the neutral midpoint of the scale (4).
In Pair 1, participants stated a preference to be presented with the opt-in format over the
standard opt-out format (M = 2.50, SD = 1.98, p < .001), would think more positively of the
choice architect using an opt-in format (M = 3.01, SD = 1.83, p < .001), and were more willing to
work with the choice architect using an opt-in again in the future (M = 3.12, SD = 1.71, p <
.001). In Pair 2, participants preferred the transparent opt-out format over the standard opt-out
format, albeit to a lesser magnitude. Specifically, they preferred being presented with the
transparent format (M = 4.36, SD = 2.45, p = .027), perceived the choice architect more
positively (M = 4.36, SD = 2.2, p = .019), and would be more willing to work with this choice
architect again in the future (M = 4.39, SD = 2.11, p < .001).
Discussion
Study 1 showed that participants perceived an opt-in default as more fair than a corresponding
opt-out default nudge. However, if subjected to an opt-out default nudge, participants considered
the choice architecture as more fair when the presence and expected influence of the nudge was
made transparent by explicitly disclosing this to them.
This finding extends previous research finding that people prefer transparent types of
nudges over non-transparent types of nudges (e.g. Gold et al., 2020), by evidencing that also a
specific instance of a nudge is perceived more favorably when made transparent.
While increased transparency had the dual effect of increasing both perceived intended
influence and sufficient informing, the latter seemingly outweighed the former in relevance for
the fairness ratings. Presumably, the joint evaluation format presented participants with a
reference point such that the disclosure statement appeared as an ethical improvement over the
non-disclosed format (cf. Hsee, 1996; Hsee, Loewenstein, Blount & Bazerman, 1999). We turn
next to whether the benefit of added transparency remains when interventions are evaluated
separately.
…intended to …intended ...intended …intended Predicted
influence the to suggest to make to inform the themselves
likelihood of a course of choosing chooser would
Fairness donating action easy sufficiently donate
Opt-in default 5.91 3.76 (1.89) 4.65 (1.70) 5.31 (1.42) 5.75 (1.28) 36%
n = 208 (1.21)
1 Standard opt-out 4.67 6.11 (1.33) 5.85 (1.41) 4.55 (1.84) 5.15 (1.64) 41%
a default (1.82)
(vs. opt-in)
n = 208
Standard opt-out 5.00 4.89 (1.76) 4.95 (1.68) 4.86 (1.64) 4.60 (1.70) 50%
default (1.61)
(vs. transparent)
n = 231
Transparent opt- 5.35 5.70 (1.59) 5.75 (1.47) 5.35 5.87 (1.43) 53%
out default, (1.64) (1.57)
n = 231
Note: For Likert-type items, numbers refer to means and standard deviations. All intention-measures are
significantly different within each comparison (row 1 vs 2; row 3 vs 4), at p ≤ .001, in paired samples t-tests.
Predicted donation is not significantly different within either pair.
n = 230
Study 2: Description-Based Separate Evaluation
Study 2 employs a between-groups design using the same choice formats and main dependent
variables as Study 1. Thus, instead of evaluating a pair of default choice formats, participants
evaluate only one format. This design can be viewed as a harder test of the hypotheses posed in
Study 1 (although not necessarily a more “correct” test), as there is no reference point facilitating
gauging the choice formats’ merits and drawbacks. Previous research has shown that integral
features of an opt-in default are sufficiently discernable, in a between-groups design, for
participants to judge it as more ethical than a transparent opt-out default nudge (Yan & Yates,
2019). However, the second focus of this paper, whether it is possible to make one-and-the-same
opt-out default nudge be perceived more favorably by increasing its transparency, has previously
not been researched. The preregistration of this study, along with analysis code, stimulus
Method
Participants
Participants were recruited through MTurk, and paid $0.40 for their time. As preregistered, we
requested 660 completed surveys, expecting a sample of 528 participants after exclusions. More
participants than expected failed our attention check however, leaving us with a final sample of
424 participants (M = 39.4 years old, SD = 13.2; 49.5% women). A sensitivity analysis showed
that the final sample was sufficient to detect differences corresponding to a Cohen’s d of 0.34,
with 80% power and α =.05, in our lowest-powered hypothesis test.
Design, Outcomes and Procedure
Materials were virtually identical to in Study 1, but changed into a between-groups design.
Additionally, a few exploratory measures were dropped. Instead of viewing and evaluating a pair
of choice formats, participants viewed and evaluated only one format, either: an opt-in default, a
standard (i.e. un-disclosed) opt-out default nudge, or a transparent opt-out default nudge with the
presence and expected influence of the intervention explicitly disclosed. As participants were
only displayed one choice format, there was no colored highlighting of differences between
choice formats, as was used in Study 1 (screenshots are provided in the Supplementary
Materials).
Participants rated the fairness of the choice architecture and four perceived intentions
behind the choice architect’s selection of choice format. They were also asked to predict whether
they would have donated the money had they encountered the choice themselves. All measures
were set up as described in Study 1. We again reminded participants that the questions concerned
their perception of how the choice was formatted, and not of the fact that a bonus was given, or
of the specific charity cause.
We posed the same two confirmatory hypotheses as in Study 1:
Hypothesis 1: An opt-out default nudge will be perceived as less fair compared to an opt-in
default.
Hypothesis 2: A standard opt-out default nudge will be perceived as less fair compared to a
transparent opt-out default that explicitly discloses the presence and expected effect of the nudge.
Results
Perceptions of fairness
We tested both hypotheses using a linear regression model with perceived fairness predicted by
experimental condition, and the opt-out default condition set as reference group. In support of
hypothesis 1, fairness ratings were higher for the opt-in default (M = 5.95, SD = 1.22) than for
the standard opt-out default nudge (M = 4.56, SD = 1.69), b = 1.39, SE = 0.19, p < .001, d = 0.93,
95% CI [0.68, 1.17]. Hypothesis 2 was however not supported, as there was no significant
difference in fairness ratings between the standard and the transparent opt-out default nudge (M
= 4.63, SD = 1.80), b = 0.07, SE = 0.19, p = .709, d = 0.004, 95% CI [-0.19, 0.27].
Exploratory analyses
in separate evaluation as well. As in Study 1, the opt-in default stood out and was significantly
different from both opt-out conditions for all intention-measures (all ps <.01, Tukey HSD). The
differences between the standard and transparent opt-out formats diminished in comparison to in
Study 1, however. There was a hint of the disclosure leading to an increase in perceived intended
influence, t(282.4) = 2.26, p = .025, d = 0.27, 95% CI [0.03, 0.50], however the effect was not
strong enough to be significant when controlling for multiple testing. No other comparisons
between the standard and transparent opt-out default conditions were significant.
…intended to …intended ...intended to …intended Predicted
influence the to suggest make to inform the themselves
likelihood of a course of choosing chooser would
Fairness donating action easy sufficiently donate
Opt-i n 5.95 3.59 (1.91) 4.26 (1.98) 5.50 (1.35) 5.50 (1.40) 38%
default (1.22)
n = 135
Standard 4.56 5.76 (1.37) 5.81 (1.24) 4.69 (1.76) 4.76 (1.60) 56%
opt-out (1.69)
default
n = 153
Transpare 4.63 6.12 (1.38) 6.04 (1.34) 4.81 (1.86) 4.92 (1.60) 52%
nt opt-out (1.80)
default,
n = 136
Note: For Likert-type items, numbers refer to means and standard deviations. For all intention-
measures, the opt-in default is significantly different from both opt-out defaults, all ps <.01 (Tukey
HSD). For predicted own donation, both opt-out defaults are significantly higher than the opt-in
default in a logistic regression model, ps ≤ .017.
Discussion
Study 2 found that in a separate evaluation, participants perceived the opt-in format as more fair
than both the standard and transparent opt-out format. People may have an intuitive, or prior,
understanding of the default effect (see Jung, Sun & Nelson, 2018, and Zlatev, Daniels, Kim &
Neale, 2017), such that an explicit reference point (as provided in Study 1’s joint evaluation
format) is not needed to understand the opt-out default as trying to solicit a certain behavior. The
sizeable difference in how people perceived the choice architect’s intention to influence them is
compatible with this interpretation, and similarly are the results for the other intention-measures
common opt-out default nudge may not be as inherently non-transparent as is sometimes feared
(Bovens, 2009). However level of transparency will necessarily vary with choice context.
Increasing the transparency of the opt-out default nudge did not increase fairness
perceptions. An examination of the intention-measures show that perceived intention to
influence, and intention to suggest a course of action, were very high in the transparent opt-out
default condition. However, intention to sufficiently inform the chooser dropped a scale point
compared to in Study 1. Thus, with no undisclosed “counterfactual” present, the disclosure
seemingly failed to mitigate perceptions of the influence attempt as negative.
Having elicited fairness perceptions in joint and separate description-based evaluations in
the first two studies, we lastly turn to actually subjecting people to the nudge in order to assess
whether the pattern of results so far transfer to a genuine application.
Study 3: Experienced-Based Separate Evaluation
In Study 3, evaluation mode is changed from the previous studies’ description-based mode, into
an experienced-based mode. In other words, participants are subjected to the same donation
choice scenario (formatted as opt-in, standard opt-out, or transparent opt-out) described in the
previous studies, and make a decision before rating the fairness of the nudge. Apart from
perceptions of fairness, we also look at how transparency affects participants’ choice of whether
or not to donate. In an equivalence test, we formally test whether the effect on choice can be
considered statistically negligible, as suggested by previous research (Bruns et al., 2018;
Loewenstein et al., 2015; Michaelsen, et al., 2020; Steffel et al., 2016; Wachner et al., 2020). The
preregistration of this study, along with analysis code, stimulus material and data can be found
Method
Participants
Participants were recruited through MTurk, and paid $0.35. After attention check exclusions, the
sample consisted of 1052 participants (M = 38 years old, SD = 12.7; 54.4% women). This was
close to our preregistered target of 1053 participants, and corresponds to 80% power to detect
differences of d = 0.21 in a between-groups t-test, with a significance level of .05. A total of 555
participants were excluded for failing an “instructional manipulation check” (IMC;
Oppenheimer, Meyvis, & Davidenko, 2009), and 80 participants were excluded for misreporting
whether or not they had chosen to donate a bonus payment in the experiment’s main choice task.
Design, Outcomes and Procedure
The study was advertised as being on comparisons of geometrical shapes. Participants rated the
similarity between six pairs of shapes before the actual experiment began. The IMC consisted of
an extension to the instructions of the shapes task. Participants who read the instructions hastily
were led to believe that for the next shape comparison, they should also account for similarity in
color. Attentive participants, reading the instructions to the end, instead learnt that they were
asked to check a specific scale point and write a +-sign in a text field.
After completing the shapes comparisons, the actual experiment began. In the
experimental choice task, participants faced the donation choice used as a vignette in the two
previous studies. Instead of rating a description of the choice formats, participants were asked to
make the decision between donating a 20¢ bonus to a pediatric charity organization or keeping it
for themselves, and afterwards rated the choice format. The donation choice was presented
identically to in the three choice formats used in the previous experiments. Participants faced
either an opt-in default, an opt-out default nudge, or a transparent opt-out default nudge with the
presence and expected effect of the intervention explicitly disclosed. Subsequent to making the
donation choice participants answered questions on perceived fairness. In order to filter out
aspects of participants’ fairness perceptions, we asked the following threefold question: “To
what extent do you perceive that... 1) ... offering you the bonus was fair?, 2) ... offering you the
option to donate the bonus was fair?, and 3) ... the two choice options (to KEEP or to DONATE)
were presented to you in a fair way?”. As preregistered, our focus was only on the third item, and
it is the one used in the analyses below. Answers were given on a seven-point scale anchored
“Completely unfair” and “Completely fair”. Each scale step was labelled numerically, from -3
through to 3.
We posed five hypotheses and one open-ended research question:
Hypothesis 1: Participants in the standard opt-out condition will donate to a higher degree than
participants in the opt-in condition.
Hypothesis 2: Participants in the transparent opt-out condition will donate to a higher degree
than participants in the opt-in condition.
Open-ended research question: Will the strength of the default effect be moderated by
participants’ attentiveness (as measured by passing or failing the IMC)?
Hypothesis 3: Donations in the standard opt-out condition and the transparent opt-out condition
will be statistically equivalent (with equivalence bounds set at a +/- .08 difference in proportions)
Hypothesis 4: Participants subjected to a transparent opt-out default will perceive the choice
format as fair (above neutral midpoint of scale).
Hypothesis 5: Participants in the transparent opt-out condition will perceive the choice format as
more fair than participants in the standard opt-out condition.
As an exploratory matter, we further asked “To what extent did you feel as if you were expected
to donate the 20¢ bonus?” (1 - Not at all to 7 - Completely), “Of all participants taking part in
this survey, how many do you think will choose to donate the 20¢ bonus?” (0-100%). At the end
of survey, we also included an exploratory measure of possible behavioral spillover from the
default-settings. These results are reported elsewhere (Hedesström et al., 2020).
Results
Donation choice
We tested Hypotheses 1 and 2 in a logistic regression model with dummy codes for the two opt-
out conditions, and the opt-in condition as reference group. Both hypotheses were supported.
Specifically, we found support for a default effect in that 47.1% of participants in the standard
opt-out default condition choose to donate their bonus, which was significantly more than the
26% of participants subjected to the opt-in default (Hypothesis 1), Wald(1), = 32.85, OR = 2.54,
p < .001. Similarly, participants subjected to the transparent opt-out default were significantly
more likely to donate their bonus (53.1%), than participants in the opt-in condition (Hypothesis
2), Wald(1) = 52.30, OR = 3.22, p < .001. The difference in donation rates was not significantly
different between the standard and transparent opt-out conditions, p = .113 (see also Hypothesis
3 below). With regards to the open-ended research question, there was no evidence that attention,
as measured by passing or failing the IMC, moderated the strength of the default effect. Neither
interaction with the two opt-out conditions was significant, ps > .25. Passing the IMC did not
itself predict donation rate, p =.91.
Hypothesis 3, predicting donation rates in the standard and transparent opt-out conditions
to be of equivalent size, was not supported. We used a TOST equivalence test (Lakens 2017;
Lakens, Scheel, & Isager, 2018), with equivalence bounds set at a +/- .08 difference in
2,3
proportions. The analysis showed the transparent opt-out condition to be significantly higher
than the low equivalence bound, Z = 3.71, p < .001, but not significantly lower than the high
low
bound, Z = -.54, p = .296. In other words, donations in the standard and transparent opt-out
high
conditions were not statistically equivalent due participants in the transparent condition donating
too much.
Perceptions of fairness
In support of Hypothesis 4, participants subjected to a transparent opt-out default nudge
perceived the choice format as fair (vs. unfair). This was tested with a one-sample t-test, which
showed that participants’ average fairness rating of 5.89 (SD = 1.66) was significantly higher
than the neutral midpoint of the scale (4) , t(353) =21.45, p < 001.
Next, we analyzed whether the transparent opt-out default was perceived as more fair
than the standard opt-out default. In opposition to Hypothesis 5 however, an independent
samples t-test showed that the transparent opt-out default (M = 5.89, SD = 1.66) was perceived as
significantly less fair than the standard opt-out default (M = 6.22, SD = 1.39), t(683.17) = 2.88, p
= .004, d = 0.22, 95% CI [0.07, 0.38]. An inspection of the frequency distributions showed no
evidence of the result being driven by a sub-group of participants reacting strongly against the
2 The TOST-procedure consists of running two one-sided tests assessing whether an effect is weaker than a specified
upper and lower smallest effect size of interest, in which case equivalence is declared.
3 Equivalence bounds were not based on theoretical reasoning, but on what we had approximately 80% power to
detect in the analysis.
4 Our preregistered analysis code incorrectly specified the midpoint of the scale as “0”, which did not correspond to
how the scale was coded. As the preregistration explicitly references “the midpoint of the scale”, we report only the
corrected analysis.
difference reflected a small but noticeable shift towards lower ratings consistently across the
scale.
Exploratory analyses
We analyzed whether the perceived fairness of the opt-in default format differed from the two
opt-out formats, using analysis of variance and Tukey HSD. Results showed that the opt-in
format (M = 6.37, SD = 1.20) was perceived as significantly more fair than the transparent opt-
out format (p < .001), but not significantly different from the standard opt-out format (p = .36),
F(2, 1048) = 10.4, p < .001.
Finally, participants in the standard opt-out (M = 4.46, SD = 2.05) and transparent opt-out
(M = 4.63, SD = 2.12) conditions felt a higher expectation to donate their bonus as compared to
participants in the opt-in condition (M = 3.77, SD = 2.13), F(2, 1049) = 16.82, p < .001, ηp =
.019. Participants’ estimations of how many percent of others would donate tracked actual
donations well: standard opt-out (M = 56.1), transparent opt-out (M = 56.4), opt-in (M = 44.5).
Felt Estimated % of
expectation to others that would
Donation Fairness donate donate
Opt-in default 26% 6.37 (1.20) 3.77 (2.13) 44.5%
n = 350
Standard opt-out default 47.1% 6.22 (1.39) 4.46 (2.05) 56.1%
n = 348
Transparent opt-out default, 53.1% 5.89 (1.66) 4.63 (2.12) 56.4%
n = 354
Discussion
Study 3 investigated whether increasing the transparency of an opt-out default nudge would
increase the perceived fairness of the nudge when people are subjected to the nudge first-hand,
and whether the transparent nudge would still successfully affect choices.
As hypothesized, participants experiencing the transparent opt-out default nudge first-
hand rated this choice format as fair. Nevertheless, and counter to our prediction, increasing the
transparency of the nudge led to a decrease in perceived fairness. The decrease in fairness runs
counter to the intention behind disclosing a nudge in the first place, and stands in contrast to
previous studies documenting positive effects of disclosing nudges (e.g. Paunov et al., 2019).
However, there are plausible explanations for the observed negative effect on perceived fairness.
The choice participants faced was a trade-off between keeping and giving away money, and the
nudged course of action was to donate the money. This is counter to most people’s immediate
self-interest, and previous research shows that such nudging is rated among the most intrusive,
and least acceptable (Jung & Mellers 2016; Hagman et al., 2015). The disclosure may then have
succeeded in increasing the transparency of the nudge, but instead of communicating openness, it
may (also) have functioned as a signal that influence was attempted.
The disclosure did, however, not decrease the effectiveness of the opt-out default nudge
in affecting choice. Regardless of being disclosed or not, participants in the nudge conditions
were about twice as likely to donate their bonus endowment to charity compared to participants
in the opt-in default condition. This finding supports the general conclusion of previous studies
on disclosing default nudges, that increased transparency has no, or a very small, diminishing
effect on behavior (Bruns et al., 2018; Loewenstein et al., 2015; Michaelsen, et al., 2020; Steffel
et al., 2016; Wachner et al., 2020). Study 3 contributes to this literature by evidencing the same
finding using demonstrably attentive participants, in a high-powered experiment, and with a
monetary choice. While statistical equivalence between donations in the standard and transparent
opt-out default nudge conditions was not evidenced, it should be reiterated that this was due to
participants in the transparent opt-out condition donating to a higher degree than participants in
the non-transparent condition (cf. Paunov et al., 2019).
General Discussion
The main focuses of this paper was to 1) systematically compare perceptions of opt-in and opt-
out defaults across description- and experience-based evaluation modes, 2) test whether making
an opt-out default nudge transparent benefits how the intervention is perceived, and 3)
investigate whether increased transparency is perceived differently across different evaluation
modes. We found that across evaluation modes, participants were highly consistent in rating an
opt-in default format as more fair than an opt-out format, even when the latter was made
transparent. Evaluation mode did, however, change how increased transparency was perceived
for the opt-out default nudge, and was found to both increase and decrease the fairness of the
choice format. Specifically, increased intervention transparency led to an increase in fairness
ratings when a standard, undisclosed, version of the nudge was provided as a reference point
(Study 1). The increase vanished without a reference point however (Study 2), and when the
nudge was experienced first-hand in an actual choice, the transparent opt-out was perceived as
less fair than a standard opt-out default (Study 3).
Additionally, to our knowledge, Study 3 provides the highest-powered test to date of
whether increased transparency influences the effectiveness of an opt-out default nudge in a
costly choice. In contrast to postulates that nudges “work best in the dark” (Bovens, 2009, p. 13),
results showed that the opt-out default nudge was similarly effective in changing behavior after
having been disclosed to the decision maker.
From a bird’s eye view, our findings are similar to those of previous studies on people’s
perceptions of nudges. We corroborate that default nudges are perceived as largely acceptable,
but that people have concerns about being influenced (cf. Hagman et al., 2015; Jung & Mellers,
2016; Yan & Yates, 2019). Likewise, we corroborate that default nudges can be disclosed to
decision makers without losing their effectiveness (Bruns et al., 2018; Loewenstein et al., 2015;
Michaelsen, et al., 2020; Steffel et al., 2016; Wachner et al., 2020). However, upon closer
inspection, an important difference emerges in that transparency is demonstrated to not be an
unambiguous asset for a nudge. Instead, when keeping the type of nudge constant (i.e. not
comparing across different types of nudges), transparency’s influence on perceptions of the
nudge depended on in which evaluation mode participants perceptions were elicited.
It is interesting that mode of evaluation could change how transparency affected the
perceived fairness of an opt-out default nudge in both directions, but showed such little effect on
how the fairness of opt-in vs standard opt-out default formats were perceived. We interpret this
as the qualities of the opt-in and standard opt-out formats being intuitive enough for participants
to grasp, whereas an increase in intervention transparency was not itself enough make the
intervention appear more fair, but needed a reference point to be appreciated (cf. research on
joint vs. separate evaluation; Hsee, 1996; Hsee, Loewenstein, Blount & Bazerman, 1999). With a
standard opt-out presented as a baseline, the otherwise hard-to-judge merits of the nudge
disclosure stood out and made providing it seem honest and fair. Without a comparison point, the
disclosure was instead seen as a part of the influence attempt (see perceived intended influence-
That the negative effect of increased transparency occurred only when participants
experienced the nudge first-hand (Study 3), and not the description-based mode (Study 2), may
be explained by the experience-mode leading to a higher engagement with the choice. In both
studies 1 and 2, there was a tendency to see the choice architect as to a higher degree trying to
exert influence on them when the nudge was transparent. Possibly first when experienced, with
money on the line, the perceived intended influence became strong enough to actually decrease
fairness ratings.
We hesitate to argue that one evaluation mode holds strict precedence over the other
concerning people’s opinions and perceptions of a nudge. Both description- and experience-
based approaches can elicit valid and informative evaluations, and each approach comes with
separate merits and drawbacks. On the one hand, people may have an easier time to take a step
back and reflect on their judgment when evaluating a nudge in a description-based format. This
is especially likely if the evaluator is presented with a comparison point, such as in a joint
evaluation format. On the other hand, an experienced-based approach is by definition better
suited for answering how a nudge is perceived by people in the “heat of the moment”, in a real
world application. This mode may pick up nuance that is lost when a nudge is rated at a distance,
based on a description. We thus recommend that policy-makers and other choice architects
gather public opinion in multiple ways, in order to be able to design policies that best meet the
public’s expectations. This is especially the case when the intervention contains elements that
may be hard to assess, such as when making a nudge transparent by providing a nudge
disclosure. One way to reap the benefits of both approaches could be to use the description-based
survey approach as an avenue for developing models of what affects people’s perceptions of
nudges (see Hagman, 2018), and experience-based studies as a stress test of theory before
interventions are converted into larger-scale policy.
Concerning generalizability, it should be remembered that only one type of nudge was
tested in the present studies. While we see little reason that our findings should not extend to
other types of nudges, the substantive findings are necessarily affected by the specific context. It
is also conceivable that the fairness ratings found were boosted from social desirability in
answers, as we, the researchers, played both the roles of choice architect and surveyor. However,
influence from social desirability should have been equal across experimental conditions,
meaning that relative differences would not have been affected.
It should also be stressed that the present findings likely only hold for situations where
the nudge targets a predominantly pro-social behavior. That is, when the nudge encourages a
behavior that inflicts a cost on the chooser for the benefit of someone else. If instead the
intervention facilitates a behavior in the nudgee’s apparent self-interest, it seems plausible the
nudge would decrease perceived fairness less, if at all.
In conclusion, it is ironic and unfortunate that a choice architect attempting to be more
ethical by making a nudge transparent may end up being perceived as less so by the people
affected by the nudge. Choice architects may thus face a trade-off between “objective” and
“subjective” fairness when implementing nudges – on the one hand catering to theoretically
based ethical demands and on the other to the perceptions of affected individuals. We suggest
that future research should more closely investigate the determinants of when increased
transparency is seen as a sign of positive openness, and when it risk primarily signaling an
influence attempt. Understanding how increased transparency affects nudge interventions may
provide a key for how to design nudges that are as effective, ethical, and socially sustainable as
possible. However, the present findings show that increased transparency is not a silver bullet in
the hunt for a nudge intervention that is both effective in changing behavior, and perceived as
fair and acceptable by people subjected to it.