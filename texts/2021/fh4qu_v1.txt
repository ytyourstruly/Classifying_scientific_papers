[DRAFT VERSION]
Improvised Music Follows Human Language Quantitative Properties to
Optimize Music Processing
Sarig Sela
The Hebrew University, Mt. Scopus, Jerusalem 9190501, Israel.
Email: sarig.sela@gmail.com
Publication status
Published. Sela, S. (2022). Improvised music follows human language quantitative properties to
[DRAFT VERSION]
Abstract
Music is a cognitively demanding task. New tones override the previous tones in
quick succession, with only a short window to process them. Language presents
similar constraints on the brain. The cognitive constraints associated with language
processing have been argued to promote the Chunk-and-Pass processing hypothesis
and may influence the statistical regularities associated with word and phenome
presentation that have been identified in language and are thought to allow optimal
communication. If this hypothesis were true, then similar statistical properties
should be identified in music as in language. By searching for real-life musical
corpora, rather than relying on the artificial generation of musical stimuli, a novel
approach to melodic fragmentation was developed specifically for a corpus
comprised of improvisation transcriptions that represent a popular performance
practice tradition from the 16th century. These improvisations were created by
following a very detailed technique, which was disseminated through music
tutorials and treatises across Europe during the 16th century. These music tutorials
present a very precise methodology for improvisation, using a pre-defined
vocabulary of melodic fragments (similar to modern jazz licks). I have found that
these corpora follow two paramount, quantitative linguistics characteristics: (1)
Zipfâ€™s rank-frequency law and (2) Zipfâ€™s abbreviation law. According to the
working hypothesis, adherence to these laws ensures the optimal coding of the
examined music corpora, which facilitates the improved cognitive processing for
both the listener and the improviser. Although these statistical characteristics are
not consciously implemented by the improviser, they might play a critical role in
music processing for both the listener and the improviser.
Keywords: Zipfâ€™s Laws; Ornamentation; Improvisation; Corpus Analysis;
Quantitative Linguistics.
[DRAFT VERSION]
Improvised Music Follows Human Language Quantitative Properties to Optimize Music
Processing
Performing music requires astonishing levels of human skill. Regardless of whether
music is performed from memory, read from a music sheet, or improvised, the number of
different parameters that must be synchronized to produce music is overwhelming: duration,
pitch, timbre, articulation, and volume are only one set of parameters required to control and
render a single note (Burton, 2015, pp.22-28; Gardner 2011, p. 111). Higher levels of abstraction
are required to produce music, even if the music is composed of only one monophonic stream of
notes. These abstraction levels are expressed on a sequence of notes, rather than on any single
note, such as accelerando/ritenuto, glissando, crescendo/diminuendo, which describe gradual
changes in note sequences (Palmer, 1989; Poli, 2004; Shaffer, 1984). The ability to group
sequences of notes into higher syntactical and conceptual units allows for the generation of
hierarchical structures. Each level of structure relates to other structures, at the same level or
between different levels, according to their associated properties (Peretz, 1989; Tan et al., 1981).
For example, to group note sequences, one can use a special delimiter to segment the note
sequences (Lerdahl & Jackendoff, 1983, pp. 45â€“52; Pearce, MÃ¼llensiefen, & Wiggins, 2010;
has been defined, it can be used as a building block for the next abstraction level. If note
grouping and the creation of building blocks are not defined arbitrarily but, instead, can be
formulated according to a set of combinatorial and compositional rules, then these rules would
represent a syntax for the stream of notes.
[DRAFT VERSION]
as one element (A) for the next surface (red line marks the sound of a note).
The differentiation between various combinatorial and compositional rules defines
musical styles.
Music is a temporal domain. Once the note duration is over, no traces of the auditory
stimuli remain.1 The order of the notes must be remembered by the listener to be able to process
the stream.
In this paper, I define music processing as a bi-directional process through which a
performer renders a stream of notes by applying a syntax, which is stored at higher abstraction
levels, whereas a listener executes the reverse processing events to reconstruct higher abstraction
levels by applying the same syntax rules to a stream of notes.
In summary, a monophonic stream of music bears three characteristics that are germane
to this discussion: (1) a hierarchical structure that is defined by a set of rules (syntax); (2)
temporality, as the auditory stimuli fade out; and (3) the significance of the order in which the
notes are rendered in the stream.
1 Up to the level of an acoustical echo effect, during which previous notes might overlap with the
current notes of a sequence for a limited timeframe.
[DRAFT VERSION]
Human language exhibits similar attributes as those in music. Humans can speak
extemporaneously by recombining meaningless speech sounds, referred to as phonemes, into
meaningful units, known as morphemes and words, which are combined into higher-level
constructs, e.g., sentences and paragraphs (Brent, 1999; Jusczyk, 1997). The combinatorial and
compositional rules for reconstructing these structures are particular to each language and
constitute elements of the acquired knowledge that both the speaker and the listener must possess
to be able to process language. Understanding the properties of these combinatorial and
compositional rules, which define language structures, is a central goal of cognitive science
(Harris, 2006).
Christiansen and Chater (2016) coined the term Now-or-Never bottleneck to describe a
key premise of language processing, which states that â€œif linguistic information is not processed
rapidly, that information is lost for good.â€ They claimed that this pressure on the brain has
fundamental implications for language processing, acquisition, and change, which is likely to
affect the structure of language itself.
Moreover, Christiansen and Chater (2016) suggested that the Now-or-Never bottleneck
was a dominant influence (although not necessarily unique) to the development of a cognitive
process hypothesis, named Chunk-and-Pass, to overcome this constraint. Chunk-and-Pass is
described as a component of the brainâ€™s comprehension system that breaks received stimuli into
groups, which are recoded and compressed into chunks of information, which are then passed to
these chunks may be compressed in a lossy manner, such that not all of the information can be
decoded at higher levels of comprehension. Moreover, the chunks themselves might not be
[DRAFT VERSION]
recombined into a single object, which can result in shallow and sometimes inaccurate
interpretations.
The Chunk-and-Pass process is bi-directional. Theoretically, the speaker is assumed to
start the speech process at a conceptual level, which is then broken down into words that are
further divided into articulated phenomes via speech. In contrast, the listener is believed to
perform the opposite processing to rebuild the structure, starting with the identification of the
phonemes and ending with the construction of the concept.
Working memory plays a crucial function during the processing of temporal stimuli
because once an information element (i.e., notes or morphemes) has passed its duration, it is
gone forever. The appearance order of temporal, arbitrary stimuli elements has been
demonstrated to be restricted in the short-term memory (STM) at ranges of 7 Â± 2 (Miller, 1956)
to 4 Â± 1 items (Cowan, 2001).
Although the stimuli stored in STM appear to be very small relative to the size of higher-
level constructs (such as sentences or musical phrases), humans are still capable of impressive
processing capabilities. This capability increases when the stimuli are not random but are instead
generated by a guided schema. For example, English speaking and listening rates are typically
150 words per minute, which corresponds to approximately 10-15 phonemes or 5â€“6 syllables per
second (Studdert-Kennedy, 1986). This high level of processing performance contrasts with the
much poorer performance detected when the stimuli is a sequence of arbitrary auditory signals,
such as a low-pitched tone (600 Hz), a high-pitched tone (1000 Hz), and a broadband noise
(Warren et al. 1969).
A similar effect on performance was described by Sloboda (1982, p. 485), who reported
that the eye-hand span (EHS), i.e., the amount of reading ahead, was typically six or seven notes
[DRAFT VERSION]
for good readers and three or four notes for poor readers, but that the performances of both good
and bad readers decreased when the sequence was random.
more concrete units, whereas the listener encodes information units into more abstract units.
The positive correlation between poor performance and the level of randomness in the
presented stimuli can be explained by Shannonâ€™s (1948) information theory concepts. The
effectiveness of compression can be measured using a data compression ratio, which is the ratio
between the length of the uncompressed stimuli (represented as a sequence of elements) and the
length of the compressed stimuli, such that a smaller ratio indicates less effective compression.
Shannon (1948) indicated that more random uncompressed stimuli are associated with smaller
compression ratios, resulting in longer compressed outputs, which might explain the
experimental results reported by Warren et al. (1969) and Sloboda (1982, p. 485) suggesting that
poor performance is associated with poor compression ratios.
Therefore, investigating the relationship between stimuli organization and compression
ratios is necessary to evaluate whether the Chunk-and-Pass process effectively encodes
[DRAFT VERSION]
information gathered at each abstraction level (based on effective compression ratios) to address
and overcome the proposed Now-or-Never bottleneck.
Two of Zipfâ€™s laws (Zipf, 1936, 1949) appear to shed light on the relationship between
the compression ratio and the organization of input data. The law of abbreviation claims that as
law of abbreviation has been verified in 986 human languages (Bentz & Ferrer-i-Cancho, 2016),
and is included in the animal communication systems of Formosan macaques and common
marmosets, in the surface behavioral patterns of dolphins (Ferrerâ€iâ€Cancho et al., 2013), and in
the Unix operating system (OS) command-line interpreter (Ellis & Hitchcock, 1986). Therefore,
this rule does not appear to be specific to any particular language but is a universal property of
human languages.
The abbreviation law can be defined mathematically as follows:
ğ¶ğ‘œğ‘Ÿğ‘Ÿ(ğ¿, ğ¹) < 0 (1)
where the magnitude of words (L) stands in negative (not necessarily proportionate)
correlation to the number of occurrences (F).
[DRAFT VERSION]
is 19 Billion words. Each word is represented by a single point (only some points are labeled). The law of
abbreviation states that the more frequently a word appears, the shorter it tends to be. The red line
represents the mean frequency of words, according to the word length in letters.
The other regularity identified by Zipf, often considered to represent the best-known law
in quantitative linguistics, is known as the rank-frequency law, which claims that the most
frequent word in a given corpus of language will occur approximately twice as often as the
second-most frequent word, three times as often as the third-most frequent word, and so on. This
law also has been verified in most written and spoken languages (Bian et al. 2016; Piantadosi,
2014).
This occurrence distribution frequency is called a Zipfian frequency and is
mathematically defined as follows:
ğ¹ âˆ , where a â‰ˆ 1,
ğ‘…!
R is the word s occurrence rank and F is its frequency (2)
[DRAFT VERSION]
The combination of these two laws allowed Mandelbrot (1953, 1961) to mathematically
prove that if an input stream follows a near-Zipfian2 distribution and the more-frequent words are
short, then the processing cost of words is minimized for average information content. Following
Mandelbrotâ€™s derivations, a few researchers have claimed that Zipfâ€™s laws provide a mechanism
for the maintenance of effective communications (Ferrer-i-Cancho, 2005a, 2005b; Mandelbrot,
1961; Salge et al., 2015; Zipf, 1936, 1949). For example, Ferrer-i-Cancho (2016) has suggested
that the reoccurrence of Zipfâ€™s distribution across multiple human languages could originate
from the realistic pressure of the Now-or-Never bottleneck (Christiansen & Chater, 2016) of
linguistic processing.
The structure of the optimal communication for processing argumentation is shown in
Never bottleneck represents a catalyst for the Chunk-and-Pass strategy to address the problem faced by
cognitive capabilities bounded by limited resources. To enhance the effectiveness of the Chunk-and-Pass
strategy, the input (music) is optimized by following the rank-frequency and abbreviation laws, which also
impact the generation of music.
Zipfâ€™s rank-frequency law has been demonstrated not only in language but also in many
other areas of science (Farmer & Geanakoplos, 2008; Mitzenmacher, 2004; Newman, 2005;
2 The term near-Zipfian is used more broadly to describe frequency distributions that
approximate the expected results based on Zipfâ€™s rank-frequency law.
[DRAFT VERSION]
Saichev, Malevergne et al., 2011). Several papers have demonstrated a Zipfian distribution for
musical elements (HsÃ¼ & HsÃ¼, 1991; Lerdahl & Jackendoff, 1983; Manaris et al., 2003, 2005;
Voss & Clarke, 1975, 1978).
None exiting papers (to my knowledge) in the field of music have attempted to explain
the phenomenon of Zipfian distribution from a communication optimization perspective, which
may be due to a lack of evidence that these derivations are based on Zipfâ€™s law of abbreviation.
For example, Manaris et al. (2005) and Zipf himself (1949) examined single melodic intervals to
demonstrate Zipfâ€™s rank-frequency law. However, comparing single melodic intervals to words
is problematic due to the magnitude of the inventory of elements, as most melodies are
constrained by an octave and a half, limiting the maximum possible number of melodic intervals
to choose from to 18, whereas more than 150,000 words exist in the English language.
A more productive analogy, in terms of the magnitude of elements, would be to compare
letters (or phonemes) to intervals and to compare words to interval sequences. Manaris et al.
(2005) appeared to have considered these comparisons and were able to demonstrate Zipfian
distributions by breaking melodies into binary and ternary sequence intervals rather than single
note occurrences.
Breaking melodies into fixed-length sequences to serve as abstraction layers can be
challenging because melodies are not typically grouped by fixed-length elements; they are
grouped according to syntactic rules, as formulated by music theoreticians (Lerdahl &
Jackendoff, 1983; Schenker, 2001). Therefore, notes are generally grouped into different sizes
depending on the syntax. In language, fixed-length is not productive either, as grouping letters
into fixed lengths without considering the boundaries of words leads to an increase in the
inventory size. For example, a sentence such as: â€œgood morning everyoneâ€ will be grouped into a
[DRAFT VERSION]
meaningless sequence of morphemes (or symbols) instead of into meaningful words (in this
example, good | morn | inge | very | one).
The two Zipf laws have been verified to be valid for a variety of corpora, including
corpora that comprise scientific texts and corpora that comprise literature (like reading books).
Moreover, the Zipf laws have also been verified to apply to spoken language, in which the
speakers and listeners process extemporaneous data. However, the argument that the Now-or-
Never bottleneck serves as a catalyst for communication optimization is stronger for spontaneous
speech because writing and reading are less temporal. The writer can plan and change the text as
much as desired, and the reader can read a sentence as many times as desired. However,
spontaneous speech and written language (or pre-planned speech) are believed to have
influenced the structures of the language. Therefore, the same characteristic statistical
regularities have been identified in both spontaneous speech and written language even though,
theoretically, the Now-or-Never bottleneck should be a less dominant influence on the written
language. The same assumption applies to music: improvisation and composed music both
influence the structure of music; therefore, similar characteristic statistical regularities are
expected to be identified for both improvised and composed music.
In this paper, I would like to go a step further than Manaris et al. (2005) by examining
both the rank-frequency law and the abbreviation law established by Zipf. These two laws are
paramount characteristics of quantitative linguistics and are considered to be universal to all
languages. I have examined melodic fragments taken from three special corpora of 16th-century
transcriptions of improvisation practice to examine their adherence to these two laws.
[DRAFT VERSION]
I present a novel approach used to extract melodic fragments from a melody using 16th-
century cultural and performance practice theories while preserving cognitive considerations
based on modern theories, including GTTM and IR.
The finding that these musical improvisations adhered to Zipfâ€™s laws supported the claim
that communication optimization in human languages occurs in all examined corpora. The
results of this study revealed parallels between the statistical characteristics of the examined
corpora and human languages. The hypothesis that the Now-or-Never bottleneck serves as a
catalyst for cognitive mechanisms and the statistical regularities that underlie the cultural
influences of different musical styles was strengthened by these findings.
Methods
To examine the hypothesis that statistical similarity exists between meaningful melodic
fragments and human language morphemes, I attempted to identify a musical style in which
melodic segmentation could be inferred, without ambiguity, from a well-defined model.
No consensus has been reached regarding how to segment a stream of any genre of
melodic sequences into meaningful elements (the equivalent of words or morphemes in
language) among music scholars (Cambouropoulos et al., 2001; Crawford et al. 1998; Lidov,
1979; Lerdahl & Jackendoff, 1983; Meredith et al., 2002, Narmour, 1990, 1992; Pearce et al.,
2010; Rolland & Ganascia, 2000; Spevak, Thom, & HÃ¶thker, 2002). A few widespread theories
have attempted to address the subject of melodic segmentation by offering different sets of rules.
Prominent examples of such theories include the generative theory of tonal music (GTTM),
presented by Lerdahl & Jackendoff (1983) and based on Gestalt principles, and Narmourâ€™s
Implication-Realization (IR) model (1990, 1992), which is based on similar Gestaltian principles,
[DRAFT VERSION]
with an emphasis on the dynamics of music over time and the intervallic expectations of music.
These proposed rules for segmentation are associated with various music fundamentals, such as
melody, rhythm, and, in some cases, harmony. However, segmentation ambiguities can arise
occur in music.
considerations suggests the grouping of the second beat, as shown in option A, due to a harmony change (E7
Ã  Am) and the onset of the third beat. In contrast, all of the melodic intervals are stepwise, except for the
first interval (a 7th interval), which suggests the grouping proposed by option B.
Unlike functional utterances, where ambiguity represents a disadvantage, in music,
ambiguity provides depth and additional meaning. However, segmentation ambiguity must be
how the two grouping options might be rendered.
3 The excerpt is the second bar of the first movement of Bachâ€™s partita for solo flute, in A minor,
BWV 1013.
[DRAFT VERSION]
Reference source not found..
The resolution depends not only on gestalt, cognitive, or computational theories but also
on culture (Spevak et al., 2002; Thom et al., 2002). The aesthetics of the musical style and
period, the performance practice, and the theory used to support the creation of the composition
are all dominant parameters in the grouping process. The same melody might change its note
grouping performance based solely on the period during which it was written.
Therefore, when comparing the regularities between language and music, the method
used to segment the melody into meaningful fragments must be carefully considered. The
minimal threshold requirement for segmentation might be the avoidance of contradicting the
main cognitive theories, such as the GTTM and IR models. If segmentation rules are defined by
considering a given periodâ€™s cultural aesthetics, performance practices, and compositional
theories, then the segmentation process is likely to be even more reliable than solely cognitive
considerations because the segmented chunks are likely to reflect the process used to construct
the music and can be used to identify the building blocks that were actually in use.
Therefore, to minimize the ambiguity of segmentation I chose a very specific type of
music genre, improvisations of 16th century music, to serve as a corpus. In this music genre, the
segmentation algorithm is well-defined and agreed upon in all the treatises of the 16th century.
[DRAFT VERSION]
Therefore, at least for the theoreticians of the 16th century, the corpus provides a consensus of the
melodic building blocks.
Moreover, a model that was designed specifically for improvisation, rather than for pure
composition, was preferable because the main research hypothesis suggests that the Now-or-
Never bottleneck serves as a catalyst for the Chunk-and-Pass processing theory and the
establishment of statistical regularities. An improvisation setting ensures that the Now-or-Never
bottleneck pressure is high, not only for a listener who is hearing the composition for the first
time but also for the performer who invents the improvisational fragments on the fly.
The chosen style for the experiment was 16th-century division ornamentation practice
(DOP). DOP proliferated during the 16th century and the first two decades of the 17th century.
Eleven division instruction manuals were published by improvisers to help assist professional
and novice musicians in the acquisition of DOP skills.4 In DOP, the performer is expected to
improvise ornamentations on the melodic line of a well-known composition. The compositions
were typically the polyphonic music of madrigals, chansons, and motets.
The improvisational section was typically not implemented by inventing melodic
sequences from scratch but instead by choosing pre-defined melodic sequences (which will be
referred to as figurations in this paper) and creatively recombining them. A substantial portion of
the division instruction manuals was used as suggestions for figurations to be used.
4 The division instruction manuals from the 16th century and the beginning of the 17th century:
Ganassi (1535); Ortiz (1553); Dalla Casa (1584); Bassano (1585); Rognoni (1592); Conforti
(1593); Bovicelli (1594); Virgiliano (ca. 1600); Brunelli (1610); Rognoni-Taeggio (1620); Spadi
(1624).
[DRAFT VERSION]
Syntactic rules exist that govern which figurations are considered to be valid for
embedding into various melodic contexts. Each improviser was intended to memorize the pre-
defined figurations presented in the division instruction manuals and to execute them during
improvisation. In this sense, the figurations are similar to words that are retrieved from memory
when parsing sentences.
The sources available for musicologists to learn about DOP include (1) improvisation
transcriptions of famous improvisers of the time, such as Bassano, Dalla Casa, and Ortiz. In this
paper, I used these sources to create the corpora (2) division manuals, designed to teach
musicians how to acquire the DOP technique, which represents a unique type of music treatise.
The division manuals provide a well-defined model describing the algorithm necessary to apply
the DOP technique. In addition, the division manuals included a â€˜dictionaryâ€™ (as mentioned
above) of thousands of pre-defined melodic fragment sequences to be applied as improvisation
fragments (the equivalent of Jazz licks or riffs). I used the model to reverse engineer the
improvisational process and to extract the melodic fragments into groups (these represent the
musical building blocks and are equivalent to human language morphemes). Some of the
manuals also provide a philosophical, cultural, and aesthetic framework for DOP. (3) Scholarly
treatises can provide quantitative advice, rules of thumbs, and aesthetic and philosophical
context; however, I did not use any of these sources in this paper.
The corpora
Bassanoâ€™s corpus consists of all of his transcribed improvisations. In Motetti, Madrigali
et Canzonie Francese (1591), 37 Soprano divisions and seven Bass divisions can be found;
[DRAFT VERSION]
however, six viola bastarda divisions were omitted.5 From his Ricercate publication (1585), two
additional Soprano divisions, written upon Roreâ€™s madrigal Signor mio caro, were included in
the corpus.
Dalla Casaâ€™s corpus consists of all of his 91 transcribed divisions (I omitted the bastarda
divisions). His transcriptions are published in the two-part 1584 treatise Il vero modo di diminuir.
Ortizâ€™s corpus consists of eight transcribed improvisations: four on the madrigal â€˜O Felici
Occhi Mieiâ€™ and the other four on the cancion â€˜Dolce Memoireâ€™ (Ortiz, 1553).
The model for generating an improvisation, as defined in the 16th-century division
manuals
The first instrumental division manual, called Opera Intitulata Fontegara, was published
by Sylvestro Ganassi, a musician of the Doge, in Venice in 1535. Fontegara is a recorder tutor
that contains a large section describing DOP. It is unique, not only from a historical outlook but
also from the viewpoint of this paper, as it provides a â€˜bilingualâ€™ dictionary and an algorithm for
how to use the dictionary to improvise.
The dictionary structure of the manual is a collection of pairs. Each pair is composed of a
key and multiple values. I call each of these values a figuration. In this paper, a figuration is the
equivalent of a morpheme, an equivalence that will be demonstrated later. The key is composed
of a sequence, ranging from two to five notes. Each figuration is a melodic fragment unit,
5 A viola bastarda division is a special form of division, as its figurations are based on more than
one specific voice of the original piece. Therefore, figuration extraction from these divisions is
not deterministic.
[DRAFT VERSION]
constructed from a longer melodic sequence of shorter note values than its associated key, in
which the first note and the last note of the figuration have the same pitch class as the first note
appear in the original melody and its associated figurations.
replacing a given melody fragment from the original composition. The improviser can choose any figuration,
depending on his taste. The figurationâ€™s first and last notes are the same as the original melodyâ€™s first and last
notes.
The algorithm for how to use the dictionary is as follows:
1. Choose a melodic fragment from the original composition to be ornamented.
2. Identify the fragment using the keys of the dictionary.
3. Choose one of the corresponding values for the key and embed it into the original
composition in place of the original melodic fragment.
In this model, three operations can be applied to the melodic fragment:
[DRAFT VERSION]
1. Diatonic transposition (DT) â€“ each key and value can be transposed diatonically to fit
into the original melody of the composition.
2. Rhythmical transformation (RT) â€“ each note in the key and value can be multiplied
by a constant because the absolute duration of the notes is relative. Therefore, every
key with quarter notes can be turned into half-notes or eighth notes.
3. Rhythmical alteration (RA) â€“some minor variations can be made to the rhymical
patterns. For example, a sequence of four eighth notes can be turned into a dotted
sequence pattern (a dotted eighth note and a sixteenth note).
figuration.
Another interesting case is when overlap exists between figurations, i.e., the last note of
one figuration is the first note of the next figuration. The sequence of these overlapping
[DRAFT VERSION]
figurations is referred to in this paper as chained figurations. Chained figurations allow the
improviser to prolong the improvisation to span across many melodic sequences.
figurations. Bracket 1 signifies a simple figuration, in which the last note of the figuration is not
the beginning of the next figuration. In contrast, Figurations 2 and 3 share the same note B
(marked in red) and, therefore, generate a chained figuration, marked by bracket 4.
partire. The red note is the last note of Figuration 2 and the first note of Figuration 3; therefore, the
combination generates a chained figuration, indicated by Bracket 4.
The model of melodic segmentation, based on the 16th-century model
Based on the 16th-century model, I developed an algorithm to reverse engineer the
improvisational process of embedding a figuration into the original melody and extracted the
generated figurations. This algorithm segments the improvised versions based on 16th-century
rules
[DRAFT VERSION]
Algorithm: Extract Figurations
Input: original melody A, improvised melody B
Output: list L of figurations in B
list Unisons = GetHarmonicUnisons(A, B)
For each unison[i], unison[i+1] in Unisons:
Figuration = ExtractNotesBetween(unison[i], unison[i+1])
If Len(Figuration) > 0:
L.append(Figuration)
return L
Func GetHarmonicUnisons(melody A, melody B):
Return all unisons between A and B where a unison is comprised of a
note x and y and x.offsetTime = y.offsetTime. (offsetTime is the
time passed from the beginning of the composition to the beginning
of the note)
Func ExtractNotesBetween(unison1, unison2):
Extract the improvised melody notes between unison1 and unison2
comparing the improvisations to the original melody using the developed algorithm.
[DRAFT VERSION]
The algorithm follows the figuration definition established by the 16th-century model and
adheres to the rules established by cognitive reasoning: all of the figurations in the corpora begin
and end on a relatively strong beat; their boundaries are always between a harmonic interval of
the same pitch class (unison or octave), which is easily distinguishable and serves as a marker;
and each figuration note is usually shorter (by two or more times) than the notes of the original
composition.
Figuration equality and figuration type definitions
Let figuration a = (a â€¦a ) and figuration b = (b â€¦b ) where a ,b are notes.
1 n 1 n i i
Equality between figurations indicates that all the notes that comprise the figurations
have the same pitch and duration and appear in the same order. Mathematically, figuration
equality can be defined as:
a=b iff a = b for each i in (1...n). (1)
i i
Figurations have the same type if it is possible to turn one figuration into the other by
transformed into one of the other figurations by using these DT, RT, or RA.
Mathematically this can be defined as follows:
Type(a) = Type(b) iff exists a sequence of figuration operations ops=(op â€¦op ) such that
1 m
op (â€¦op (op (a))) = b. (2)
m 2 1
the operations performed on figurations are DT, RT, and RA (as defined in the previous
section)
[DRAFT VERSION]
Computational Tools
I used the Music21 framework (Cuthbert, & Ariza, 2010) to perform figuration extraction
and Python 3 and SciPy library (Jones E., Oliphant T. E., Peterson P., 2001-) for all other
statistical analyses.
Results
Corpora figuration extraction
Using the described algorithm, I extracted all figurations directly from the corpora,
classified them by type, and calculated the frequency of occurrence for each figuration type, as
itemises the figurations and the frequencies of their occurrence. In this format, each dictionary is
superior to the 16th-century dictionaries provided by the division manuals because they also
contain information regarding the frequency of occurrence for each figuration type. Moreover,
these usage statistics were derived directly and automatically from the improvisations
themselves, rather than from the performersâ€™ subjective views on which figurations were being
used.
As observed, Oritz provided a relatively small sample of transcriptions and, therefore,
uses fewer figuration types and instances. Bassano tends to reuse the same figuration types more
frequently than the others.
Extracted figurations from the Corpora
Bassan Dalla Casa Ortiz Global
Figuration Instances 2435 3482 463 6380
[DRAFT VERSION]
Extracted figurations from the Corpora
Figuration Types 771 1270 192 2233
Reuse of figuration types 3.16 2.74 2.41 2.86
ratio (Instances/Types)
No. of Transcriptions 52 91 8 151
The rank-frequency law
represents figuration types, ordered by their rank from the most to least frequent. The Y-axis
represents the frequency of each figuration type. As observed, the distribution of these
figurations is fairly similar to human language distribution.
a near-Zipfian distribution for each corpus, as well as the union of the corpora (labeled â€œGlobalâ€).
[DRAFT VERSION]
Moreover, a recent study (Mehri & Jamaati, 2017) calculated the range of values on a
corpus that consisted of the Bible text translated into 100 languages. The study showed that the a
values (defined in Equation 1) ranged from 0.765 to 1.442.
Mehri & Jamaatiâ€™s (2017) study, associated with high coefficients of determination (R2) values;
therefore, the corpora followed a near-Zipfian distribution, similar to natural languages.
Linear regression of the log-log plot yields the following exponent a and
R2 values
Bassano Dalla Casa Ortiz Global
a 0.79 0.9 0.77 1.1
R2 0.86 0.95 0.90 0.97
The abbreviation law
To investigate the relationship between the frequency of each figuration type and its
length, as described by Zipfâ€™s abbreviation law, the average frequencies of figuration types for
each length value were calculated. A similar approach has been applied to human language
corpora (Kanwal et al., 2017).
Zipfâ€™s word length can be interpreted in music using two reasonable methods:
Length can be described as the figuration sequence length, i.e., the number of melodic
intervals that comprise the figuration sequence. Formally:
ğ‘™ğ‘’ğ‘› (ğ‘) = ğ‘›, ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘›ğ‘¦ ğ‘“ğ‘–ğ‘”ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ = (ğ‘ . . ğ‘ ) (2)
#$% & â€™
Length can also be described as the figuration duration length, i.e., the duration
summation of the figuration note sequence. Formally:
ğ‘™ğ‘’ğ‘› (ğ‘) = P ğ‘‘ğ‘¢ğ‘Ÿ(ğ‘ ) , ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘›ğ‘¦ ğ‘“ğ‘–ğ‘”ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ = (ğ‘ . . ğ‘ ) (3)
()* + & â€™
+,&
[DRAFT VERSION]
its length is non-linear, and the figuration type distribution is not normally distributed. Therefore,
instead of using a linear association, such as the Pearson correlation coefficient, a Kendall and
Spearman non-parametric rank-correlation coefficient was used.
measured in quarter notes.
[DRAFT VERSION]
The correlation coefficient results demonstrated a moderate inverse relationship, which
demonstrated in human languages.
Inverse correlation between figuration type frequency and length.
Bassano Dalla Casa Ortiz Global
Figuration sequence length
value p-value value p-value value p-value value p-value
Ï âˆ’0.347 3Eâˆ’23 âˆ’0.361 2Eâˆ’40 âˆ’0.505 8Eâˆ’14 âˆ’0.36 5Eâˆ’63
Ï„ âˆ’0.280 2Eâˆ’22 âˆ’0.294 9Eâˆ’39 âˆ’0.420 4Eâˆ’13 âˆ’0.29 4Eâˆ’60
Figuration duration length
Ï âˆ’0.346 3Eâˆ’26 âˆ’0.343 2Eâˆ’41 âˆ’0.423 3Eâˆ’11 âˆ’0.35 6Eâˆ’70
Ï„ âˆ’0.279 3Eâˆ’25 âˆ’0.280 6Eâˆ’40 âˆ’0.354 5Eâˆ’11 âˆ’0.28 1Eâˆ’66
Note: p-value denotes the two-sided values for a hypothesis test whose null hypothesis is an absence of
association, i.e., correlation coefficient = 0. Ï = Spearmanâ€™s rank correlation coefficient; Ï„ = Kendallâ€™s rank
correlation coefficient.
Discussion
In this paper, I showed that statistical similarities exist in the organization of improvised
16th-century music and language. These similarities are exemplified by their adherence to the
two paramount linguistic laws, identified by Zipf (1949): the rank-frequency and abbreviation
laws. The music corpora in this paper consisted of 151 transcribed improvisations from the 16th
century (Bassano, 1591; Dalla Casa, 1584; Ortiz, 1553). This study hypothesized that 16th-
[DRAFT VERSION]
century improvisation practice aesthetics were influenced not only by culture but also by
communicative optimization principles.
Moreover, the statistical regularities that were found were valid to all improvisers even
though each improviser used his own unique vocabulary of figurations. The validity across
vocabularies strengthens the universality property of the hypothesis as it is not bounded to a
specific vocabulary.6
In language, communicative optimization has been suggested to be necessary for
overcoming the Now-or-Never bottleneck constraint (Ferrier-i-Cancho, 2016; Pinatadosi, 2014).
Because this same Now-or-Never bottleneck constraint exists in music, I suggest that the
adherence to Zipfâ€™s laws in music occurs for the same reasons that these laws apply to language
and, therefore, this case study serves as a concrete example that music is not only influenced by
culture.
Although earlier studies of music have established that the rank-frequency law applies to
melodic sequences (e.g., HsÃ¼ & HsÃ¼, 1991; Manaris et al., 2003, 2005; Voss & Clarke, 1975,
1978), the abbreviation law, which is necessary for the communicative optimization hypothesis
(Ferrer-i-Cancho, 2016) as a result of Mandelbrotâ€™s derivation (1953) was not able to be
examined because the melody elements that have been used previously were constructed by
dividing the melody into single, binary and ternary melodic sequence elements, in an arbitrary
manner.
6 The claim for universality of Zipfâ€™s laws have been examined across human languages, see
Bentz & Ferrer-i-Cancho, R. (2016)
[DRAFT VERSION]
In this paper, I divided the melodies into their building blocks, called figurations,
according to the syntactic rules defined by 16th-century theoreticians (e.g., Conforti, 1593;
Ganassi, 1535; Ortiz, 1553) of the period. These rules also adhere to modern cognitive melodic
segmentation theories (e.g., GTTM and IR). This syntactically aware method yielded melodic
elements of variable lengths called figurations. In the 16th century, every professional and
amateur musician was expected to learn and apply these figurations according to the syntactic
rules of the period. I have demonstrated that these figurations, with their variable sequence
lengths, follow the abbreviation law similar to all natural languages. This adherence to Zipfâ€™s
laws suggests that the communicative optimization claim is valid, not only for language but also
for the musical corpora I examined.
Moreover, discovering that statistical similarities exist between language and music (as
expected based on the Now-or-Never bottleneck hypothesis regarding the processing of
information) suggests that either a shared neural network exists in the brain that processes both
language and music (as the domain-general theory posits) or that the two modalities (language
and music) are processed by different areas of the brain using a common mechanism that is
sensitive to these statistical characteristics.
Nevertheless, the corpora I examined was not a collection of any type of music but rather
a very specific type of improvisation practice. Therefore, further research is required to be able
to generalize the conclusions to more musical genres and other semiotic systems to support the
claim that Zipfâ€™s laws are not only linguistic universals but are also semiotic system universals.
[DRAFT VERSION]