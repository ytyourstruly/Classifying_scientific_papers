STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 1
The Statistical, Theoretical, and Practical Significance of an Empirical Effect
1 2
Erich H. Witte , and Frank Zenker
Institute for Psychology, University of Hamburg, Hamburg, Germany
Nankai University, College of Philosophy, Tianjin, China
Author Note
We have no known conflict of interest to declare.
E.W. wrote a draft that both authors jointly developed and which F.Z. edited. Both
authors approved the final submitted manuscript.
Part of this research was conducted while F.Z. was funded by TUBITAK (The
Scientific and Technological Research Council of T√ºrkiye), Project No. 118C257.
Correspondence concerning this article should be addressed to Frank Zenker, Nankai
University, College of Philosophy, Jinnan New Campus, Haihe River Education Park,
300350 Tianjin, China, Email: frank.zenker@nankai.edu.cn, fzenker@gmail.com
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 2
Abstract
A comprehensive evaluation of an empirical effect should address the size of the effect
(statistical aspect), the ability to explain and predict this effect using a theoretical construct
(theoretical aspect), and the utility ascribed to it (practical aspect). In the case of the
empirical aspect, researchers do not so much evaluate but statistically describe an effect as a
parameter estimate. In the case of the theoretical aspect, the evaluation addresses whether this
parameter estimate explains the observations that inductively ground the effect size estimate,
respectively whether a theoretical construct predicts new observations of this effect. Contrary
to what the synonymous use of ‚Äòexplain‚Äô and ‚Äòpredict‚Äô suggests, the standards for an adequate
explanation differ from those for an adequate prediction. This, we argue, has important
implications for the smallest effect of interest that offers a worthwhile parameter for theory
construction research. In the case of the practical aspect, finally, the evaluation concerns the
amount of utility that is ascribed to a stable population effect.
Keywords: effect size; error-rates; evaluation; explanation; prediction; significance;
utility
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 3
The Statistical, Theoretical, and Practical Significance of an Empirical Effect
Expressing ‚Äúthe degree to which [a] phenomenon under study is manifested‚Äù (Cohen,
1977, 9), an effect size estimate quantifies the amount of causal influence or correlation
between independent and dependent variables. As the central role of effect size estimates in
behavioral science is widely recognized today (Huberty, 2002; Lakens, 2013; Wilkinson &
APA Task Force, 1999), the point of repeatedly estimating an effect in population samples is
to estimate a population effect. These estimates inform not only treatments, interventions, or
policies but also the development of valid theoretical constructs, that explain and predict true
population effects.
As behavioral scientists increasingly follow the recommendation to ‚Äúprovide some
effect size estimate when reporting a p-value‚Äù (Wilkinson & APA Task Force, 1999, 599;
notation adapted; Huberty, 2002), the effect size measures most frequently used by published
studies (Sch√§fer & Schwarz, 2019) are the standardized measures of Cohen‚Äôs d and Pearson‚Äôs
r (Cohen, 1977; Pearson, 1895), where . Conventionally, r = .10, .24, .37,
ùëüùëü = ùëëùëë/‚àöùëëùëë + 4
respectively d = .20, .50, .80, are called small, medium, and large effects (Cohen, 1977).
Already some 40 years ago, Rosenthal and Rubin (1982, 166) had observed that
‚Äú[‚Ä¶] experienced behavioral researchers and experienced statisticians [were]
quite surprised when we showed them that the Pearson r of .32 [or d = .67]
associated with a coefficient of determination (r ) of only .10 was the
correlational equivalent of increasing a [treatment‚Äôs] success rate from 34% to
66% by means of an experimental treatment procedure.‚Äù (Rosenthal & Rubin,
1982, 166)
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 4
While an increase in a treatment‚Äôs success rate of r = .32 would likely be evaluated as
a practically significant effect, an effect of similar size need not be theoretically significant.
Although both issues presuppose different evaluative standards (detailed below), the
discussion sections of behavioral science publications often ‚Äúcherry-pick‚Äù a single evaluative
standard. A case in point is how Greenwald, Banaji, and Nosek (2015) respond to a
theoretically motivated criticism based on a meta-analytically estimated correlation of merely
r = .148 (i.e., a small effect) between racially or ethnically discriminatory behavior and
measures of the Implicit Association Test (IAT) (Oswald, Mitchell, Blanton, Jaccard &
Tetlock, 2013).
‚ÄúOswald, Mitchell, Blanton, Jaccard, and Tetlock concluded that IAT
measures show ‚Äòpoor prediction of racial and ethnic discrimination‚Äô (pp. 171,
183) and provide ‚Äòlittle insight into who will discriminate against whom‚Äô (p.
188). [But] Oswald, Mitchell, Blanton, Jaccard, and Tetlock‚Äôs (2013)
conclusion did not take into account that small effect sizes affecting many
people or affecting individual people repeatedly can have great societal
significance.‚Äù (Greenwald, Banaji, and Nosek, 2015, 560; italics added)
While relying on the same statistical and meta-analytical methods as Oswald et al.
(2013) did, and although Greenwald et al.‚Äôs (2015) meta-analysis of the IAT even arrives at a
slightly larger estimate (r = .22), what their response cites is not the effect‚Äôs size but its
practical utility in a large population. What Oswald et al. (2013) criticized, however, is ‚Äú[t]he
low predictive utility for the race and ethnicity IATs present problems for contemporary
theories of prejudice and discrimination that assign a central role to implicit constructs‚Äù
(Oswald et al., 2013, 183; italics added). As is shown below, a theoretical construct that
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 5
states r = .22 yields a poor prediction because the vast majority of the effect‚Äôs observed
variance is left unaccounted for‚Äîand citing the IAT‚Äôs practical utility cannot change that.
Greenwald et al.‚Äôs (2015) response thus shifts the issue from evaluating an effect‚Äôs theoretical
to its practical significance.
A comprehensive evaluation of an empirical effect should address three aspects: the
size of the effect (statistical aspect), the ability to explain and predict this effect size using a
theoretical construct (theoretical aspect), and the utility ascribed to it (practical aspect). An
empirical effect can thus be evaluated in terms of its statistical, theoretical, and practical
significance. However, how behavioral scientists normally use Cohen‚Äôs d or Pearson‚Äôs r, as
well as related effect size measures (partial r, Œ∑¬≤, odds-ratio, etc.; see Ferguson, 2009), often
suggests that this evaluation can be reduced to a single aspect. This, we argue, oversimplifies
things. Although we address only simple hypotheses that state a difference between means,
what follows applies equally to hypotheses that state multiple means, complex correlations,
interactions, etc.
In the case of the empirical aspect, researchers do not so much evaluate but
statistically describe an effect as a parameter estimate. In the case of the theoretical aspect,
the evaluation addresses whether this parameter estimate explains the observations that
inductively ground the effect size estimate, respectively whether a theoretical construct
predicts new observations of this effect. Contrary to what the synonymous use of ‚Äòexplain‚Äô
and ‚Äòpredict‚Äô suggests, the standards for an adequate explanation differ from those for an
adequate prediction. This, we argue, has important implications for the smallest effect of
interest that offers a worthwhile parameter for theory construction. In the case of the
practical aspect, finally, the evaluation concerns the amount of utility that is ascribed to a
stable population effect.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 6
The statistical aspect
Statistical significance and effect size
When researchers conduct statistical tests to establish whether an observed difference
in behavioral responses is statistically significant, an effect size corresponding to this
observed difference is induced from observations as a parameter estimate. Parameter
estimating research relies on a statistical significance measure such as the p-value (Fisher,
1959) or the Œ±-error rate (Neyman & Pearson, 1967) and an effect size measure such as
‚àí ‚àí
Cohen‚Äôs |d| = [(m m ) / s], the difference between the observed means in two groups (m
1 0 1
m ) relative to the pooled standard deviation (s) in both groups (e.g., the experimental and
control groups). Crucial to understanding the statistical aspect of an empirical effect is that
the significance level (p, Œ±) and the d-value are independent.
To appreciate their independence, it suffices to review why, given a sufficiently large
sample (N = (N + N )), a t-test for statistical significance lets even the smallest mean
1 2
difference pass any statistical significance level (Bakan, 1966). In brief, the p-value/Œ±-error
rate decreases as the t-value increases and the t-value increases with N. In detail, the means in
two groups (N , N ) are associated with a standard error represented by the denominator of:
1 2
. So, if (m ‚Äì m ) is assumed to be constant and other things are
1 0
2 2
ùë†ùë†1 ùë†ùë†2
1 0
ùë°ùë° = (ùëöùëö ‚àí ùëöùëö )/ ùëÅùëÅ + ùëÅùëÅ
equal, then the only way of increasing the t-value is to increase N. For instance, for (m ‚Äì m )
1 0
= 0 we find t = 0, independently of N. Whereas if (m ‚Äì m ) ‚â† 0, then there is some sample
1 0
On the epistemological difference in the statistical approaches of Fisher (1959) and Neyman-Pearson
(1967) and why this difference often disappears in praxis, see Hubbard & Bayarri (2008). Fisher
interprets the p-value as a measure of evidence that is inductively inferred from observations, while
Neyman and Pearson interpret the Œ±-error rate as the long-run chance that this inductive inference is
erroneous.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 7
size that lets (m ‚Äì m ) pass any threshold of t. Therefore, if N is assumed to be constant, then
1 0
the p-value/Œ±-error-rate and the d-value are seen to be independent.
The statistical significance of an empirical effect thus depends only indirectly on the
d-value but directly on N. Of course, as N grows larger, what improves is the quality of a
parameter estimate (law of large numbers), allowing for greater confidence that the observed
d-value states the true effect size in a sample. Nevertheless, N is a property not of an
empirical effect, but of the statistical tool used to estimate its size.
Observed d-values are non-neutral parameter estimates
An observed effect size varies not only with factors such as the measurement scale
(nominal, ordinal, interval, ratio) or the design characteristics of a study but crucially with the
observed variance of behavioral responses (s ). A measure like Cohen‚Äôs d relates the mean
‚àí ‚àö 2
difference (m m ) explicitly to the observed standard deviation ( s = s) and implicitly to
1 0
‚àö ‚àí
the statistical measurement error (s / N 1) associated with m and m . In a given sample of
0 1
observations, therefore, the observed d-value depends on both (m m ) and s. So, the
1 0
‚àí ‚àí
measure of variation in a sample is never just (m m ) but always by (m m ) relative to s.
1 0 1 0
To anticipate the theoretical aspect of an empirical effect, the presence of s creates
challenges when an observed effect size shall be modeled by a valid theoretical construct
because, for reasons given below, theoretical constructs in behavioral science normally state
only (m m ), but not s. Solely relevant to evaluating the validity of a theoretical construct,
1 0
therefore, are variations of the observed d-value that point to (m m ), whereas variations
1 0
that point to s are theoretically irrelevant. Although the parameter s is in this sense
theoretically irrelevant, s can be interpreted as the implicit unit of a measurement scale, a unit
that is induced from observations. In contexts of statistical testing and description, this
interpretation is a ‚Äúblessing‚Äù because the formally permissible transformations of a
measurement scale allow for measurements that occur on different scales to be treated as if
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 8
they occurred on a single alternative scale (Baguley, 2009). This possibility counts whenever
effects that are measured on different scales shall be compared or meta-analytically
aggregated.
But this interpretation is also a ‚Äúcurse‚Äù because, as a measurement unit, the parameter
s can never be eliminated. The main reason is that the measurement scales in behavioral
science normally lack a measurement unit that is theoretically motivated (Flake & Fried,
2020), in turn forcing researchers to resort to the induced measurement unit s. An x-point
Likert scale, for instance, has an overt measurement unit of 1 / x. If x differs between two
Likert scales that measure an identical mean difference underlain by individual behavioral
responses that are normally distributed around the sample mean, one necessarily obtains two
different s-values, and thus two different d-values. As any observed d-value thus ‚Äútraces‚Äù s,
that d-value cannot state a neutral parameter.
A pair of non-neutral d-values (measured on Likert scales with different units) can
nevertheless be made comparable by setting the origin of each Likert scale to zero and the
observed standard deviations to one (s = 1), then adjusting (m m ) accordingly. Known as
1 0
standardization, this process yields a new, neutral measurement unit of s = 1. Formally, this
process changes a d-value into a standard z-value. For example, d = [(.3 .1) / .5] = .4
becomes z = [(.6 .2) / 1] = .4. While z-values are comparable on a single standardized
measurement scale, the standardization process necessarily relies on s. In the context of
statistical testing and description, therefore, the parameter s can be ‚Äúneutralized,‚Äù yet it
cannot be eliminated.
What the statistical aspect of an empirical effect thus reveals is the necessity of
(somehow) handling the parameter s. This need is readily apparent when researchers plan the
sample size for a study with sufficient statistical test power (Sect 3.1). What is more, the
parameter s has distinct consequences when an empirical effect is explained or predicted
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 9
(Sect. 3.2-3.6). These consequences are what inform the smallest effect of interest for theory
construction (Sect. 3.7, 3.8).
The theoretical aspect
A parameter for well-powered research
The upshot of the standardization process is that the neutral measurement unit s = 1
conceptually separates (m m ) from the observed s. This separation is the key to
1 0
understanding the theoretical aspect of an empirical effect, under which (m m ) can be
1 0
stated (non-inductively) as the theoretically expected difference in the average behavioral
response. While a theoretically expected difference is rarely used in behavioral science, some
such expected difference is required to calculate the sample for a study with sufficient test
power (Bakker et al., 2012; Sch√§fer & Schwartz, 2019; Stanley, Carter, & Doucouliagos,
2018).
To illustrate, if the average measurements in the experimental and control groups, i.e.,
the means m and m , are standardized to the grant mean M = 0, one retains the differences
E C
(m ‚Äì M) and (m ‚Äì M). Because M cancels out, one obtains the standardized difference
E C
measure (m ‚Äì m ). The same idea underlies the standardization of d-values into z-values:
E C
standardize the control group mean to m = 0, the experimental group mean to m = d, and the
0 1
standard deviation in both groups to s = 1. Crucially, the standardization process assumes that
z-values are standard normal curve distributed or, as an approximation, t-distributed. Only
relative to a normal distribution, therefore, can (m m ) and s be treated as conceptually
1 0
separate parameters.
The assumption of a normal distribution not only provides the warrant to interpret (m
m ) as a non-inductive parameter that states a theoretically expected mean difference
between average behavioral responses (Witte, 1996). This assumption is also what enables
researchers to plan studies with sufficient test power to register a theoretically expected (m
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 10
m ) as a statistically significant mean difference (Neyman & Pearson, 1967). Regardless of
whether a theoretically expected (m m ) derives from a theoretical reconstruction of
1 0
observations or pure speculation, a test power calculation requires a definite effect. A
theoretically expected mean difference, therefore, must be specified as a lower (l) and upper
‚àí ‚àí
(u) bound, as in l < (m m ) < u, or as a single point, as in (m m ). Of course, a point-
1 0 1 0
specific (m m ) assumes a highly informative theoretical construct‚Äîand, in behavioral
1 0
science, such constructs are rare. Absent a definite effect, however, a sufficiently powered
study simply cannot be planned (Cohen, 1977; Witte & Zenker, 2017; Krefeld-Schwalb,
Witte, Zenker, 2018).
Genuine theoretical constructs
In the case of the theoretical aspect, then, the effect size is ideally deduced from a
highly informative theoretical construct. As a rule, the content of a genuine theoretical
construct exceeds that of an inductive generalization (Witte, Stanciu, Zenker, 2022) and is
acquired by terms/functions not occurring in the observational theory used to arrive at the
inductive generalization. In behavioral science, by contrast, what normally stands proxy for a
genuine theoretical construct is an inductive generalization, namely the parameter estimate
for an observed effect. This proxy relation allows for a d-value that is induced as a parameter
estimate from observations to be treated as if it represented a d-value that is deduced from a
theoretical construct.
With any such quasi-deduced effect, what remains in play are two inductive
parameters that reflect knowledge from experience, namely that observations vary around a
mean and that empirical measurement is prone to error. In behavioral science, even a genuine
theoretical construct normally states only the mean difference (m m ) but neither the
1 0
variance (s )‚Äîcapturing how individual behavioral responses cluster around m and m ‚Äînor
0 1
the associated amount of measurement error. Therefore, as the variance is modeled by two
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 11
probability density distributions centered on m and m , which represent the logical
0 1
conjunction of the variance and the measurement error, these two inductive parameters
continue to co-determine a quasi-deduced effect.
However, what these inductive parameters ought not to co-determine is the validity of
a theoretical construct. A meaningful concept of validity simply cannot arise if repeated
measurements with a poorly calibrated instrument or the presence of uncontrolled factors that
affect behavioral responses would potentially invalidate a theoretically deduced effect, one
that repeated measurements with a well-calibrated instrument would demonstrate, in the
absence of uncontrolled factors, to adequately subsume observed behavioral responses.
Indeed, regardless of whether an effect is deduced from a genuine theoretical construct or
quasi-deduced from an inductive parameter estimate, underlying the plethora of validity-
related concepts (Boorsboom, Mellenberg, van Heerden, 2004) is the idea that a valid
theoretical construct is empirically adequate, a property that is evaluated as the proportion of
observations that sufficiently match a theoretical effect. For this property to be evaluated
properly, then, the theoretical effect must be ‚Äúsevered‚Äù from the inductive parameter s.
Explanation vs. prediction
An empirically adequate theoretical construct must not only explain an observed
effect but also predict future observations of it. Both abilities are evaluated as the proportion
of old and new observations that a theoretical construct manages to subsume. While both
abilities improve if a theoretical construct states a more definite effect, of special relevance
for the proportion of subsumed observations are the statistical error rates.
As for the definiteness of an effect, the larger the class of theoretically unmodelled
factors by which behavioral responses are potentially affected, the larger the class of possible
observations that are consistent with a theoretical construct, i.e., not ruled out by it. Citing
this class of unmodelled factors thus provides a hand-waving ‚Äúexplanation‚Äù for virtually any
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 12
non-random effect‚Äîthe suboptimum being a theoretical construct that states something as
uninformative as a (non-random) bi-directional effect. Conversely, as a theoretical construct
states a more informative (non-random) effect and thus becomes inconsistent with an ever-
larger class of possible observations, all theoretically unmodelled factors that fail to account
for the theoretically modeled effect can be excluded as potential explanations‚Äîthe optimum
being a point-specific effect. To be preferred, then, is a theoretical construct that states a more
definite effect.
As for the size of the effect, when observations are modeled as probability density
distributions, they are simply called data. If the measurement quality and the (interval scale-
based) standard error rates associated with m and m are constant, then as the effect size
1 0
continues to increase, the normal distributions under H (m = 0) and H (m = d)
0 0 1 1
continuously overlap less. (The measure of overlap is Cohen‚Äôs U (Cohen 1977), to which we
return below). So, as the probability of data according to H keeps exceeding the probability
of data according to H , a theoretical construct subsumes an ever-larger share of data. Prima
facie, then, to be preferred is a theoretical construct stating a larger effect.
While the preference for a larger effect already anticipates the task of predicting the
effect in new observations, this preference applies less strongly when an observed effect is
explained. To appreciate the difference between explaining and predicting an effect, the
relevance of the statistical error rates is best illustrated by example.
Explaining old vs. predicting new observations
As an example of nominal scale measurement, according to the orthodox
understanding of biological sex the presence of the Y-chromosome is correlated perfectly
with the male sex and its absence with the female sex. Modulo the measurement error, the Y-
chromosome‚Äôs ‚Äúeffect on sex‚Äù would thus be 100%. Empirically, however, while the average
male person is taller than the average female person‚Äîrendering the Y-chromosome strongly
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 13
correlated with body height‚Äîthe height of the average male person is nevertheless exceeded
by that of some female persons. Body height and biological sex, therefore, are after all
imperfectly correlated. For the cohort born between 1980 and 1994, this imperfect correlation
can be quantified as d = [(m ‚Äì m ) / s] = [(178.4 ‚Äì 164.7 cm) / 7.28 cm] = 1.88, or r = .69,
1 0
conventionally called a very large effect (Jelenkovic et al., 2016).
In behavioral science, very large effects are rare. One such rarity is the effect of the
best single measure of employee job performance, defined as behaviors with a positive
expected value to an organization. The degree of the linear relationship between adequate
performance on a moderately demanding job and an employee‚Äôs IQ-test score is meta-
analytically estimated as a correlation of r = .51, or d = z = 1.2 (Schmidt & Hunter, 1998). As
this and the following subsections show, providing a statistical explanation for d = z = 1.2
under conventional explanation error rates (Œ± = Œ≤ ‚â§ .05) requires only a very small sample (N
= N = 8). Whereas if d = z = 1.2 is used to predict the future job performance of individual
job applicants under symmetrical prediction error rates (Œ≥ = Œ¥), then the prediction error rates
associated with the critical z-value for d = 1.2 (i.e., critical z = .60) entail that if an
organization only hires applicants whose IQ-test score exceeds z = .60, then 27% of hired
applicants are not expected to show adequate job performance and adequate job performance
is expected of 27% of applicants not hired (Faul et al., 2007) (Fig. 1).
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 14
‚àí ‚àí
Illustration of d = z = (m m ) = 1.2; s = 1; critical z = .60; Œ≥ = Œ¥ = .27; (1 Œ¥) = .73
1 0
Note: The x-axis plots possible intelligence scores as z-values. The y-axis plots the
probability density of two normal distributions centered on m : d = 0 (solid line) and m : d =
0 1
1.2 (dashed line). The shaded areas (Œ≥, Œ¥) describe the prediction error rates given z = .60.
(Image generated using G*power (Faul et al., 2007).)
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 15
The example may make it seem as if the future job performance of individual
applicants was predicted theoretically. However, the predicted effect size is squarely based
on meta-analytically aggregated observations of how job performance correlated with IQ test
scores in the past, a correlation that is based on ‚Äúold‚Äù observations. This means the proportion
of individual job applicants ‚Äúpredicted‚Äù to (not) show adequate job performance is arrived at
by induction. So, absent a genuine theoretical construct, this ‚Äúprediction‚Äù cannot rely on a
theoretical parameter. What instead stands proxy for a theoretical parameter is the meta-
analytical parameter estimate.
For this reason, the example cannot address the question of whether an induced
parameter estimate of size d = z = 1.2, as an assumedly true effect, probabilistically confirms
a genuine theoretical construct stating a theoretically deduced parameter of size d = z = 1.2.
And that is the question when evaluating the empirical adequacy of a theoretical construct
that is used predictively. Addressing this question not only goes beyond the statistical
approaches of Fisher or Neyman Pearson (as these approaches lack a formal measure of
confirmation) but also requires new observations, because the empirical basis from which a
parameter estimate was induced cannot validly be ‚Äúrecycled‚Äù as the empirical basis to
probabilistically confirm a theoretically predicted parameter. A version of ‚Äúdouble dipping‚Äù
(Kriegeskorte, Simmons, Bellgowan, & Baker, 2009), this circular reasoning would
necessarily result in a ‚Äúconfirmation.‚Äù
Explanation vs prediction error rates
In evaluating the empirical adequacy of a theoretical construct, the use of old or new
observations marks the conceptual distinction between a statistical explanation and a
statistical prediction. In the case of a statistical explanation, the proportion of old
observations that indicate a mistaken decision to accept/reject a parameter estimate is
estimated by the Œ±- and Œ≤-error rates. Whereas in the case of a statistical prediction, the
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 16
proportion of new observations that a theoretical parameter would erroneously predict is
estimated by the Œ≥- and Œ¥-error rates. Both pairs of error rates differ qualitatively. To decrease
the explanation error rates, it suffices to increase the number of observations (N). However,
the prediction error rates can be decreased only by increasing the effect size (d).
Regardless of whether the effect size is stated as a genuine theoretical parameter or as
a parameter estimate standing proxy for it, both pairs of error rates vary with the d-value,
respectively the z-value. Unlike the Œ≥- and Œ¥-error rates, however, the Œ±- and Œ≤-error rates also
vary with N. Hence, if N is constant, then as the effect size decreases, the explanation error
rates increase, rendering the decision to accept/reject a parameter estimate more error-prone.
Conversely, as the effect size decreases, constant Œ±- and Œ≤-error rates can only be maintained
by increasing N. This explains why making N sufficiently large to meet conventional Œ±- and
Œ≤-error rates (e.g., Œ± = Œ≤ ‚â§ .05) can in principle provide a statistical explanation for even the
smallest (non-random) effect.
Things are different in the case of a statistical prediction. Like in the case of a
statistical explanation, the empirical adequacy of a theoretical construct must be evaluated
relative to non-zero error rates. Any measurement instance thus retains a non-zero probability
of being represented as an element of the H distribution (m : d = z = 0) or the H distribution
0 0 1
(m : d = z = 1.2). Unlike in the case of a statistical explanation, as the prediction error rates (Œ≥,
Œ¥) vary only with the z-value, a measurement instance is more probable to be represented as
an element of the H than the H distribution in the area z < .60, vice versa in the area z > .60,
0 1
and equally probable to be represented as an element of either distribution at z = .60. And, for
critical z = .60, we find a prediction error rate of (1 Œ¥) = .73. So, 27% of all measurement
instances are represented as elements of the other distribution.
In this example, then, about ¬º-th of individual applicants would be erroneously
predicted to adequately perform their jobs and the same error rate applies to a predicted
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 17
inadequate performance. Thus, using r = .51 (d = z = 1.2) to inform an organization‚Äôs hiring
decisions would make it expected that about ¬º-th of all hiring decisions lead to ‚Äúbad hires.‚Äù
At the same time, the organization also misses out on about ¬º-th of ‚Äúgood hires.‚Äù The meta-
analysis by Sacket et al. (2023) even arrives at a correlation of only r = .31 (d = .65) (see
Sacket et al., 2022). As the predicted effect becomes smaller, of course, the prediction error
rates become larger, and vice versa. For instance, for d = z = .80, we find Œ≥ = Œ¥ = .34 (Fig. 2).
But a prediction associated with an error rate of (1 Œ¥) = .66 would outperform the tossing of
a fair coin (50 / 100 cases) in but 16 out of 100 cases. This explains why the preference for a
larger effect anticipates the task of predicting it in new observations.
In sum, while an observed effect of d = .80 is no less of a rarity in behavioral science,
explaining it relative to the conventional error rates (e.g., Œ± = Œ≤ ‚â§ .05) merely requires
sufficiently many observations. Whereas if the same effect is to be predicted, then the amount
of theoretically unintended, yet unavoidable variation associated with d = .80 translates into a
mere 16% improvement over a chance-based prediction. An adequate statistical prediction
given Œ≥ = Œ¥ ‚â§ .05 thus demands a much larger effect or a valid reason for accepting prediction
error rates that deviate from the conventionally accepted explanation error rates. That no such
reason can be given is what motivates the symmetry thesis.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 18
‚àí ‚àí
Illustration of d = z = (m m ) = .80; s = 1; (1 Œ¥) = .66
1 0
Note: compare Fig. 1
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 19
The symmetry thesis
That the ability to explain an effect depends on N, whereas the ability to predict it
depends on the effect‚Äôs size, marks an important difference between using a theoretical
construct as an explanatory and a predictive tool. No doubt, ‚Äú[o]ur goal as scientists is not to
publish as many articles as we can, but to discover and disseminate truth‚Äù (Simmons, Nelson
& Simonsohn, 2011, 1365). In behavioral science today, the main goal appears to be the
explanation of (true) effects, ‚Äúdiscovered‚Äù using old observations as the empirical basis for a
parameter estimate. This estimate, as we saw, is technically an inductive generalization that
stands proxy for a theoretical construct that explains the average observed behavioral
response.
Arguably no less important is the ability for the average behavioral response to be
predicted in new observations. And, for this ‚Äúgame against nature‚Äù to be played fairly, the
prediction error rates for theory-based applications or interventions (Œ≥, Œ¥) must match the
proportion of data leading to a mistaken decision that a true effect is explained (Œ±, Œ≤). So, as a
principle, the prediction error rates must match the statistical explanation error rates. This
symmetry thesis has implications for the smallest effect of interest that offers a worthwhile
parameter for theory construction research.
The smallest effect of interest for theory construction
That theoretical constructs in behavioral science tend to be subject to an asymmetrical
evaluation, that focuses on explaining, reflects the goal of an inductive strategy: to explain
what is already observed. But even the largest possible effect that is assumed to be observed
in the largest possible sample, can justify using its associated parameter estimate to predict
new observations merely in the sense of pragmatic justification. Even if the predicted effect
satisfies a preferred criterion of empirical adequacy, this would provide no more than a
circular justification for considering the parameter estimate as a valid theoretical construct.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 20
While this only restates Hume‚Äôs problem of induction (Witte, Stanciu, Zenker, 2022),
it helps to understand why a genuine theoretical construct must postulate content exceeding
that of the inductive generalization that is a parameter estimate. To generate this excess
content is the task of theory construction research. In this context, the symmetry thesis‚Äî
stating that the abilities to explain and predict an effect are equally important‚Äîis part and
parcel of the Hempel-Oppenheim schema of explanation (Hempel, 1970). This schema also
applies to a prediction. According to this schema, the explanans, consisting of initial
conditions and a covering law, is what allows for the explanandum to be logically deduced.
The Hempel-Oppenheim schema thus reflects a deductive strategy for evaluating
theoretical constructs. This strategy is standardly applied in physics, for instance, where a
predicted effect to be (dis-)confirmed is deduced from a physical theory before observations
are made (Meehl, 1990a; 1990b). The importance of this strategy for a progressive version of
behavioral science is difficult to overstate. Unless researchers employ even only rudimentary
theoretical considerations to deduce a predicted effect before confronting it with data, a
probabilistic (dis-)confirmation of a predictive theoretical construct is simply out of reach.
And, without this ability, a valid theoretical construct cannot be distinguished from its invalid
counterpart.
On a deductive strategy for evaluating a theoretical construct, considering the amount
of observed variance that keeps from adequately predicting a yet smaller effect size is what
creates the need to specify the smallest effect of interest (SEOI). Indeed, ‚Äú[‚Ä¶] not specifying
a[n] SEOI for research questions at all will severely hinder theoretical progress‚Äù (Lakens,
Scheel & Isager, 2018, 267). We propose that an SEOI that offers a worthwhile parameter for
theory construction research must be observed repeatedly‚Äîthe more often the better‚Äîunder
statistical error rates of Œ± = Œ≤ ‚â§ .05 in a sample of at least N = N = 53 and must explain at
1 2
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 21
least 10% of the observed variance. This proposal translates into an SEOI of at least d = z =
.65, a compromise between a medium and a large effect.
In Rosenthal and Rubin‚Äôs (1982) case (see Sect. 1), for instance, r = .32 (d = z = .65)
would satisfy error rates of Œ± = Œ≤ ‚â§ .05 in a sample of N = N = 100 and explain 10% of the
1 2
the observed variance would be explained by r = .40 (d = z = .90), the average effect for
behavioral models based on personality attitudes (Eckes & Six, 1994). By contrast, d = z =
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 22
Select parameter values determining the smallest effect of interest (SEOI) that offers a
worthwhile parameter for theory construction
Effect size Proportion of Error rates of a
measures observed Total non-overlapping Sample statistical
(Cohen‚Äôs d, variance area of two normal size per significance test
Pearson‚Äôs r) explained distributions group (one-sided)
d = z r r¬≤ Cohen‚Äôs U N = N Œ± = Œ≤
1 1 2
.50 .24 .06 % 33% 88 .05
.65 .33 .10 % 41% 53 .05
1.00 .45 .20 % 55% 22 .05
3.3 .85 .73 % 95% 11 .05
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 23
While the proposed thresholds for the two focal criteria (the amount of explained
variance and the statistical error rates) may be found too lax, the decision to tighten these
thresholds is best left to researchers. The criteria are nevertheless the right ones to spell out a
motivated constraint on identifying observed effect sizes that offer worthwhile parameters for
theory construction. Any such constraint would not only inform theoreticians about the
observed effects that are (not) worth modeling but also allow a theoretically oriented journal
editor to decide which observed effects are (not) worth publishing. Prima facie, then,
adopting such a constraint is useful to foster theory construction research in behavioral
science.
What behavioral science studies normally observe
The motivation for the above constraint is a practical consideration: The theoretical
modeling of an observed effect that is smaller than the SEOI wastes theoretical efforts
because the magnitude of the associated prediction error rates leaves it impossible to confirm
a corresponding theoretical construct as an empirically adequate predictive construct. While
this constraint appears useful, an empirical consideration strongly suggests that applying it in
behavioral science is problematic. One part of the problem is that publications normally
report small to medium-sized observed effects (.20 ‚â§ d ‚â§ .60) (Bakker, et al., 2012). A
second part of the problem, which makes theory construction research in behavioral science
almost impossible, is that several recent meta-meta-analyses of published observed effects
suggest that observed effects here describe a specific pattern.
The specific pattern is that a meta-analytically estimated effect in behavioral science
is normally either small and homogenous or otherwise large(r) and highly heterogeneous
(Linden & H√∂nekopp, 2021; Olsson-Collentine, Wicherts & van Assen, 2020; Schauer &
Hedges, 2020). The heterogeneity of a meta-analytical effect size estimate is measured as the
degree to which (topically related) object-level observed effect size estimates vary around the
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 24
population effect size estimate. A high degree of heterogeneity indicates that the underlying
object-level estimates differ vastly in size and a low degree of heterogeneity (or a high degree
of homogeneity) indicates that the underlying object-level estimates are similar in size. This
pattern suggests that object-level effects that are clearly observed normally yield a small
meta-level effect size estimate, whereas a large(r) meta-level effect size estimate tends to be
underlain by object-level effects that are diffusely observed (Fig. 3).
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 25
Heterogeneity as a function of meta-analyses‚Äô mean effect size
Note: ‚ÄúThe funnel plot in (a) shows 150 meta-analyses from cognitive, social, and
organizational psychology; the funnel plot in (b) shows 57 meta-analyses for close
replications‚Äù (Linden & H√∂nekopp, 2021, 366, Fig. 5; Copyright: Sage Publications).
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 26
Given what meta-analytical research in behavioral science makes available, this
pattern normally keeps from identifying population effect sizes that are worthwhile
parameters for theory construction research. After all, a homogenous population effect that is
small can explain hardly more of the observed variance (r ) than is explained by a random
effect (d = 0). For any effect size d = z < .45, for instance, we find r < .05 (Cohen, 1977,
accounted for by the measurement error alone, leaving it unclear whether there is any effect
at all. Small homogenous effects, therefore, are poor parameters for theory construction.
Conversely, a large(r) but highly heterogeneous population effect injects vast uncertainty into
the process of theoretically modeling it, leaving it unclear exactly which d-value a valid
theoretical construct should model. So, large(r) but heterogeneous effects are equally poor
parameters for theory construction. What merely adds gravity is that recent critiques of meta-
analytical and replication methods strongly suggest that publication bias in behavioral science
normally causes population effect size estimates to state overestimates (Klein et al., 2018;
Sch√§fer & Schwartz, 2019).
The vast heterogeneity of (topically-related) object-level observed effects suggests
that observed behavioral responses are normally affected by moderators, the influence of
which is non-uniformly controlled across various implementations of an experimental setting
(Stanley et al., 2018; Viechtbauer, 2007). In principle, cues as to how moderators may be
controlled more uniformly are provided by a moderator analysis, allowing the observed
heterogeneity to be post-hoc explained or potentially reduced in future implementations of an
experimental setting. In praxis, however, promising moderators are what moderator analyses
normally fail to identify (Linden & H√∂nekopp, 2021). And, even if promising moderators
were identified, their effect on observed behavioral responses would be no less in need of a
theoretical model than the observed behavioral responses themselves (Bryan et al., 2021).
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 27
The upshot thus states a disheartening counterfactual: the development of a valid
theoretical construct in behavioral science normally requires a meta-analytic population effect
size estimate that is underlain by more homogeneous object-level effect size estimates or is
otherwise much larger than the kind of estimate that meta-analytical research normally can
provide. Essentially the same conclusion, here reached on formal grounds, can also be
reached on empirical grounds (McManus, Young & Sweetman, 2023). Because theoreticians
have good reason to reject an effect size estimate that is underlain by heterogeneous
observations (Sixtl, 1993), whether on the object- or the meta-level, this upshot may also
suggest why valid theoretical constructs in behavioral science are so rare: Absent large
enough and homogenous effect size estimates, valid theoretical constructs simply cannot be
developed. Rather than as an apology, this can be read simply as a potential explanation of
the status quo.
The practical aspect
The utility ascribed to a population effect
In behavioral science, as we saw, not only are meta-analytical population effects
thought to state overestimates but already the observed object-level effects remain modest in
size. In social psychology, for instance, the average observed effect size is estimated as r =
.21, or d = .42 (Richard et al., 2003), and for general psychology as r = .16, or d = .33 (Cafri,
Kromrey, & Brannick, 2010). A median observed effect of d = .60 in an original study is
even thought to shrink to d = .15 in a replication study (Klein et al., 2018). In the case of the
practical aspect of an empirical effect, then, and pace critical considerations concerning the
validity of meta-analytical methods (Grissom & Kim, 2012; Schulze, 2004; Cafri, Kromrey
& Brannick, 2010; Mitchell, 2012; Richard et al., 2003; Schmidt, Oh & Hayes, 2009), what
counts for an evaluation is the amount of utility ascribed to a meta-analytically estimated
population effect.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 28
In the context of evaluating an empirical study, utility considerations dictate to ascribe
four kinds of utility (Miller & Ulrich, 2016): utilities of a false positive finding (erroneously
rejecting H ), a true positive (correctly accepting H ), a false negative (erroneously rejecting
0 1
H ), and a true negative (correctly accepting H ). Given that an ‚Äúintervention with weak
1 0
effect size but no risks may [still] be valuable‚Äù (Ferguson, 2009, 536), the main challenge is
to avoid an evaluation that depends exclusively on these four kinds of utility.
A case in point is Greenwald et al.‚Äôs (2015) evaluation of the IAT as a practically
significant intervention (see Introduction), which pivots on a small effect benefitting a large
number of individuals. Similarly, a one percent increase in life satisfaction that results from
fine-tuning how a commercial product matches consumer preferences, may apply to no fewer
than 244 million people. Or so Matz, Gladstone & Stillwell (2017) respond to a criticism by
Boyce et al. (2017) that a similarly small effect would hardly matter, a response that exactly
mirrors how Greenwald et al. (2015) evaluated the IAT. That this kind of evaluation becomes
more readily available in the age of ‚Äúbig data,‚Äù may be one reason why behavioral science
Miller & Ulrich, 2016, 671).
Stability
Because the evaluation of an empirical effect as practically significant relies not only
on its ascribed utility but also on the associated statistical error rates, this evaluation can in
principle rely on the same assumptions as those that underlie a statistical theory. If the same
amount of positive or negative utility is ascribed to a correct or mistaken decision to
accept/reject an effect size estimate, then the ascribed utilities simply mirror the statistical
error rates. Given symmetrical error rates, therefore, the reason for evaluating treatments or
interventions with a large effect as practically significant may simply state that, if other
things are equal, greater utility is expected than in the case of a small(er) effect. In the case of
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 29
a small effect, by contrast, the statistical error rates must not only be symmetrical but also
small enough to indicate that the effect is stable.
For example, a 29% death rate among patients treated with cardiovascular drug A vs.
a 35% death rate in the case of drug B (Poole-Wilson et al., 2003) amounts to a 6% difference
in the death rate, making for less than a small effect. A rational choice between these two
treatments is driven less by the death rate difference than by the disutility of death. Given
both drugs have a non-zero effect, it is the infinite amount of negative utility that warrants
preferring the treatment with drug A. As an instance of Pascal‚Äôs wager, this preference can be
modeled as a treatment‚Äôs expected value. With ‚ÄòP‚Äô for the probability of a positive outcome
(‚Äúlife‚Äù), ‚ÄòU ‚Äô for the ascribed gain, ‚Äò(1 P)‚Äô for the probability of a negative outcome
(‚Äúdeath‚Äù), and ‚Äò U ‚Äô for the ascribed loss, the expected value (E) of treatments with the
drugs A or B is:
√ó ‚àí √ó ‚àí
E = P U + (1 P ) U
A A L A D
√ó ‚àí √ó ‚àí
E = P U + (1 P ) U (1)
B B L B D
‚àí ‚àí
As the death rates for the drugs A and B come to (1 P ) = .29 < (1 P ) = .35,
A B
‚àí ‚àí ‚àû
while U = (‚Äúnegative infinity‚Äù), the rational choice according to (1) is to opt for the
treatment associated with the lower death rate. With matters of life or death, however, an
observed death rate difference of 6% can arguably be called stable only if its associated
statistical error rates are < 1% (Œ± = Œ≤ < 0.01), or even less. Demonstrating that this difference
is stable thus requires a sample of at least N = N = 6100 (Faul et al., 2007). In any smaller
1 2
sample, the true difference is simply too uncertain.
Thus, if preferences are driven mainly by the amount of ascribed utility, then a small
(non-zero) effect that shall be evaluated as practically significant must be stable. Of course,
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 30
the smaller the effect, the more observations are needed to obtain a confidence interval that
statistically excludes the inverse effect (here: the negative version of a positive death rate
difference.) So, regardless of whether the practical significance of an empirical effect grounds
in a small or large amount of ascribed utility for one or many individuals, the larger the
effect, the fewer resources are needed to demonstrate that the effect is practically significant.
Conclusion
While we have addressed only simple hypotheses, that state a difference between two means,
it equally holds for more complex hypotheses (e.g., correlations, multiple means, interactions,
dimensional hypotheses, etc.) that a transparent evaluation of an empirical effect must
address three distinct aspects: the size of the effect (statistical aspect), the ability to explain
and predict this effect size with a (genuine or proxy) theoretical construct (theoretical
aspect), and the utility ascribed to the effect (practical aspect). In the case of a positive
evaluation, this offers an intelligible sense in which an empirical effect can be endowed with
statistical, theoretical, or practical significance.
Keeping these three aspects apart clarifies evaluative complexities that tend to be
‚Äúhidden‚Äù when effect size measures like Cohen‚Äôs d are used to evaluate empirical effects as
small, medium, and large (d = .20, .50, .80). These three categories can be are meaningfully
related only to the statistical explanation error rates (Œ±, Œ≤), themselves integral to a test for
statistical significance. In the case of an explanation, the amount of uncertainty that is
incurred if a theoretical construct subsumes observations squarely depends on the sample
size. In the case of a prediction, by contrast, the prediction error rates (Œ≥, Œ¥)‚Äîrepresented by
the non-overlapping area of two data distributions‚Äîlet the amount of uncertainty depend on
the effect size. For this reason, the smallest effect of interest for an empirically adequate
prediction was seen to be (much) larger than that for an empirically adequate explanation.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 31
Given the right utility assignment, a small statistically significant (non-zero) effect
can nevertheless be practically significant, provided it is stable (i.e., is observed in large
enough samples). On this stability condition, a practically significant empirical effect can
vary freely in size, rendering the practical and statistical aspects mutually independent. Yet,
as the homogeneous share of observed effects in behavioral science comprises small effects
(d = .20), what a small (non-zero) empirical effect normally lacks, even if it is statistically
significant and stable, is theoretical significance.
Just as a statistical estimation of an observed effect size is distinct from a predictive
application of a theoretical construct, explaining or predicting the average behavioral
response (i.e., the observed mean difference) is distinct from explaining or predicting
individual behavioral responses. In behavioral science today, a theoretical construct normally
can adequately explain, yet cannot adequately predict observations because the amount of
observed variance keeps a theoretician from modeling individual behavioral responses.
Modeled instead is the average behavioral response, as affected by various theoretically non-
modelled factors. That non-modeled factors are always at play is the main reason for
insisting, as we did, that an empirical effect in behavioral science should currently be
evaluated by the mean difference alone, rather than also by the observed variance.
STATISTICAL, THEORETICAL, AND PRACTICAL SIGNIFICANCE 32