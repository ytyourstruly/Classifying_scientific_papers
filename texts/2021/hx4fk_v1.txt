The t-test, resource pooling, and psi: why the minimum sample size counts
towards theoretical progress in behavioral science
1 2*
Erich H. Witte & Frank Zenker
University of Hamburg, Institute for Psychology, Von-Melle-Park 5, 20146
Hamburg, Germany; ORCID: 0000-0002-9446-3211
Boğaziçi University, Department of Philosophy, 34342 Bebek, Istanbul, Turkey;
ORCID: 0000-0001-7173-7964; corresponding author: fzenker@gmail.com
Abstract: A theoretical construct that subsumes an empirical phenomenon should
rest on statistically significant test-results with high replication probably. To
statistically establish such test-results, behavioral science publications typically rely
on a t-test, and researchers typically operate under limited data collection resources.
To publish more test-results, or to publish an individual result sooner, questionable
strategies are commonly used to reduce data collection cost. One strategy is to
increase the β-error rate from 0.05 to 0.20; a second strategy is to treat the control
group as a constant, collapsing a two sample t-test into a one sample t-test. Both
strategies happen to underlie Bem’s (2011) rightly controversial results on human
precognition (“psi hypothesis” [Journal of Personality and Social Psychology, 100,
3, 407-425]). Since both strategies undermine theoretical research in behavioral
science, their ubiquity partially explains the rarity of well-corroborated theoretical
constructs there. We advocate collaboration between individual labs as a viable
route to enabling theoretical research by collecting a large enough sample jointly.
Keywords: Fisher; Neyman-Pearson; psi research; power; t-test, theory construction
The t-test, resource pooling, and psi: why the minimum sample size counts
towards theoretical progress in behavioral science
Erich H. Witte & Frank Zenker
1. Introduction
In behavioral science, the probably most frequently used statistical test to compare
the independent means in a controlled experimental study is the t-test. To reduce the
minimum sample size (N ) for a t-test, two strategies are commonly applied to
MIN
save data collection cost. The first strategy lowers test-power from (1−β)=0.95 to
(1−β)=0.80, lowering the probability that the original test-result is independently
replicated. The second strategy collapses a Neyman Pearson-test into a Fisher-test,
making test-results useless to develop an empirically adequate theoretical construct
that is more informative than a non-random effect. Both strategies happen to
underlie Bem’s (2011) rightly controversial and misleading results on human
precognition (“psi hypothesis”).
Researchers seemingly deploy both strategies to publish test-results although
a study’s actual sample (N) fails to meet N , itself an element of Neyman-Pearson
MIN
test-theory (NPTT) (Neyman & Pearson, 1967). We argue that, in order to develop a
theoretical construct, a theoretician needs well-powered test-results, (1−β)≥0.95.
Conversely, spending less than what NPTT identifies as necessary data collection
cost is guaranteed to undermine theoretical efforts. This claim extends beyond a t-
test, to NPTT-based tests that compare multiple means, variances, correlations, or
frequencies (Cohen, 1977; ²1988).
That both strategies are normal in behavioral science today goes along with
research here focusing not on the development of theoretical constructs, but on the
discovery of non-random test-results in individual studies. This focus partially
explains the replication crisis in behavioral science, whose root cause is the inability
to independently repeat a non-random test-result (Pashler & Wagenmakers, 2012;
Klein et al., 2018; Camerer et al., 2018). To support the development of informative
theoretical constructs (more precise than a two-sided non-random hypothesis or even
a one-sided directional non-random hypothesis), we advocate that behavioral
scientists should bear the cost of data collection collectively.
2. Induction vs. corroboration
With rare exception, behavioral science today regularly operates with small samples.
In psychology, for instance, the typical sample size is N<30 (Bakker, van Dijk &
Wicherts, 2012). But a “small N-study” can discover a statistically significant effect,
only if the empirically observed effect is large. Yet, behavioral science studies
typically find small to medium size empirical effects, the actual N regularly being
much smaller than N . This implies that published statistically significant test-
MIN
results tend to be underpowered, leaving their independent replication improbable.
A statistically significant but underpowered test-result generally is a poor
instance of an effect that a theoretician would seek to subsume under a theoretical
construct. If a good theory predicts a true effect, then a statistically significant but
underpowered effect is what a theoretician should ignore. Conversely, only well-
powered statistically significant test-result warrant the effort of developing a
theoretical construct (Witte & Zenker, 2018). Therefore, if N<N , then researchers
MIN
avoid what from a theoretical point of view are necessary data collection cost.
To fully appreciate this point, it helps to contrast induction with deduction as
two central modes of statistical reasoning. Researchers invariably collect data to
induce an empirically observed effect size, often estimating it as Cohen’s d, the
difference between the means in experimental and control group, divided by the
standard deviation, d=(m −m )/s, where 0≤|d|≤. By convention, d=0.01 is
1 2
considered a very small effect, d=0.20 is small, d=0.50 medium, d=0.80 large,
d=1.20 very large; and some call d=2.0 huge. Crucially, inducing d in ways a
theoretician can take seriously requires that empirical data satisfy two criteria:
(C1) Statistical significance criterion
Preacher and Kelley (2012) report that d is probably the most widely used effect
size measure today, while Schaefer et al. (2019) report that it probably is r. Another
common measure is Hedges’ g. Regardless which measure is most common, one can
safely transform d into r or g, if experimental and control groups have similar base
rates, because this maximizes a binary variable’s variance (McGrath & Meyer,
2006). Compared to d, only extremely diverging base rates (e.g., 0.95 vs. 0.05)
would reduce r artificially. For other transformations, see Schulze (2004, 30ff).
On the assumption that the (random) H , itself postulating d=0, is the
best-fitting description of data, D, the observed empirical effect, d=x, is
sufficiently improbable (p), where the statistical significance threshold
is conventionally set to 0.05. In form: p(D,H )<0.05.
0(d=0)
(C2) Replicability criterion
Assuming that H is the true value, replicating the point-specified
1(d=x)
effect d=x in new data is sufficiently probable, i.e., p(D,H )>(1−β),
where the conventional replication threshold is set to (1−β)=0.80.
Since ‘p(D,H )’ denotes neither a conditional nor a joint probability, it
conveys a meaning that is distinct from p(D|H ) or p(D&H ). If d is transformed into
x x
a statistic, e.g., a t-value, then p(D,H ) denotes the probability of observing this
statistic, or more extreme values of it. As a natural language expression for p(D,H ),
this suggests: ‘the probability of D in view of H ’.
Fisher’s (1956) approach to statistical inference had merely introduce the p-
value, which suffices to estimate p(D,H ). The p-value thus succeeds in addressing
C1. But Fisher’s approach ignores H , and so fails to address C2. Fisher’s approach
therefore offers neither an effect size measure, nor a replication probability measure
(Mayo, 1996).
These two measures were only introduced in NPTT, which estimates p(D,H )
and p(D,H ) by drawing on:
(i) the α-error rate (aka p-value): the probability of mistakenly accepting that
observed data deviate significantly from H (false positive error rate);
(ii) the β-error rate: the probability of mistakenly accepting that observed
data deviate significantly from H in the direction of H (false negative
1 0
error rate); respectively the derived magnitude (1−β-error), i.e., the
probability that data replicate if H is true (aka. test-power);
(iii) the effect size, d: the theoretically predicted or the empirically observed
(non-random) causal effect, or correlation, between independent and
dependent variables;
(iv) the minimum sample size, N , given both d and the α- and β-error rates.
MIN
While origins, uses, and interpretations of the p-value and the α-error rate
differ, and although both concepts are never equivalent, the p-value nevertheless
relates to the α-error rate. In fact, both concepts can be combined to arrive at a
binary decision. Originating in NPTT, the α-error rate represents the theoretical
long-run probability of committing a Type I-error (false positive error rate). The α-
error rate thus serves as a decision criterion when choosing between the two sides of
a simple hypothesis. NPTT requires two such hypotheses, at least one of which, the
H , differs from a data-description because, in NPTT, the H derives from a theory.
1 1
Originating in Fisher’s approach, by contrast, the p-value represents the
empirical probability of observing data at least as extreme as data in fact sampled, if
H is true. The p-value thus describes an empirical test-result in view of a single
hypothesis, typically a random version of the H . Since Fisher’s approach offers
merely H and the p-value, but no H -hypothesis, his approach can only measure
0 1
whether data deviate from a random distribution (H ). The p-value thus is useful to
decide that something non-random has occurred, yet without implying a substantial
hypothesis as to what has occurred. Given these constraints, the commonly used t-
test with two error-probabilities (α, β) is therefore perfectly unintended in Fisher
statistics; it becomes possible only under the constraints of NPTT.
A researcher seeking to decide whether, given data, one should pursue the H
or the H would consider the probability of a mistaken decision. The p-value bases
this probability on data (objective), while the α-error-rate bases that probability on
the researcher’s personal expected error-rate (subjective). Since the decision should
be based on data, a researcher would typically set the expected personal error-rate no
higher than the p-value serving as the statistical significance criterion. In practice,
therefore, the differences between both concepts are effectively “hidden.”
Thus, although the p-value and the α-error rate are non-equivalent concepts,
one can meaningfully treat the empirically observed p-value as if it were the
theoretical α-error, and thus arrive at an empirically-based binary decision whether
to accept or reject a hypothesis. Moreover, since NPTT has only three degrees of
freedom, fixing any three of α, β, d and N lets the fourth magnitude “fall out.”
MIN
Hence, given specified α- and β-error rates, as well as a point-specific value for d
motivated by theoretical reasoning, NPTT deduces N . This feature recommends
MIN
NPTT as a planning tool to estimate N , and thus determine necessary data
MIN
collection cost, using an a priori test-power analysis (Cohen, 1977; 1988²) as
follows:
N =2[z + z ]² / d² (1)
MIN (1−α) (1−β)
Here, z is the value of d for which the area under a standard normal
distribution has a probability density that corresponds to the α- and β-error rates.
Given α=β=0.05 and d=0.50, for instance, N =2[1.65+1.65]²/0.50²=87.12, or
MIN
approximately N =88.
MIN
Interpreted as normative magnitudes, the α- and β-error rates state error-rates
that researchers have determined by convention. The effect size, by contrast, can be
determined in but one of three ways: (i) again by convention (e.g., following Cohen,
as very small, small, medium, or large), (ii) by induction, as an observed d-value, or
(iii) by deduction based on theoretical considerations, as a theoretical d-value. Thus,
if data collection cost are calculated before data collection starts, then N is the
MIN
only material resource a researcher has to determine.
Since the induction vs. deduction-distinction reflects the basic purpose of
research, it also constrains the shape of the appropriate formal support-measure. It is
by induction that the probability measure p(D,H ) estimates the probability of an
observed test-result in view of the data-distribution postulated by H . Here, ‘D’
denotes an interval comprising empirically observed data, as well as more extreme
possible data. To corroborate a hypothesis, by contrast, is to measure the extent to
which D—now interpreted as theory-deduced point-statistic—supports H or H . As
1 0
a distinct measure, corroboration requires the likelihood ratio (LR):
L(H |D)/L(H |D)=p(H )×p(D,H )/p(H )×p(D,H ), a likelihood being a probability
1 0 1 1 0 0
multiplied by a positive constant (Edwards, Lindman & Savage, 1963; Edwards,
1972). (Decisive reason against using the Bayes factor as a corroboration measure
are given in Witte and Zenker (2017b), and Krefeld-Schwalb, Zenker, and Witte
(2018).)
Unlike in the inductive case, ‘D’ now denotes a point-specified observation,
for instance the point-difference between the expected mean effect in experimental
and control group. In view of criteria C1 and C2, then, only data that are statistically
significant and well-powered can offer sufficient empirical support to corroborate a
theoretical hypothesis. In behavioral science, however, researchers show little
interest in inducing effect sizes that enable a statistical hypothesis corroboration. In
fact, a recent survey of some 200 meta-analyses based on almost 8,000 publications
in psychology journals reports median test-power of (1−β)=0.36 (Stanley, Carter &
Doucouliagos, 2018). This analysis also notes that only some 8% (!) of studies here
achieve (1−β)=0.80. Bakker et al. (2012) report similar results.
The main telos of research thus appears to be the publication of statistically
significant test-results under minimal data collection cost, results that address C1 but
fail to address C2. That this must play out negatively for theory construction is what
we now show in the context of a t-test.
3. The t-test
To plan a proper control group study, where N≥N , one must stipulate not only α,
MIN
β, and d, but also two data distributions. For a small sample (N<30) and an estimated
standard deviation, the distributions are known as t-distributions (Fig. 1). Although a
t-distribution can only approximates observed data, statistical inferences are possible
by acting as if observed data mirrored a normally distributed variable with a
corresponding small sample standard deviation, s. For example, we may hypothesize
d =0 as the mean value in the control group, and d =0.50 in the experimental
H0 H1
group. When transforming d into the t-distribution’s non-centrality parameter
H1
=d×(N/2), and provided the sample size is the same in both groups (N =N ), we
1 2
find that  depends only on N. Given N =N =88, it thus follows that d =0 yields
1 2 H0
=d×(N/2)=0, whereas the largest probability density for d is found at the point
H1
=3.32 (Fig. 1.) So, beyond the ordinate t=1.65 (aka ‘critical t’), the probability
density for H exceeds that for H . For any observed data point exceeding critical t,
1 0
therefore, the probability of falsely rejecting H is smaller than α=0.05.
2 If N ≠N , a useful sample must be neither too small, nor too large. Compared to a
1 2
t-test where N =N , the harmonic, geometric, and arithmetic mean deliver an ever-
1 2
increasing value, and thus estimate the actual sample size ever less conservatively.
Fig. 1 One-sided, two sample t-test for d=0.50 given N =N =88 and
1 2
α=β=0.05. The hypothetical H -distribution (solid line) is centered on
=0; the observed H -distribution (dashed line) is centered on =3.32.
The abscissa (x-axis) states the t-value given N; the ordinate (y-axis)
states the probability density for a given t-value. Up to critical
t=1.65366, each shaded area (α, β) covers 5% of the other distribution.
2007).
To ensure that a study meets N , computer software readily facilitates an a
MIN
priori test-power analysis (e.g., G*Power; Faul, Erdfelder, Lang & Buchner, 2007).
But if median test-power is in fact no larger than about (1−β)=0.36 (Stanley, Carter
& Doucouliagos, 2018), then the majority of published behavioral science studies
that report statistically significant results would have determined N either at
liberty—e.g., by stopping data collection once a test-result is statistically significant,
which Bayarri et al. (2016) rightly call “cheating”—or would have determined N
strictly according to available resources (e.g., the number of students in a class).
This suggests that the aim of publishing an empirical test-result regularly
dominates the aim of securing sufficient test-power. Of course, if a researcher
focuses only on the observed effect in the first place, but ignores the theoretically
expected effect, then test-power cannot be computed other than post-hoc. Absent any
theoretical considerations, however, Bakan’s (1966) well-known objection applies.
Given N60.000, even a very small effect can become statistically significant by
splitting the sample randomly. So, absent theoretical considerations, statistical
significance is guaranteed to be a trivial criterion once N=60.000.
Irrespective of the specific effect size a statistically significant study reports,
to the extent that the α- and β-error rates both increase, the effect size becomes ever
less useful for constructing a theory that predicts a true effect. Ceteris paribus, if
H were true, then as the β-error-rate increases, the probability that a statistically
1(d=x)
significant effect replicates decreases. Conversely, ceteris paribus, if α=β, then d=x
becomes more useful for the same purpose as N increases, because both error-rates
decrease. (Formally, decreasing the error rates increases the non-centrality parameter
δ, which reduces the t-distributions’ overlap in Fig. 1.) So, if α=β, then already in
case a prediction postulates nothing stronger than a directional deviation from
random, a statistically significant effect’s use-value for theory construction stands or
falls with the β-error-rate.
Cohen (1977; ²1988) had recommended asymmetric error rates (α≠β), viz.
α=0.05, β=0.20. He stated that, given α=0.05 as the antecedently accepted error rate,
“[w]hen the investigator has no other basis for setting the desired power value, the
value 0.80 is used” (Cohen, 1977, 56). Rather than motivating the value of 0.80
independently, however, Cohen accepted—without further argument—that it is
more important to avoid false positive test-results than to avoid false negative ones
(ibid.). Despite being text-book example of circular reasoning, his recommendation
became so entrenched that α=0.05 and β=0.20 are widely accepted today as the
“right” error-rates.
Asymmetric error rates imply that researchers weigh the risk of falsely
accepting that an effect is statistically significant (α) against the risk of failing to
replicate a true non-random effect (β) (Witte, 1994; Witte & Kaufman, 1997). Given
N =N =50 (N =100), for instance, if d=0.50, then α=0.05 one-sided yields
1 2 TOTAL
(1−β)=0.80, whereas α=0.05 two-sided yields (1−β)=0.70, and α=0.01 one-sided
even only yields (1−β)=0.56, i.e., close to chance level. Yet, to meaningfully
develop a theoretical construct, a theoretician needs error rates of α=β<0.05, or
lower. What else should a theoretical construct subsume if not the true effect?
Worse yet, researchers in fact pay lip service to Cohen’s recommendation,
while undermining it regularly. As publishing such test-results renders them of very
limited use for theory development, we proceed to critique two common strategies
meant to reduce necessary data collection cost. That both strategies happen to
underlie Bem’s (2011) results on human precognition (aka “psi-effect”) entails
strong reasons to doubt these results.
4. Two cost reduction strategies
4.1 From N =176 to N =27
MIN MIN
Since the behavioral science literature typically reports small to medium sized
effects (Cafri, Kromrey, & Brannick, 2010), and given theoretically motivated error-
rates of α=β<0.05, collecting all of N can quickly generate massive data
MIN
MIN
test as a function of test-power (1–β) and effect size, d, given α=0.05
0.01 0.20 0.50 0.80
(1−β)
very small small medium large
0.40 38726 97 15 6
0.50 54111 135 22 8
0.80 123651 309 49 19
0.95 216443 541 88 34
Necessary data collection cost thus are what many researchers today appear
to shun. For test-results in empirical psychology, for instance, the largest behavioral
science field, meta-analyses have consistently estimated median test-power as
0.35≤(1−β)≤0.50 (Cohen, 1962; Bakker, van Dijk & Wicherts, 2012; Stanley et al.,
2018). Given publication bias, moreover, the behavioral science literature typically
underrepresents the actual proportion of statistically non-significant results (Cafri,
Kromrey, & Brannick, 2010; Francis, 2012; Sedlmeier & Gigerenzer, 1989).
Although preregistration can eliminate this bias, even the recent well-powered
preregistered replication studies could replicate the original effect size in only 25%
of all cases (Klein et al., 2018; Schäfer & Schwartz, 2019).
To understand how underpowered test-results arise, let us critically consider
two strategies that seemingly help researchers “save” up to 85% of necessary data
collection cost. We already saw that a one-sided two sample t-test for a medium
effect (d=0.50) under α=β=0.05 requires N =N =N =88 (N =176). The first
MIN 1 2 TOTAL
strategy maintains α=0.05 but increases β=0.05—the β-error value that theoretically
considerations warrant—by a factor of 4, to β=0.20. This reduces N by some
MIN
42% to N =N =51 (1020.58×176). Thus, rather than obtaining only one in twenty
1 2
false positive statistically significant test-results that probably fail to replicate, one
obtains one in five such results.
Underlying the second strategy is the assumption that random influences
among data cancel out mutually. A given causal influence or correlation of size x in
one direction would thus be matched by a corresponding influence of size x in the
opposite direction. This assumption underlies Bem’s (2011) psi-effect research (aka
precognition), allowing Bem to treat the hypothesis his experiment shall falsify, the
H , as postulating the constant zero, or when expressed as a binary variable as
postulating a 50% chance of a fair coin landing heads up. Experimental participants’
average empirical reactions that fail to display a psi-effect are thus assumed to
describe a random distribution.
This assumption, or so the following quote shows, allows Bem to avoid an
actual (empirical) control group, and thus allows him to deploy a one sample t-test
against a theoretically postulated constant random effect in an imaginary control
group (Bem et al., 2011; Bem, Tressoldi, Rabeyron, & Duggan, 2016).
“I set 100 as the minimum number [aka N ] of participants/sessions
MIN
for each of the experiments reported in this article because most effect
sizes (d ) reported in the psi literature range between [d =]0.20 and
emp emp
0.30. If [one sets the theoretical effect size to the point-value] d =0.25
theo
and [the sample to] N=100, the power to detect an effect as significant at
[α=]0.05 by a one-tail, one sample t-test is [(1−β)=]0.80.” (Bem, 2011,
409, fn. 1, notation adapted)
As a consequence, data collection cost are massively reduced. A (one-sided)
two sample t-test given d=0.25, α=0.05, and (1−β)=0.80, requires N +N =N =398.
1 2 MIN
Yet, given the same values of d, α, and β, a (one-sided) one sample t-test only
requires the N =100 that Bem mentions. And, a one sample t-test given a
MIN
medium-sized effect of d =0.50, α=0.05, and (1−β)=0.80 even only requires
theo
N2=N =27, close to the median sample size of published studies in psychology,
MIN
N=24 (Wetzels et al., 2011). This makes a “median-study” with N =24 nearly
MIN
equivalent to a one sample t-test given α=0.05, (1−β)=0.80, and d=0.50 (Fig. 2). If
data collection cost are a linear function of N , which often is safe to assume, then
MIN
compared to N =398, this saves some 75% of cost (398×0.25100).
MIN
Fig. 2 One sample t-test for the difference from a constant of zero,
given α=0.05, β=0.20, d=0.50, N=27, t=1.70562. Despite the term
‘constant’, one must employ two distributions rather than one to
calculate test-power, including a second, theoretically predicted
distribution centered on a theoretically motivated constant. (Compare
labels in Fig. 1.)
The successive application of both strategies—increasing the β-error-rate
from 0.05 to 0.20; treating the control group as a constant—thus results in 25% of
N . But it risks masking Bem’s own inference strategy. By citing α, β, d and N ,
MIN MIN
after all, Bem presents the inference strategy as if it were based on NPTT. Empirical
measurements, however, occur only in the experimental group. (There is no actual
control group.) Bem’s point-H thus effectively predicts his own empirical setting’s
unknown influences. Such influences, however, cannot be measured systematically
because they arise from factors besides the focal experimental effect. In fact, the
control-group strategy was developed precisely because uncontrollable influences
cannot be controlled. So, a genuine NPTT-test must use an actual control group.
4.2 One- vs. two sample t-tests
Unlike the theoretically expected effect size, only the empirically observed effect
size can describe a specific experimental condition’s sample size, as well as that
sample’s specific standard deviation. In a two sample t-test, as we saw, the d-value
reports the difference between the independent mean effect sizes in the experimental
group and control group, m −m , divided by the standard deviation, s. Since
1 2
independent samples correlate to degree zero (r =0), one can derive s by adding the
square root of the variances from each sample: s=s , where s² =s² +s² –
(x1−x2) (x1−x2) (x1) (x2)
2×r ×s ×s . So, if s² =s² , then d=(m −m )/√2s. By normalizing the standard
12 (x1) (x2) (x1) (x2) 1 2
deviation to s=1, one obtains d=(m −m )/√2=(m −m )/1.41.
1 2 1 2
Compared to a two sample t-test, then, if only a single sample’s standard
deviation is used to define the effect size, then it is by the value 1.41 that the
standard deviation of a one sample t-test necessarily decreases. After all, the t-test’s
one sample variant sheds m , retaining only m −c, the difference between the mean
1 2
of the experimental group’s data distribution and a constant random effect, c. Yet,
the constant random effect c is a point-specified effect, rather than a distributed
effect. To a constant effect, however, one cannot meaningfully associate a standard
deviation. In d*=(m −c)/s , therefore, the independently estimated value of
2 (x1−x2)
s must be lower than in d=(m −m )/s .
(x1−x2) 1 2 (x1−x2)
As explained above, the observed effect size is an inductive concept, while
the theoretically expected effect size is a deductive concept. Test-power, however, is
meaningfully associated only to the latter concept, serving to plan experiments with
N . Based on NPTT, such planning necessitates associating two hypotheses and
MIN
two error-distributions with each theoretically postulated mean. Reducing this to one
error-distribution implicitly increases d, because the inference is now based on only
one distributions, where α- and β-error are undefined (see Figs. 1 and 2). In brief, if
a statistical inference shall be based on NPTT (featuring defined α- and β-errors),
then the numerator will be increased, because two distributions with an estimated
standard deviation, s, are needed. In all other cases, the inference simply is not
“according to the art.”
Specifically, d relates to the t-value as: t=d/√2·s. A one sample t-test’s t-
value is therefore more sensitive to empirical data than a two sample t-test, namely
by the factor 1.41, above. For instance, d=0.50 given two samples would increase to
d*=0.705 given one sample. Since test-power depends on d, it follows that, if one
holds N constant, then a one sample t-test has greater test-power than a two sample
t-test. Put differently, if test-power is constant, then N decreases. This explains
MIN
why a one sample t-test incurs lower data collection cost than a two sample t-test.
According to Cohen (1977), the shift from two samples to one is mandatory.
The increase in effect size between d and d* shall “compensate for [his own test-
power] tables’ assumption of double error variance” (ibid., 46). Cohen motivates
this assumption by having simplified s to s . The constant c itself, Cohen
(x1−x2) (x2)
“conceive[s] as the mean of a normal distribution whose standard deviation is also
s” (ibid., 46). Thus, “values of power are sought in the power tables by means of
d*×√2” (ibid., 47; notation adapted). We consider this an illegitimate transformation
of a Fisher-test into an NPTT-test, which must feature two error-prone hypotheses.
4.3 A one sample t-test is a Fisher-test
Having assumed a medium effect (d=0.50), we saw how the second strategy lowers
N for a two sample t-test by some 85% (186×0.15=27), while incurring the
MIN
assumption that H can be modeled without an actual control group, i.e., without an
empirical data distribution. This assumption is problematic, because if one now fails
to obtain d as the difference between two means of a statistical t-distribution, and so
fails to obtain a test-power value. Yet, without a test-power value, one cannot deploy
a genuine NPTT t-test. One has rather altogether abandoned this statistical concept.
For both the experimental and the control group, moreover, the standard
deviations are normally established empirically, from data. A theory, by contrast,
predicts d as a point-specific difference between means. But a reasonable
theoretician would not go as far as to predict the exclusively empirical magnitude
that a point-specific standard deviation is.
Even if the standard deviation in the experimental group is identical to that in
the control group, there remains a non-zero probability that the experimental group’s
t-value (H =d) is an element of the t-distribution centered on H =c (Figs. 1 and 2).
1 0
Call this probability the p-value. But to properly deploy an NPTT-test, the constant c
fails to represent H properly. (One would additionally have to center a random
distribution on c; otherwise the β-error is undefined.) Since Bem’s t-test uses the
improper representation of the H that a “distributed constant” c is, his t-test treats
experimental group data as if they originated from the random distribution
representing the group mean, c. The test-result, however, is meant to report the
probability to which data deviate statistically significantly from the distribution
placed around the constant c. But Bem’s t-test can evaluate data only in view of
H =c. The empirical decision criterion, therefore, is not the α-error-rate, as defined
in NPTT, but the empirically derived p-value.
This entails that Bem deploys not an NPTT-test, but a Fisher-test. After all,
the attempt to interpret a one sample t-test as an NPPT-test would incur the
unacceptable assumption that the control group’s mean can be specified as the
theoretical point-value c, without any variation. But point-specification is unrealistic
because any experimental condition is subject to uncontrolled empirical influences.
That, indeed, is why researchers should use an actual control group, to which
participants are randomly assigned vis-à-vis an experimental group.
As a Fisher-test, a one sample t-test requires three values: the random
distribution around c, the observed mean of the experimental group, m , and the
estimated standard deviation placed around c, the latter obtained from the observed
values of the experimental group, s . The standardized mean difference-measure
(x2)
thus is d=m −c/s . Transformed into a statistic, it becomes t=d ×√N/2. So, if
2 (x2) emp
d*=0.50, then given N=27, one finds t=1.84. Under a random distribution, therefore,
if t=1.84, then a mere 3% of all t-values are more extreme than the empirically
observed t-value. But nowhere is there a H -hypothesis, and thus no test-power. All
Bem’s t-test allows to obtain is the p-value that, given a random distribution,
potential empirical test-results are the same, or yet more extreme than, an observed
result. This is all that a one sample t-test reports.
Starting with an NPTT-test under conventionally accepted error rates, but
then to reducing N to 15% while continuing to present it as a NPTT t-test under
MIN
constant error rates, appears highly questionable. Given N =N =27 (N =54),
1 2 TOTAL
α=0.05, and d=0.50, even a proper two (independent) sample t-test would at most
have test-power of (1−β)=0.57. Close to chance, this nearly implies speculative
decision-making. At any rate, already Cohen’s own (lenient) standard (1−β)=0.80
entails N =N =N =51 (N =102).
MIN 1 2 TOTAL
4.4 Double testing
A proper t-test for d=0.50 (one-sided) given α=β=0.05 thus needs N =N =N =88
MIN 1 2
(N =176). Given the typical effect size range of published observed effects in
TOTAL
behavioral science (0.10<d<0.50), as the d-value shrinks, N quickly reaches
MIN
beyond the data collection resources that individual research groups commands.
This suggests that resource restrictions are a major reason for N to stay below N ,
MIN
while letting a t-test seem a genuine NPTT-test (with conventionally accepted error
rates).
Even if resource restrictions keep researchers from collecting all of N , the
MIN
one strategy that NPTT offers to legitimately reduce N is double testing. ‘Double
MIN
testing’ refers to testing the same participants in random order, in both the control
and the experimental group condition, and then calculating for each subject the
difference between both conditions (repeated measurement design). Double testing
obviously creates a dependent sample. And, compared to how the measurement error
spreads in two independent samples (N =176), one can arguably reduce s
TOTAL (x1−x2)
legitimately—and thereby increase the effect size—only if participants’ responses in
both groups are sufficiently correlated, e.g., to degree r =0.50.
Notice that the following holds: s² =s² +s² –2×r ×s ×s . Given
(x1−x2) (x1) (x2) 12 (x1) (x2)
independent samples (which correlate to degree r =0), therefore, the product
2×r ×s ×s goes to zero. Generally, as the standard deviation of two groups,
12 (x1) (x2)
s , converges to that of a single group, s , the variance of the mean of the
(x1−x2) (x)
differences between both groups continuously grows smaller. If s =s =1, for
x1 x2
instance, then given r =0.50, one finds s² =s² =1. So, on formal grounds, the
12 (x1−x2) (x1)
study of experimental participants’ individual differences in behavior using two
groups makes an empirical test-condition statistically comparatively more sensitive
that when using but one group. This because the formal model is now less error-
prone.
Whether the psychological processes in focus are thought to be identical or
even influenced by double testing, any reduction of N using double testing thus
MIN
requires the necessary assumption that the value of r is positive. (Absent contrary
evidence, a compromise between not knowing the true correlation and one’s
resource restrictions may be the initial assumption that r =0.50.) Compared to a two
independent sample t-test, lowering the error variance by means of double testing
increases the effect size, and thereby lowers N , and so decreases data collection
MIN
cost. Given α=β=0.05, for instance, a matched pairs t-test for d=0.50 with dependent
samples requires N =N =N =45. Deploying the test twice yields 90 data points yet
MIN 1 2
requires only 45 participants. This legitimate way of reducing the minimum sample
thus lowers N from 176 to 45, or to some 26%.
MIN
On this background, we turn to resource pooling as a way for labs to reach
N jointly.
MIN
5. Resource pooling
5.1 Resource pooling is possible and meaningful
A literature review would show the ubiquity of both strategies critiqued in Sect 5.
The strategies’ negative consequences for theoretical knowledge, of course, may be
widely unknown. Alternatively, these negative consequences, though known, may
for the following reasons seem acceptable.
The first reason is practical, citing a research group’s lack of sufficient
resources to individually meet N , as well as obstacles encountered when seeking
MIN
to increase N to N by pooling several labs’ resources (e.g., lack of experience,
MIN
infrastructure, or peer-support). The second reason is motivational, stating that
statistically significant, well-powered test-results are a goal that actual resource
restrictions simply make unachievable. The third reason cites the impetus to avoid
the hypocrisy of desiring what one should want (significant, well-powered test-
results), but because of insufficient resources also cannot want.
The cogency of the third reason stands or falls with that of the second reason.
As for that second reason, since no law of nature keeps from coordinating resources,
several labs can in principle perform the same study under similar conditions to
reach N jointly. They also should, because even underpowered test-results
MIN
See Landy et al. (2020), who report the replication efforts of 15 research teams
investigating five social-psychological hypotheses with more than 15.000
experimental participants. They show that the typically large unexplained
heterogeneity in results plausibly relates to researchers’ subjective choices in
designing experimental stimuli. The authors observe, “one way to reduce the role of
subjective researcher choices in research outcomes may be to more fully flesh out
the underlying theory at the outset” (ibid., 22).
remain theoretically useful to corroborate/falsify a hypothesis if one aggregates their
likelihood ratios. Simulation results show that, if the corroboration threshold is
sensitive to both error-rates—as holds for LR >(1−β)/α (Wald, 1947)—then log-
H1/H0
likelihood addition combines underpowered test-results safely (Krefeld-Schwalb,
Witte & Zenker, 2018). For exemplary analyses of combined test-results, see Witte
and Zenker (2016) on macro-social stress (Chou, Parmar & Galinsky, 2016) and
Witte and Zenker (2017a) on ego-depletion (Hagger & Chatzisarantis, 2016).
Related proposals on how to limit the publication of underpowered data include
Nosek and Bar-Anan (2012), and Nosek, Spies and Motyl (2012). Resource pooling
thus compensates fully for an individual lab’s limited resources. The second and
third reasons thus are non-cogent.
The first reason exclusively cites practical obstacles, which often require
innovative solutions to old problems (e.g., how to share academic credit fairly;
Rehg, 2009). These practical obstacles, however, certainly cannot be overcome by
sticking to the approaches of Fisher or Neyman-Pearson, because their statistical
concepts only enable decisions for an individual study. In fact, a method to combine
test-results of a series of studies had long remained absent (Birnbaum, 1954). To our
best knowledge, the only fair and firm combined statistical test for a series of studies
is log-likelihood-ratio addition (Krefeld-Schwalb, Witte & Zenker, 2018; Witte &
Zenker, 2017b).
5.2 Research diversity is a weak con-reason
The one reason against resource pooling cites research diversity (Schönbrodt &
Wagenmakers, 2018). Given constant resources, if labs pool individual resources to
achieve sufficient test-power, ceteris paribus, then each lab can “discover” only a
comparatively smaller number of statistically significant effects. Yet, assigning
value to diverse but underpowered statistically significant test-results, as opposed to
less diverse but well-powered significant test-results, does not only subordinate
theoretical progress (Witte & Strohmeier, 2013). An overly strong focus on research
diversity without aggregation and subsequent theory development indeed kills
theory. After all, the aggregation of diverse yet dissimilar experimental test-results is
theoretically as pointless as the comparison of apples and oranges is proverbial (cf.
Landy et al., 2020). Not pointless, by contrast, is a differentiation (with respect to
the same theory) of empirical test-results obtained under dissimilar experimental
conditions, But this already implies undertaking at least initial efforts in theory
construction. In brief, research diversity can be at most as important as obtaining
well-powered significant test-results.
5.3 Resource pooling is necessary
Few researchers today manage resource pooling within an integrated research
program in service of theoretical goals, even if several recent publications report
preregistered replications that look as if an integrated research program were
pursued (e.g., Hagger & Chatzisarantis, 2016; Landy, et al., 2020). Although
preregistered replications aggregate the effect sizes of individual studies, no
construct is specified as a point-alternative hypothesis. Researchers thus take a first
step towards analyzing test-results of similar experimental conditions from different
labs, but leave the full theoretical potential unused (Witte & Zenker, 2017a).
If the theoretical motivation for resource pooling were better understood, and
properly incentivized, then training and infrastructure would probably develop
accordingly. Test-power would then complement the p-value, both in job interviews
and on review checklists. And, as journals predominantly published well-powered
significant test-results, with the use of likelihood-ratios (rather than probability
measures) becoming normal, too, substantial hypothesis verifications could inform
theoretical research (Witte & Zenker, 2017b).
Instead, what researchers in behavioral science do today is seek to replicate
originally underpowered effects at a test-power level of around (1−β)=0.80, the very
value Cohen recommended (e.g., Cova et al., 2018). Counter-intuitively, however,
once resource pooling leads to test-results under α=β=0.01, for instance, then the
typically effort-intensive replication attempts undertaken today would become
superfluous (Witte & Zenker, 2018). In terms of statistical test-power, after all,
independently replicating the statistically significant (non-zero) observed effect size
of an already well-powered original study (α=β=0.05) yields merely a warrant to
restate confidence in the originally observed effect. But unless the error-rates are yet
further decreased (α=β<<0.05), independently replicating this original effect would
fail to offer a warrant to increase confidence in the originally observed effect.
This is so because the joint replication probability of n independent studies
(aka ‘aggregate test power’) is the product of these studies’ test-power, or (1−β) . If
a first successful replication yields (1−β)=0.80×0.80=0.64, for instance, then already
the second successful replication yields only (1−β)=0.512. But this is too close to
chance-level to use this observed effect size in theory construction. Unlike log-
likelihood aggregation paired with LR >(1−β)/α as the corroboration threshold
H1/H0
(Krefeld-Schwalb, Witte & Zenker, 2018), therefore, to determine whether a test-
result is informative for theory construction, aggregate test-power simply is a
misleading formal measure.
5.4 Inadequate samples sizes incur negative consequences
Hiring decisions regularly assign great importance to a job candidate’s number of a
publications, using it as a proxy measure for research productivity. Gervais, Jewell,
Najle, and Ng (2015) operationalize research productivity as the sum of two sample
control group studies (N , N ) that two hypothetical candidates publish over five
1 2
years. Ceteris paribus, given d=0.40 and N =N =100, the candidate whose studies
1 2
consistently achieve (1−β)=0.95 can in principle publish 21 studies with statistically
significant results, compared to 33 studies if a candidate makes do with N =N =25.
1 2
Given α=0.05 (one-sided), however, N =N =25 implies (1−β)=0.40, which is below
1 2
chance level. Even N =N =100 yields only (1−β)=0.88. Thus, avoiding an erroneous
1 2
discovery of a non-random effect under α=0.05 would thus be evaluated as being 2.4
times more important than avoiding an erroneous rejection of a true alternative
hypothesis (β=0.12=0.052.4).
From a theoretical viewpoint, this evaluation is unreasonable. An under-sized
sample, after all, necessarily leaves vague what α=β=0.05, or even α=β=0.01, would
instead turn into the “well-hardened” (Lakatos, 1978) empirical knowledge of an
effect size, knowledge that a theoretician needs to develop a theoretical construct.
Indeed, if we assume a medium effect (d=0.50) as typical, then most published
effects in behavioral science publications would be underpowered. Behavioral
science would thus generally underestimate the negative theoretical consequences of
conducting underpowered studies. Indeed, “[r]unning underpowered studies […]
might constitute a type of performance-enhancing design that inflates an individual
researcher’s productivity while having deleterious consequences for the collective
enterprise of science” (Gervais et al., 2015, 848). Though a hiring committee would
reportedly reverse their decision upon learning that little of theoretical value speaks
for underpowered test-results (Gervais et al., 2015), inadequate samples size clearly
incur negative consequences for theory construction.
6. Discussion
As several recent guidelines make clear, reliable scientific knowledge requires
independently replicated test-results (e.g., “Reproducibility and Replicability in
Science” (NAS, 2019)). Replicable results in turn are more important than novel
results, and the ability to theoretically predict replicable results is more important
yet. The replication crisis in behavioral science, as we saw, can be explained as a
consequence of typically low (1−β)-error-rates, that is, an overly low probability of
replicating an original test-result. The root cause of this crisis, we submit, is a wide
misunderstanding or misapplication of statistical methods in contexts of limited data
collection resources, because reducing N to a value smaller than N must be
MIN
“paid” for in terms of test-power. And, both strategies critiqued here—lowering the
(1−β)-error-rate from 0.95 to 0.80; treating the control group mean as a constant—
are typical examples of questionable research practices to reduce N .
MIN
As most behavioral science research today is data-driven, not only do many
researchers misunderstand/misapply the best statistical methods (Gigerenzer, 2018),
they also tend to over-interpret or overstate a test-result’s significance. An indicator
of this is the verbal transition—without warrant—from ‘is (statistically) significant’
to ‘is (scientifically) important’. A related issue arises from assuming “exactly zero
effect of site, experimenter, stimuli, task, instructions, and every other factor except
subject” (Yarkoni, 2020), resulting in a mismatch between general verbal statements
of theoretical hypotheses and their statistical expressions (aka ‘generalizability
crisis’). Finally, the preference for novel results in behavioral science has long kept
from appreciating the full value of replication research (Witte & Strohmeier, 2013;
Ioannidis, 2005; Pasher & Wagenmakers, 2012; Open Science Collaboration, 2015).
In fact, these causes were already discussed in the 1970s (e.g., Meehl, 1967; 1978).
A theoretically progressive behavioral science requires the ability to derive
point-specific predictions from theories that subsume well-replicated experimental
effects, thereby contributing to explaining, predicting, and intervening upon these
effects. Indeed, the practical value of theoretical knowledge rests squarely on
successful interventions. Though the philosophy of science has long recognized
induction and deduction as distinct modes of reasoning, behavioral science regularly
conflates these modes, here taking it for granted that learning from already observed
(‘old”) data would provide a substitute for a theory-based prediction of new data.
Particularly Bayesian methods embrace this idea.
But predictions derive from theoretical knowledge, whereas induction can
practically succeed without any theoretical accountability. While induction describes
the process of arriving at a parameter estimate given old data (aka ‘retrodiction’), a
point-specific version of this estimate is what a theoretical construct must predict for
a specific empirical condition, and then test this prediction against new data. A
confirmation by data, therefore, can be nothing other than a confirmation of a
theory-derived point-prediction. Conversely, theories failing to point-predict cannot
be confirmed by data. Indeed, absent point-predicting theories that retrodict old data
and predict new data, theoretical progress is hard to define and hard to achieve.
Since the construction of empirically adequate theoretical constructs is
impossible without collecting much larger samples (that guaranteed acceptable
replication probability), collaboration between labs in the form of resource pooling
remains a promising and viable route to avoid the negative consequences for
theoretical knowledge in behavioral science. Even Nobel prize winner Daniel
Kahneman admitted, in 2017 (Schimmack et al., 2017), that his bestselling
“Thinking fast and slow” (Kahneman, 2011) relied on too many non-replicated
empirical effects. Of course, it does not help that “published papers in top
psychology, economics, and general interest journals that fail to replicate are cited
more than those that replicate” (Serra-Garcia & Gneezy, 2021, 1).
7. Conclusion
The functional dependence between the α- and β-error-rates, d, and N allows
MIN
researchers to plan empirical studies with sufficiently large samples to achieve test-
results under α=β=0.05, test-results that alone can warrant the effort of subsuming
the corresponding observations under a theoretical construct. Presently, however,
these very error-rates would filter out the bulk of published test-results in behavioral
science as broadly useless for theory construction.
We have here critiqued two common strategies—lowering the (1−β)-error-
rate from 0.95 to 0.80; treating the control group mean as a constant—as typical
examples of questionable research practices to reduce N . We saw that, even if
MIN
α=β=0.05, a proper two sample t-test of the means in experimental and control
group always requires a point-specification of the H and the H . Provided that r is
0 1 12
positive, moreover, double testing is the only admissible strategy to reduce N .
MIN
A one sample t-test, by contrast, collapses a two sample Neyman-Person test
into a Fisher test, which merely reports the p-value of data in view of H . Although
the p-value can have heuristic value, a Fisher test cannot estimate test-power, hence
cannot statistically corroborate/falsify a theoretical hypothesis, so fails to inform
theory construction. As we have argued, collaboration between individual labs in the
form of resource pooling remains a viable route to increasing the sample sizes in
behavioral science.
Acknowledgements: For comments on earlier versions, we thank audiences at
Beijing Normal University, PRC, the Universities of Gdansk and Poznan, Poland,
Hamburg, Germany, Lund, Sweden, and Waterloo, Canada. FZ acknowledges
funding from the European Union (CA17132) and TUBITAK (118C257).
Declarations: Both authors declare that they have no conflicts of interest with
respect to the authorship or the publication of this article. EHW wrote a first draft of
the manuscript, both authors edited it, and both approved the final submitted
version. All values were calculated using G*Power software (Faul et al., 2007).