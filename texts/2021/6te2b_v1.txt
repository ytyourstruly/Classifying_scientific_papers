MODEL FIT AND INTERPRETATION 1
***Please note that this manuscript has been accepted for publication in the Journal of
Personality Assessment. This paper version is not the copy of record and may not exactly
replicate the final, authoritative version of the article.
Focusing Narrowly on Model Fit in Factor Analysis can Mask Construct Heterogeneity and
Model Misspecification: Applied Demonstrations Across Sample and Assessment Types
Kasey Stanton
University of Wyoming
Ashley L. Watts
University of Missouri
Holly F. Levin-Aspenson
Brown University
Ryan W. Carpenter
University of Missouri-St. Louis
Noah N. Emery
Colorado State University
Mark Zimmerman
Rhode Island Hospital
Correspondence concerning this paper should be addressed to Kasey Stanton, University of
Wyoming, Department of Psychology, 1000 E. University Avenue, Laramie, Wyoming 82071.
Email: kaseyjstanton@gmail.com. Please note that the ideas appearing in this manuscript have
not been disseminated previously.
Department affiliations for study authors:
= Department of Psychology
= Department of Psychological Sciences
= Department of Psychiatry and Human Behavior
= Department of Psychological Sciences
= Department of Psychology
= No separate departmental affiliation
MODEL FIT AND INTERPRETATION 2
Abstract
This study builds upon research indicating that focusing narrowly on model fit when evaluating
factor analytic models can lead to problematic inferences regarding the nature of item sets, as
well as how models should be applied to inform measure development and validation. To
advance research in this area, we present concrete examples relevant to researchers in clinical,
personality, and related subfields highlighting two specific scenarios when an overreliance on
model fit may be problematic. Specifically, we present data analytic examples showing that
focusing narrowly on model fit may lead to (a) incorrect conclusions that heterogeneous item
sets reflect narrower homogeneous constructs and (b) the retention of potentially problematic
items when developing assessment measures. We use both interview data from adult outpatients
(N = 2,149) and self-report data from adults recruited online (N = 547) to demonstrate the
importance of these issues across sample types and assessment methods. Following
demonstrations with these data, we make recommendations focusing on how other model
characteristics (e.g., factor loading patterns; carefully considering the content and nature of
factor indicators) should be considered in addition to information provided by model fit indices
when evaluating factor analytic models.
MODEL FIT AND INTERPRETATION 3
Focusing Narrowly on Model Fit in Factor Analysis can Mask Construct Heterogeneity and
Model Misspecification: Applied Demonstrations Across Sample and Assessment Types
Exploratory factor analytic (EFA), confirmatory factor analytic (CFA), and “hybrid”
exploratory structural equation modeling (ESEM) approaches feature prominently in measure
development efforts and examinations of personality and psychopathology structure (Greiff &
Heene, 2017; Sellbom & Tellegen, 2019; Wright, 2017). Many crucial decision points arise
when using factor analysis, including how many latent factors to extract in analyses and how to
best specify item loadings a priori with use of CFA, or in some cases, ESEM models (Wright,
2017). When using CFA, ESEM, and, to a lesser extent, EFA, researchers also examine model
fit indices to guide model selection. These indices typically are compared against “benchmarks”
presumed to reflect an “acceptable” or “well-fitting” model (e.g., interpreting fit based on root
mean square error of approximation [RMSEA] values; Hopwood & Donnellan, 2010).
Information derived from model fit indices can be useful for guiding model interpretation
and selection by providing a general sense of the degree to which models align with observed
data. That being said, concerns have been raised about an overreliance on model fit as an
indicator of model validity (Barrett, 2007; Gignac, 2007; Sellbom & Tellegen, 2019). Sellbom
and Tellegen (2019) describe that it is often the case that “limited theoretical consideration […]
goes into decision making when selecting an ‘optimal’ model” when using factor analysis, such
that “researchers seem to allow themselves to be dictated by model fit indices” (p. 1431).
Various simulation efforts also have demonstrated limitations of fit indices for identifying the
true (“population”) structural model, as model fit may be stronger for a model other than the true
or known structure (e.g., Bonifay & Cai, 2017; Greene et al., 2019).
MODEL FIT AND INTERPRETATION 4
This research indicates the need to consider additional model characteristics to determine
model validity and viability, as ignoring other model characteristics (e.g., factor loadings,
external factor associations) can result in a focus on problematic models that serve as guiding
frameworks for literatures (Roberts & Pashler, 2000; Watts et al., 2019). For example, measure
validation efforts for some measures of social-cognitive vulnerabilities (e.g., intolerance of
uncertainty; Carleton et al., 2007) have focused heavily on identifying well-fitting models to
guide measure development, with less attention given to other aspects of the measure
development process. As a result of focusing on identifying well-fitting models without
thoroughly considering key issues such as discriminant validity, some social-cognitive
vulnerability measures have later been shown to include item content that is difficult to
differentiate from general distress (Naragon-Gainey & Watson, 2018; Stanton, 2020).
Herein, we focus on two problems that can arise from an overreliance on model fit indices
when identifying models. First, researchers may fail to consider valid, alternative, multifactor
solutions when single-factor models fit well. Second, even when multifactor solutions are
considered, model fit indices may indicate acceptable to good fit even when item loadings on
specific factors are misassigned. Previous theoretical articles provide general recommendations
regarding the need to consider model characteristics other than fit as reviewed (e.g., Greiff &
Heene, 2017; Roberts & Pashler, 2000; Sellbom & Tellegen, 2019), and other studies offer
insights into interpretations of model fit in other specific data analytic scenarios (e.g., when
evaluating higher-order factor models; Gignac, 2007). However, the two specific issues of focus
here have received relatively little attention in psychometric research to date, and we provide
practical demonstrations relevant to clinical, personality, and other subfields to facilitate
researchers’ awareness of these issues when applying factor analysis in their own work.
MODEL FIT AND INTERPRETATION 5
Issues with Failing to Recognize Heterogeneity in Item Sets
Researchers may mistakenly interpret heterogeneous item sets as being homogeneous in
nature if model fit is “good” or “acceptable” for single-factor models (Watts et al., 2021).
Similarly, researchers sometimes conflate “good” internal consistency (e.g., as determined using
coefficient alpha) with scale homogeneity (Dunn et al., 2014). As a result, an overreliance on
these indices or their misinterpretation can result in the conclusion that an item set reflects a
single, narrow construct even when it does not (Chmielewski et al., 2011). In such cases,
heterogeneous item sets may then be summed to create global composite scores, even though
more homogeneous item sets within broader composites may associate differentially with
external criteria (Jackson et al., 1976; Smith & McCarthy 1995; Smith et al., 2009).
For example, different psychopathy dimensions (e.g., disinhibition, callousness) show
distinct neural correlates that may be obscured when focusing analyses solely on global
psychopathy scores (Latzman et al., 2019). Similarly, the borderline personality disorder (PD)
criteria from the Diagnostic and Statistical Manual of Mental Disorders (currently fifth edition;
DSM–5; American Psychiatric Association) are heterogenous in nature, and ratings of individual
borderline PD criteria show distinctive associations with other variables (e.g., inappropriate
anger shows some specificity with aggression; Chmielewski et al., 2011; Sharp et al., 2015).
However, item sets used to assess borderline PD criteria sometimes have been interpreted as
being homogeneous because single-factor CFA models of borderline PD ratings show good fit,
with fit for single-factor models appearing better than multidimensional model configurations in
some studies (e.g., Clifton & Pilkonis, 2007; Feske et al., 2007; Johansen et al., 2004). Although
illustrative, these examples are not limited to the personality pathology literature. In fact, Greiff
and Heene (2017) noted that these issues persist across substantive research areas (e.g., substance
MODEL FIT AND INTERPRETATION 6
use assessment; see Watts et al., 2021), as they describe that when single-factor CFA structures
fit well, one might “conclude, probably just as 99% of other researchers working in assessment
would, that you found support for the unidimensional structure” (p. 313) without carefully
considering other model characteristics.
Issues with Failing to Recognize Misassigned or Problematic Items
As a second related issue, if researchers focus narrowly on model fit, they may retain
potentially problematic items for scale/subscale scoring. For example, if model fit indices
suggest acceptable fit, researchers may overlook items that are problematic to include in scales
due to loading weakly on their targeted factors (e.g., items assessing emptiness may load much
less strongly on a latent borderline PD factor than affective instability items; Johansen et al.,
2004). Related issues include the possibility that well-fitting multifactor confirmatory models
could have items that (a) show strong cross-loadings on other factors on which they are not
specified to load or (b) are assigned to load onto factors other than those on which they load most
strongly. However, such aspects of model misspecification may go undetected without careful
consideration of alternative structures (Greene et al., 2021), which may hinder measure
development efforts and the application of structural models for informing assessment more
generally (Loevinger, 1957; Jackson, 1970; Sellbom & Tellegen, 2019).
The issues of items being misassigned to factors or failing to be clear indicators of a single
factor may be particularly salient when examining structural models of symptoms, given that
different symptom experiences often are closely interrelated (Clark & Watson, 2019; Kotov et
al., 2017). For example, items assessing “having thoughts that don’t make sense to others” often
are used to score thought disorder scales, but individuals with high levels of internalizing
psychopathology may have worries that seem irrational to others and may strongly endorse these
MODEL FIT AND INTERPRETATION 7
items as a result (Samuel et al., 2018). In such cases then, a two-factor model consisting of
thought disorder and internalizing dimensions potentially could fit well if an item assessing
“thoughts that don’t make sense to others” is allowed to load only on thought disorder, even
when such an item could have a loading of equal or stronger magnitude on internalizing.
Study Aims and Demonstrations
In this study, we show how focusing narrowly on the interpretation of model fit indices
when evaluating factor models may lead to the previously described issues of (a) incorrectly
concluding that heterogeneous item sets reflect narrow homogeneous constructs and (b) the
retention of potentially problematic items for scoring scales/subscales. We address these issues
through a series of demonstrations using data from multiple samples and assessment methods as
We present results from both EFA and CFA models illustrating these issues regarding
model fit interpretation, beginning with an initial focus on single-factor CFA models. We focus
on CFA models first because researchers often rely more heavily on making inferences based on
model fit when using more confirmatory modeling approaches to (a) evaluate the acceptability of
models and (b) inform interpretations of the degree to which item sets are homogeneous when
examining single-factor structures (acknowledging parallels for using CFA, ESEM, and EFA
when examining single factor solutions; Greene et al., 2021; Greiff & Heene, 2017).
After reporting the results of single-factor CFA models, we proceed to conduct follow-up
EFAs to show that additional interpretable factors can be extracted even when model fit indices
indicate acceptable to good fit for single-factor CFA structures. We then score subscales based
on the results of these follow-up EFAs and examine these subscales’ external correlates with
measures of other psychopathology, personality, and psychosocial functioning. These analyses
MODEL FIT AND INTERPRETATION 8
mimic how factor analytic approaches often are used to inform scale/subscale scoring (Clark &
Watson, 2019) and illustrate that subscales corresponding with distinct dimensions from our
EFAs show differential patterns of external correlates that would be obscured by focusing
analyses solely on total scores reflecting a single dimension. Following that, we demonstrate
that factor interpretability is important to consider in addition to model fit even when examining
multifactorial CFA structures.
When reviewing the subsequent demonstrations, it is important to recognize that factor
analytic approaches range in the degree to which they are exploratory to confirmatory rather than
representing an exploratory versus confirmatory dichotomy (Chabrol et al., 2005; Greene et al.,
2021; Schmitt et al., 2018; Wright, 2017). Although we use follow-up EFAs to highlight aspects
of model misspecification for CFA models showing acceptable to good fit according to
traditional interpretative cutoffs, we do not intend to suggest that more exploratory analyses are
superior to applications of more confirmatory approaches in all situations and contexts. Indeed,
other recent demonstrations highlight how use of more exploratory and confirmatory approaches
in tandem can be useful for advancing knowledge of personality and psychopathology structure
(Greene et al., 2021; Schmitt et al., 2018). Furthermore, in addition to EFA, we could have used
other data analytic approaches such as comparing multiple CFA model configurations. We
chose to use EFA after examining initial CFA models because it represented a straightforward
option for demonstrating that models can have adequate to good fit according to traditional
benchmarks even when item loadings are clearly misspecified and/or other model features are
problematic.
Many of the issues discussed here also are applicable to measure development efforts and
maximizing measures’ construct validity (also see Clark & Watson, 2019 & Loevinger, 1957 for
MODEL FIT AND INTERPRETATION 9
discussion of the distinction between the reliability and validity of “measures” versus
“measurements” of constructs). More exploratory approaches may be particularly useful in
earlier stages of the measure development process (e.g., EFAs with no loadings specified) to
determine the extent to which emergent structures are consistent with general theoretical
expectations (e.g., is each factor well-defined when extracting a specific number of factors based
on theoretical considerations; Clark & Watson, 2019; Greene et al., 2021). Approaches
traditionally described as confirmatory are useful for evaluating models with an increasing
number of constraints. At minimum, CFA requires specifying which indicators are allowed to
load onto which factors. However, even with use of CFA, other model features (e.g., the
magnitude of factor loadings, the magnitude of interfactor correlations) typically are not
specified a priori, such that these models rarely are entirely confirmatory (Greene et al., 2021).
Keeping these issues in mind, our EFAs following our initial CFAs demonstrate potential pitfalls
with focusing narrowly on model fit when evaluating model suitability through a series of basic
illustrative examples spanning different personality and psychopathology domains.
Method
Sample One: Interview Data from the Adult Outpatient Sample
Sample Description
All participants across samples provided their informed consent for participation, and all
research procedures received institutional ethics board approval. We did not receive ethics board
permission to share data from either of these samples to open source repositories. However,
frequencies and descriptive statistics for all items included in subsequent factor analyses are
syntax and output for these factor analyses and polychoric correlation matrices for all items used
MODEL FIT AND INTERPRETATION 10
in these analyses also are available at the link provided previously. Full study datasets are
available upon request from the first author.
Regarding the first sample, participants were 2,149 adult outpatients (mean age = 38.5, SD
= 12.5) who completed interview assessments as part of the Rhode Island Methods to Improve
Diagnostic Assessment and Services (MIDAS) Project (Zimmerman, 2016). Data were collected
at treatment intake, and we included data from all participants regardless of specific diagnosis or
diagnoses. The majority of participants were female (61.0%). Most participants identified as
being White or European American (90.7%), 4.4% identified as Black or African American, with
remaining participants endorsing other identities. Highest level of education was as follows:
40.6% with an associate degree or some college; 21.9% high school degree or equivalent; 15.1%
with a 4-year college degree; 14.2% with some graduate school or a graduate/professional
degree; and 8.3% with less than a high school or equivalent level of education.
Interview Measures and Procedure
Our factor analyses focus on nine item-level ratings of borderline, avoidant, schizotypal,
and paranoid PD traits drawn from the Structured Interview for DSM-IV Personality (SIDP-IV;
Pfohl et al., 1997). Items were rated on a 0 (not present) to 3 (strongly present) scale, and
criteria 1, 2, 3, and 7) and avoidant PD ratings (criterion 2 rating; fear of not being liked) were
used to assess identity and relationship disturbance. Schizotypal (criterion 9 rating; social
anxiety due to paranoia) and paranoid PD ratings (ratings for criteria 1, 4, and 5) were used to
assess mistrust. These ratings were selected because they (a) all reflect various aspects of
interpersonal dysfunction and/or (b) load strongly onto a general PD factor (Sharp et al., 2015).
We focused on limited item sets in this and subsequent examples to provide straightforward
MODEL FIT AND INTERPRETATION 11
demonstrations of issues related to focusing narrowly on model fit, not because we intended to
conduct comprehensive examinations of psychopathology structure.
Other variables used for our analyses focused on determining external correlates of PD
factors included (a) antisocial PD ratings obtained via the SIDP-IV, (b) lifetime internalizing and
externalizing disorder ratings based on the Structured Clinical Interview for DSM-IV Axis I
Disorders (SCID-IV; First et al., 1995), and (c) other clinically-relevant ratings (e.g., suicide
attempt history) from the Schedule for Affective Disorders and Schizophrenia (SADS; Endicott
antisocial PD ratings, complete data for specific conduct disorder ratings were not available for
many participants. As a result, participants were rated as meeting criteria for antisocial PD based
on having three or more antisocial PD traits present (4.3% sample prevalence).
Internalizing disorders included major depressive disorder (MDD; lifetime prevalence
64%), social anxiety disorder (29.8%), panic disorder (25.8%), generalized anxiety disorder
(19.7%), and persistent depressive disorder (10.3%). Externalizing disorders included alcohol
use disorder (40.9%), tobacco use disorder (20.6%), cannabis use disorder (16.1%), intermittent
explosive disorder (6.1%), and antisocial PD (4.3%). We also summed scores on these
dichotomous diagnostic ratings (e.g., if MDD history was rated as present, then a score of “1”
was computed toward the composite total) to create internalizing and externalizing composite
variables; thus, internalizing and externalizing scores each ranged from 0 to 5. Extensive
information about the training process for interviewers (PhD-level psychologists or bachelor’s
level research assistants) and interrater reliability data described in other articles (Stanton et al.,
2018; Zimmerman, 2016). For example, interrater reliability analyses for all internalizing
diagnoses in these data exceed .80, with similar estimates reported for externalizing ratings.
MODEL FIT AND INTERPRETATION 12
Sample Two: Self-Report Data from the Online Community Sample
Sample Description
Participants were 547 US adults (mean age = 38.0 years, SD = 12.3) recruited using
Amazon Mechanical Turk. Most participants were women (59%), 40% were men, and 0.5%
were nonbinary (the small remaining percentage of participants did not provide this information).
A small percentage of participants (1.2%) were transgender. Most participants identified as
White or European American (74.2%; 9.7% Asian American; 7.9% Black or African American;
remaining participants endorsed other identities); 5.3% reported being Hispanic or Latino/Latina.
The most common responses for highest level of education were 38.6% having bachelor’s degree
and 37.7% having completed some college or an associate’s degree. Finally, 18.1% of
participants reported receiving medication to treat psychiatric issues, and 9.1% reported currently
receiving psychotherapy.
Self-Report Measures
Our first set of factor analyses using data from this sample focused on the 27 items from
the Dysphoria (10 items), Lassitude (6 items), Social Anxiety (6 items), and Ill-Temper (5 items)
scales of the Expanded Version of the Inventory of Depression and Anxiety Symptoms (IDAS-
II; Watson et al., 2012). Items from these scales were selected to assess negative emotion and
cognitive patterns characteristic of the internalizing domain. Participants responded to the
IDAS-II items in reference to the past 2 weeks using a 5-point scale ranging from 1 (not at all) to
5 (extremely). Additional factor analyses focused on item-level data from the 18-item Adult
ADHD Self-Report Scale (ASRS; Kessler et al., 2005), which assesses inattentiveness, motor
hyperactivity/impulsivity, and verbal hyperactivity/impulsivity symptoms (Stanton et al., 2018).
MODEL FIT AND INTERPRETATION 13
Participants responded to these items using a scale ranging from 0 (never) to 4 (very often) in
reference to the past 6 months.
We also examined how subscales created based on factor analyses of the IDAS-II items
associated with scores from both (a) the second edition of the Big Five Inventory (BFI-2; Soto &
John, 2017) and (b) the Short Dark Triad (SD3; Jones & Paulhus, 2014). The BFI-2 assesses
five-factor model personality traits, and the SD3 assesses Machiavellianism, psychopathy, and
grandiose narcissism using 9 items to assess each construct. Participants responded to the BFI-2
and SD3 using a scale ranging from 1 (disagree strongly) to 5 (agree strongly). Finally, we
included trait scores from the Broad Autism Phenotype Questionnaire (BAPQ; Hurley et al.,
2007). The BAPQ assesses three phenotypic dimensions relevant to autism: Aloofness (12
items), Pragmatic Language Difficulties (e.g., “out of sync in conversations”), and Rigidity (12
items; “very set in my ways”). Participants responded to the BAPQ items using a scale ranging
from 1 (very rarely) to 6 (very often). Descriptive statistics and coefficient omega estimates for
Data Analytic Overview
results from single-factor CFA models in both datasets. Second, we conduct EFAs showing that
distinct dimensions can be identified using these same item sets. The magnitude of factor
loadings in CFA models were not specified even though all items were specified to load onto a
single factor in all initial CFA models. We created scales to represent dimensions from both
single-factor CFA and multifactor EFA models for subsequent analyses examining external
correlates. When creating scales to model single-factor solutions, scores for items loading > |.40|
on factors were included in scale scoring. For multifactor solutions, we scored subscales using
MODEL FIT AND INTERPRETATION 14
items with (a) absolute loadings ≥ .40 on their primary factor and (b) absolute cross-loadings ≤
.30 on other factors, consistent with measure development recommendations (Clark & Watson,
2019). Where relevant, we compared subscale correlations using a Fisher’s r-to-z transformation
and conducted two-tailed within-sample z-tests of their differences. We used a threshold of p <
.001 for evaluating all difference tests, acknowledging that relatively small differences in
correlations still may have been likely to be significant at this level due to our use of large
sample sizes for analyses. Finally, we present results from multifactor CFA models to show that
model fit indices can indicate good fit even when item loading specifications are problematic.
We made no specifications when estimating these CFA models other than specifying which item
indicators were allowed to load on specific factors, with items being allowed to load on a single
factor only across CFA analyses.
All factor analyses were conducted using Mplus Version 8 (Muthén & Muthén, 2017). We
estimated all factor models using a weighted least squares mean and variance adjusted
(WLSMV) estimator given our focus on analyzing item-level data, and all EFA analyses were
conducted using a promax rotation. To evaluate fit for CFA models, we considered fit indices
that commonly are reported when evaluating models, including the confirmatory fit index (CFI),
the Tucker-Lewis Index (TLI), RMSEA, and the standardized root mean squared residual
(SRMR). For RMSEA and SRMR, values ≤ .08 often are interpreted as indicating acceptable fit,
and CFI and TLI values ≥ .95 commonly are interpreted as indicating adequate to good fit (Hu &
Bentler, 1999). Nevertheless, there is not clear consensus on specific cutoff values (McNeish &
Wolf, 2021). For example, CFI and TLI values ≥ .90 commonly are interpreted as indicating
acceptable fit across literatures, as are RMSEA values of ≤ .10 (Hopwood & Donnellan, 2010).
We also present McDonald’s omega (ω) and coefficient alpha (α) estimates where relevant.
MODEL FIT AND INTERPRETATION 15
Results
Single-Factor Confirmatory Factor Analytic Models and Masked Item Heterogeneity
Sample One: Interview Ratings in Adult Outpatients
Single-factor CFA solution. The single-factor CFA model of the PD ratings yielded
acceptable to good model fit (RMSEA = .063; CFI = .941; TLI = .921; SRMR = .060; χ =
on this factor, which we labeled Interpersonal Dysfunction.
Follow-up exploratory factor analysis and external validation. Although a single-factor
CFA model was viable, we conducted a follow-up EFA to show that distinct dimensions could
be identified in these data. Prior studies have not examined the factor structure and
multidimensionality of the nine specific PD ratings used here to our knowledge. Therefore, we
conducted a parallel analysis to help inform how many dimensions should be extracted in these
data. Parallel analysis indicated that up to two factors could be extracted (sample eigenvalues =
2.82, 1.11, 1.02; random data eigenvalues = 1.10, 1.07, 1.04). Consistent with this, two
interpretable dimensions emerged when extracting two factors, as shown in the middle columns
needing validation from others (e.g., fearing abandonment) loaded strongly on Factor I, which
we labeled Identity Disturbance. Items assessing mistrust of others (e.g., suspicious of being
exploited) loaded strongly onto Factor II, which we labeled Suspiciousness (correlation between
Factor I and II = .65).
We scored Identity Disturbance (5 items; α= .66; ω = .67; M = 2.0, SD = 2.5) and
Suspiciousness (4 items; α= .55; ω = .59; M = 1.0, SD = 1.5) subscales based on these results.
Estimates for α and ω values for these subscales were slightly lower than cutoff values (e.g., ≥
MODEL FIT AND INTERPRETATION 16
.70) commonly interpreted as indicating acceptable internal consistency; this likely was due at
least in part to these subscales’ brevity, as indicators of internal consistency often increase as a
function of item number (Clark & Watson, 2019; Dunn et al., 2014). We also scored a 9-item
scale reflecting the broader Interpersonal Dysfunction factor (α = .72; ω = .73; M = 3.0, SD =
3.4) to evaluate the degree to which subscale correlations differed from those for this composite.
Identity Disturbance and Suspiciousness subscales showed similar, weak associations with
externalizing ratings. However, Identity Disturbance showed correlates that were significantly
stronger in magnitude than those for Suspiciousness with all clinically relevant ratings and nearly
all internalizing ratings when examining z-test correlation differences. Several differences for
Identity Disturbance and Suspiciousness were pronounced, including differences in associations
with social anxiety (rs = .43 and .18, respectively) and the internalizing composite rating (rs =
.36 and .17, respectively). When considering subscale versus composite correlations, patterns of
associations for the general Interpersonal Dysfunction composite paralleled associations for the
Identity Disturbance subscale very closely, such that scores on this subscale appear to have
driven observed associations for the overall composite. Collectively then, Identity Disturbance
and Suspiciousness showed key, distinctive associations that would be masked when focusing
analyses solely on total scores reflecting a more general PD dimension.
Sample Two: Self-Rated Internalizing Symptoms in the Online Community Sample
Single-factor CFA. Next, we examined these same issues using item-level data drawn
from the four different IDAS-II scales. A single-factor model on which all items were specified
to load generally fit well (CFI = .956; TLI = .952; SRMR = .052; RMSEA = .088; model χ =
1684.412, df = 324). The RMSEA value for this model slightly exceeded some cutoffs values
MODEL FIT AND INTERPRETATION 17
for determining acceptable fit (e.g., .08; Hu & Bentler, 1999), but we anticipate that many
researchers would deem this model acceptable given other index values and because this
RMSEA value still was lower than other commonly used cutoffs (i.e., ≤ .10; see Hopwood &
on this single factor, with one exception (i.e., “slept more than usual”; loading = .64).
Follow-up exploratory factor analysis and external validation. Next, we examined a
four-factor EFA model. We extracted four factors given that the items used for these analyses
were drawn from four IDAS-II scales that have been validated extensively in prior factor analytic
research across sample types (e.g., patient, community, undergraduate; Watson et al., 2012).
had at least five items with primary loadings ≥ .40 on them and with cross-loadings ≤ .30 on
interfactor correlations ≥ .60), but no interfactor correlation exceeded .70.
We scored Dysphoria (5 items; both α and ω = .93; M = 10.8, SD = 5.8), Lassitude (8
items; α and ω = .90; M = 16.0, SD = 7.5), Ill-Temper (5 items; α and ω = .90; M = 8.4, SD =
4.5), and Social Anxiety (6 items; α and ω = .91; M = 10.9, SD = 5.9) subscales for examining
not included in subscale scoring because they were not clear indicators of any single factor. We
also created a General Internalizing composite scale including scores from all 27 IDAS-II items
(both α and ω = .97; M = 51.4, SD = 23.7).
The General Internalizing composite and internalizing subscales had similar correlates in many
ways (e.g., robust negative associations with BFI-2 Conscientiousness). Still, individual
MODEL FIT AND INTERPRETATION 18
subscales showed some correlates that were distinctive from those for other subscales and the
General Internalizing composite. For example, all subscales associated robustly with BFI-2
Negative Emotionality, but the correlation for Dysphoria (r = .71) was significantly stronger than
that for any other subscale based on z-test correlation comparisons. Ill-Temper also showed
some distinctive associations with BFI-2 Extraversion (r = -.25) and SD3 Psychopathy (r = .47;
both correlations significantly different from all other subscale correlations).
Comparing correlations for two specific subscales also was informative. For instance,
Dysphoria and Ill-Temper had significantly different correlates in the opposite direction with
SD3 Narcissism (rs = .12 and -.10, respectively; p < .001 for difference). As another example,
Social Anxiety correlated significantly more strongly with BAPQ Aloofness than did Ill-Temper
(rs = .44 and .28; p < .001 for difference). Many of these correlations are not novel or surprising
(e.g., social anxiety with aloofness), and differences in correlational patterns for subscales were
less pronounced than in our example focused on PD interview ratings. However, examining
these associations indicates that in addition to be differentiable on content grounds, these four
internalizing subscales showed specificity in their associations in some ways.
Acceptable Model Fit in Multifactor Models but Potential Item Misspecification
Sample One: Interview Ratings in Adult Outpatients
Next, we demonstrate that even if multifactor solutions are examined, problematic item
misspecification can be missed by focusing too heavily on model fit. Returning to the PD
misspecified two-factor CFA model of Identity Disturbance and Suspiciousness. All items
showed strong loadings (i.e., > .45) on their assigned factors. Model factors correlated strongly
(.73), though at a level that may be deemed acceptable by many researchers given that CFA can
MODEL FIT AND INTERPRETATION 19
inflate interfactor correlations (Hopwood & Donnellan, 2010). Furthermore, this model fit well
(RMSEA = .046; CFI = .970; TLI = .959; SRMR = .051; model χ = 142.824, df = 26), such that
it likely would be deemed sufficient to guide subscale scoring when developing measures.
aspects of CFA model misspecification that could be overlooked. Specifically, the schizotypal
PD criterion rating reflecting “social anxiety due to paranoid fears” is assigned to and loads
strongly on the Identity Disturbance factor in the CFA model, even though this item loads much
more strongly on the Suspiciousness factor in the EFA model (loadings = .16 and .43,
respectively in the two-factor EFA). Assigning this schizotypal PD rating to Identity
Disturbance could be plausible theoretically given that it assesses social anxiousness, as do some
of the specific avoidant and borderline PD items.
Sample Two: Self-Rated ADHD Symptoms in the Online Community Sample
Next, we tested a misspecified variation of a three-factor model of ADHD Inattentiveness,
Verbal Hyperactivity/Impulsivity, and Motor Hyperactivity/Impulsivity identified in prior
examinations of the ASRS’s factor structure (e.g., Stanton et al., 2018) to demonstrate these
same issues with item misspecification using self-report data. Note that we also could have
demonstrated item misspecification for well-fitting models with the previously reviewed IDAS-II
internalizing symptom data (e.g., all model fit indices indicated good fit when select items
assessing irritability were assigned to load onto a Lassitude rather than Ill-Temper factor);
however, we leverage the range of symptom measures included in this dataset to highlight the
relevance of these issues across measures assessing a range of constructs.
1 Other well-fitting but misspecified models also could be identified by assigning items to load onto factors in a
manner different from the example here, such that this is only a one demonstrative example of misspecification. For
example, model fit also was good (e.g., RMSEA = .047, CFI = .986, TLI = .956) for another model where the
avoidant PD item assessing fear of not being liked was specified to load onto the Suspiciousness factor.
MODEL FIT AND INTERPRETATION 20
to good fit based on information provided from different fit indices (RMSEA = .078; CFI = .960;
TLI = .954; SRMR = .038; model χ2 = 571.904, df = 132). All items also loaded strongly on
their assigned factors (i.e., > .60). Similar to other examples, interfactor correlations were
strong in magnitude (i.e., rs for Inattentiveness with Verbal and Motor Hyperactivity/Impulsivity
= .68 and .82, respectively, r for Verbal and Motor Hyperactivity/Impulsivity = .73), but this
model still could be deemed suitable for guiding ASRS subscale scoring on the basis of these
item loadings and this model generally fitting well. Importantly, however, this model still fit
well despite ASRS item 9 (“difficulty concentrating), a clear inattentiveness indicator, being
assigned to load onto the Motor Hyperactivity/Impulsivity factor. In fact, this item had the
strongest loading on Motor/Hyperactivity of any item.
Based on existing structural research indicating that the ASRS items may reflect defined as
many as three distinct factors (Gibbins et al., 2012; Stanton et al., 2018), we examined a three-
most strongly onto Inattentiveness as anticipated when extracting three factors using EFA
(loading = .50, loadings < .25 on other factors). This was the case even though this item had a
very strong loading on Motor Hyperactivity/Impulsivity when specified to load onto that factor
in the CFA model reviewed previously.
A more subtle aspect related to this specific example with ADHD ratings focuses on item
12 (i.e., “leaves seat”), which loaded strongly (loading = .68) onto its assigned Motor
Hyperactivity/Impulsivity factor in the CFA model. However, in the EFA model, it showed
2 Once again, other configurations of assigned factor loadings could have been used to demonstrate model fit indices
indicating good fit even in the context of model misspecification (e.g., well-fitting models can be identified when
specific items assessing motor hyperactivity/impulsivity are specified to load onto the Verbal
Hyperactivity/Impulsivity factor).
MODEL FIT AND INTERPRETATION 21
loadings of the same magnitude (.30) on Verbal Hyperactivity/Impulsivity and Motor
Hyperactivity/Impulsivity. Consequently, if these analyses were conducted to inform subscale
scoring, this item would be identified as a “splitter” and may not be included in subscale scoring
as a result (Clark & Watson, 2019). Thus, had the original misspecified three-factor CFA model
been identified as suitable, both items 9 and 12 potentially would have been inappropriately
scored for subscales representing factors. Finally, interfactor correlations for the three-factor
EFA model were weaker than those from the CFA model (e.g., rs for Verbal and Motor
factor intercorrelations).
Discussion
These examples using data from multiple samples and methods demonstrate potential
problems with failing to (a) consider valid, alternative multifactor solutions when single-factor
models fit well and (b) identify items with misassigned loadings on specific factors in multifactor
models. Regarding this first issue, we showed that well-fitting models could lead to potentially
problematic interpretations that heterogeneous item sets are instead homogeneous. First, our
analyses using a heterogenous set of interview ratings assessing various criteria from four PDs
demonstrated that even when a single-factor model fits well, distinguishable factors showing
meaningfully different patterns of external correlates also could be identified. Additionally, our
example focusing on self-rated internalizing symptoms also indicated that a single-factor model
generally fit well according to most indices examined, even though these analyses were based on
27 items drawn from four different symptom scales. Taken together then, these demonstrations
illustrate how focusing narrowly on model fit could lead researchers astray without careful
consideration of item content and alternative multidimensional structures.
MODEL FIT AND INTERPRETATION 22
At the same time, we recognize that a narrow focus on model fit can lead to overfactoring
and prioritizing models that are overly complex representations of data, even though our
demonstrations did not focus explicitly on this issue. Model fit will almost invariably improve
with each additional factor extracted, and narrowly focusing on model fit may result in
researchers extracting additional factors that are poorly defined or difficult to interpret regardless
of whether more exploratory or confirmatory approaches are applied (Montoya & Edwards,
2021). Thus, the goal of our demonstrations was simply to show that researchers may make
problematic inferences when fit indices for single-factor models satisfy widely used cutoff
values. For instance, borderline PD may be reified as a unidimensional construct based on
single-factor models of borderline PD ratings showing good fit as noted (e.g., Clifton & Pilkonis,
2007). Along with other research, our results underscore the importance of considering other
model characteristics when adjudicating model appropriateness, including factor loading
patterns, item content alignment with construct definitions, the extent to which prioritizing more
complex models meaningfully increments clinical assessment and prediction, and so on (Greiff
& Heene, 2017; Sellbom & Tellegen, 2019).
Another important point related to balancing parsimony with capturing heterogeneity is that
the same item scores may be valid indicators of both broad and narrow constructs (Clark &
Watson, 2019). This is especially relevant to the application of hierarchical, dimensional
frameworks such as the Hierarchical Taxonomy of Psychopathology (HiTOP; Kotov et al.,
2017), which classifies symptom dimensions at both broad (e.g., the internalizing spectrum) and
narrow (e.g., a dysphoria facet within internalizing) levels of abstraction. For instance, consider
our examples focused on PD ratings. In this case, items reflecting both (a) Identity Disturbance
and (b) Suspiciousness also were clear indicators of a general Interpersonal Dysfunction factor.
MODEL FIT AND INTERPRETATION 23
As a result, they could be validly used to assess their respective specific factors and a more
general factor depending on the goals of an analysis. Measures such as the Spectra: Indices of
Psychopathology (Blais & Sinclair, 2018) that can be used to assess both specific dimensions
(e.g., Aggression) and broad factors (e.g., Externalizing) demonstrate this approach. According
to this perspective then, assessment focused on broad levels of abstraction (e.g., assessing broad
spectra such as internalizing) are not “competing” with assessment of more specific dimensions
(e.g., worry), as they could be complementary approaches for assessing psychopathology (e.g.,
see Blais & Sinclair, 2018; Stanton et al., 2020).
Identifying Misspecification in the Presence of Acceptable Model Fit
As our other examples with the PD interview ratings and self-ratings of ADHD
demonstrate, overemphasizing model fit can result in failure to detect other problematic aspects
of models even when examining multifactor models. Although both examples presented
reinforce these points, our demonstration with ADHD is particularly striking. In this example,
even when items clearly assessing inattentiveness (e.g., concentration difficulties) were assigned
to load onto hyperactivity/impulsivity factors when using CFA, model fit was acceptable to good
when making comparisons against widely used interpretive benchmarks. There are ongoing
debates in many substantive areas of personality and psychopathology research regarding the
nature of specific model constellations that should be used as frameworks for guiding
measurement (e.g., different conceptualizations of psychopathy structure; posttraumatic stress
disorder structure; Schmitt et al., 2018; Veal et al., 2021). Keeping in mind that statistical
models are imperfect representations of complex systems of traits and processes, assigning
concentration items to load onto hyperactivity/impulsivity factors when examining ADHD
symptom models represents an obvious error theoretically and based on prior research (Kessler et
MODEL FIT AND INTERPRETATION 24
al., 2005; Martel et al., 2010; Stanton et al., 2016). However, it illustrates very well that models
can fit well even when item misspecifications clearly are theoretically inconsistent. In many
other cases, item misassignments often can be much more difficult to detect without careful
consideration.
Recommendations for Factor Analytic and Psychometric Research
In what follows, we provide concrete recommendations to reduce the likelihood of the
issues described occurring in measure development contexts and more generally. First,
foundational steps include (a) clearly articulating construct definitions and (b) generating
homogeneous item composites (HICs) representing each dimension predicted to underlie an item
set when developing measures. These recommendations likely are familiar to researchers with
expertise in psychometrics and have long been recognized as a fundamental in the measure
development process (e.g., Jackson, 1970; Loevinger, 1957) but often remain neglected in
measure validation and other research areas (Greiff & Heene, 2017; Sellbom & Tellegen, 2019).
We recognize that researchers hold differing viewpoints about the utility of more
exploratory versus confirmatory factor analytic approaches at various phases of the measure
development process and when examining personality and psychopathology structure (e.g., Veal
et al., 2021). Acknowledging this, first considering more exploratory approaches may be
particularly useful in early phases of the measure development process, rather than applying
more confirmatory structures in initial development phases to “prove” that a structure is
sufficient if fit indices indicate acceptable to good fit (Greiff & Heene, 2017; Sellbom &
Tellegen, 2019). We also encourage researchers to consider the application of more exploratory
approaches in cases where relatively little is known about factor structures a priori and/or when
factor structures to be examined are likely to be complex in nature (Greene et al., 2021).
MODEL FIT AND INTERPRETATION 25
For example, the classification of dimensions traditionally defining DSM
neurodevelopmental disorders within dimensional models such as the HiTOP remains unclear in
some ways (Michelini et al., 2019). As a result, specifying more confirmatory models would be
challenging and/or likely would requiring adjudicating amongst a very large number of possible
model configurations. Model fit increasingly has become used when applying more exploratory
factor analytic approaches as well, such that we caution against an overreliance on model fit
regardless of whether more exploratory or confirmatory approaches are used (see Montoya &
Edwards, 2021 for discussion of issues that may arise when using model fit interpretation to
adjudicate amongst exploratory structures).
In addition to being useful for obtaining an initial understanding of indicator structure, use
of more exploratory approaches early in the measure development process can improve
assessment efficiency when developing measures. In our examples with the ADHD and
internalizing item sets, exploratory approaches were useful for identifying items showing
loadings of equal (or roughly equal) magnitudes on multiple factors when examining multifactor
solutions. As described in contemporary measure development guidelines (Clark & Watson,
2019), it can be helpful to examine patterns of loadings across samples, to ensure that pattern
loadings are not due to sample-specific idiosyncrasies. If these factor analytic results for
internalizing and ADHD symptoms were used to guide measure development and were
consistent with results from other samples, items that were not clear indicators of a single factor
may not be included when scoring scales, allowing constructs to be assessed with fewer items.
Lastly, we have stressed the importance of careful scrutiny of item sets. However, we are
not suggesting that researchers generate a theoretical mapping of anticipated dimensions and
adhere to it rigidly when provided with contrary evidence. It could be problematic, for instance,
MODEL FIT AND INTERPRETATION 26
to rigidly adhere to a theoretical model by ignoring important contradictory information provided
by EFA. As an example of an alternative, more appropriate, data-driven approach, Watson et al.
(2012) hypothesized that multiple dimensions assessing different aspects of social anxiety would
emerge as distinct when developing the IDAS-II. However, results across samples indicated that
multiple, well-defined dimensions could not be identified when analyzing social anxiety item
sets, such that the IDAS-II assesses social anxiety using a single scale.
Future Directions, Limitations, and Conclusion
Several limitations and related future directions would be useful for advancing
understanding of these issues related to factor analytic model interpretation. First, we did not
examine other related issues such as how the same items may function differently across sample
types. Examining cross-sample issues such as these has been informative both in measure
development research and other work focused on interpreting the replicability and substantive
nature of factor structures (e.g., the p factor of psychopathology; Greene et al., 2021; Levin-
Aspenson et al., 2021). Considering the relevance of this to our study, analyses involving the PD
ratings included items assessing low-base rate symptoms such as paranoia, such that model fit
and patterns of factor loadings could have varied in other samples (e.g., inpatient samples).
Other extensions of this research also would be interesting. For example, recent research
indicates that the number of items used to assess the criteria for specific DSM-5 disorders
influences model fit and the extent to which criteria defining disorders such as alcohol use
disorder appear unidimensional (Watts et al., 2021). However, our self-report dataset did not
include assessment of the criteria for specific disorders, and our interview data included only
single-item ratings for different PD criteria and no item level data for other diagnoses, which
precluded direct examination of these issues across samples. Fit for all CFA models was good
MODEL FIT AND INTERPRETATION 27
according to relaxed interpretative guidelines (e.g., CFI and TLI ≥ .90) and often exceeded more
stringent cutoffs (e.g., CFI ≥ .960), such that we believe that it is plausible that researchers would
deem these models acceptable in many cases. We again would like to acknowledge debate
regarding the use of stringent versus more relaxed cutoffs (see Hopwood & Donnellan, 2010 for
discussion), and we anticipate continued discussion and investigations into these issues. Future
research in this area also would be useful for determining the utility of different interpretative
cutoffs based on varying study design characteristics (e.g., fit varying based on the number of
items used as reviewed; also see McNeish & Wolf, 2021). We anticipate ongoing debate
regarding the suitability of more exploratory versus confirmatory factor analytic approaches in
specific contexts (e.g., Veal et al., 2021; Hopwood & Donnellan, 2010; Perry et al., 2015). We
agree with sentiments expressed by Hoekstra and Vazire (2021) that an openness to differing
perspectives as evidence accumulates will be useful for improving measure development efforts
and understanding of personality and psychopathology structure.
Acknowledging these future directions, our demonstrations illustrate that care is needed
when interpreting factor analytic results in clinical and personality research. Although model fit
indices provide important information and should not be disregarded, failing to consider other
model characteristics can lead to misinterpretations regarding the nature of item sets. We hope
that researchers will integrate these considerations when conducting factor analytic research and
that other studies will extend investigation on these topics in the ways described.
MODEL FIT AND INTERPRETATION 28