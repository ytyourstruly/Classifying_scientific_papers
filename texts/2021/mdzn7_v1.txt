Delayed lexical access and cascading effects on semantic activation during spoken word
recognition in children with hearing aids and cochlear implants: Evidence from eye-tracking
Kelsey E. Klein, AuD, PhD 1, Elizabeth A. Walker, PhD 2, Bob McMurray, PhD 3
1Department of Audiology and Speech Pathology, University of Tennessee Health Science
Center, Knoxville, TN
2Department of Communication Sciences and Disorders, University of Iowa, Iowa City, IA
3Department of Psychological and Brain Sciences, Department of Communication Sciences and
Disorders, and Department of Otolaryngology, University of Iowa, Iowa City, IA
All correspondence should be addressed to:
Kelsey Klein, AuD, PhD
kklein11@uthsc.edu
865-974-1793
600 Henley St., Office B031
Knoxville, TN 37996
Financial Disclosures/Conflicts of Interest:
The authors have no financial relationships relevant to this article to disclose. This research was
supported by NIH/NIDCD R01 DC0019081 and DC0013591 awarded to EW and Ryan
McCreery; R01 DC008089 and P50 DC000242 awarded to BM; and F30 DC017638, an
Interdisciplinary Research Grant from the University of Iowa DeLTA Center, and a PhD
Scholarship from the Council of Academic Programs in Communication Sciences and Disorders
awarded to KK.
ABSTRACT
Objective: The objective of this study was to characterize the dynamics of real-time lexical
access, including lexical competition among phonologically similar words, and semantic
activation in school-age children with hearing aids (HAs) and children with cochlear implants
(CIs). We hypothesized that developing spoken language via degraded auditory input would lead
children with HAs or CIs to adapt their approach to spoken word recognition, especially by
slowing down lexical access.
Design: Participants were children ages 9-12 years old with normal hearing (NH), HAs, or CIs.
Participants completed a Visual World Paradigm task in which they heard a spoken word and
selected the matching picture from four options. Competitor items were either phonologically
similar, semantically similar, or unrelated to the target word. As the target word unfolded,
children’s fixations to the target word, cohort competitor, rhyme competitor, semantically related
item, and unrelated item were recorded as indices of ongoing lexical and semantic activation.
Results: Children with HAs and children with CIs showed slower fixations to the target, reduced
fixations to the cohort, and increased fixations to the rhyme, relative to children with NH. This
wait-and-see profile was more pronounced in the children with CIs than the children with HAs.
Children with HAs and children with CIs also showed delayed fixations to the semantically
related item, though this delay was attributable to their delay in activating words in general, not
to a distinct semantic source.
Conclusions: Children with HAs and children with CIs showed qualitatively similar patterns of
real-time spoken word recognition. Findings suggest that developing spoken language via
degraded auditory input causes long-term cognitive adaptations to how listeners recognize
spoken words, regardless of the type of hearing device used. Delayed lexical activation directly
led to delayed semantic activation in children with HAs and CIs. This delay in semantic
processing may impact these children’s ability to understand connected speech in everyday life.
INTRODUCTION
Childhood hearing loss (HL) is a common condition, affecting approximately 3% of
children and adolescents in the United States (Mehra et al., 2009). Hearing aids (HAs) and
cochlear implants (CIs) can improve access to speech, but these devices are imperfect: HAs are
poor at transmitting high frequencies (Stelmachowicz et al., 2000) and CIs do not clearly
separate frequencies. Due to their degraded and often inconsistent access to speech signals,
children with any degree of HL are at risk for problems developing spoken language and
listening skills (e.g., Tomblin et al., 2015), especially in higher-level language. Many school-age
children with HL perform within the normative range on standardized measures of vocabulary
and grammar (Halliday et al., 2017; Klein et al., 2017; Lund, 2016; Nittrouer et al., 2020;
Nittrouer et al., 2018; Wie et al., 2020), yet they lag behind their normal-hearing (NH) peers on
more complex tasks like understanding sequential directions, ambiguous sentences, or multi-
sentence stories (Griffin et al., 2020; Lewis et al., 2015; Nittrouer & Lowenstein, 2021; Walker
et al., 2020), and recognizing malapropisms (Lowenstein & Nittrouer, 2021). These complex
aspects of spoken language are likely to be crucial for classroom success (Lowenstein &
Nittrouer, 2021).
To develop effective interventions, it is necessary to pinpoint the underlying cause of
higher-level language deficits in children with HL. One possible explanation is that lower-level
language skills of these children are intact, but their degraded input or inconsistent access limits
their ability to develop more complex language skills. An alternative explanation is that children
with HL experience subtle deficits in lower-level language skills, but these deficits are missed by
most standardized assessments. It is possible that small differences in lower-level skills (such as
a delay in recognizing individual words) compound into greater difficulties in the context of
more complex multi-sentence input.
One such lower-level skill is real-time spoken word recognition. In the guise of accuracy,
word recognition is a useful outcome measure for listeners with HL. However, cognitive science
suggests that even in listeners with NH, word recognition is a complex, cognitively rich process
that unfolds over time (Dahan & Magnuson, 2006). The process of word recognition unfolds
over several hundred milliseconds and affords multiple dimensions in which individuals can vary
over development or across levels of hearing or language ability (cf. McMurray et al., in press).
Thus, examining the real-time processes by which children with HL recognize words may reveal
a deficit that is not detected by standard accuracy measures, and it may reveal how the specific
aspects of these processes differ. If children with HL show similar dynamics of real-time spoken
word recognition as children with NH, it would suggest that the lower-level lexical skills of
children with HL are intact, and children with HL simply struggle to apply those skills in more
demanding language situations. On the other hand, if children with HL differ from children with
NH in how they approach real-time word recognition, these lower-level differences may scale up
to lead to more pronounced higher-level language deficits (Kronenberger & Pisoni, 2019).
The present study used eye-tracking in the Visual World Paradigm (VWP) to characterize
the cognitive mechanisms that children with HL use to resolve competition among phonological
competitors and recognize words. As a model for the notion that lower-level skills could cascade
to affect higher level language, we also ask how differences in these basic word recognition
mechanisms are associated with differences in how quickly children with HL process word-level
semantics.
Lexical Access during Spoken Word Recognition
Even under ideal listening conditions, word recognition is complex. Speech unfolds over
time, raising the problem of temporary ambiguity: at any given moment, a listener likely has not
yet heard the complete word. For example, when a listener hears the word sandal, upon hearing
san- they cannot know if the target word is sandal or sandwich.
Adults with NH address the problem of temporary ambiguity by using immediate
competition and incremental processing. As soon as any auditory input arrives, listeners make
inferences about likely words, immediately activating multiple lexical candidates (e.g., sandal
and sandwich) that are consistent with the signal (san-) up to that point (Allopenna et al., 1998;
Luce & Pisoni, 1998; Marslen-Wilson, 1987; McClelland & Elman, 1986; Zwitserlood, 1989).
The relative activation of these lexical candidates is updated as more auditory input arrives.
Lexical competition is thus a direct result of incremental processing. As more input arrives to
disambiguate the target from other candidates, the listener resolves the lexical competition by
suppressing incorrect competitors (sandwich) as the listener becomes more confident in the
identity of the target word (sandal).
Lexical competition can be seen as a largely passive process that reflects whatever words
are consistent with the auditory input thus far. However, it is far more complex. Listeners
activate rhymes (e.g., sandal and candle) even though they can be ruled out from the first
phoneme (Allopenna et al., 1998; Connine et al., 1993). Further, factors such as word frequency
(Dahan, Magnuson, & Tanenhaus, 2001), neighborhood density (Apfelbaum et al., 2011; Luce &
Pisoni, 1998; Magnuson et al., 2007), and lexical inhibition (Dahan, Magnuson, Tanenhaus, et
al., 2001) also affect the timing and extent to which listeners activate lexical candidates. Finally,
the dynamics of spoken word recognition develop slowly, through adolescence (Rigler et al.,
2015). Together, these findings indicate that spoken word recognition is a complex and flexible
process that is tuned over development to balance efficiency and accuracy (McMurray et al., in
press).
An effective tool for precisely characterizing the time course of lexical competition is the
Visual World Paradigm (VWP; Tanenhaus et al., 1995). In the VWP, participants hear a word
and match it to a picture of its referent on a screen containing multiple pictures. Pictures
instantiate candidates that may compete. For example, for the target word sandal, pictures may
include a sandwich (a cohort competitor) and a candle (a rhyme) along with the target and an
unrelated item. As participants perform this task, they make one or more eye movements to
prepare the response. Critically, participants can make 3-5 fixations per second while word
recognition is unfolding; consequently, the amount that the participant is looking at a particular
picture at a given moment is used as a proxy for the extent to which the participant is activating
the lexical representation of the picture’s label at that moment.
Effects of Degraded Input on Spoken Word Recognition
Researchers have recently begun to ask how signal degradation affects lexical access and
competition. This has identified two profiles of lexical competition that appear across studies.
One profile can be termed sustained competitor activation (Farris-Trimble et al., 2014). This
profile is characterized by increased competitor activation that is sustained over time, often in
combination with small delays in target word activation. Listeners initiate lexical access
immediately upon hearing the start of the word, but then are slow to suppress activation of
competitors because it is possible that one of those competitors may be the target. This approach
may allow for easier later revisions if the initial perception was not correct (Winn & Teece,
2021). The sustained competitor activation profile has been demonstrated in adults with NH
listening to speech in background noise (Ben-David et al., 2011; Brouwer & Bradlow, 2016), 8-
channel noise-vocoded speech (Farris-Trimble et al., 2014), and slightly soft speech
(Hendrickson et al., 2020). It has also been shown in postlingually deaf adults with CIs listening
to speech in quiet (Farris-Trimble et al., 2014; Nagels et al., 2020). All these situations include
only a moderate amount of uncertainty about the speech signal.
The alternative profile has been colloquially termed wait-and-see (McMurray et al.,
2017). In this profile listeners do not process speech as incrementally, and instead wait until
substantial auditory input accrues before activating any candidates. Because of this delay,
listeners appear to activate cohorts less than they would with a clear signal because by the time
lexical competition is underway, more information is available to rule the cohorts out. In
contrast, listeners activate rhyme competitors more than they would with a clear signal. The
wait-and-see profile has been demonstrated by NH adults and children listening to very soft
speech (Hendrickson, Oleson, et al., 2021; Hendrickson et al., 2020), NH adults listening to
highly degraded 4-channel vocoded speech (McMurray et al., 2017), and prelingually deaf
adolescents with CIs (McMurray et al., 2017). All these situations involve highly degraded
listening conditions and high uncertainty about the speech signal. The wait-and-see profile may
be an adaptive approach to reducing perceptual errors, though it comes at the cost of recognizing
words slower. Importantly, wait-and-see represents a substantial departure from the immediate
competition approach that was long thought to be the universal way of resolving lexical
competition.
Children with prelingual HL develop language via a degraded auditory signal. These
children must therefore cope with two sources of uncertainty when recognizing speech: they
must process an auditory signal that is degraded in the moment, and they must compare that
input to phonological and lexical representations that are built upon years of degraded input. It is
not clear how these jointly contribute to the wait-and-see profile of lexical competition that has
been observed in children who use CIs. Investigating these processes in children with more
residual hearing (e.g., those who use HAs) may help to clarify this issue.
On the one hand, the sensory and lexical-representation problems may compound, such
that a moderate signal degradation combines with poorer representations built up over
development to lead to difficulties with spoken word recognition. McMurray et al. (2017) found
that prelingually deaf adolescents (ages 12-25 years) with CIs exhibited a wait-and-see profile,
consistent with earlier work showing that children with CIs as young as 2 years old also show
delayed word recognition (Grieco-Calub et al., 2009). Farris-Trimble et al. (2014) found, in a
comparable task to McMurray et al., that postlingually deaf adults with CIs showed a sustained
competitor activation approach to spoken word recognition. The fact that prelingually and
postlingually deaf CI users showed differences in their lexical activation dynamics despite
experiencing what is likely similar in-the-moment degradation suggests that developing language
via a HL may fundamentally alter the listener’s cognitive approach to recognizing words.
On the other hand, the wait-and-see approach may simply be due to poor perceptual
acuity. In the studies mentioned above, the listeners who showed wait-and-see also had relatively
poor word recognition accuracy in the task. The prelingually deaf CI users averaged 88.5%
correct and NH adults listening to 4-channel vocoded speech averaged 81.7% correct, whereas
the NH adults listening to a clear signal averaged over 99% correct (McMurray et al., 2017). In
contrast, the postlingually deaf CI users and NH adults listening to 8-channel vocoded speech
(i.e., those who showed sustained competitor activation) averaged 94.8% and 98.4% accuracy,
respectively (Farris-Trimble et al., 2014). Based on this pattern of results, the wait-and-see and
sustained competitor activation approaches may simply represent the predictable effects of a
lexical system confronted with degraded input that is not easily recognizable.
Children who use HAs—who experience less degraded input than listeners with CIs—
may help disentangle these hypotheses. With appropriately fit HAs there is likely to be less loss
of spectral and temporal detail, therefore providing a substantially better signal than a CI. If
children with HAs show high accuracy but a wait-and-see profile, this would suggest that this
profile can in part arise from long-term experience with a moderately poor signal, not just a
severely degraded signal. On the other hand, if wait-and-see is entirely due to the periphery, we
might expect the less extreme sustained activation profile in these listeners.
Semantic Activation during Spoken Word Recognition
At the broadest level, it is unknown whether differences in higher-level language seen in
children with HL may derive from differences in lower-level skills, like word recognition. As a
first step in addressing this, we examine semantic processing. Ultimately, a goal of word
recognition is to activate the semantics of the speech, so the listener can understand the meaning
of what is being said. Thus, one way to examine the downstream consequences of differences in
lexical competition resolution is to observe their effects on semantics.
Lexical competition and semantic activation co-occur through a process of cascading
activation (Apfelbaum et al., 2011; Moss et al., 1997; Zwitserlood, 1989). Activation of semantic
information occurs as soon as any degree of lexical activation has occurred, and lexical
competition need not be resolved before semantic activation begins. That is, the first stages of
comprehending speech (i.e., semantic activation) occur while lexical competition is being
resolved. For example, at the time that a listener has heard san-, the word forms of both sandal
and sandwich are active. The listener also activates semantic features of the lexical candidates
(e.g., “worn on feet,” a semantic feature of sandal and “is edible,” a semantic feature of
sandwich).
VWP studies have generally approached this by using items that are semantically related
to the target word (e.g., after hearing sandal, examining looks to the foot). A number of studies
have shown that semantic activation is graded based on the degree of semantic overlap between
two words (Huettig & Altmann, 2005; Mirman & Magnuson, 2009). This process of cascading
activation begins to develop as early as 2 years old (Mani et al., 2012), though the ability to
resolve lexical and semantic competition continues to develop throughout childhood (Huang &
Snedeker, 2011). The VWP has also confirmed that lexical competition and semantic activation
occur in a cascading process, showing, for example, that phonological density affects the time
course of fixations to semantically related words (Apfelbaum et al., 2011; Huettig & McQueen,
2007; Yee & Sedivy, 2006). Thus, examining real-time semantic activation can be a useful way
to assess the consequences of differences in word recognition for downstream processes.
Indeed, signal degradation can affect the extent to which listeners utilize semantic
information. Using a VWP task, Sajin and Connine (2014) found that when adults listened to
words in background babble, the semantic richness of a word (i.e., the number of semantic
features the word had) facilitated fast word recognition more than when listening in quiet.
Additionally, van der Feest et al. (2019) used a VWP task to show that for NH adults listening to
sentences presented in background noise, semantic context in the carrier sentence only facilitated
looks to the target item when the sentence was presented using a clear speaking style; no effect
of semantic context was evident when a conversational speaking style was used. When listening
in quiet, however, the addition of semantic context increased looks to the target item regardless
of the speaking style used. Finally, Nagels et al. (2020) found that when listening to sentences in
a VWP task, adults with CIs looked slower but overall more to a semantically related item,
relative to listeners with NH. Together, these results indicate that listeners may rely on available
semantic information when the auditory signal is degraded, but the extent to which listeners are
able to utilize semantics may depend on how difficult the listening conditions are.
Critically, however, this issue has received little attention among populations with early-
onset HL. Consequently, it is not clear the degree to which learning language under adverse
conditions may affect a listener’s real-time ability to access semantic information. One
possibility is that effects on semantic processing are completely gated by poor input – that is,
poor input delays the resolution of phonological competitors, which in turn alters semantic
activation (via this continuous cascade). In this view, children with HL may not have true
semantic deficits at all, but instead any differences in semantic activation are simply a product of
their poorer word recognition abilities.
However, it is possible that adverse development leads to distinct semantic deficits that
cannot be accounted for by phonological-level word recognition. When children with HL
develop language via inconsistent auditory input, they may hear words used in fewer semantic
contexts than children with NH (Benitez-Barrera et al., 2018). This may lead to weaker semantic
connections between words, or between words and their sematic features (Löfkvist et al., 2012;
Lund & Dinsmoor, 2016; Wechsler-Kashi et al., 2014). Some studies using picture-word priming
tasks have suggested that children with HL may have deficits in their lexical-semantic network
organization, though the findings are mixed (de Hoog et al., 2015; Jerger et al., 2002; Jerger et
al., 2013). However, these priming studies required children to name words aloud; thus, it is not
clear if these semantic deficits extend to speech comprehension, which is arguably a more
challenging domain for children with HL.
Thus, an investigation of semantic effects in spoken word recognition could help resolve
these questions. Critically, we can ask first if there are semantic processing differences in
children with HL. We then ask if these differences are observed after accounting for differences
in resolution of phonologically driven lexical competition. A comparison of children using HAs
and CIs can help clarify these questions, given the large differences in the quality of the
perceptual input experienced by these two groups.
Current Study
To date, research on real-time word recognition in prelingually deaf children with HL has
focused on CI users. It remains unknown whether children with HAs adapt lexical access
mechanisms in response to learning language via what can be considered a moderately degraded
signal. It may be the case that developing language via any degree of signal degradation leads to
poorly defined phonological representations, causing children with HAs to process speech less
incrementally and show a similar wait-and-see approach as prelingually deaf adolescents with
CIs (McMurray et al., 2017; Walker, Kessler, et al., 2019). On the other hand, children with HAs
may show lexical competition characterized by slightly delayed target word activation with
sustained competitor activation (Farris-Trimble et al., 2014). One goal of the present study was
to characterize the cognitive mechanisms that children with HAs use to recognize spoken words,
in comparison to children with CIs and children with NH. This will inform our theoretical
understanding of the effects of in-the-moment versus long-term signal degradation on spoken
word recognition, as well as inform our knowledge about potential cognitive mechanisms
underlying persistent language difficulties in this clinical population.
Previous research on real-time spoken word recognition in children with HL is
additionally limited by sample characteristics. The adolescent CI users in McMurray et al. (2017)
represented a wide chronological age range (12-25 years old) and age at implantation range (1.5
to 7.5 years old; mean = 4 years). It is unclear if the findings from McMurray et al. remain
applicable to children with severe to profound HL who receive the current standard of audiologic
care (CI by age 2 years; Yoshinaga-Itano et al., 2018). It is possible that the wait-and-see profile
previously shown by adolescent CI users reflects prolonged early auditory deprivation, rather
than developing language via a degraded signal. In this study, we addressed this question by
investigating spoken word recognition in children with CIs who were implanted at an early age.
A final goal of the present study was to characterize the time course of semantic
activation during spoken word recognition in children with HAs and children with CIs as a way
to investigate the downstream consequences of poorer real-time spoken word recognition, and to
determine if there are true semantic deficits in children with HL. We examined semantic
activation while children with HAs and children with CIs recognized spoken words to clarify the
extent to which these children appreciate semantic similarities between words while a speech
signal unfolds. We also examined the extent to which differences in the time course of semantic
activation are due to differences in lexical access, rather than representing true differences in
semantic processing.
METHOD
Participants
Data from 68 children (25 with NH, 24 with HAs, and 19 with CIs) were included in this
study. Mean age was 11.0 years (SD = 0.89) for the children with NH, 11.0 years (SD = 0.94) for
the children with HAs, and 10.8 years (SD = 0.79) for the children with CIs. Age did not
significantly differ between the groups, F(2,65) = 0.292, p = .748. Females comprised 15 (60%)
of the children with NH, 12 (50%) of the children with HAs, and 15 (78.9%) of the children with
CIs; the rest were male. Maternal education (based on parent-reported education level) did not
differ significantly between groups, F(2,64) = 0.05, p = .95 (not available for one child with
All children with HAs had permanent bilateral sensorineural (n = 21) or mixed (n = 3)
hearing loss and used bilateral behind-the-ear HAs (n = 23) or receiver-in-the-canal HAs (n = 1).
Degree of HL ranged from mild to moderately severe. All children with CIs had permanent
bilateral sensorineural hearing loss and used either bilateral CIs (n = 18) or a unilateral CI with a
contralateral HA (n = 1). All children with CIs received a CI by 48 months (M = 19.8).
Children were invited to participate in the study if they had at least one caregiver who
primarily used spoken English, had vision within normal limits (with correction, if necessary),
did not have a diagnosed disability affecting cognition or language (other than HL, if applicable),
and were fluent in no languages other than English. Children with HAs and CIs were not invited
to participate if the HL was unilateral, the child relied on manual communication in most
settings, or HL onset was after 18 months of age1.
Two children with CIs were excluded after testing due to eye disorders (glaucoma,
cataract, and/or spontaneous nystagmus) that led to very poor calibration of the eye-tracker. One
child with CIs was excluded because of a diagnosis of Autism Spectrum Disorder and because
they scored below the normative range on nonverbal cognition. No children with NH or HAs
were excluded after completing testing.
General Procedure
Several standardized assessments were administered to characterize the listening,
language, and cognition skills of the participants in each group. These tasks included a hearing
1 Note that children whose formal diagnosis occurred after 18 months were retained if their audiologist suspected
that the HL was present prior to 18 months.
assessment; the Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence,
Second Edition (WASI-II; Wechsler & Hsiao-pin, 2011) to assess nonverbal cognition; the
Peabody Picture Vocabulary Test, Fourth Edition (PPVT; Dunn & Dunn, 2007) to assess
receptive vocabulary; and the Bamford-Kowal-Bench Speech-in-Noise (BKB-SIN) sentence
recognition task (Etymotic Research, 2005) to assess speech recognition in noise. The main
experimental task was an auditory VWP task, which assessed real-time spoken word recognition.
Participants also completed a nonverbal VWP task, which assessed the dynamics of visual
processing in the absence of auditory of verbal stimuli. Participants either completed the test
battery in a mobile testing unit or in a quiet testing room. All study procedures were approved by
the appropriate Institutional Review Board.
Standardized Assessments
Participants were administered the following measures via spoken English by a trained
researcher.
Audiological assessment. Children with HAs completed air conduction threshold testing
from 250 to 8000 Hz and bone-conduction thresholds at octave frequencies from 500 to 4000 Hz
in both ears. Children with NH completed a hearing screening at 20 dB HL at 500, 1000, 2000,
and 4000 Hz at the start of the research visit. They were required to pass in both ears at all
frequencies. No children with NH were excluded based on this criterion.
Nonverbal cognition. Most children (16/19 children with NH, 22/24 children with HAs,
and 15/19 children with CIs) completed the Matrix Reasoning subtest of the WASI-II. This
measure comprises a series of progressively more difficult pattern completion items; children are
shown a pattern with a piece missing and must choose the correct piece from five possible
alternatives. Children were excluded if they scored 1.5 SD below the normative mean. Some
children did not complete this assessment due to time constraints. Two children with HAs did not
complete Matrix Reasoning at the research visit, but they had performed within the normal range
on this measure at a previous research visit. For the rest of the children who did not complete
Matrix Reasoning, parents reported no concern about the child’s cognitive development. One
child with CIs was excluded because he scored more than 1.5 SD below the normative mean.
Receptive vocabulary. Receptive vocabulary was measured with the PPVT. In this
assessment the examiner says a word that describes one of the pictures on a page, and the
participant identifies the correct picture. A standard score of 100 represents average performance
(SD = 15). PPVT data were not available for one child with NH. PPVT standard scores differed
using Tukey’s Honestly Significant Difference (HSD) test showed that children with CIs had
significantly lower vocabulary scores than children with NH (M =112.5, M =93.3, adjusted p
NH CI
< .001) and children with HAs (M =106.8, adjusted p = .005), while the vocabulary scores of
HA
children with NH and children with HAs were not significantly different (adjusted p = .32).
Speech recognition in noise. All children completed the BKB-SIN as a measure of
speech recognition ability in noise. In this task, participants repeated back sentences presented at
65 dBA from a loudspeaker at 0º azimuth. The sentences were accompanied by four-talker
babble that became increasingly louder with each sentence. The outcome measure was the
signal-to-noise ratio at which participants would be expected to correctly repeat back 50% of the
target words (SNR-50). Accuracy was scored live by a trained experimenter, and participants
were asked to repeat themselves if the experimenter was unsure of what a participant said.
Participants each completed one list pair for a total of 20 sentences. List pairs were
counterbalanced across participants. Sentence repetition in noise differed significantly between
better) SNR-50 scores than the HA and CI groups, and the HA group had significantly lower
scores than the CI group (all adjusted p-values < .001).
NH Group HA Group CI Group
Variable
(n = 25) (n = 24) (n = 19)
Age (years) 11.0 (0.89) 11.0 (0.94) 10.8 (0.79)
Sex (% female) 60% 50% 78.9%
Maternal Education
16.0 (2.15) 16.3 (2.29) 16.1 (2.70)
(years)
Age at HL Identification
- 2.04 (9.79) 4.95 (9.63)
(months)
st
Age at 1 HA (months) - 6.02 (9.49) 7.63 (7.41)
st
Age at 1 CI (months) - - 19.8 (11.6)
Better-Ear Unaided PTA
< 20 48.9 (9.90) -
(dB HL)
PPVT Standard Score 112.5 (13.7) 106.8 (12.6) 93.3 (14.2)
BKB-SIN SNR-50 -2.88 (2.14) 0.12 (2.52) 4.29 (3.13)
Note: Values are entered as M (SD), unless otherwise specified. Maternal education was not
available for one child with CIs. NH = normal hearing, HA = hearing aid, CI = cochlear implant,
HL = hearing loss, PTA = pure-tone average, PPVT = Peabody Picture Vocabulary Test, BKB-
SIN = Bamford-Kowal-Bench Speech-in-Noise Test, SNR = signal-to-noise ratio.
VWP for Spoken Word Recognition
Design. We identified word pairs (target + competitor) that captured three types of
relationships between the words: cohorts, in which the words began with the same phonemes
(e.g., sandal and sandwich); rhymes, in which the words ended with the same phonemes (e.g.,
wizard and lizard); and semantics, in which the two words shared semantic features (e.g., apple
and lemon). There were 30 pairs of each type, leading to 180 words.
Individual trials were constructed using a “pairs-of-pairs” VWP design, similar to
Hendrickson, Apfelbaum, et al. (2021). Each trial consisted of the items from two pairs, which
were not related to each other. Consequently, depending on the auditory stimulus heard, one pair
served as the target + competitor and the other pair served as two unrelated items. For example,
if sandal/sandwich (a cohort pair) was paired with wizard/lizard (rhymes), when sandal was the
auditory stimulus, then sandwich was the cohort, and wizard and lizard served as unrelated items
on this cohort trial. However, when wizard was the auditory stimulus, then lizard was the rhyme,
and sandal and sandwich served as unrelated items on this rhyme trial. This pairs-of-pairs design
is highly efficient as all trials could be included in the analyses (there are no truly unrelated
words).
To minimize the role of any unforeseen phonological, semantic, or visual similarities
between the pair-of-pairs (as one pair was intended to be unrelated to the other pair), three
versions of the four-item sets were created. For example, wizard and lizard (rhymes) were
matched with trombone and guitar (semantics) in Version A, with baseball and soccer
(semantics) in Version B, and with market and marble (cohorts) in Version C. Pairs of the same
type (e.g., two rhyme pairs) were never matched together, and a given pair was never matched
with the same type of pair in all three versions (e.g., a cohort pair was not matched with a rhyme
pair in all three versions). Each participant completed either Version A, B, or C (randomly
assigned within participant group).
Each trial was either a cohort, rhyme, or semantic trial; only one type of competitor
roles on a cohort trial. Across trials, each word in an item set served as the target twice. This
prevented participants from guessing which word would be the target on a given trial by
mentally eliminating words that had already served as targets, before hearing the auditory
stimulus. Each trial used a unique exemplar of the auditory stimulus so that participants could
not utilize idiosyncrasies of a given auditory exemplar to help identify an item on its second
presentation. This led to 45 sets (pairs of pairs) ´ 4 words/set ´ 2 repetitions for 360 total trials
(randomized).
bubble indicates the auditory stimulus (i.e., the target word). Italicized words indicate picture
labels, and red words indicate the role the item plays in the trial. Note that no written words were
actually present on the screen during the VWP task.
Trial order was randomized for each participant. The spatial locations of the item types
were counterbalanced across trials so that each item type (i.e., target, cohort, rhyme, semantically
related item, and unrelated items) occurred in each screen location (i.e., top left, top right, bottom
left, bottom right) approximately the same number of times for each participant. Presentation of
the cohort, rhyme, and semantic trials was interleaved.
Item selection. Item selection balanced on several factors. First, all items were bisyllabic.
In studies of spoken word recognition, cohort effects and semantic priming are robust and easy to
detect. However, rhyme effects are smaller and observed less consistently than cohort effects
(McQueen & Huettig, 2012). Furthermore, larger rhyme effects are often observed for bisyllabic
than monosyllabic words (Hendrickson et al., 2020; Simmons & Magnuson, 2018) and often not
observed at all for monosyllabic words (Hendrickson, Apfelbaum, et al., 2021). Thus, to
maximize the likelihood that rhyme effects would be observed, all items in this study were
bisyllabic.
Second, all items were in the expected vocabulary of all children in this study. Most
items appeared in the Children’s Printed Word Database (Masterson et al., 2010), an online
database of words in books targeted at children ages 5 to 9. Other items did not appear in the
Children’s Printed Word Database but were found in the Child Corpus Calculator (Storkel &
Hoover, 2010). Four items did not appear in the Children’s Printed Word Database or the Child
Corpus Calculator, but close variants of the items did appear in one of these databases (e.g.,
sandal was not found, but sandals was found in the Children’s Printed Word Database). In these
cases, lexical characteristics of the close variant were used. Twelve items were found in neither
the Children’s Printed Word Database nor the Child Corpus Calculator, but were kept in the
study because it was intuitively expected that even 9-year-olds with below-average vocabularies
item pairs and item characteristics.
Semantically related item pairs were primarily drawn from the McRae et al. (2005)
database, which provides shared feature norms for word pairs. Possible semantic pairs were
excluded if they shared an initial phoneme or had more than one shared phoneme in the same
position in both items. Pairs were chosen to represent a variety of semantic categories (e.g.,
animals, tools, food). To increase the diversity of semantic categories represented in the items,
five pairs were drawn from De Deyne et al. (2008) and three were drawn from Vinson and
Vigliocco (2008). Both these databases determined semantic similarity based on shared features.
Cohort and rhyme pairs were chosen from similar semantic categories as the items used
in the semantic pairs, though phonological competitors never came from the same semantic
category. Pairs were chosen so that the two words shared at least three phonemes, except for
words that only contained three total phonemes (e.g., wire-fire). Two-tailed two-sample t-tests
indicated that cohort and rhyme pairs did not differ significantly on the average number of
shared phonemes per pair, t(58) = -0.67, p = .507, and cohort pairs had more distinct phonemes
type of pair (i.e., cohort, rhyme, or semantic). Because lexical frequency is calculated differently
in the Children’s Printed Word Database and the Child Corpus Calculator, only the words
appearing in the Children’s Printed Word Database were included in this comparison (n = 49
cohort items, n = 53 rhyme items, n = 51 semantic items). Lexical frequency did not significantly
differ across the three competitor types, F(2,150) = 1.19, p = .307. Neighborhood density was
calculated as the number of phonological neighbors in the Children’s Printed Word Database.
Neighborhood density was significantly different between the three competitor types, F(2,148) =
24.07, p < .001. Post-hoc comparisons using Tukey’s HSD test showed that rhymes had
significantly higher density than both cohort (adjusted p < .001) and semantic items (adjusted p <
.001), which did not differ from each other (adjusted p = .126).
Test
Cohort Rhyme Semantic p
Statistic
M 81.43 68.70 50.82
Lexical
F = 1.19 0.307
Frequency
SD 121.74 80.32 94.25
M 2.21 4.68 1.16
Neighborhood
F = 24.07 < .001
Density
SD 2.44 3.52 1.53
M 3.10 3.20 -
Shared
t = -0.67 0.507
Phonemes/Pair
SD 0.48 0.66 -
M 1.75 1.10 -
Distinct
t = 5.02 < .00001
Phonemes/Pair
SD 0.67 0.24 -
M 529.9 504.1 549.6
Stimulus
F = 5.73 < .01
Duration (ms)
SD 111.3 89.2 110.9
Note: Lexical frequency (frequency/million) and neighborhood density were calculated based on
the Children’s Printed Word Database (Masterson et al., 2010).
Visual stimuli. Visual stimuli were developed with a standard lab protocol (e.g.,
McMurray et al., 2010). Several images were chosen from a commercial clipart database to
represent each word. The images were then viewed and discussed by a focus group of lab
members, and the most prototypical depiction of each word was selected by consensus. Each
selected image was then edited to remove extraneous details, use a more prototypical color or
orientation, and maintain stylistic similarity with other pictures. Care was also taken to minimize
the visual similarity of items that would appear together on VWP trials to ensure that looking
behavior was driven primarily by phonological overlap and semantic priming, rather than visual
similarity. Each final, edited picture was approved by a senior lab member with extensive
experience working with the VWP.
Auditory stimuli. Each target word was recorded at a sampling rate of 44.1 kHz by a
native English-speaking female adult with a Midwestern dialect. Words were recorded at the end
of a neutral sentence context to ensure consistent intonation across exemplars (e.g., He said
apple). The speaker included a brief pause before saying each target word to reduce
coarticulation. At least five exemplars of each target word were recorded. Exemplars were then
digitally extracted from the sentence context, and the best two exemplars for each item were
chosen. Exemplars were manually edited to reduce background noise and remove unnecessary
clicks, thuds, releases of air, etc., from the recordings. Fifty ms of silence was added to the
beginning and end of each recording.
Auditory stimuli in the three types of pairs (cohort, rhyme, and semantic) differed
Tukey’s HSD test showed that cohort stimuli duration did not significantly differ from the
durations of rhyme or semantic stimuli; semantic stimuli were significantly longer than rhyme
stimuli, p = .002.
Procedure. Participants sat in front of a 17” (1280 ´ 1024 pixel) computer monitor.
Before beginning the VWP task, each participant completed a familiarization task so they knew
which image would correspond to each target word during the experiment. In this task, each
image that would be used during the VWP appeared one-by-one in the center of the computer
screen, and the label for each item was written above the image. Participants were instructed to
pay attention to the images as the experimenter read the label for each item aloud. For each item,
participants were instructed to say “yes” if they knew what the word meant and “no” or “I don’t
know” if they did not know what the word meant. When participants indicated that they did not
know what a word meant, the experimenter gave a short explanation of the word. The
experimenter noted which words were unknown to each participant, if any.
Next, participants completed six practice trials to familiarize themselves with the VWP
task prior to starting the experiment. Practice trials included auditory and visual stimuli used in
the main experiment, but items were not shown in the same sets as in the main experiment.
During the practice trials, participants could ask the experimenter to increase or decrease the
sound level of the auditory stimuli to achieve a comfortable level.
Participants next began the primary experimental trials. On each trial, a picture was
presented in each of the four corners of a computer screen, with a red dot in the center of the
screen. Each picture was 300 × 300 pixels and located 50 pixels away from the edge of the
screen, vertically and horizontally. After 500 ms the center dot turned blue, at which time the
participant clicked on the dot to initiate the auditory stimulus. Then, the label for one picture was
presented at 70 dBA over two speakers positioned directly to the left and right of the computer
monitor, and the participant clicked on the corresponding image. Trials were grouped into 10
blocks of 36 trials with a drift correction procedure after every block. Participants were permitted
to take a short break between each block.
Nonverbal VWP Task
Participants completed a nonverbal, completely visual analog to the auditory VWP task
to estimate the dynamics of visual processing (i.e., overall looking behavior) in the absence of
auditory or verbal stimuli. This task was completed immediately after the auditory VWP task,
and it included 192 trials, split into 6 blocks of 32 trials. Four pictures of varying shapes and
colors appeared on the screen, with one picture per corner. Participants clicked on a blue dot in
the center of the screen to initiate each trial. Then, a target shape appeared in the center of the
screen for 100 ms. Participants clicked on the picture in a corner that exactly matched the target
shape. The four alternatives always included one shape that matched the target in shape and
color, one shape that matched the target in color but not shape, and two unrelated shapes that
matched the target in neither shape nor color. The two unrelated items were always the same
color as each other, so every trial included two pairs of color-matched shapes. This task was
similar to that used by Farris-Trimble and McMurray (2013), with the exception that Farris-
Trimble and McMurray used basic shapes (e.g., circles and triangles), and the present study used
more complex shapes with non-primary colors (e.g., burgundy, lavender) to reduce the
possibility of participants internally naming the shapes and colors during the task. The sets of
four pictures were presented in a random order for each participant.
Eye-Tracking Recording and Data Processing
Data Processing. During both eye-tracking tasks, eye gaze was recorded by an EyeLink
1000 Plus desktop-mounted remote eye-tracker at a sampling rate of 250 Hz. Participants used a
chin rest, with height adjusted for comfort. Eye gaze was calibrated using a standard 9-point
calibration procedure. Participants completed a drift correction every 36 trials (in the auditory
task) or every 32 trials (in the nonverbal task) to account for natural eye drift over time. If
participants did not pass the drift correct, the full calibration procedure was repeated.
In the VWP tasks, both eyes were tracked if possible, but only the data from one eye
were used for analysis. The eye used for analysis was chosen based on which eye had better
calibration and/or more samples available. Both the pupil and corneal reflection were used to
determine fixation position.
EyelinkAnal (McMurray, 2019) was used to pre-process the eye-tracking data. During
analysis, eye movements were classified as saccades, fixations, and blinks; saccades and
subsequent fixations were grouped into a single “look” which began at saccade onset and ended
at fixation offset. When determining the item to which a look was directed, the ports of each area
of interest (i.e., the locations of the four items on the screen) were increased by 100 pixels both
horizontally and vertically to account for noise in the eye-tracker. This did not result in any
overlap between the four ports.
Data were processed from 0 to 2000 ms. The start of this time window corresponded to
the time at which the participant initiated presentation of the auditory stimulus via mouse click.
Fixations launched prior to this time window were ignored. Furthermore, eye movements during
the first 250 ms of each trial were not analyzed because 1) the first 50 ms of each trial was
silence, and 2) it takes about 200 ms to plan and launch an eye movement (Matin et al., 1993).
Thus, any eye movements within the first 250 ms of a trial are due to random looking behavior,
rather than information in the auditory signal. On trials in which the participant responded before
2000 ms, the location of their final fixation was extended over the rest of the 2000 ms time
window. On trials in which the participant had a response time of greater than 2000 ms, eye
movements after the 2000 ms mark were ignored.
Trial Exclusions. VWP trials were excluded from analysis for three reasons: 1) the
subject chose the incorrect item, 2) the child did not know a word that appeared in the trial, and
3) the child had an atypically long response time. The VWP is intended to measure the time
course of recognizing known words. For this reason, trials containing words that children did not
know (either as a target or a competitor) were excluded. A word was considered unfamiliar based
on the child’s self-report during the familiarization task. All children were familiar with at least
86% (155/180) of the words. Fourteen children with NH (56% of the group), 14 children with
HAs (58.3% of the group), and four children with CIs (21.1% of the group) were familiar with
with at least one unfamiliar word (on a subject-by-subject basis) led to an average of 7.04 (of 360
trials, SD = 12.5) trials excluded for the NH group, 7.67 (SD = 12.1) excluded for the HA group,
and 43.4 (SD = 46.2) for the CI group.
NH Group HA Group CI Group F Statistic p-value
M 0.92 1.00 6.00
Number of Words
SD 1.73 1.59 6.74 11.93 <.001
Unfamiliar
Range 0-8 0-5 0-25
Number of Trials M 1.12 4.33 7.05
with Response SD 1.48 10.1 10.9 2.77 .07
Time > 5 seconds Range 0-5 0-45 0-45
Note: NH = normal hearing, HA = hearing aid, CI = cochlear implant
Response time was measured on each VWP trial. The zero timepoint corresponded to the
time at which the child initiated the trial via mouse click, which was 50 ms prior to the onset of
the auditory stimulus. On some trials, children had particularly long response times, likely due to
being off task or inattentive. Trials with a response time of greater than 5 seconds were excluded
After excluding trials as described above, all further analyses included a mean of 349.2
trials (SD = 13.1, range = 298 to 360) for the NH group, a mean of 342.9 trials (SD = 19.0, range
= 280 to 359) for the HA group, and a mean of 303.0 trials (SD = 49.9, range = 183 to 354) for
the CI group, out of a total of 360 trials that were presented to each participant. The number of
trials differed significantly between groups, F(2,65) = 14.77, p < .001, driven by the fact that
children with CIs had fewer trials than children with NH (adjusted p < .001) and children with
HAs (adjusted p < .001), who did differ from each other (adjusted p = .74). Despite these
differences, at least 50% of trials were retained for every child, which was sufficient for the
VWP analyses.
Analyzing Fixation Time Courses. The proportion of looks to each item type across
VWP trials was calculated every 4 ms from 0 to 2000 ms after the onset of the auditory stimulus.
This was done for each participant in each trial type (cohort, rhyme, semantic). Looks to the
unrelated items were quantified as the mean looks to the two items. We used Bootstrapped
Differences of Timeseries (BDOTS; Seedorff et al., 2018) to compare fixation curves across
competitor types (e.g., cohort vs. unrelated) and groups. BDOTS takes as input any two sets of
timeseries data and determines the periods during which the timeseries significantly differ. The
first step in the BDOTS analyses is to fit a curve for each item type, for each participant (details
on this curvefitting process below). Next, curves are bootstrapped to obtain confidence intervals.
These confidence intervals are then used to compute t-tests at every time point (i.e., every 4 ms).
The autocorrelation among the resulting t-values is computed, and the alpha-value is adjusted
based on family-wise error rate. This corrects for multiple comparisons without being as
conservative as a traditional Bonferroni approach. Finally, the time periods during which the two
curves significantly differed was calculated to determine if and when each group showed
significant cohort, rhyme, and semantic effects.
The first step in implementing BDOTS was to fit each participant’s fixation curves to a
nonlinear function. Logistic models were fit to the curves for target fixations. The logistic curve
has four parameters: a baseline corresponding to the lower asymptote or minimum looks, a peak
corresponding to the upper asymptote or maximum looks, a slope corresponding to the maximum
derivative of the curve, and a crossover corresponding to the time at which the slope occurs.
Competitor fixation curves (i.e., cohorts, rhymes, semantically related items, and unrelated
items) were fit with an asymmetric Gaussian. This function has six free parameters: an onset
baseline corresponding to the initial asymptote, an onset slope corresponding to the rate at which
proportion of looks to the item type increases, a peak corresponding to the maximum proportion
of looks, a peak location corresponding to the time at which the peak occurs, an offset slope
corresponding to the rate at which proportion of looks to the item type decreases after the peak,
and an offset baseline corresponding to the final asymptote.
Functions were fit using a constrained gradient descent algorithm that minimizes the least
squared error between the function and the data, while obeying reasonable constraints (e.g., the
function must be between 0 and 1). Functions were fit using the curvefitting software of
McMurray (2020), and these fits were imported into BDOTS for analysis. For the unrelated
items, separate curves were fit for the items that were included in cohort, rhyme, and semantic
trials. Curvefits for each participant/item-type were compared visually to the participant’s
representing the match between the fit and the data. The values indicate that the curvefits
represented the data well.
Standard
Item Type Mean Minimum Maximum
Deviation
Target .997 .001 .991 .999
Cohort .972 .020 .870 .993
Rhyme .938 .039 .769 .984
Semantic .945 .037 .769 .984
Unrelated (Cohort) .961 .025 .880 .989
Unrelated (Rhyme) .969 .019 .869 .991
Unrelated (Semantic) .968 .018 .903 .992
Note: Looks to unrelated items were fit according to the trial type they appeared in, indicated by
words in parentheses.
For all BDOTs comparisons, any statistically significant findings that occurred within the
first 250 ms of the trial were ignored because the first 50 ms of the trial consisted of silence, and
it takes approximately 200 ms to plan and launch an eye movement. Therefore, any differences
occurring within this early time period represent an artifact of the curvefitting process.
RESULTS
Accuracy and RT on the Auditory VWP
When including only trials in which the child was familiar with all four words, accuracy
of the final mouse click was high for all three groups (M = 99.1%; M = 98.3%; M =
NH HA CI
p = .004. Children with NH were significantly more accurate than children with CIs (adjusted p
= .003). The accuracy of children with HAs did not differ significantly from that of children with
NH (adjusted p = .32) or children with CIs (adjusted p = .10). Notably, both the children with
HAs and CIs had substantially higher accuracy than has previously been shown by postlingually
deaf adults with CIs (94.8%; Farris-Trimble et al., 2014) and prelingually deaf 12- to 25-year
olds with CIs (88.5%; McMurray et al., 2017).
NH Group HA Group CI Group F Statistic p-value
M 99.1% 98.3% 97.1%
Accuracy SD 0.67% 1.34% 3.35% 5.99 .004
Range 97.2-100% 95.3-100% 87.2-100%
M 1509.0 1734.0 1842.5
Response
SD 171.8 220.6 246.1 14.52 <.001
Time (ms)
Range 1255-1882 1363-2402 1431-2439
Note: NH = normal hearing, HA = hearing aid, CI = cochlear implant
Mean response times on the correct trials (after trial exclusions described in the methods)
.001. Post-hoc comparisons showed that children with NH responded significantly faster than
children with HAs (p = .001) and children with CIs (adjusted p < .001), and the response times of
children with HAs and children with CIs did not differ significantly (adjusted p = .23).
Cohort, Rhyme, and Semantic Effects by Group
BDOTS to confirm that participants looked to the meaningful competitors (cohorts, rhymes, and
semantically related items) more than the unrelated items (i.e., that participants showed a cohort,
rhyme, and semantic effect). Each competitor was compared to the unrelated items that appeared
in the corresponding trial types with separate analyses for each hearing group. For each
competitor type, all three groups had time periods during which fixations to the competitor were
tended to be active early (250 ms after trial onset), while rhymes and semantically related items
saw significant fixations much later (between 400-600 ms). All effects were significant
throughout the trial. These findings confirmed that the NH, HA, and CI groups each showed
significant cohort, rhyme, and semantic effects.
hearing aid (HA) group, and c) cochlear implant (CI) group, averaged across trials. Data are
pooled across all three trial-types. Trial onset corresponds to 0 ms. Note that fixations during the
first 250 ms after stimulus onset are set to 0 because trials began with 50 ms of silence, and eye
movements take 200 ms to plan and launch.
from fixations to unrelated items, based on Bootstrapped Differences of Timeseries.
Time Period of
Subject Adjusted Direction of
Trial Type Autocorrelation Significant
Group Alpha Effect
Difference (ms)
Cohort >
Cohort .9843 .0009 250-2000
Unrelated
Unrelated >
332-456
NH Rhyme
Rhyme .9911 .0015
Group Rhyme >
504-2000
Unrelated
Semantic >
Semantic .9911 .0015 460-2000
Unrelated
Cohort >
Cohort .986 .001 250-1696
Unrelated
Unrelated >
352-540
Rhyme
Rhyme .9907 .0015
Rhyme >
HA
592-2000
Unrelated
Group
Unrelated >
308-496
Semantic
Semantic .9896 .0014
Semantic >
528-2000
Unrelated
Cohort >
Cohort .9843 .0010 250-2000
Unrelated
Rhyme >
Rhyme .9935 .0022 428-2000
CI Unrelated
Group Unrelated >
312-544
Semantic
Semantic .9888 .0013
Semantic >
584-2000
Unrelated
Note: Bold values indicate the time periods of the main cohort, rhyme, and semantic effects for
each group. Any statistically significant differences occurring within the first 250 ms were
ignored because they represented an artifact of the curvefitting process, rather than any
meaningful difference in fixations. NH = normal hearing, HA = hearing aid, CI = cochlear
implant
Group Differences in Fixation Time Courses
We next used BDOTS to examine the effect of listener group for each of the competitor
types. Because BDOTS can only compare two timeseries, three BDOTS comparisons were
conducted to compare the NH and HA groups, the NH and CI groups, and the HA and CI groups.
Separate BDOTS analyses were conducted for the target and each competitor type. For the
cohorts, rhymes, and semantically related items, we used a difference of differences analysis to
control for differences in overall looking (estimated by the unrelated fixations). This asked if the
difference between fixations to competitors and the corresponding unrelated items differed
each item type, ignoring the first 250 ms time period. These significant time periods are also
denoted by horizontal bars at the top of each panel in Figures 3-6.
Considering the fixation patterns to all item types, both the HA and CI groups show a
pattern of spoken word recognition consistent with a wait-and-see profile. Children in both the
HA and CI groups were substantially slower to look at the target item relative to the children
delayed by 97.7 ms (based on the average crossover parameter for each group). Further, both the
both the HA and CI groups showed increased fixations to the rhyme relative to the NH group
NH group (M = 0.900, M = 0.846, M = 0.859). Together, the fixation patterns in the HA
NH HA CI
and CI groups are consistent with delayed lexical access and lexical competition, which leads to
decreased cohort activation and increased rhyme activation.
and cochlear implant (CI) groups differ significantly, based on Bootstrapped Differences of
Timeseries
Time Period
Adjusted Direction of of Significant
Comparison Autocorrelation
Alpha Effect Difference
(ms)
Target
NH vs HA 0.9929 .0020 NH > HA 292-2000
NH vs CI .9916 .0017 NH > CI 250-2000
HA > CI 250-1300
HA vs CI .9896 .0014
CI > HA 1388-2000
Cohort
NH > HA 388-768
NH vs HA .9807 .0008 HA > NH 848-1256
NH > HA 1428-2000
NH > CI 324-740
NH vs CI .9776 .0007
CI > NH 788-1532
HA > CI 250-680
HA vs CI .985 .001
CI > HA 752-2000
Rhyme
NH > HA 468-672
NH vs HA .9829 .0009
HA > NH 760-1548
NH vs CI .9903 .0015 CI > NH 376-1972
HA vs CI .9874 .0012 CI > HA 380-1948
Semantic
NH > HA 292-864
NH vs HA .9824 .0008
NH > HA 1872-2000
NH > CI 292-892
NH vs CI .9799 .0008
NH > CI 1776-2000
HA > CI 492-712
HA vs CI .9805 .0008
HA > CI 1708-1824
Note: Any statistically significant differences occurring within the first 250 ms were ignored
because they represented an artifact of the curvefitting process, rather than any meaningful
difference in fixations.
of each panel indicate the time periods during which groups differ significantly based on BDOTS
analyses. The color of the horizontal significance bars indicates which group had higher
proportion of fixations. Panel a) shows the comparisons between the normal hearing (NH) group
and each group with hearing loss, and panel b) shows the comparison between the cochlear
implant (CI) group and the hearing aid (HA) group.
fixations to unrelated items in cohort trials. Horizontal bars at the top of each panel indicate the
time periods during which groups differ significantly based on BDOTs analyses. The color of the
horizontal significance bars indicates which group had higher proportion of fixations. Grey
regions indicate 95% confidence intervals. Panel a) shows the comparisons between the normal
hearing (NH) group and each group with hearing loss, and panel b) shows the comparison
between the cochlear implant (CI) group and the hearing aid (HA) group.
fixations to unrelated items in rhyme trials. Horizontal bars at the top of each panel indicate the
time periods during which groups differ significantly based on BDOTs analyses. The color of the
horizontal significance bars indicates which group had higher proportion of fixations. Grey
regions indicate 95% confidence intervals. Dashed line indicates y = 0. Panel a) shows the
comparisons between the normal hearing (NH) group and each group with hearing loss, and
panel b) shows the comparison between the cochlear implant (CI) group and the hearing aid
(HA) group.
minus curvefits for fixations to unrelated items in semantic trials. Horizontal bars at the top of
each panel indicate the time periods during which groups differ significantly based on BDOTs
analyses. The color of the horizontal significance bars indicates which group had higher
proportion of fixations. Grey regions indicate 95% confidence intervals. Dashed line indicates y
= 0. Panel a) shows the comparisons between the normal hearing (NH) group and each group
with hearing loss, and panel b) shows the comparison between the cochlear implant (CI) group
and the hearing aid (HA) group.
Nonetheless, this pattern was somewhat more pronounced in the CI group than the HA
Moreover, although the CI group was slower to look to the cohort relative to the HA group, peak
accuracy in the task, the CI group seems to show an enhanced wait and see profile relative to the
HA group.
When considering looks to the semantically related item, the HA and CI groups both
the CI group was slower than the HA group to look to the semantically related item, these two
examined the extent to which differences in lexical access and competition can explain group
differences in semantic fixations.
Semantic Activation Patterns while Controlling for Lexical Access and Lexical Competition
Semantic activation occurs downstream from phonological word form recognition: a
listener cannot access the semantics of a word until they have at least partially activated the
corresponding word form. For this reason, slower fixations to a semantically related item may
not reflect a distinctly semantic deficit, but rather could derive from the fact that the listener is
slower to activate lexical candidates and resolve competition from phonologically related items.
We conducted a hierarchical regression to address this. The first level of the model ignored
listener group and examined the extent to which variance in speed of fixations to the
semantically related item is explained by looks to the target and cohort competitor. The next
level of the model then asked whether hearing status explains any additional variance in semantic
fixations over and above the variance explained by phonological competition. If hearing status
explains additional variance, it would suggest that children with HAs and/or CIs may have
underlying differences in real-time semantic activation that cannot be explained by differences in
resolving competition among phonological word forms.
To perform this analysis, we needed to collapse the fixation curves into individual
estimates that could be used as the dependent or independent variables in the regression. Two
aspects of phonological competition (target timing and cohort peak time) were used as the
independent variables. We used speed of fixations to the semantically related item as the
dependent variable because timing was the aspect of the semantic fixation curves that differed
most obviously between the groups.
To control for the resolution of lexical (phonological wordform) competition, we
identified two key measures. First, speed of lexical access was quantified with a target timing
variable (McMurray et al., 2019). This was based on the slope and crossover parameters from
each participant’s target curvefit; these parameters were strongly correlated (r = -.67). Target
slope and crossover were each log-transformed and converted to Z-scores, based on the available
data from all participants. The Z-score for crossover was multiplied by -1 (because a slower
response function is indicated by a larger crossover, but a smaller slope). Finally, these two Z-
scores were averaged to compute the target timing. A larger target timing value indicated faster
looks to the target.
Second, lexical competition was quantified using the speed of fixations to the cohort. The
cohort peak time variable was defined as the time at which the peak fixations to the cohort
occurred. This cohort peak was calculated as the maximum looks to the cohort, minus looks to
unrelated items. Specifically, the cohort peak parameter from the double-Gaussian curvefit was
obtained, and the proportion of looks to the unrelated items (based on the curvefit for unrelated
items) at the same time as the cohort peak was subtracted from the cohort peak.
The semantic timing variable quantified the speed at which participants looked to the
semantically related item. To compute this, we used a procedure based on McMurray et al.
(2008). First, we smoothed the fixations to the semantically related items and unrelated items
using a 48 ms triangular window. Next, looks to the unrelated items were subtracted from the
looks to the semantically related items for each participant. The difference in looks to the two
item types was then normalized for each participant based on the maximum difference in looks.
The semantic timing variable was defined as the first time point at which the participant’s looks
reached 50% of that participant’s maximum semantic looks and stayed above the 50% criterion
for at least 100 ms.
Before including the semantic timing variable as the dependent variable in regression
analyses, we needed to determine whether this variable was sensitive to the group differences
observed in the BDOTS analysis. Because the HA and CI groups showed similar time courses of
semantic fixations, these two groups were collapsed into a single group of children with HL. A
two-tailed two-sample t-test indicated that the HL group (M = 675.1 ms, SD = 203.4) was 86.4
ms slower to look to the semantically related item than the NH group (M = 588.6 ms, SD =
147.8), t(66) = 1.86, p = .068, d = 0.486. Although this group difference did not meet the
significance threshold of p < .05, the semantic timing variable was deemed appropriately
sensitive for use in the regression analyses due to the effect size and the a priori hypothesis that
semantic timing would be affected by the timing of lexical access and competition.
We examined the contributions of phonological variables (target timing and cohort peak
time) and hearing status to the semantic timing variable. We first conducted a linear regression to
predict semantic timing from the two phonological variables. In the R environment (R Core
Team, 2017) this model was entered as the following using the lm command (1):
semantic timing ~ target timing + cohort peak time (1)
On the second level of the model, we added hearing status to determine whether hearing
status explained any variance in semantic timing over and above the variance explained by the
phonological variables. Groups were dummy coded so that NH = 0 and HL = 1. The second level
of the model took the following form (2):
semantic timing ~ target timing + cohort peak time + HLvNH (2)
main effect on semantic timing (p = .002): participants who were faster to look to the target also
looked faster to the semantically related item, relative to participants who were slower to look to
the target. Cohort peak time was not a significant predictor of semantic timing (p = .26).
Together, target timing and cohort peak time explained 10.5% of the variance in semantic
timing.
In the second level model, hearing status (i.e., NH vs. HL) did not explain any additional
variance above the variance explained by the phonological variables (p = .478). This finding
suggests that group differences in looking speed to the semantically related item can primarily be
explained by the fact that children with HL are slower to initiate lexical access and resolve
lexical competition, rather than by any semantic-specific differences between the groups with
NH and HL.
Level 1: r = .105, F(2,65) = 3.81, p = .027
Variable Estimate Standard Error t-value p-value
Intercept 783.5 185.0 4.26 < .001
Target Timing -69.4 25.1 -2.76 .008
Cohort Peak Time -0.224 0.292 -0.767 .446
Level 2: r = .112, F(3,64) = 2.69, p = .054
Variable Estimate Standard Error t-value p-value
Intercept 792.8 185.1 4.28 < .001
Target Timing -58.7 29.3 -2.00 .049
Cohort Peak Time -0.280 0.303 -0.923 .359
Hearing Status 40.8 57.2 0.713 .478
Difference between levels: r change = .007, F(1,64) = 0.509, p = .478
Visual VWP
One concern with the VWP is that differences could derive from more basic changes in
decision making (e.g., speed of processing), visual search, or eye-movement control. To rule this
out, we used the visual VWP task to determine if children with HL differ from children with NH
on these fundamental processes. Data from the visual VWP task were processed similarly to the
spoken word task. Looks to targets were fit with the logistic function, and looks to color
competitor and unrelated shapes were fit with asymmetric-Gaussian function (McMurray, 2020).
Curvefit parameters were obtained for each model for each participant. Parameters were
compared between the NH, HA, and CI groups using one-way ANOVAs. None of the curvefit
parameters from the visual VWP task differed significantly between the groups (all p-values >
(2017). The fact that children with NH, children with HAs, and children with CIs did not
significantly differ on the visual VWP task indicates that any differences observed between the
groups on the auditory VWP task are due to the auditory/lexical nature of the task, rather than
underlying differences in eye movement behavior, visual search, or the dynamics of general
decision making.
DISCUSSION
We used a VWP task to characterize the dynamics of lexical competition and semantic
activation during spoken word recognition in school-age children with HAs, children with CIs,
and children with NH. Relative to the children with NH, both the children with HAs and children
with CIs demonstrated a wait-and-see profile of spoken word recognition, characterized by
delayed looks to the target item, reduced looks to the cohort competitor, and increased looks to
the rhyme competitor. The children with HAs and children with CIs also showed delayed looks
to the semantically related item, an effect that could be attributed to the cascading effects of
delayed lexical access.
Effects of HL on Real-Time Lexical Access and Competition
An emerging body of literature using VWP tasks has suggested that when faced with
degraded auditory input, listeners tend to show either a sustained competitor activation profile or
a wait-and-see profile (Farris-Trimble et al., 2014; Hendrickson et al., 2020; McMurray et al.,
2017; McQueen & Huettig, 2012). These profiles of spoken word recognition have been
explained in terms of adaptation to uncertainty: when the listener has a moderate amount of
uncertainty in the signal, they are slightly slower to initiate lexical access and are slower to
suppress activation of lexical candidates (consistent with sustained competitor activation). When
the listener has a high amount of uncertainty in the signal, they instead wait to initiate lexical
access until substantially more input has accrued, thus reducing the need for lexical competition
(consistent with wait-and-see).
A child with HAs or CIs has two sources of uncertainty when recognizing spoken words.
First, the auditory signal they are listening to in-the-moment is degraded due to the child’s HL
and hearing device signal processing. Second, the child must map the speech signal onto lexical
representations that are built on long-term signal degradation. At the onset of this study, it was
unclear whether children with HAs would show a sustained competitor activation profile due to
their moderately degraded auditory signal, or a wait-and-see profile due to their early and long-
term signal degradation. Because they showed delayed looks to the target, decreased looks to the
cohort, and increased looks to the rhyme in the VWP task, the children with HAs in this study
showed a profile most consistent with wait-and-see.
We also aimed to characterize the dynamics of spoken word recognition in 9- to 12-year
old children with CIs who were implanted at an early age. McMurray et al. (2017) found that 12-
to 25-year-old adolescents with CIs showed a wait-and-see profile. In addition to being older
than the children in the present study, the participants in the McMurray et al. study had a much
later age at implantation (M = 47.9 months) than the children in the present study (M = 19.8
months). Despite these differences between participant samples, the children with CIs in the
current study showed a similar wait-and-see profile to spoken word recognition as the
adolescents with CIs in the McMurray et al. study.
The children with HAs and the children with CIs in this study both showed a general
pattern of looking behavior consistent with the wait-and-see profile of the CI users of McMurray
et al. (2017). However, the results from the children with HL in the present study differ from
those of McMurray et al. in important ways. The first difference is the length of delay in target
looking. In this study, the children with HAs were 60.7 ms slower to look to the target than the
children with NH, and the children with CIs were 97.7 ms slower than the children with NH
(based on target crossover). In McMurray et al., the CI users showed a much longer delay of 236
ms. The longer delay in lexical access found by McMurray et al. could be because the
participants in that study received their CIs much later than the children in the present study.
Prolonged auditory deprivation early in life may permanently affect the speed and/or efficiency
with which listeners can recognize spoken words later in life.
The difference in target delay between the children with HL in this study and the CI users
from McMurray et al. (2017) was unexpected, given the fact that the participants with HL in both
studies showed decreased looks to the cohort. It was previously believed that the decreased
lexical competition seen in the wait-and-see profile is a direct consequence of substantially
delayed target word activation. However, the present findings indicate that decreased
competition can occur even when the delay in target word activation is relatively small. This
suggests that decreased lexical competition is a specific cognitive adaptation to uncertainty,
rather than being solely the natural result of slow lexical access. Indeed, the postlingually deaf
adult CI users in Farris-Trimble et al. (2014) were 74 ms slower to initiate lexical access than the
adults with NH, but they did not show decreased lexical competition.
Notably, the children with HL in this study were highly accurate at recognizing words in
the VWP task, with an average of 98.3% accuracy for the HA group and 97.1% for the CI group.
Both groups were substantially more accurate than the prelingually deaf CI users (M = 88.5%) in
McMurray et al. (2017). Accuracy is generally not emphasized as a metric of performance in the
VWP. However, VWP accuracy is important to consider in conjunction with the VWP time
course data because accuracy can be considered as a measure of in-the-moment uncertainty. If
listeners are highly uncertain about the sounds they are hearing, they will make more errors in
word identification and show lower accuracy. With their high overall accuracy, it is clear that the
VWP task used in this study was quite easy for both the children with HAs and children with
CIs. In other words, they had high certainty about the auditory input. The CI users in McMurray
et al., on the other hand, were less certain about the auditory input, even on the trials in which
they selected the correct picture. The difference in overall accuracy between participants with
HL in the present study and the CI users in McMurray et al. may help explain the difference in
lexical access delay between these studies.
Importantly, the fact that children with HL showed a wait-and-see profile to spoken word
recognition despite showing relatively high certainty about the in-the-moment auditory signal
extends our understanding about the role of uncertainty in spoken word recognition. Previous
research has shown that a wait-and-see profile arises when NH listeners are required to listen to
highly degraded speech (i.e., 4-channel vocoded speech or very soft speech) or when
prelingually deaf CI users listen to speech (Hendrickson et al., 2020; McMurray et al., 2017).
Importantly, NH listeners do not show wait-and-see when listening to moderately degraded
speech (8-channel vocoded speech or slightly soft speech): they only show this pattern when they
are forced to be highly uncertain about the input (81.7% accuracy with 4-channel vocoded
speech), or when they must adapt quickly to a novel stimulus (98.8% to 99.1% accuracy with 40
dBA speech). To our knowledge, this is the first study to show a wait-and-see profile in listeners
who are highly accurate while listening to speech as they normally hear it. The findings indicate
that although some listeners may utilize a wait-and-see profile to cope with in-the-moment signal
degradation, this profile fundamentally represents a cognitive strategy that listeners may utilize
regardless of the quality of the input in-the-moment.
As a whole, this suggests that that the wait-and-see profile shown by the children with
HL in this study is not likely due to their in-the-moment uncertainty, but rather is likely due to
their long-term degraded auditory input. Children who learn spoken language via any degree of
HL must develop phonological representations while coping with multiple sources of
inconsistent auditory access. Many children do not use their hearing devices full time (Walker et
al., 2015; Wiseman & Warner-Czyz, 2018), and all hearing devices degrade the auditory signal
to a certain extent (e.g., due to limited bandwidth and spectral degradation; Moeller & Tomblin,
2015; Stelmachowicz et al., 2001). This prolonged inconsistency, even if relatively mild, may
lead to permanent differences in how children with HL mentally represent sounds and words:
these mental templates for speech sound categories have been proposed to be underspecified
relative to children who develop language via a consistently clear signal (AuBuchon et al., 2015;
McMurray et al., 2017; Pisoni et al., 2008; Wechsler-Kashi et al., 2014). In this study, children
with HAs and children with CIs showed similar overall profiles of spoken word recognition in
the VWP task, despite the fact that CIs typically provide the listener with a much more degraded
signal than HAs. These findings suggest that these children’s long-term experience with auditory
uncertainty, rather than solely their in-the-moment uncertainty, is responsible for their wait-and-
see approach to spoken word recognition. In other words, the wait-and-see profile of children
with early-onset HL is not (only) a strategy they adopt to deal with speech that is difficult to
understand in that moment, but rather a long-term strategy they have adopted in dealing with a
lifetime of auditory uncertainty.
Although the children with HAs and children with CIs in this study showed similar
dynamics of spoken word recognition in the VWP task, the children with CIs showed a more
exaggerated wait-and-see profile than the children with HAs. Relative to the children with HAs,
the children with CIs showed slower looks to the target, decreased looks to the cohort, increased
looks to the rhyme, and slower looks to the semantically related item in the VWP task. Thus,
although the two HL groups showed qualitatively similar profiles of spoken word recognition,
they were quantitatively different. It is possible that the higher signal degradation provided by a
CI than a HA causes a more extreme wait-and-see profile. It is also possible that differences
between the HA and CI groups in terms of language skills, such as vocabulary, are responsible
for the differences between these groups. In this study, the children with CIs had vocabulary
scores that were on average one standard deviation lower than the children with HAs. Previous
work has shown that vocabulary influences children’s word recognition skills, including in VWP
and other experimental paradigms (Borovsky & Peters, 2019; Evans et al., 2018; Klein et al.,
2017; Law et al., 2017; Walker, Kessler, et al., 2019). However, little is known about how
vocabulary influences the real-time word recognition skills of children with HL. Further research
is needed to tease apart the factors affecting the dynamics of spoken word recognition in children
with a range of hearing abilities.
Effects of HL on Real-Time Semantic Activation
All three groups of children showed higher looks to the semantically related item than the
unrelated item. This finding suggests that like the children with NH, the children with HAs and
children with CIs activated the semantics of the target word as they were hearing it. This finding
is consistent with recent work showing that children with HAs and children with CIs use
semantic information to facilitate spoken word recognition (Blomquist et al., 2021; Holt et al.,
2021; Simeon & Grieco-Calub, 2021; Walker, Kessler, et al., 2019).
Compared to children with NH, the children with HAs and children with CIs were slower
to look to the semantically related item. This is likely indicative of delayed semantic activation.
However, delayed semantic activation during spoken word recognition can be a downstream
effect of delayed lexical access. Through the process of cascading activation, the listener does
not access the semantics of a word until after the word form has been at least partially activated
in the mental lexicon (Apfelbaum et al., 2011; Moss et al., 1997; Zwitserlood, 1989). Relative to
children with NH, the mean semantic delays for the HL groups (75.2 ms for the HA group and
100.6 ms for the CI group, based on the semantic timing variable) were similar to the mean
lexical access delays (60.7 ms for the HA group and 97.7 ms for the CI group, based on target
crossover). When considering all children, the time at which participants looked at the
semantically related item (i.e., the semantic timing variable) was directly associated with how
additional variance in semantic timing. This suggests that the delay shown by children with HL
in looking to the semantically related item was due to different speeds of lexical access, rather
than hearing status per se.
On the one hand, it is encouraging that hearing status did not explain unique variance in
semantic timing. Based on the analyses in the present study, there is no evidence for weaker
semantic connections between words in the mental lexicon of children with HAs or children with
CIs. Previous research has suggested children with HL may have weaker semantic connections
between words (Jerger et al., 2002; Walker, Redfern, et al., 2019), though past findings have
been mixed (de Hoog et al., 2015; Jerger et al., 2013). Although it is possible that children with
HL do experience deficits in the quality of their lexical-semantic networks due to their HL, this
was not evident in the VWP task used in this study.
On the other hand, the children with HL did show a delay in looks to the semantically
related item, and this delay can be attributed to differences in speed of lexical access. HL appears
to indirectly contribute to delayed semantic activation, by way of delayed lexical access. The
findings support the idea that the wait-and-see dynamics of resolving word form competition
directly affects access to word meaning as a continuous cascade.
In everyday listening situations, listeners must access word meaning quickly, or risk
falling behind in terms of understanding what is being said (Nation, 2014). Children with as little
as minimal HL show deficits in discourse comprehension, even when they are highly accurate at
repeating back spoken sentences (Griffin et al., 2020; Lewis et al., 2015). It is possible that the
relatively small delay in semantic activation at the single-word level builds up during connected
speech, as words are uttered sequentially. This may cause the listener to fall behind and fail to
retain the meaning of what was said. Concluding that a direct link exists between the real-time
dynamics of individual word recognition and functional understanding of real-world discourse is
beyond the scope of this study. However, future research should examine the potential role of
these single-word processing mechanisms during more complex comprehension tasks, especially
among children with HL. Critically, our work illustrates how a small deficit in low level skills
can cascade to create higher level impairments, even if higher level skills are intact.
Clinical Implications
Although children with HAs and children with CIs achieved high word recognition
accuracy on the VWP task, they showed different patterns of lexical access, lexical competition,
and semantic activation than the children with NH. This suggests that even when children with
HL perform well on clinical speech recognition tasks, we cannot assume that they are processing
speech as quickly or efficiently as children with NH. To ensure the best possible access to
speech, children with HL should continue receiving classroom accommodations, such as remote
microphone technology, as they progress through middle and high school.
The findings of this study have implications for clinical intervention approaches for
children with HL. The fact that the semantic activation delay of children with HL was due to
delayed lexical access, rather than being a semantic-specific delay, suggests that intervention
targeting lexical access speed may in turn speed up semantic activation. In other words, helping
children with HL to recognize words faster should help them to understand the meanings of
those words more automatically. Kapnoula and McMurray (2016) showed that when adult
listeners were required to attend to small phonological differences between words during training
tasks, they were better able to resolve in-the-moment lexical competition. This suggests that the
dynamics of real-time spoken word recognition processes are amenable to intervention. Further
research is needed to understand the effects of word recognition training on the listening
comprehension skills of children with HL.
Limitations and Future Directions
One limitation of this study is the generalizability of the results, due to participant
characteristics. All participants were required to be native English speakers and have no
disabilities known to affect language or cognitive skills. Children with HL were required to have
HL onset prior to 18 months old and rely primarily on spoken language. These inclusion criteria
resulted in relatively homogenous participant groups. Furthermore, the participants in this study
represented a socioeconomically advantaged group. Of the 42 children with HAs or CIs for
whom parent education data were available, all but two (95.2%) had a mother who attended
college and 29 (69.0%) had a mother with at least a bachelor’s degree. For these reasons, we
cannot generalize the findings of this study to children with more diverse language and cognitive
abilities, auditory experience, and socioeconomic backgrounds.
In this study, auditory stimuli consisted of only single words. In everyday life, however,
children must be able to process and understand multi-word sentences and multi-sentence
discourse. It is possible that the relatively small delay in lexical access shown by children with
HL at the single-word level compounds into an even greater delay when these children are
listening to multi-word utterances (but see Smith & McMurray, in press, for evidence against this
hypothesis in postlingually deaf adults with CIs). On the other hand, most sentences contain
semantic and syntactic cues that help the listener predict upcoming words or fill in the blanks of
words and sounds that were missed. Children with HL might use these sentential cues to avoid
falling further behind children with NH while listening to sentences. Future research should
investigate the spoken word recognition dynamics of children with HL while recognizing full
sentences to better understand how these children recognize words under more realistic
conditions.
Finally, it is unclear from this study whether the differences in spoken word recognition
between children with NH and children with HL should be considered a deficit or simply a
difference. It is possible that the wait-and-see profile shown by children with HL represents an
adaptive strategy for coping with uncertainty. If children with HL have difficulty recognizing
individual speech sounds due to fuzzy phonological templates or noise in the auditory signal, the
most effective approach to recognizing the word may indeed be to wait until additional input
arrives before activating lexical candidates. In this case, faster lexical access may actually
impede word recognition and speech understanding if the listener over-activates lexical
candidates that are inconsistent with the speech signal. Examining the association between real-
time dynamics of spoken word recognition and children’s ability to retain meaning from speech
would provide insight into the extent to which the wait-and-see profile of children with HL
represents an effective adaptation or a speech processing deficit.
Conclusion
This study used a VWP task to examine the dynamics of lexical access, lexical
competition, and semantic activation during spoken word recognition in school-age children with
and without HL. Consistent with a wait-and-see profile of spoken word recognition, both
children with HAs and children with CIs showed slower real-time lexical access and reduced
lexical competition while recognizing spoken words, relative to children with NH. This delay in
lexical access directly led to a delay in activating the semantics of the target word. Despite
having access to auditory signals with very different degrees of degradation, the children with
HAs and children with CIs showed remarkably similar profiles of real-time spoken word
recognition. The findings indicate that developing language via inconsistent or degraded input
can permanently alter the dynamics of spoken word recognition, even when a listener has high
certainty about the in-the-moment auditory signal. These findings provide insight into the
mechanisms that may underlie the persistent spoken language deficits seen in children with any
degree of HL.
ACKNOWLEDGMENTS AND AUTHOR CONTRIBUTION STATEMENT
This research was supported by NIH/NIDCD R01 DC0019081 and DC0013591 awarded
to EW and Ryan McCreery; R01 DC008089 and P50 DC000242 awarded to BM; and F30
DC017638, an Interdisciplinary Research Grant from the University of Iowa DeLTA Center, and
a PhD Scholarship from the Council of Academic Programs in Communication Sciences and
Disorders awarded to KK. The content of this project is solely the responsibility of the authors
and does not necessarily represent the official views of the National Institute on Deafness and
Other Communication Disorders or the National Institutes of Health. The authors have no
financial relationships relevant to this article to disclose.
We are grateful for the involvement of the participating children and their families,
without whom this research would not have been possible. We would like to thank Carolyn
Brown, Kristi Hendrickson, and Karla McGregor for helpful comments on an earlier draft of this
manuscript. We also thank Meggie Dallapiazza, Jeff Shymanski, Wendy Fick, and Jamie Klein-
Packard for their support throughout this study.
K.E.K., E.A.W., and B.M. conceived of and designed the study. K.E.K. conducted the
study, analyzed data, and wrote the manuscript. E.A.W. and B.M. provided guidance on
statistical analyses, offered critical revisions, and reviewed the manuscript at all stages.