The Dual Mechanisms of Cognitive Control (DMCC) project:
Validation of an on-line behavioral task battery
1 1 2 2 1*
Rongxiang Tang , Julie M. Bugg , Jean-Paul Snijder , Andrew R. A. Conway , Todd S. Braver
Department of Psychological and Brain Sciences, Washington University in St. Louis
Division of Behavioral & Organizational Sciences, Claremont Graduate University
*Corresponding Author:
Todd S. Braver
Email: tbraver@wustl.edu
Telephone: 314-935-5143
Address: Washington University,
Campus Box 1125,
One Brookings Drive,
St. Louis, Missouri 63130-4899
Abstract
Cognitive control serves a crucial role in human higher mental functions. The Dual Mechanisms
of Control (DMC) account provides a unifying theoretical framework that decomposes cognitive
control into two qualitatively distinct mechanisms – proactive control and reactive control. While
prior behavioral and neuroimaging work has demonstrated the validity of individual tasks in
isolating these two mechanisms of control, there has not been a comprehensive, theoretically-
guided task battery specifically designed to tap into proactive and reactive control across different
domains of cognition. To address this critical limitation and provide useful methodological tools
for future investigations, the Dual Mechanisms of Cognitive Control (DMCC) task battery was
developed to probe these two control modes, as well as their intra-individual and inter-individual
differences, across four prototypical domains of cognition: selective attention, context processing,
multi-tasking, and working memory. We present this task battery, along with detailed descriptions
of the experimental manipulations used to encourage shifts to proactive or reactive control in each
of the four task domains. We rigorously evaluate the group effects of these manipulations in
primary indices of proactive and reactive control, establishing the validity of the DMCC task
battery in providing dissociable yet convergent measures of the two cognitive control modes.
Introduction
There is widespread agreement that the capacity for cognitive control is a central element
of human adaptability, achievement, and flourishing. While this ability to flexibly regulate, update,
and coordinate thoughts and actions in accordance with internally maintained goals is one of
humans’ most cherished higher mental functions, it is also quite vulnerable to impairment, and
even in healthy individuals can vary substantially (Braver, 2012). Importantly, cognitive control
is not a unitary process, as it encompasses a diverse range of mental functions spanning different
domains of cognition (i.e., attention, working memory, and decision-making; Kane & Engle, 2002;
Miller & Cohen; 2001; Miyake et al., 2000). Consequently, a major challenge has been to
characterize and explain cognitive control through a unifying and coherent theoretical framework,
which ideally should provide meaningful and operationalizable core constructs that can account
for both contextual (i.e., state) and individual (i.e., trait) variation in these functions.
The Dual Mechanisms of Control (DMC) account provides a theoretical framework that
decomposes cognitive control into two qualitatively distinct mechanisms – proactive control and
reactive control (Braver et al., 2007; Braver, 2012). Proactive control refers to a sustained and
anticipatory mode of control that is goal-directed, allowing individuals to actively and optimally
configure processing resources prior to the onset of task demands. Reactive control, by contrast,
involves a transient mode of control that is stimulus-driven, and relies upon retrieval of task goals
and the rapid mobilization of processing resources following the onset of a cognitively demanding
event (Braver et al., 2007; Braver, 2012). In other words, proactive control is preparatory, while
reactive control operates in a just-in-time manner. These two mechanisms proposed by the DMC
framework have been dissociated based on both their temporal dynamics, computational
mechanisms, and neural substrates in healthy and impaired populations (Braver et al., 2005;
DePisapia & Braver, 2006; Braver et al., 2009), and based on their behavioral signatures in young
adults (Gonthier, Braver, & Bugg, 2016) and in cross-sectional studies examining age differences
(Paxton et al., 2008; Bugg, 2014a; Bugg, 2014b). Critically, extant empirical findings provide
evidence that these two modes of control can be manipulated via distinct situational factors, while
also pointing to an important source of variation in control function at the individual and group
level (i.e., age group or in clinical groups with differing levels of well-being and characteristics),
in terms of the bias or preference to adopt one control mode over the other mode (Braver, 2012;
Barch & Ceaser, 2012).
In addition to providing a unifying account for understanding intra-individual, inter-
individual, and between-groups variability in cognitive control, the DMC framework describes a
domain-general account of these two control mechanisms, postulating the presence of proactive
and reactive control across multiple cognitive domains. However, there have been empirical and
theoretical challenges in developing and optimizing valid and reliable paradigms of proactive and
reactive control in different task domains, and in establishing behavioral markers that provide
robust indices for these two modes of control. The Dual Mechanisms of Cognitive Control (DMCC)
project was started by our group to address these shortcomings. The DMCC aimed to develop and
systematically examine the validity and reliability of a battery of cognitive control tasks across
four distinct cognitive domains: selective attention, context processing, multi-tasking, and working
memory (Braver et al., 2021). Previous small-scale behavioral studies stemming from the DMCC
project have demonstrated the robustness of some task variants in isolating proactive control and
reactive control in the abovementioned domains (Bugg & Braver, 2016; Gonthier et al., 2016;
Gonthier, Braver, & Bugg, 2016), but a systematic and rigorous behavioral analysis of all
optimized variants of all four task domains has not yet been conducted in a large sample using a
within-subject design. Indeed, most studies of proactive and reactive control to date, whether they
are designed to examine individual variation, temporal dynamics, or neural signatures of these two
modes of cognitive control, have mostly focused on a single task in limited domains of cognition,
and with fairly restricted participant samples. Although the focus on single tasks in measuring the
two modes of control has been informative in contributing to our understanding of their
mechanisms, it precluded a rigorous test of the validity and the domain generality of cognitive
control modes across multiple cognitive domains.
Leveraging the strengths of classic cognitive control paradigms, the DMCC battery
includes four well-established cognitive tasks (Stroop, AX-CPT, Cued Task-Switching and
Sternberg Working Memory), one for each of the abovementioned domains, respectively, that were
theoretically optimized to capture variability in proactive and reactive control. Specifically, there
were three variants of each task representing different experimental conditions: 1) a baseline
condition that maximizes within- and between-individual variability, which does not bias the
adoption of proactive or reactive control; 2) a proactive condition that shifts individuals toward
proactive control; 3) a reactive condition that independently engages the reactive mode of control.
As will be detailed for each task in the later sections, we contrasted theoretically-specified
behavioral performance patterns across the three variants, to determine whether proactive and
reactive control variants did indeed produce the predicted shifts in control.
The present study implemented a multi-session within-subject design to systematically
evaluate the validity and reliability of the entire DMCC task battery. As the first large-scale study
of the DMCC task battery, the study has several advantages and innovative features that had rarely
been implemented in prior experimental studies of cognitive control. First, all tasks were
computerized and made available through an online platform (Amazon Mechanical Turk) for data
collection, which enables recruitment of a large sample size within a short time period, while also
allowing open accessibility and dissemination of the task battery for future investigations – both
in laboratory settings and through online platforms. Second, the study was methodologically
innovative in that previously unexamined novel variants of the task battery were investigated for
the first time, in order to assess the effects of experimental manipulations in inducing variability
in the utilization of proactive and reactive control modes. Third, the validation of this task battery
can be used to provide a firm foundation for more costly and time-consuming neuroimaging
investigations of cognitive control, by providing behavioral markers and metrics that can be linked
to underlying neural mechanisms (Braver et al., 2021). Finally, this work can provide a foundation
for future translational efforts, given that the battery can provide: 1) assessment tools by which to
evaluate the domain-generality (unity) and diversity of cognitive control function in different
populations, including those with impaired cognitive control; and 2) potential targets for
intervention efforts aimed at enhancing proactive and/or reactive control (Braver, 2012).
In the following section, we describe the experimental manipulations and rationale
underlying the theoretically-targeted variants of all four cognitive control tasks (Stroop, AX-CPT,
Cued Task-Switching and Sternberg Working Memory), specifically highlighting the innovative
features of the task conditions included in the battery. This manuscript focuses on the effectiveness
of the experimental manipulations at the group level, in independently assessing proactive and
reactive control modes. In particular, we compare task performance and primary outcome indices
among the three conditions (baseline, proactive, reactive) to evaluate both divergent (discriminant)
and convergent (cross-task) validity of the DMCC task battery in capturing variations in the two
cognitive control modes. The primary goal of the paper is to introduce the battery and associated
data, such that the scientific community can fully evaluate and make use of the acquired data
(which will be made available on public repositories at the time of publication).
Dual Mechanisms of Cognitive Control Task Battery
Stroop. The color-word Stroop is widely recognized as a canonical task of cognitive control, in
which top-down selective attention is required to focus processing on the task-relevant font color
of printed words, while ignoring the irrelevant but otherwise dominant word name. A commonly
used approach to manipulating cognitive control demands in the Stroop task is to vary list-wide
proportion congruence (PC; Logan & Zbrodoff, 1979; Lindsay & Jacoby, 1994). Under high list-
wide PC conditions, congruent trials (word name matches font color, e.g., BLUE in blue font) are
frequent and incongruent trials (word name indicates a different color than the font color, e.g.,
RED in blue font) are rare within a block, such that control demands are on average low and
intermittent. In contrast, under low list-wide PC conditions (rare congruent trials, frequent
incongruent), the high probability that interference will occur within a block should lead to an up-
regulated cognitive control state.
In particular, we and others have hypothesized that under low list-wide PC conditions, the
tendency to utilize proactive control will increase (Gonthier, Braver, & Bugg, 2016; see also Bugg,
2014a; Bugg & Chanani, 2011; Hutchison, 2011; Spinelli, Perry, & Lupker, 2019 for evidence in
confound-minimized designs). In this case, proactive control is theoretically associated with
sustained maintenance of the task goal to attend to the ink color and ignore the word, which should
be present in a consistent (i.e., global; present on all trials) and preparatory manner (i.e., engaged
even prior to stimulus onset). Thus, the key prediction is that the Stroop effect (average slowing
or increase in errors on incongruent relative to congruent trials) should be reduced on all trials,
relative to a baseline, high list-wide PC condition, reflecting improved performance on
incongruent trials and a reduction of facilitation on congruent trials (i.e., a congruency cost; see
Gonthier et al., 2016).
In contrast, PC can also be manipulated in an item-specific, rather than list-wide fashion
(Jacoby, Lindsay, & Hessels, 2003). In this case, specific colors will occur with low PC (e.g., items
appearing in green font will frequently be incongruent), while others may occur with high PC (e.g.,
items appearing in red font will frequently be congruent), and these “items” are randomly
intermixed such that participants cannot predict whether a low PC or high PC item will appear on
a given trial. This type of item-specific PC manipulation is theoretically predicted to enhance the
utilization of reactive control for low PC items (for evidence in confound minimized designs, see
Bugg, Jacoby, & Chanani, 2011; Bugg & Hutchison, 2013; Bugg & Dey, 2018). For these items,
strong associations develop between a critical feature (a specific font color, such as green) and
increased control demands (i.e., high interference), leading to more effective goal retrieval and
utilization upon presentation of a stimulus that includes this feature (e.g., a word printed in a green
font). The engagement of reactive control is expected to be transient, present only after stimulus
onset, and only engaged by low PC incongruent items, particularly when these occur within the
context of 50% congruent, or even higher, list-wide PC conditions.
The three Stroop task variants in the present battery varied as follows: the baseline
condition had a high list-wide PC (67% congruent, 33% incongruent trials), whereas the proactive
condition had a low list-wide PC (33% congruent, 67% incongruent trials). In contrast, the reactive
condition approximated the high list-wide PC of the baseline condition (60% congruent, 40%
incongruent) due to the inclusion of many high PC (100% congruent) filler items, but also featured
specific items that were low PC (25% congruent, 75% incongruent). Another feature of the battery
is the inclusion, in each condition, of a set of unbiased, diagnostic items (“PC-50”, 50% congruent,
50% incongruent) that did not share features (i.e., words or colors) with the other items in the
condition. These PC-50 (diagnostic) items provide clearer behavioral markers from which to
dissociate proactive and reactive control (Braem et al., 2019 . Similar versions of these Stroop
conditions have been examined in prior work, using both picture-word (Gonthier et al., 2016) and
color-word variants (Dey & Bugg, 2021). Finally, it is worth noting that because of the large
numbers of different font colors (8) included in each of the conditions, the task was implemented
with vocal rather than manual responding, using built-in voice recognition software to extract
response latencies.
AX-CPT. The AX-CPT has become increasingly utilized as a task of context processing and
cognitive control, given its simplicity, flexibility and applicability in a wide-range of populations
(Barch et al., 2008; Chatham et al., 2009; Chun et al., 2018; Janowich & Cavanagh, 2018 Servan-
Schreiber, Cohen, & Steingard, 1996). In the paradigm, participants respond to letters presented
one at a time, with each trial consisting of a cue-probe letter pair. When an A-cue is followed by
an X-probe, a target response is required. Since the AX pairing occurs frequently, strong cue-probe
associations develop. Cognitive control is postulated to be needed to maintain and utilize the
information provided by contextual cues, particularly to minimize errors and response interference
occurring on BX trials (where B refers to any letter except A), which occur when the X-probe is
presented, but is not preceded by an A-cue. In prior work, shifts in the tendency to utilize proactive
or reactive control have not only been observed when comparing different populations or groups,
but have also been manipulated within-subjects (Braver et al., 2009).
The AX-CPT conditions included in the battery extend prior recent work using a task
variant in which the A- and B-type contextual cues occur with equal frequency, thus eliminating
confounds in earlier versions that could be due to the lower overall frequency of encountering B-
cues (Richmond, Redick, & Braver, 2015; Gonthier et al., 2016). Further, these conditions also
include no-go trials, in which the probe is a digit rather than letter. Because of the increase in
response uncertainty (i.e., three types of probe response are possible: target, nontarget, no-go), the
addition of no-go trials decreases the overall predictive utility of context information for
responding, and as a consequence was found to reduce the overall proactive control bias typically
observed in healthy young adults. As such the no-go conditions result in a “low control” baseline,
from which to more sensitively observe condition-related changes in control mode (Gonthier et al.,
2016). In all of the current AX-CPT versions tested in this battery, the task structure, trial types
and frequencies are identical, except for the specific manipulations described below for proactive
and reactive conditions.
The proactive condition replicates prior work using context strategy training (Gonthier et
al., 2016), as a means of increasing the predictive preparation of responses following contextual
cue information. Specifically, participants are provided with explicit information regarding the
frequencies of these cue-response associations, and receive training and practice in utilizing them
to prepare the dominant responses. In addition, during inter-trial intervals, participants are
provided with visual instructions to “remember to use the strategy”. The key prediction is that the
increased utilization of contextual cue information will lead to a bias to prepare a target response
following an A-cue (analyzed in terms of both AX and AY trials) and a nontarget response
following a B-cue, leading to reduced interference on BX trials. Yet a side effect of this preparatory
bias is a predicted increase in errors and response interference on AY trials, which occur when the
A-cue is not followed by an X-probe.
The reactive condition involved a new manipulation which has not previously been
examined in prior work. Specifically, the reactive condition utilizes context-specific probe cueing
(similar to other context cueing manipulations in tasks, such as Stroop and flanker; for review, see
Bugg & Crump, 2012), in that for high control demand trials (AY, BX, nogo) the probe item
appears in a distinct spatial location, and with a distinct border color surrounding it (presented
briefly before the onset of the probe). Critically, because these featural associations are only
present at the time of probe onset, they were not hypothesized to modulate the utilization of
proactive control strategies. Likewise, the probe features could not drive direct stimulus-response
learning, since they do not directly indicate the appropriate response to be made (i.e., either a non-
target or no-go response could be required). In contrast, the probe features do serve as contextual
cues signaling high control demand, and thus prompt more rapid and effective retrieval of
contextual information to resolve the conflict. Because information about high-conflict probe
features is not provided explicitly to participants (in contrast to the proactive condition), it has to
be learned implicitly through experience. The key prediction is that utilization of probe features
should reduce the tendency to make BX errors but could increase BX reaction time interference
(due to the tendency to utilize the probe to drive context retrieval).
Cued-TS. Cued task-switching (Cued-TS) has long been recognized as a critical paradigm
to assess a core component of cognitive control – the ability to activate and update task-
representations in an on-line manner, in order to configure attention and action systems to process
the task-relevant features of a current target. The key aspect of the paradigm is that two or more
tasks randomly alternate across trials, with target items typically being ambiguous, so that they
can be processed according to multiple task rules. Consequently, the advance presentation of the
task cue, prior to target onset, is what disambiguates the target and specifies the appropriate
stimulus-response rules.
An important metric of cognitive control in task-switching paradigms is the task-rule
congruency effect (TRCE), which refers to the increased interference (both errors and reaction
time) when the target response required for the current task is incongruent with the response that
would be required to the same target stimulus if the alternative task had been cued (Meiran &
Kessler, 2008). Consider the letter-digit task-switching (also called consonant-vowel, odd-even
[CVOE]) task comprising a letter task and a digit task. If in the letter task, a right button press is
required for a consonant and a left button press for a vowel, while in the digit task, a right button
press is required for odd and a left button press for even, the “D4” target stimulus would be
incongruent (whereas the “A2” target stimulus would be congruent, since for either task, the left
button press would be correct). Two additional important metrics are switch costs, which refer to
the decrement to performance when the task to be performed on the current trial switches from
that on the previous trial (relative to task-repeats, when the same task is performed on two
consecutive trials; Meiran 1996; Rogers & Monsell, 1995), and mixing costs, which refer to the
decrement to performance that occurs on task-repeat trials (relative to performance within a single-
task block; Los, 1996; Braver et al., 2003); these have also served as indices of cognitive control
demands.
In prior work, including reward incentives on a subset of trials, with reward cues presented
at the time of the task cue, led to a strong reduction in the mixing cost – and this was present even
on the trials that were non-incentivized – but there was no effect on the TRCE (Bugg & Braver,
2016). This finding was interpreted as indicating that the mixing cost reductions reflected a list-
wide (global) enhancement of proactive control, whereas the TRCE effect is primarily influenced
by reactive control, and so less impacted by advance reward incentive manipulations. The Cued-
TS conditions included in the current battery build on this prior work by using variants of the
CVOE (letter/digit) paradigm that aim to accentuate the robustness of the TRCE, while also
enabling clear utilization of proactive control through the use of advance task cues with a long
cue-to-target interval (CTI). A robust finding from prior work is that performance improves with
longer preparation times (CTI), suggesting advanced preparation for relevant task rules and
stimulus-response mappings for the upcoming target (Meiran, 1996).
In the baseline condition, target stimuli are list-wide mostly congruent (67%), as prior
work has found that mostly congruent conditions result in a large and robust TRCE (Bugg &
Braver, 2016). The proactive condition builds on Bugg and Braver (2016) in keeping the same list-
wide mostly congruent structure as the baseline condition but adding reward incentives on a subset
of trials. Specifically, on 33% of trials, reward cues are presented simultaneously with advance
task cues (i.e., by presenting the task cue in green font), and indicate the opportunity to earn
monetary bonuses if performance is accurate and fast (relative to baseline performance) on that
trial. By only presenting reward cues on a subset of trials, the remaining subset of non-incentivized
trials and target stimuli can be directly compared across the proactive and baseline conditions. A
divergence from Bugg and Braver (2016) is that single-task conditions are not included as part of
the battery (due to length constraints), which precludes direct calculation of mixing costs.
Nevertheless, the key prediction is that enhanced proactive control will lead to a global
improvement of performance (i.e., faster RTs without a loss in accuracy).
The reactive condition utilizes a new manipulation which has not previously been
examined in prior work. Specifically, the reactive condition includes punishment (rather than
reward) incentives, again on the same 33% subset of trials that were incentivized in the proactive
condition. However, in the reactive condition the incentive cue is presented at the time of the target
stimulus, rather than with the task cue, which precludes the use of incentive motivation in a
preparatory fashion. Participants are instructed that they will lose a component of their potential
monetary bonus if they make an error on these incentivized trials. Critically, the incentivized trials
occur preferentially (75%) with incongruent target stimuli. This manipulation is intended to
associate punishment-related motivation with these high-conflict items, potentially leading to
increased response monitoring and caution when incongruence is detected. As such, the key
prediction is that enhanced reactive control should reduce the error TRCE, even on the non-
incentivized trials, when compared to baseline and proactive conditions. Conversely, the RT TRCE
should be increased, due to the tendency to utilize target features (detection of incongruency) to
drive retrieval of task rules.
Sternberg WM. The Sternberg item-recognition task has been one of the most popular
experimental paradigms used to assess short-term / working memory for over 50 years (Sternberg,
1966), but more recently has been adapted particularly for the study of cognitive control with the
“recent probes” version (Jonides & Nee, 2006). Like standard versions of the paradigm, the recent
probes version presents participants with a memory set of various load levels (number of items),
to maintain over a short delay (retention period), after which a single item probe is presented,
which requires a target response if the probe was a part of the memory set. A classic finding in the
literature is that as the memory set increases in size, WM load increases, and performance declines
accordingly (higher error rates, longer RTs; Sternberg, 1966; Schiffrin & Schneider, 1977). Under
conditions in which the WM load is below capacity (3-4 items), active maintenance and rehearsal
processes can be used to keep the memory set accessible, as an attentional template from which to
prospectively match against the probe item (i.e., utilizing proactive control strategies). In contrast,
when the WM load is above capacity (~7 items), probe responses are likely to be driven by
retrieval-focused processes, such as familiarity (i.e., reactive control strategies).
In recent probes versions, the key manipulation is that the probe item can also be a part of
the memory set of the previous trial, but not the current trial, which is termed a “recent negative”
(RN) probe. On these RN trials, the probe is associated with high familiarity, which can increase
response interference and errors, unless cognitive control is utilized to successfully determine that
the probe familiarity is a misleading cue regarding its status (target or nontarget). The current
versions of the Sternberg WM paradigm included in the battery are adapted from both Burgess &
Braver (2010) and Speer et al. (2003), in using manipulations of WM load expectancy and RN
frequency. Specifically, in all conditions, trials randomly vary in set size, with words used as
stimuli, such that all items are novel on each trial, with the exception of RN probes. Under such
conditions, Burgess & Braver (2010) found strong RN interference effects in both RT and errors.
Likewise, following Speer et al. (2003), the set size in a given trial is revealed sequentially, leading
to unpredictability and reliance on WM load expectancies to engage control strategies.
In the baseline condition, most trials have high WM load (6-8 items; 60%) and RN
frequency is low (20% of nontarget probes), which should reduce tendencies to engage either
proactive or reactive control strategies. However, in the proactive condition, most trials have low
WM load (2-4 items; 60%), leading to the expectancy that active maintenance-focused and
proactive attentional strategies will be effective, while RN frequency remains low (matched at 20%
nontarget probes), such that the utility of reactive control should be unchanged. The critical
prediction concerns the 5-item set size which occurs equivalently in all conditions (40% of trials),
and thus can be equivalently compared between them. The key hypothesis is that use of proactive
control strategies, will improve both RT and accuracy, primarily for the target probe items (termed
novel positive, or NP, since they never overlap across trials).
In the reactive condition, WM loads are identical to the baseline condition, while the
frequency of RN trials is increased (80% of nontarget probes). Thus, in the reactive condition, it
is familiarity-based interference expectancy that increases, rather than WM load expectancy.
Based on the increased interference-expectancy, the theoretical hypothesis is that participants will
not rely on familiarity as a cue for responding, and will rather evaluate the match of the probe to
items stored in WM. Consequently, the key prediction is that performance on RN (or rather the
RN effect, computed by subtracting performance on novel negative or NN trials) will be
significantly improved relative to baseline.
Methods
Participants
Participants were recruited for the study via the Amazon Mechanical Turk (MTurk) on-
line platform. The TurkPrime interface was used to post study descriptions, manage recruitment
and payment, send out reminder emails and handle all other communication with the participants.
After reading a description of the study that indicated its multi-session nature and time
commitment, interested participants accessed a link which allowed them to review and sign the
consent form. After signing the consent, the web-links for the first session of the study were made
available over MTurk. The study protocol was approved by the institutional review board of
Washington University, St. Louis. All data were collected across two separate testing waves held
a few months apart; however, since the procedures for the tasks described below were identical
across waves, the data from both waves are aggregated for reporting purposes below.
A total of 129 participants were included in the analyses. Participants were not restricted
with regard to age range, and as such a wide range was included in the sample (22-64, M=37.11,
SD=9.90; 82 females, 47 males). The excluded participants were either rejected for not completing
sessions in the required period of time, for technical problems that precluded data analysis
(particularly for the Stroop task, which involved vocal responses), or for data that indicated a
failure to comply with task instructions. Data were analyzed separately for each task, and only for
participants that had complete useable data for that task; thus, tasks are not equivalent in terms of
sample size (AX-CPT: 121, Stroop: 126, Cued-TS: 128, Sternberg: 128).
Design and Procedure
The study protocol consisted of 30 separate testing sessions that subjects completed in a
sequential manner (15 for the test phase, and another 15 for retest). Participants were asked to
complete the sessions at a rate of 5 per week, i.e., 6 weeks to complete the full protocol. Each
session lasted approximately 20-40 minutes in duration, with the exception of the first session,
which was 1 hour in duration (and included a Stroop practice block to validate operation of vocal
response recording, plus a battery of demographic and self-report questionnaires). To both
incentivize and prorate study completion, completion of the first session in each set of 5, for both
test and retest phases resulted in a $4 payment, and the others resulted in a $2 payment. Additional
bonuses of $20 were paid for completion of the test phase and $30 for full study completion.
Together, successful completion of the entire protocol resulted in a payment of $122, plus
additional monetary bonuses associated with incentives in the Cued-TS sessions.
Each set of 5 sessions was posted at the beginning of the week through MTurk and sent
through emails to the participants. Two reminder emails were also typically sent during the week
to remind subjects of the completion deadline for the set (by the end of the week). If subjects
failed to complete the weeks’ sessions by the designated deadline, they were not invited back to
participate in subsequent sessions. For each completed session, subjects would enter in a
completion code and the experimenter would review each session results for completion and
approve the payment within a week through TurkPrime. If subjects dropped from the study, they
still received prorated payment for all sessions completed.
For each completed session, the experimenter checked for overall accuracy and completion
of each task and questionnaire to make sure that subjects were complying with instructions and
maintaining sufficient attention to the task. A criterion of 60% accuracy and response rate was
used to determine whether the data would be included, and the subject invited to remain in the
study. For each task or questionnaire that did not meet the criterion, the experimenter attempted to
communicate with the subject first to determine if they had trouble understanding the instructions
or had technical difficulties. If so, the subject was given a second chance to complete the task
before a designated deadline.
Within each of the test and retest phases, sessions were conducted in a fixed order for all
participants, with the baseline conditions of all tasks performed first, followed by reactive
conditions of all tasks, and then proactive conditions last. The AX-CPT, Cued-TS, and Sternberg
were programmed with in-house JavaScript code (available upon request at
and delivered using Inquisit software, as it included capabilities for online vocal response
recording (script also available at link above).
Tasks
illustration of the four tasks and their associated experimental manipulations in each variant.
Stroop
In this vocal Stroop task, color words are presented in colored font and participants name
the font color out loud. For each trial, vocal response latencies were recorded, and the spoken word
was detected using the computer’s built in voice recognition software. Accuracy was then
automatically coded through the Inquisit software. Participants were given standard instructions
to respond as quickly as possible (in a normal voice) while retaining accuracy. Adequacy of the
automated voice recognition was validated in previous pilot testing, and individually for each
subject based on their first testing session, which contained a practice block of 25 standard Stroop
trials. If responses could not be detected for most of the trials, the subject was not asked to continue
with further testing.
The current versions of the Stroop were based on the design of our previously reported
work (Gonthier et al., 2016; Gourley et al., 2016), and constructed using two different sets of four
colors, in which the relative proportion of congruent and incongruent trials were manipulated in
different ways. One set of four colors (red, blue, purple, white) was biased in the proportion of
congruent and incongruent trials, either mostly congruent or mostly incongruent, varied across
conditions. The other set (black, green, pink, yellow) was an unbiased/diagnostic set in that the
proportion of congruent to incongruent stimuli was 50:50 (hereafter, this set is termed PC-50
items). The two sets of stimuli were nonoverlapping, such that on incongruent trials, the word
name was one of the three remaining colors from that set (e.g., green font with “black”, “pink” or
“yellow”; red font with “blue”, “purple” or “white”). All trials consisted of the following stimulus
parameters: items were presented centrally on a gray screen for 5000 msec duration or until a
response was detected, followed by a 250 msec inter-trial interval during which a blank screen was
presented.
Baseline Session. In the baseline session the trials were manipulated in a list-wide, mostly
congruent (LW-MC) manner. Participants completed a total of 288 trials during the baseline
session, in which there were 96 PC-50 trials (48 congruent, 48 incongruent), and 192 biased trials.
The biased set had 75% congruent (144 trials) and 25% incongruent (48 trials) trials. Consequently,
the list-wide proportion congruency for the baseline session was 66%. The session was divided
into two blocks of 144 trials each, between which participants were instructed to rest for one
minute. Participants practiced a slightly simpler version of the baseline condition in a practice
block on the first session, in order to validate that response latencies could be accurately captured.
Proactive Session. In the proactive session, the trials were manipulated in a list-wide, mostly
incongruent (LW-MI) manner. Participants completed a total of 288 trials during the proactive
session, in which there were 96 PC-50 trials (48 congruent, 48 incongruent), and 192 biased trials.
The biased set had 25% congruent (48 trials) and 75% incongruent (144 trials) trials. Consequently,
the list-wide proportion congruency for the proactive session was 33%. The session was divided
into two blocks of 144 trials each, between which participants were instructed to rest for one
minute.
Reactive Session. In the reactive session the proportion congruency manipulation was at the item-
level, i.e., item-specific proportion congruency (IS-PC). Specifically, blue and red color-font items
were manipulated to be biased trials with PC-100 (i.e., these font-color words were only presented
as congruent trials; 192 trials). Purple and white color-font items served as biased trials, with PC-
25 (i.e., 25% congruent, 48 trials; 75% incongruent, 144 trials). Finally, as in the baseline and
proactive conditions, the remaining 96 trials were unbiased trials with PC-50 (i.e., equal amount
of congruent and incongruent trials). Thus, subjects completed a total of 480 trials during the
reactive session. The session was divided into three blocks of 160 trials each, between which
subjects were instructed to rest for one minute.
Cognitive Control Measures. Average response times (RTs) on correct trials and error rates were
calculated for both congruent and incongruent trials for each subject in each session. The key
measure of cognitive control was the Stroop interference effect (incongruent – congruent). We
focus primarily on the PC-50 items (since these were matched across conditions) and RT (as is
standard in the literature), though we also examined biased items and error rates for both types of
items. To directly compare proactive and reactive conditions, two additional derived indices were
also calculated: the transfer cost and congruency cost (see Gonthier et al., 2016, for further
descriptions). The transfer cost was computed as the difference in Stroop RT interference on PC-
50 items relative to biased items; the congruency cost was computed by subtracting the baseline
congruent trial RT from the congruent trial RT in proactive and reactive conditions (again focusing
on PC-50 items, but also computed for biased items).
AX-CPT
In this version of the AX-CPT, participants make button press responses to visually
presented cue-probe pairs. A target key press (“/”) is made to the probe on AX trials; a nontarget
key press (“.”) is made to the probe on the other nontarget (AY, BX, BY) trials, as well as to the
cue on all trials. In addition to the four primary trial types, the task also includes no-go trials, which
require withholding response to the probe; no-go trials are indicated by a digit (1-9) rather than
letter probe. The task comprised 216 trials total, and included 72 AX trials, 72 BY trials, 18 AY
trials, 18 BX trials and 36 nogo trials (18 following an A-cue, 18 following a B-cue). All trial types
and no-go trials were presented in random order. The task was performed in three 72 trial blocks,
between which subjects were instructed to take a minimum of 1-minute rest break. All trials
consisted of the following parameters. The cue was presented centrally on a white screen for 500
msec duration. After a 4000 msec blank cue-probe interval, the target (in same size font) was
presented for 500 msec but immediately preceded by a 250 msec period during which a bounding
box was presented. A 1500 msec inter-trial interval ended the trial (indicated by a central triangle
of fixation crosses).
Baseline Session. The baseline session identically followed the description above. After receiving
task instructions, subjects performed a 12-trial practice block before beginning the actual task.
Proactive Session. In the proactive condition, participants received strategy training before
completing the AX-CPT. The strategy training occurred during a practice block consisting of two
phases. In the first phase of 6 trials an audio clip was played, which instructed subjects which
button to prepare following the cue. In the second phase of 6 trials, after the cue was presented,
they were asked to type which button they were preparing to press in response to the second item.
Participants typed out “left” or “right” and the program told subjects if they were correct or not. If
they were not correct, they were reminded what letter the first item was and asked to try again.
This procedure was implemented to accommodate the on-line testing format, and deviated slightly
from in-person versions, in which subjects responded verbally regarding the button they were
preparing to press. Additionally, during the test phase, in the inter-trial interval periods, subjects
were given the visual message to “Use the strategy!”. Otherwise, task structure was identical to
the baseline session.
Reactive Session. The occurrence of high conflict trials (AY, BX, nogo) was implicitly signaled
by presenting the probe in a distinct spatial location and preceded by a distinct border color.
Specifically, while cues were always presented centrally (as in the baseline and proactive
conditions) the probe stimuli were either presented in the upper half (AX, BY) or lower half (AY,
BX, nogo) of the visual display. Furthermore, probe stimuli were immediately preceded (250 msec
before probe onset) by either a white border (AX, BY) or red border (AY, BX, nogo). Otherwise,
the task structure and trial proportions were identical to baseline and proactive sessions.
Cognitive Control Measures. Average response times (RTs) on correct trials and error rates were
calculated for each of the 4 primary trial types (AX, AY, BX, BY) for each subject in each session.
Average error rates for no-go trials were calculated as well. The key measure of cognitive control
was BX probe interference, which is calculated as the difference score on B-cue trials (BX – BY).
This index allows for examination of the interference that occurs when an “X” probe follows a
non-target cue “A” and a target trial response must be inhibited. We focused on BX probe
interference in both errors and RT. To directly compare proactive and reactive conditions, we also
computed an additional derived index, the A-cue bias. The A-cue bias measure reflects the bias to
make a target response following an A-cue, and is calculated by computing a c criterion from hits
on AX trials and false alarms on AY trials as 1/2*(Z[H] + Z[F]), with H representing hits on AX
trials and F representing false alarms on AY trials (Richmond et al., 2015). Because BX probe
interference and A-cue bias involve different trial types they can be examined fully independently,
which is useful when directly comparing proactive and reactive control conditions.
Although not a primary focus of interest in the current report, additional derived indices
were computed and reported to maintain continuity with prior work: d’-context and the Proactive
Behavioral Index (PBI) (Gonthier et al., 2016). The d’-context index was calculated by computing
a d’ index from hits on AX trials and false alarms on BX trials as Z(H) – Z(F), with H representing
hits on AX trials, F representing false alarms on BX trials, and Z representing the z-transform of a
value (Servan-Schreiber et al., 1996). The PBI is calculated as (AY – BX)/(AY + BX) (Braver et
al., 2009). This index reflects the relative balance of interference between AY and BX trials; a
positive PBI reflects higher interference on AY trials, indicating proactive control, whereas a
negative PBI reflects higher interference on BX trials, indicating reactive control. The PBI was
computed separately for error rates (based on average error rates on AY and BX trials) and for
RTs (based on average RTs on AY and BX trials). In order to correct for error rates that were equal
to 1.00, a log-linear correction was applied to all error rate data prior to computing the d’-context,
the A-cue bias, PBI, and BX interference (Braver et al., 2009; see also Hautus, 1995). This
correction was applied as error rate = (number of errors + 0.5)/(number of trials + 1).
Cued Task Switching
In the current Cued-TS paradigm, we used the letter-digit task, which involves bivalent
target stimuli consisting of a letter and a digit (e.g., E3). On each trial the subject is cued to perform
either a letter task – consonant/vowel discrimination – or a digit task – odd/even discrimination
(Rogers & Monsell, 1995; Minear & Shah, 2008). For the letter task, consonants required right
key press (“L”) and vowels required a left key press (“A”). For the digit task, even numbers
required a right (“L”) key press and odd numbers required a left (“A”) key press. At the start of
every trial the task is cued by an on-screen message that indicates either “ATTEND LETTER” or
“ATTEND NUMBER”, indicating whether attention and responding should be based on the letter
or digit, respectively. Critically, because of the response mappings, certain stimuli are congruent,
in that they require the same key press irrespective of the relevant task rule (e.g., H6, E3), while
other stimuli are incongruent, in that the two tasks were associated with different required
responses to the same target (e.g., I6, D4).
The target stimuli were constructed in terms of two distinct stimulus sets. One set of stimuli
(A1, A2, B1, B2, 1A, 2A, 1B, 2B) was mostly congruent (80% congruent; 20% incongruent). The
second set of stimuli (D4, E3, H5, I6, 4D, 3E, 5H, I6) was unbiased (50% congruent, 50%
congruent). Trials randomly alternated between an equal number of “ATTEND LETTER” and
“ATTEND NUMBER” trials. Due to the random presentation order of the cues, switch and repeat
trials were on average equivalent, but deviated slightly in number across conditions and subjects.
Each session consisted of 192 total trials, 96 mostly congruent (80 congruent, 16 incongruent) and
96 unbiased (48 congruent, 48 incongruent) and also equally split between the two tasks (i.e., 96
letter, 96 digit). Trials were separated into three 64 trial blocks, between which subjects were
required to take a minimum of 1-minute rest break. Prior to starting each session subjects learned
(or refreshed their memory) of these response mappings through a set of 16 practice trials. All
trials consisted of the following stimulus parameters: trial initiation with a 300 msec alerting cue
(flashing cross), followed by the task cue presented on a gray screen for 500 msec duration. After
a 3500 msec blank cue-target interval, the target was presented until a response was made. The
response was followed by a 1250 msec feedback period, then a 1000 msec inter-trial interval
(indicated by a central triangle of fixation crosses).
Baseline Session. In this condition, no manipulations were made to the unbiased stimuli. However,
to maintain consistency with the proactive and reactive sessions described below, for these stimuli
task cues and target stimuli could appear in either red or green font. Nonetheless, this distinction
was irrelevant with regard to the instructions given subjects.
Proactive Session. The proactive version of Cued-TS was identical to the baseline version except
for the addition of a reward-based motivational incentive. This motivational incentive involved
presenting subjects with a reward cue (font color) indicated during presentation of the task cue.
When subjects responded to incentive trials faster than the baseline session’s median RT, while
beginning of each session), they received a monetary bonus for that trial added to their
compensation amount. Before the start of the proactive sessions, participants were given the
following instructions: “from here to end, you can obtain more payment on top of regular
compensation by responding faster than before and maintaining accuracy. A green cue will let you
know if you are performing a trial where you can obtain a larger reward.” Non-incentive trials
were indicated by the task cue appearing in red font, while incentive trials were indicated by the
task cue appearing in green font. Only the unbiased set of stimuli were incentivized (66% of
unbiased, 33% of total, 64 trials) and presentation order was random with respect to the task cue
and target stimuli pre-determined pairs. Subjects received feedback on all trials. The word
“Reward!” appeared on the screen for 1250 ms if the subject earned the reward. If subjects were
too slow or made an incorrect response, the words “Too Slow!” or “Incorrect!”, respectively,
appeared on the screen. The non-incentive trials also included feedback, showing “Correct” or
“Incorrect” after each trial.
Reactive Session. The reactive version of Cued-TS was identical to the baseline version except
for the addition of a punishment-based motivational incentive. This motivational incentive
involved presenting subjects with a punishment cue (font color), that was indicated during
presentation of the target. When subjects made errors on incentive trials, they received a monetary
penalty for that trial that was subtracted from their compensation amount. Before the start of the
reactive sessions, subjects were given the following instructions: “from here to end, you can lose
money from your regular compensation by making errors. A green cue will let you know if you
are performing a trial where you might receive a penalty.” Non-incentive trials were indicated by
the target stimulus appearing in red font, while incentive (i.e., punishment) trials were indicated
by the target stimulus appearing in green font. Only the unbiased set of stimuli were incentivized,
and these were applied in an item-specific manner such that all of the incongruent stimuli (H5, 6I,
5H, 6I; 48 trials) were incentivized while only 33% of the congruent stimuli were associated with
incentives (D4, E3, 3E, 4D; 16 trials). The sentence “Loss of 25 cents!” appeared on the screen for
1250 msec if the subject made an incorrect response. If subjects were correct or were too slow, or
the words “Correct” or “Too Slow!” respectively, appeared on the screen. The non-incentive trials
also included feedback, showing “Correct” or “Incorrect” after each trial.
Cognitive Control Measures. Average response times (RTs) on correct trials and error rates were
calculated for separately for congruent/incongruent trials, and for both the non-incentivized
(biased) items and incentivized (unbiased) items, for each subject in each session. A key measure
of cognitive control is the Task Rule Congruency Effect (TRCE; Meiran & Kessler, 2008), which
is the difference between incongruent and congruent trials. We focus primarily on the non-
incentivized items since these can be most straight-forwardly compared across proactive and
reactive conditions, though we also examined and report effects on incentivized items.
Additionally, although not a primary focus of interest in the current report, we also report the
switch cost as another index of cognitive control. The switch cost is calculated by subtracting task-
repeat trials from task-switch trials.
Sternberg
In the current Sternberg item-recognition task, participants are presented with a new, short
list of words on each trial that served as a memory set (e.g., “WINE”, “SPLIT”, “GRILL”,
“INTENT”). After encoding and a retention interval delay period, a probe item is presented, which
requires a judgment as to whether it was part of the current trial’s memory set (i.e., a positive probe)
or not (i.e., a negative probe). If the word was in the most recent list, a left key press (“Z”) is
required. If the word was not in the most recent list, a right key press (“M”) is required. The current
versions of the Sternberg were based on the design of Speer et al (2003) and constructed using two
distinct sets of memory set items: critical items, had a constant memory set of 5 words; the other,
variable-load set consisted of either low-load items (memory sets of 2-4 words) or high-load items
(memory sets of 6-8 words). Additionally, the probe consisted of three trial types: 1) novel positive
(NP), 2) novel negative (NN), and (3) recent negative (RN).
Each session consisted of 120 total trials, broken down into 48 critical items, and 72
variable-load items. Trials were separated into three 40 trial blocks, between which subjects were
required to take a minimum of 1-minute rest break. Prior to starting each session subjects learned
(or refreshed their memory) of the task through a set of 10 practice trials. All trials consisted of
the following stimulus parameters: visual presentation of the memory set across two encoding
screens each of 2000 msec duration; in the first screen, were presented above a central fixation
cross, and in the second screen, below the cross. Following memory set presentation, a retention
interval of 4000 msec was presented (during which the fixation cross remained on screen),
followed by 1500 msec presentation of the probe item, and then a 1000 msec inter-trial interval.
Baseline Session. The baseline session involved high-load variable-items and a low proportion of
RN trials (20% of negative probes; 10% of total trials). Specifically, the variable-load set consisted
of a mixture of high-load memory sets (12 6-item, 24 7-item, 36 8-item) and very few RN trials (4
RN, 32 NN, 36 NP). For the critical 5-item set, the proportion was slightly adjusted, to increase
the number of RN trials for analysis (8 RN, 16 NN, 24 NP).
Proactive Session. In the proactive session, the variable-load items were instead a mixture of low-
load memory sets (36 2-item, 24 3-item, 12 4-item). The proportion of RN, NN, and NP trials was
identical to the baseline session for both variable-load (4 RN, 32 NN, 36 NP) and critical item sets
(8 RN, 16 NN, 24 NP).
Reactive Session. In the reactive session, the variable-load set used the identical mixture of high-
load memory set items as the baseline session (12 6-item, 24 7-item, 36 8-item). However, the
relative proportion of RN to NN trials was increased in both the variable-load (32 RN, 4 NN, 36
NP) and critical items (16 RN, 8 NN, 24 NP).
Cognitive Control Measures. Average response times (RTs) on correct trials and error rates were
calculated per trial type (i.e., NN, NP, RN trials), and separately for critical items (5-item lists)
and non-critical items (collapsed across the remaining list lengths). The key measure of cognitive
control was the recency effect, which is calculated as the difference score on negative trials (RN –
NN; Jonides & Nee, 2006). We focused on critical item performance, both in errors and RT and
in terms of the recency effect, since these are most easily compared across proactive and reactive
conditions, though we also report findings on non-critical (high or low-load) items as well.
Data Preprocessing and Analysis
The data were pre-processed in two steps: (1) removal of extreme outliers, and (2)
winsorization of remaining outliers. In step 1, all 128 subjects were screened for severe
abnormalities such as extremely slow RTs or high error rates. RT plots were examined and cutoff
decisions were made for each task separately. Trials with RTs slower than the cutoff threshold
were discarded. The threshold for Stroop was 4000 ms; no RTs on correct trials surpassed the
threshold. The threshold for AX-CPT was 2000 ms; no RTs on correct trials surpassed the
threshold. The threshold for Cued Task-Switching was 5000 ms and resulted in 0.3% of the task’s
data discarded. The threshold for Sternberg was 3000 ms; no RTs on correct trials surpassed the
threshold. After discarding trials with these RT outliers, the number of trials per condition
remained sufficient for analyses.
In step 2, a winsorization procedure was conducted on RT data at the trial level (i.e., data
split by phase, session, trial type, and subject). The winsorization parameters for RTs were as
follows: RTs lower than 200 ms were replaced by RTs of 200 ms and RTs above the mean plus 3
standard deviations were replaced by RTs of the mean plus 3 standard deviations. Across the four
tasks 1.9% of RT observations were adjusted by the procedure. The adjustments did not vary
considerably across tasks, sessions, or trial types. For error rate, the winsorization procedure was
conducted at the level of trial type (data split by phase, session, and trial type), instead of at the
subject level, which was examined in the first step of pre-processing. Following the cutoff used by
Gonthier et al. (2016), error rates above 40% were replaced with error rates of 40%. This resulted
in nearly 5% of error rates being adjusted for the AX-CPT and Sternberg tasks (i.e., 4.78%, 4.69%,
respectively). The Stroop and Cued Task-Switching adjustments were much lower at .07% and
1.69%, respectively. Examining this more carefully revealed repeated subpar performance for
some subjects (e.g., greater than 80% error rate in some conditions, large proportion of
observations without responses) which inflated the winsorization adjustment rates. Those subjects
were excluded from the final sample. It should be noted that for all tasks (with the exception of
no-go trials in the AX-CPT), trials in which no response was recorded were treated as incorrect
trials. Finally, we retained 126 subjects for Stroop, 121 for AX-CPT, 128 for Cued Task-Switching,
and 126 for Sternberg.
After these pre-processing steps, statistical inference was conducted both within and across
conditions using paired t-tests. Both classical frequentist and also Bayesian analyses were
conducted and both sets of results are reported, in terms of both effect sizes (Cohen’s d) and Bayes
Factors. The reported t-values are always positive when the pattern followed the predicted pattern;
thus, a negative t-value refers to a pattern than went opposite to that predicted. We refer to an effect
as having strong evidence in favor of the hypothesis when BF > 10, and strong evidence for a null
effect with BF < 0.1. In cases where an effect yielded statistical significance via classical
frequentist conventions (i.e., p < .05), but with BF < 10, we refer it as significant but lacking strong
evidence.
Results
Primary and secondary predictions for each task, are summarized in Figures 1-4 and Tables
1-4, respectively; the tables also indicate which predictions were confirmed. Detailed descriptive
data are presented in Supplementary Tables 1-4. Below, we present the key results and test
statistics for each task measure, separately for each condition.
Stroop
Baseline effects. We first verified the presence of a standard Stroop interference effect in terms of
increased RT on incongruent (IC) relative to congruent (C) trials. We examined the biased (PC-
75) and PC-50 items separately. In both cases, highly robust effects (> 100 msec) were observed:
biased items (IC: M=920.27, SD=381.32, C: M=768.51, SD=372.96; t(125)=24.49, p<0.001,
Cohen’s d= 2.18, BF >100), PC-50 items (IC: M=910.60, SD=376.78, C: M=792.41, SD=380.73;
t(125)=17.86, p<0.001, Cohen’s d = 2.18, BF >100). In the Stroop task, error rates tend to be
very low overall, but also typically show Stroop interference effects as well. This pattern held in
the current dataset: biased items (IC: M=0.07, SD=0.08, C: M=0.03, SD=0.06; t(125)=7.314,
p<0.001, Cohen’s d = 1.59, BF >100), PC-50 items (IC: M=0.05, SD=0.06, C: M=0.03, SD=0.06;
t(125)=5.23, p<0.001, Cohen’s d = 0.47, BF >100).
Proactive condition. In the proactive condition, the list-wide PC manipulation was predicted to
lead to a reduction in the Stroop RT interference effect. Critically, because of the list-wide nature
of the manipulation, this reduction was predicted to impact PC-50 items as well as biased items
(PC-25), This prediction was confirmed: both of these interference effects yielded strong evidence
of reduction relative to the baseline condition (biased: baseline M=151.76, SD=69.57, proactive
M=83.74, SD=53.43, t(125)= 12.10, p<0.001, Cohen’s d=1.08, BF >100; PC-50: baseline
M=118.19, SD=74.30, proactive M=92.96, SD=68.66, t(125)= 3.76, p<0.001, Cohen’s d=0.34,
BF =69.83). Similar numerical patterns were present in the error rate data, but weaker, in that the
effects were significant, but some lacked strong evidence (biased: baseline M=0.03, SD=0.05,
proactive M=0.01, SD=0.0), t(125)= 4.64, p<0.001, Cohen’s d=0.41, BF >100; PC-50: baseline
M=0.02, SD=0.04, proactive M=0.01, SD=0.03, t(125)= 2.07, p=0.041, Cohen’s d=0.18,
BF =1.29).
Reactive condition. In the reactive condition, the item-specific PC manipulation was predicted to
lead to a reduction in the Stroop RT interference effect on the biased (MI) items (PC-25).
Conversely, because of the item-specific nature of the manipulation the reduction in Stroop
interference was predicted to not transfer to PC-50 items, with no change from baseline. This
prediction was confirmed: biased interference effect (M=93.53, SD=66.24) and PC-50 interference
effect (M=127.01, SD=73.96). Only the biased items were significantly reduced relative to the
baseline condition with strong evidence; in contrast, the PC-50 items showed some evidence for a
null effect (biased: t(125)= 9.00, p<0.001, Cohen’s d=0.80, BF >100; PC-50: t(125)= -1.50,
p=0.136, Cohen’s d=0.13, BF =0.29). Again, similar numerical patterns were present in the error
rate data (biased: M=0.03, SD=0.03, t(125)= 1.96 , p=0.053, Cohen’s d=0.21, BF =0.63; PC-50:
M=0.02, SD=0.03, t(125)= -0.90, p=0.370, Cohen’s d=0.09, BF =0.15).
Proactive vs. Reactive. Based on prior work, we predicted that the Stroop interference effect (in
RT) would be reduced for PC-50 items in proactive, due to the differential transfer effect. This
prediction was confirmed, with strong evidence: PC-50 (t(125)=6.43, p<0.001, Cohen’s d=0.57,
BF >100). Conversely, we predicted no difference between conditions for biased items; there
was some evidence in favor of the null effect (t(125)= -1.97, p=0.051, Cohen’s d=0.18, BF =0.64).
To more directly quantify these contrasting effects, we computed the “transfer cost”, which is the
difference in Stroop interference across PC-50 and biased items. As predicted, the transfer cost
was significantly greater in reactive (M=33.48, SD=47.91) than proactive, with strong evidence
(M=9.22, SD=58.63), t(125)=3.97, p<0.001, Cohen’s d=0.35, BF >100).
Another prediction from prior work was that the proactive condition would reduce facilitation on
congruent trials to a greater degree than reactive. The congruency cost was computed by
subtracting out the baseline congruency from both proactive and reactive on PC-50 items (and on
biased items as well). Although the congruency cost effects were numerically in the predicted
direction (i.e., greater in proactive), these were not close to statistically reliable (PC-50: proactive=
-12.56, reactive= -17.03, t(125)=0.36, p=0.717, Cohen’s d=0.03, BF =0.11; biased: proactive=
4.07, reactive= -7.06, t(125)=0.78, p=0.436, Cohen’s d=0.07, BF =0.13). Thus, the congruency
cost prediction was not supported in this dataset. Key predicted effects for the Stroop are
Primary Results
Measures Formula Predictions Confirmation
Reaction Time
Stroop Effect Incongruent – Congruent P<B (Biased, PC-50) +*
R<B (Biased) +*
R=B (PC-50) +
Congruency Cost Proactive/Reactive Congruent – R<P (Biased, PC-50) +
Baseline Congruent
Transfer Cost PC-50 incongruent – Biased P<R +*
incongruent
Secondary Validation Results
Error
Stroop Effect Incongruent – Congruent P<B (Biased, PC-50) +*
R<B (Biased) +
R=B (PC-50) +
B=Baseline, P=Proactive, R=Reactive
+Prediction Confirmed
*Strong evidence (BF > 10 or BF <0.1)
AX-CPT
Baseline effects. We verified the presence of standard AX-CPT interference effects, which include
higher RTs and error rates on high-conflict AY (RT: M=548.77, SD=73.45; errors: M=0.06,
SD=0.07) and BX (RT: M=553.75, SD=148.53; errors: M=0.19, SD=0.18) non-target trials,
relative to low-conflict BY non-target trials (RT: M=462.65, SD=66.51; errors: M=0.01, SD=0.03).
For both trial types, highly robust effects with strong evidence were observed: AY (RT:
t(120)=21.25, p<0.001, Cohen’s d=1.93, BF >100; Errors: t(120)=6.67, p<0.001, Cohen’s
d=0.61, BF >100) and BX (RT: t(120)=9.38, p<0.001, Cohen’s d=0.85, BF >100; errors:
10 10
t(120)=10.99, p<0.001, Cohen’s d=1.00, BF >100). Moreover, as predicted from the inclusion
of no-go trials, participants showed poorer performance on BX than AY trials in terms of error
rates (t(120)=7.44, p<0.001, Cohen’s d=0.68, BF >100) and were even numerically, though not
reliably slower in RT (t(120)=0.48, p=0.633, Cohen’s d=0.04, BF =0.11).
Proactive condition. In the proactive condition, the instructed strategy manipulation was predicted
to lead to an increased utilization of contextual cue information, as indexed by a significantly
positive A-cue bias (the tendency to make a target response following an A-cue). This prediction
was confirmed with strong evidence (M=0.42, SD=0.46, t(120)=10.02, p<0.001, Cohen’s d=0.91,
BF >100). Moreover, the A-cue bias also exhibited strong evidence of increase relative to
baseline (M=0.03, SD=0.30), t(120)=9.06, p<0.001, Cohen’s d=0.82, BF >100). Additionally,
the utilization of context was also predicted to reduce BX interference effects in the proactive
condition (errors: M= 0.11, SD=0.10; RT: M=56.45, SD=74.55), in both errors and RT, relative
to baseline (errors: M=0.19, SD=0.17; RT: M=91.11, SD=106.84). This prediction was also
confirmed with strong evidence (error: t(120)=5.07, p<0.001, Cohen’s d=0.46, BF >100; RT:
t(120)= 3.74, p<0.001, Cohen’s d=0.34, BF =65.42). Although we now prefer the A-cue bias
measure, because it more selectively indexes proactive control, for continuity with prior literature
we further examined d’-context, which was also predicted to be improved in the proactive
condition (M=3.12, SD=0.88), relative to baseline (M=2.61, SD=0.93). This prediction was
confirmed, as the proactive condition d’-context was significantly greater, with strong evidence
(t(120)=6.13, p<0.001, Cohen’s d=0.56, BF >100).
Reactive condition. In the reactive condition, the probe cueing manipulation was predicted to lead
to a reduction in BX error interference, but at a cost of increased BX RT interference (due to probe-
triggered context retrieval). These predictions were also both confirmed, BX error interference in
reactive (M=0.14, SD=0.14) showed strong evidence of reduction, relative to baseline (M=0.19,
SD=0.17), t(120)=3.27, p=0.001, Cohen’s d=0.30, BF =15.15, whereas BX RT interference in
reactive (M=130.07, SD=77.40), was significantly increased relative to baseline, also with strong
evidence (M=91.11, SD=106.84), t(120)=3.99, p<0.001, Cohen’s d=0.36, BF >100. Although
we now prefer the BX RT interference effect as a selective index of reactive control, for continuity
with prior literature we further examined d’-context, which was also predicted to be improved in
the reactive condition (M=2.84, SD=0.85), relative to baseline (M=2.61, SD=0.93). This
prediction was confirmed, as the reactive condition d’-context was significantly greater than the
baseline condition, but was lacking strong evidence (t(120)=2.76, p=0.007, Cohen’s d=0.26, BF
=3.74).
Proactive vs. Reactive. We predicted that the A-cue bias would be greater in proactive than reactive,
whereas BX RT interference would be greater in reactive compared to proactive. Both effects were
confirmed with strong evidence (A-cue bias: t(120)=7.97, p<0.001, Cohen’s d=0.72, BF >100;
BX RT interference: t(120)=10.13, p<0.001, Cohen’s d=0.92, BF >100). Although we prefer
these two measures as they are doubly dissociable, for completeness and comparison with prior
studies we also examined the PBI and d’-context measures. In the proactive condition, the PBI had
strong evidence of being positive in both error (M=0.18, SD=0.53, t(120)=3.68, p<0.001, Cohen’s
d=0.33, BF =53.79) and RT indices (M=0.08, SD=0.10, t(120)=9.39, p<0.001, Cohen’s d=0.85,
BF >100), whereas in reactive the PBI had strong evidence of being negative in errors (M=-0.19,
SD=0.52, t(120)=4.12, p<0.001, Cohen’s d=0.37, BF >100). For RT, the PBI in reactive had
strong evidence of being lower than proactive (t(120)=8.74, p<0.001, Cohen’s d=0.79, BF >100),
consistent with predictions. Likewise, there was strong evidence for the d’-context measure being
greater in proactive, relative to reactive (M=2.84, SD=0.85), t(120)=3.73, p<0.001, Cohen’s
Primary Results
Measures Formula Predictions Confirmation
Reaction Time
BX Interference Effect BX – BY P<B +*
R>B +*
R>P +*
Error
BX Interference Effect BX – BY P<B +*
R<B +*
A-cue bias 0.5*(Z[Hits on AX] + Z[False alarms P>0 +*
on AY]) P>B +*
P>R +*
Secondary Validation Results
Reaction Time
PBI (AY – BX)/(AY + BX) P>0 +*
R<P +*
Error
d’-context Z(Hits on AX) – Z(False alarms on P>B +*
BX)
P>R +*
PBI (AY – BX)/(AY + BX) P>0 +*
R<0 +*
B=Baseline, P=Proactive, R=Reactive
+Prediction Confirmed
*Strong evidence (BF > 10)
Cued Task-Switching
Baseline effects. We verified the presence of standard cued task-switching effects, which include
both the TRCE and (residual) switch costs. For RT, there was strong evidence for both effects, in
both the biased (mostly congruent) and unbiased items: TRCE (biased: M=40.51, SD=126.90,
t(127)=3.61, p<0.001, Cohen’s d=0.32, BF =42.78; unbiased: M=46.30, SD=132.36, t(127)=3.96,
p<0.001, Cohen’s d=0.35, BF >100), switch cost (biased: M=38.63, SD=76.68, t(127)=5.70,
p<0.001, Cohen’s d=0.50, BF >100; unbiased: M=39.96, SD=115.66, t(127)=3.91, p<0.001,
Cohen’s d=0.35, BF >100). These effects also tend to be present in error rate, and there was
strong evidence confirming this pattern in the data as well, for both the TRCE (biased: M=0.06,
SD=0.11, t(127)=6.33, p<0.001, Cohen’s d=0.56, BF >100; unbiased: M=0.06, SD=0.10,
t(127)=6.94, p<0.001, Cohen’s d=0.61, BF >100) and switch cost (biased: M=0.02, SD=0.04,
t(127)=4.91, p<0.001, Cohen’s d=0.43, BF >100; unbiased: M=0.03, SD=0.08, t(127)=3.87,
p<0.001, Cohen’s d=0.34, BF =97.60).
Proactive condition. We first examined the effects of the reward incentive manipulation. This
manipulation was predicted to speed RTs, but also increase errors, which was confirmed with
strong evidence, when comparing incentivized trials (RT: M=709.11, SD=211.00; errors: M=0.16,
SD=0.10) to non-incentivized trials (RT: M=770.22, SD=208.36, t(127)=7.25, p<0.001, Cohen’s
d=0.64, BF >100; errors: M=0.09, SD=0.07, t(127)=8.89, p<0.001, Cohen’s d=0.79, BF >100).
10 10
This shift in control strategy was predicted to impact even the non-incentive (biased) trials, which
could be compared directly with baseline. Confirming this prediction, there was strong evidence
for faster RTs on these non-incentivized trials in the proactive condition (M=770.22, SD=208.36),
relative to baseline (M=988.41, SD=254.83), t(127)=16.66, p<0.001, Cohen’s d=1.47, BF >100).
Moreover, even when restricting the focus to just low-conflict congruent trials, there was still
strong evidence for this effect (proactive: M=746.98, SD=197.89; baseline: M=968.16,
SD=257.56; t(127)=18.71, p<0.001, Cohen’s d=1.65, BF >100). Moreover, the RT speeding on
these trials occurred in the absence of a change in error rate, relative to baseline, with some
evidence for the null (proactive: M=0.05, SD=0.04; baseline: M=0.05, SD=0.06; t(127)=0.79,
p=0.431, Cohen’s d=0.07, BF =0.13), suggesting more than just a speed-accuracy shift.
Reactive condition. We first examined the effects of the punishment incentive manipulation. This
manipulation was predicted to slow RTs, but also decrease errors. The RT prediction was
confirmed with strong evidence when comparing incentivized trials (RT: M=1202.03, SD=338.29)
to non-incentivized trials (RT: M=1094.76, SD=303.19), t(127)=9.55, p<0.001, Cohen’s d=0.84,
BF >100. However, the effect of reduced errors was not detected (incentivized: M=0.04,
SD=0.05; non-incentivized: M=0.04, SD=0.05; t(127)=0.83, p=0.407, Cohen’s d=0.07,
BF =0.14). This shift in control strategy was predicted to affect even non-incentivized (biased)
trials, which could be directly compared with the baseline condition. Confirming this prediction,
there was strong evidence for RTs being slower in reactive (M=1094.76, SD=303.19) relative to
baseline (M=988.41, SD=254.83; t(127)=5.48, p<0.001, Cohen’s d=0.48, BF >100), and in this
comparison there was also strong evidence for errors being lower as well (reactive: M=0.04,
SD=0.05, baseline: M=0.08, SD=0.09; t(127)=-6.50, p<0.001, Cohen’s d=0.55, BF >100). A
stronger prediction was that this effect might be related to TRCE interference, which was also
predicted to be reduced for error interference in reactive, but not TRCE RT interference. This
prediction was partially confirmed, in that TRCE error interference in non-incentivized trials was
numerically lower and close to statistically significant (M=0.04, SD=0.09), relative to baseline
(M=0.06, SD=0.11; t(127)=1.97, p=0.051, Cohen’s d=0.17, BF =1.57), but the effect lacked
strong evidence. Conversely, TRCE RT interference in reactive (M=80.45, SD=166.06) was
statistically greater when compared to baseline (M=40.51, SD=126.90; t(127)=2.62, p=0.010,
Cohen’s d=0.23, BF =2.59), but again this effect lacked strong evidence.
Proactive vs. Reactive. We predicted that on non-incentivized trials, the TRCE error effect would
be reduced in reactive relative to proactive. This prediction was confirmed, in that there was strong
evidence that the TRCE error effect was lower in reactive (M=0.04, SD=0.09) compared to
proactive (M=0.09, SD=0.12; t(127)=4.56, p<0.001, Cohen’s d=0.40, BF >100). In addition, we
predicted that in the proactive condition, there would be general response speeding relative to
reactive, even on non-incentivized and low-conflict congruent trials. This effect was confirmed
and also with strong evidence (proactive congruent RT: M=746.98, SD=197.89, reactive
congruent RT: M=1054.53, SD=308.78; t(127)=-16.93, p<0.001, Cohen’s d=1.50, BF >100).
3.
Primary Results
Measures Formula Predictions Confirmation
Reaction Time
Non-Incentivized – P<B (Biased, Low-conflict +*
(Biased) trials Biased)
R>B (Biased) +*
P<R (Low-conflict Biased) +*
Error
TRCE Interference Incongruent – Congruent R<B +
R<P +*
(Non-Incentivized)
Secondary Validation Results
Reaction Time
TRCE Interference Incongruent – Congruent R>B +*
R>P +
(Non-Incentivized)
Error
Non-Incentivized – P=B (Low-conflict Biased) +
R<B +*
(Biased) trials
B=Baseline, P=Proactive, R=Reactive
+Prediction Confirmed
*Strong evidence (BF > 10)
Sternberg
Baseline effects. We first verified the presence of standard working memory load effects, by
comparing the critical to higher load items. We expected higher error rates and longer RTs on the
high load items, there was strong evidence for these effects (errors: critical=0.13 (0.09), high
load=0.21 (0.09), t(127)=11.31, p<0.001, Cohen’s d=1.00, BF >100; RT: critical=897.02
(160.91), high load=943.34 (169.91), t(125)=6.36, p<0.001, Cohen’s d=0.57, BF >100). We also
tested for the recent negative effect (RN-NN), which had strong evidence for both errors (M=0.16,
SD=0.15, t(127)=12.11, p<0.001, Cohen’s d=1.07, BF >100) and RT (M=128.05, SD=110.77,
t(127)=13.08 p<0.001, Cohen’s d=1.16, BF >100). Likewise, strong evidence was similarly
obtained when examining non-critical (high load) items for both errors (M=0.32, SD=0.18,
t(127)=20.39, p<0.001, Cohen’s d=1.80, BF >100) and RT (M=174.25, SD=190.14,
t(127)=10.29, p<0.001, Cohen’s d=0.92, BF >100).
Proactive condition. We first tested that the load manipulation was successful, comparing critical
to low-load items, predicting lower error rates and faster RTs with lower-load. These effects were
confirmed with strong evidence (errors: critical=0.15 (0.11), low load=0.05 (0.08), t(127)=11.35,
p<0.001, Cohen’s d=1.00, BF >100; RT: critical=912.65 (173.84), low load=828.87 (163.50),
t(126)=11.28, p<0.001, Cohen’s d=1.00, BF >100). The key prediction involved the critical items,
which could be directly compared with baseline, which was predicted to show better performance
on NP trials. This prediction was only partially confirmed, in that the effects were statistically
significant for RT but lacking strong evidence (proactive: M=858.48, SD=157.29, baseline:
M=890.12, SD=166.92, t(127)=2.60, p=0.01, Cohen’s d=0.23, BF =2.47); the effects were in the
correct numerical direction, but not significant for errors (proactive: M=0.12, SD=0.11, baseline:
M=0.14, SD=0.13, t(127)=1.42, p=0.159, Cohen’s d=0.13, BF =3.82).
Reactive condition. We first tested that the load manipulation was successful, as in the baseline
condition, comparing critical to high-load items, predicting higher error rates and slower RTs with
higher-load. These effects were confirmed with strong evidence (errors: critical=0.09 (0.08), high
load=0.18 (0.09), t(127)=14.52, p<0.001, Cohen’s d=1.28, BF >100; RT: critical=890.79
(143.43), high load=948.47 (149.41), t(127)=11.62, p<0.001, Cohen’s d=1.03, BF >100). The
key prediction was on critical items, which could be directly compared with baseline; we predicted
a reduced recent negative effect in reactive. This prediction was confirmed. For errors, the recent
negative interference effect was reliably reduced in the reactive condition, with strong evidence
(M=0.10, SD=0.12) relative to baseline (M=0.16, SD=0.15, t(127)=4.37, p<0.001, Cohen’s d=0.39,
BF >100). For RT, the effect was statistically significant, but lacked strong evidence (reactive:
M=98.10, SD=96.40; baseline: M=128.05, SD=110.77; t(127)=2.48, p=0.014, Cohen’s d=0.22,
BF =1.86).
Proactive vs. Reactive. We predicted that for critical items, performance would be better in
proactive on NP trials, but that the recent negative interference effect would be reduced in the
reactive condition. This prediction was only partially confirmed. Although RTs were numerically
faster on NP trials in proactive, relative to reactive, this effect was not statistically significant
(proactive: M=858.48, SD=157.29, reactive: M=871.40, SD=149.05; t(127)=1.26, p=0.210,
Cohen’s d=0.11, BF =4.70); furthermore, NP error rates were actually significantly higher in
proactive relative to reactive (though this lacked strong evidence; proactive: M=0.12, SD=0.11,
reactive: M=0.10, SD=0.10; t(127)=-2.40, p=0.018, Cohen’s d=0.21, BF =1.55), which was
contrary to our prediction. Conversely, the recent negative effect provided strong evidence in
support of the prediction, both in terms of errors (proactive: M=0.22, SD=0.20, reactive: M=0.10,
SD=0.12, t(127)=6.72, p<0.001, Cohen’s d=0.59, BF >100) and RT (proactive: M=194.77,
SD=148.99, reactive: M=98.17, SD=96.78, t(126)=7.24, p<0.001, Cohen’s d=0.64, BF >100).
Measures Formula Predictions Confirmation
Reaction Time
Critical Load Performance NP P<B +*
P<R +
Recent Negative Interference RN – NN R<B +*
Effect R<P +*
Error
Critical Load Performance NP P<B +
P<R –
Recent Negative Interference RN – NN R<B +*
Effect R<P +*
B=Baseline, P=Proactive, R=Reactive
+Prediction Confirmed
–Opposite of Prediction
*Strong evidence (BF > 10)
Discussion
The primary goals of this report were to comprehensively describe the newly developed
DMCC task battery and rigorously evaluate the degree to which experimental manipulations
produce group-level shifts in proactive control and reactive control, as predicted by the DMC
framework. In each of the four tasks, we compared task performance and primary outcome indices
among the three conditions (baseline, proactive, reactive) to evaluate both convergent (cross-task)
and divergent (discriminant) validity of the DMCC task battery in capturing variations in the two
cognitive control modes.
In the Stroop task, the list-wide and item-specific PC manipulations were generally
successful in producing the predicted shifts toward proactive control and reactive control. In
particular, 3 of the 5 key predictions were confirmed with strong evidence, except for the Stroop
effect in reactive relative to baseline for the PC-50 items (reactive = baseline) and congruency cost
(proactive < reactive), even though both of these effects were in the correct numerical direction.
For AX-CPT, findings indicated that the context strategy manipulation and the probe cueing
manipulation were successful in dissociating the two modes of control. All 8 of the key predictions
were confirmed with strong evidence. Likewise, many additional measures of historical interest
(e.g., d’-context, PBI) also exhibited consistent patterns. For the Cued-TS, the reward and
punishment incentive manipulations successfully produced differential effects on RT and error
rates, supporting the dissociable nature of proactive control and reactive control. Specifically, 4 of
the 5 key predictions were confirmed with strong evidence; for the TRCE interference effect on
error rate, the difference between reactive and baseline conditions was in the predicted direction
but was not statistically significant. Finally, for the Sternberg working memory task, the
manipulations of working memory load and recent negative trials did impact task performance in
differential ways. In particular, 5 of the 8 key predictions were confirmed with strong evidence.
However, the performance of NP trials was the one condition across the four DMCC tasks, in
which our predictions were clearly disconfirmed. Specifically, the RT on NP trials in the proactive
condition was not significantly faster than the reactive condition, and the NP trial error rate was in
fact numerically higher in proactive compared to the reactive condition.
The key results from the Sternberg task, namely less reliable effects and, in some cases,
contrary patterns, may suggest a potential need for further task development and optimization.
Importantly, to our knowledge this is the first time that the proactive and reactive Sternberg task
variants have been directly compared. Nevertheless, both the recent negative interference and
working memory load effects were reliably demonstrated in each condition, as longer RT and
higher error rate were detected in high working memory load trials relative to low load trials, and
in recent negative trials relative to novel negative trials. These results suggested that both
experimental manipulations were valid in terms of producing the basic effects.
Thus, when considered together, our evaluation of the DMCC task battery suggests that it
exhibits substantial convergent and divergent validity. In terms of convergent validity, as just
described, the experimental manipulations were generally effective in producing common
experimental patterns in all four tasks, suggesting robust cross-task sensitivity to cognitive control
demands (summarized in Tables 1-4). In terms of divergent validity, there were clear patterns of
double dissociation, in that the behavioral markers of proactive and reactive control could
effectively be distinguished in all 4 tasks, with one set of measures showing the predicted
proactive > reactive pattern, at least numerically (Stroop congruency cost, AX-CPT A-cue bias,
Cued-TS TRCE error interference, Sternberg recent negative RT effect; see Figures 1-4 Panel B,
left side), and another set of measures showing the reverse predicted reactive > proactive pattern,
again at least numerically (Stroop transfer cost, AX-CPT BX RT interference, Cued-TS Non-
incentivized congruent RT, Sternberg Novel Positive RT; see Figures 1-4 Panel B, right side).
Notably, the on-line format of data collection proved to be a strength but may have also
resulted in some limitations for this study. From a practical standpoint, the nature of this multi-
session and multi-task study made frequent laboratory visits less optimal and more time-
consuming for data collection of a large sample size. As such, the utilization of an online format
helped to lower the barrier for participation in this type of large-scale data collection effort. In
particular, both researcher and participant burden were much reduced, since administration
demands were largely automated and therefore less time-consuming. For participants, completing
each session at their own convenience and from the comfort of their own home, made study
completion a much more attractive proposition. Nevertheless, by allowing participants to take the
tasks in a non-laboratory setting that precluded monitoring by the researchers, it is quite possible
that potential distractions could have occurred during participant completion of study sessions.
This is a well-known problem with online studies (Skitka & Sargis, 2006), and some results have
suggested possible impacts on task performance (Bauer et al., 2012; Skitka & Sargis, 2006).
However, in prior on-line studies in related domains, many key effects have been well-replicated
and indicate comparable patterns to those observed in laboratory settings (e.g., Crump et al., 2013;
Germine et al., 2012; Hicks, Foster, & Engle, 2016). Likewise, in the current study, we were able
to reproduce some of the same effects (e.g., Stroop; Gonthier et al., 2016) previously observed in
laboratory settings, which provides some reassurance regarding the feasibility and validity of
administering the task battery in an online format. Most critically, the key advantage of the online
format of the DMCC task battery, is that it can enable rapid future large-scale replication of the
present findings, as well as additional investigation of cognitive control modes in different labs
and in various populations.
Although the goal of this report was to describe and evaluate the validity of the DMCC
task battery in terms of group effects, another important area of focus relates to the utility of the
battery for individual differences analyses. A potential fruitful research direction is to examine
individual differences in proactive control and reactive control modes, exploring putative state and
trait factors that may influence cognitive control biases and task performance. In fact, a set of self-
report questionnaires on personality traits and psychological well-being was collected as part of
the study for this purpose. Although analysis of the individual differences data was beyond the
scope of the current paper, it can readily be utilized in future analyses to directly investigate the
role of individual difference variables in predicting utilization of distinct cognitive control modes.
Our interest in optimizing the study design for future investigations of individual difference
led to the use of a fixed condition order across participants. For detection of individual differences,
it is beneficial to have every participant perform tasks in the same order, so that order effects do
not serve as a between-individual confound variable. Conversely, for studies of group and
condition differences, as in the current paper, counterbalancing of condition order is often a key
feature, in order to enable examination and control of systematic order effects. For example, it is
possible that some of the predicted patterns in proactive versus reactive comparisons that we did
not observe in the current study (e.g., Stroop congruency cost, Sternberg NP effects) may have
been impacted by the order in which conditions were performed (i.e., all participants performed
the proactive condition after reactive). Conversely, it is also possible that some of the observed
effects with strong evidence may have been weaker, if aggregating data from other condition
orders. Thus, a useful extension of the present study would be to run additional waves of data
collection, with participants performing the battery with a different condition order to both
replicate the current findings, and also to test the impact of a different condition order (e.g.,
proactive before reactive) on the pattern of data. In this case, the on-line feature of the battery
lowers the barrier for future waves of data collection in which to examine the effect of these types
of design changes.
A central tenet of the DMC framework is the domain-generality of proactive and reactive
control modes. The current findings provide support for domain-generality in that consistent shifts
in control mode could be induced in each of the four tasks. Nevertheless, stronger evidence for
domain-generality will require in-depth analyses of relationships among the tasks and indices of
each control mode, potentially through multi-level, Bayesian, or latent-variable modeling. Future
investigations concerning the DMCC task battery will need to more systematically evaluate this
domain-general hypothesis regarding cognitive control modes. To facilitate future development
and investigation of this DMCC task battery, after publication of these findings, we will be making
the full dataset available on a public repository for any interested investigators to conduct further
explorations. We hope that the richness of this dataset might open new avenues of research and
help investigators in addressing key questions regarding the mechanisms of cognitive control.
Acknowledgement
We would like to thank Erin Gourley for her contributions to task programming and data
collection for the DMCC project.
Condition Trial Type RT Mean (SD) Error Mean (SD)
Baseline Biased Congruent: 768.51 (372.96) Congruent: 0.03 (0.06)
Incongruent: 920.27 (381.32) Incongruent: 0.07 (0.08)
Stroop Effect: 151.76 (69.57) Stroop Effect: 0.03 (0.05)
PC-50 Congruent: 792.41 (380.73) Congruent: 0.03 (0.06)
Incongruent: 910.60 (376.78) Incongruent: 0.05 (0.06)
Stroop Effect: 118.19 (74.30) Stroop Effect: 0.02 (0.04)
Proactive Biased Congruent: 772.58 (368.51) Congruent: 0.01 (0.04)
Incongruent: 856.32 (365.03) Incongruent: 0.03 (0.04)
Stroop Effect: 83.74 (53.43) Stroop Effect: 0.01 (0.03)
PC-50 Congruent: 779.85 (369.01) Congruent: 0.01 (0.05)
Incongruent: 872.81 (369.54) Incongruent: 0.03 (0.05)
Stroop Effect: 92.96 (68.66) Stroop Effect: 0.01 (0.03)
Reactive Biased Congruent: 761.45 (414.05) Congruent: 0.02 (0.04)
Incongruent: 854.99 (397.29) Incongruent: 0.04 (0.05)
MC Filler: 761.80 (381.44) MC Filler: 0.00 (0.00)
Stroop Effect: 93.54 (66.24) Stroop Effect: 0.03 (0.03)
PC-50 Congruent: 775.38 (386.95) Congruent: 0.02 (0.05)
Incongruent: 902.38 (378.99) Incongruent: 0.04 (0.06)
Stroop Effect: 127.01 (73.96) Stroop Effect: 0.02 (0.03)
Condition Derived Measure Trial Type RT Mean (SD)
Proactive Congruency Cost Biased 4.07 (171.43)
PC-50 -12.56 (167.72)
Transfer Cost – 9.22 (58.63)
Reactive Congruency Cost Biased -7.06 (155.40)
PC-50 -17.03 (121.78)
Transfer Cost – 33.48 (47.91)
Condition Trial Type RT Mean (SD) Error Mean (SD)
Baseline AX 465.04 (80.45) 0.07 (0.10)
AY 548.77 (73.45) 0.06 (0.07)
A-nogo – 0.13 (0.13)
BX 314.93 (148.53) 0.19 (0.18)
BY 324.66 (66.51) 0.01 (0.03)
B-nogo – 0.21 (0.17)
Proactive AX 422.73 (86.61) 0.05 (0.08)
AY 548.96 (85.40) 0.20 (0.19)
A-nogo – 0.18 (0.19)
BX 474.79 (121.82) 0.10 (0.11)
BY 418.34 (68.78) 0.01 (0.02)
B-nogo – 0.33 (0.22)
Reactive AX 449.38 (82.66) 0.07 (0.08)
AY 572.11 (88.97) 0.07 (0.08)
A-nogo – 0.09 (0.09)
BX 565.90 (117.92) 0.14 (0.15)
BY 435.83 (72.53) 0.01 (0.03)
B-nogo – 0.13 (0.12)
Condition Derived Measure RT Mean (SD) Error Mean (SD) Z-scores Mean (SD)
Baseline A-cue Bias – – 0.03 (0.30)
BX Interference 91.11 (106.84) 0.19 (0.17) –
d’-context – – 2.61 (0.93)
PBI 0.01 (0.08) -0.35 (0.44) –
Proactive A-cue Bias – – 0.42 (0.46)
BX Interference 56.45 (74.55) 0.11 (0.10) –
d’-context – – 3.12 (0.88)
PBI 0.10 (0.01) 0.18 (0.53) –
Reactive A-cue Bias – – 0.06 (0.31)
BX Interference 130.07 (77.40) 0.14 (0.14) –
d’-context – – 2.84 (0.85)
PBI 0.01 (0.07) -0.19 (0.52) –
Condition Trial Type RT Mean (SD) Error Mean (SD)
Baseline Biased (Non-Incentivized) Congruent: 968.16 (257.56) Congruent: 0.05 (0.06)
Incongruent: 1008.66 (267.56) Incongruent: 0.11 (0.13)
TRCE: 40.51 (126.90) TRCE: 0.06 (0.11)
Switch Cost: 38.63 (76.68) Switch Cost: 0.02 (0.04)
Unbiased (Non-Incentivized) Congruent: 1005.86 (301.22) Congruent: 0.07 (0.09)
Incongruent: 1052.15 (254.24) Incongruent: 0.13 (0.11)
TRCE: 46.30 (132.36) TRCE: 0.06 (0.10)
Switch Cost: 39.96 (115.66) Switch Cost: 0.03 (0.08)
Proactive Biased (Non-Incentivized) Congruent: 746.98 (197.89) Congruent: 0.05 (0.04)
Incongruent: 793.46 (229.47) Incongruent: 0.13 (0.12)
TRCE: 46.48 (99.92) TRCE: 0.09 (0.12)
Switch Cost: 33.56 (49.39) Switch Cost: 0.01 (0.04)
Unbiased (Incentivized) Congruent: 694.74 (208.80) Congruent: 0.10 (0.11)
Incongruent: 723.47 (217.78) Incongruent: 0.23 (0.12)
TRCE: 28.73 (63.02) TRCE: 0.13 (0.13)
Switch Cost:12.56 (61.62) Switch Cost: 0.03 (0.09)
Reactive Biased (Non-Incentivized) Congruent: 1054.53 (308.78) Congruent: 0.02 (0.02)
Incongruent: 1134.98 (319.83) Incongruent: 0.06 (0.09)
TRCE: 80.45 (166.06) TRCE: 0.04 (0.09)
Switch Cost: 58.31 (82.11) Switch Cost: 0.01 (0.03)
Unbiased (Incentivized) Congruent: 1175.97 (366.90) Congruent: 0.01 (0.03)
Incongruent: 1228.09 (322.15) Incongruent: 0.07 (0.08)
TRCE: 52.12 (137.97) TRCE: 0.05 (0.08)
Switch Cost: 37.93 (127.82) Switch Cost: 0.03 (0.07)
Condition Trial Type RT Mean (SD) Error Mean (SD)
Baseline NN Critical: 839.11 (165.20) Critical: 0.04 (0.08)
High: 883.58 (165.00) High: 0.07 (0.08)
NP Critical: 890.12 (166.92) Critical: 0.14 (0.13)
High: 909.63 (170.34) High: 0.18 (0.11)
RN Critical: 967.16 (186.84) Critical: 0.20 (0.16)
High: 1039.74 (236.12) High: 0.39 (0.19)
Proactive NN Critical: 843.25 (170.37) Critical: 0.05 (0.08)
Low: 826.06 (162.12) Low: 0.05 (0.07)
NP Critical: 858.48 (157.29) Critical: 0.12 (0.11)
Low: 788.47 (148.90) Low: 0.05 (0.06)
RN Critical: 1037.27 (238.65) Critical: 0.27 (0.22)
Low: 870.60 (209.79) Low: 0.07 (0.14)
Reactive NN Critical: 851.44 (157.70) Critical: 0.03 (0.08)
High: 922.91 (189.34) High: 0.06 (0.13)
NP Critical: 871.39 (149.05) Critical: 0.10 (0.10)
High: 902.71 (144.34) High: 0.19 (0.11)
RN Critical: 949.53 (161.04) Critical: 0.13 (0.13)
High: 1019.81 (164.22) High: 0.28 (0.16)
Condition Derived Measure RT Mean (SD) Error Mean (SD)
Baseline Critical: 128.05 (110.77) Critical: 0.16 (0.15)
Non-critical: 174.25 (190.14) Non-critical: 0.32 (0.18)
Proactive Critical: 194.77 (148.99) Critical: 0.22 (0.20)
Recent Negative
Non-critical: 47.27 (138.02) Non-critical: 0.02 (0.12)
Effect
Reactive Critical: 98.10 (96.40) Critical: 0.10 (0.12)
Non-critical: 94.62 (138.55) Non-critical: 0.22 (0.16)